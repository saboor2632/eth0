[
    {
        "title": "ULTRA TX - Programmable blocks: One transaction is all you need for a unified and extendable Ethereum",
        "link": "https://ethresear.ch/t/ultra-tx-programmable-blocks-one-transaction-is-all-you-need-for-a-unified-and-extendable-ethereum/21673",
        "article": "image1024×1024 185 KB\nULTRA TX is a way to achieve programmable L1 blocks, unlocking capabilities well beyond what is possible using standard L1 transactions in an L1 block. You could say ULTRA TX is to blocks what account abstraction is to EOAs.\nThis post will focus on what this means for L2s and the interoperability between L1 and L2s. Other possible use cases will not be explored here.\nWhen you combine the L1 meta tx bundler and the (based) rollup block builders, you have an entity we’ll call the master builder. The master builder will build L1 blocks containing just a single extremely powerful and fat L1 transaction. This single transaction will henceforth be referred to as the ULTRA TX.\nThis setup makes it efficient and straightforward to do composability and aggregation for L2s and the L1.\nFor easy L1 composability, the ULTRA TX should be at the top of the L1 block so that the latest L1 state is directly available.\nThis approach requires no changes to L1.\nFor these reasons I believe we are moving towards a future where almost everything will be done by a single L1 transaction. ULTRA TX can be adopted gradually, the rest of the block can be built in the traditional way.\nTogether with real time proving (in some reasonable capacity), this can achieve the ideal future where Ethereum truly feels like a single chain, where L1 and all L2s can call into each other, and every L2 can (but does not need to) settle in each L1 block, generally without loss in efficiency.\nGoing forward, Gwyneth will often be referenced to make things more concrete on how things could actually work. This is simply because it is the one I am most familiar with.\nDoing L1 → L2 well is hard with EOA transactions. There’s only so much you can do on L1 to achieve this:\nYou propose an actual L2 tx as part of the L1 transaction. This in theory can work, but the problem is that it is generally not known that these L1 transactions will end up proposing an L2 transaction without first executing the transaction. This L2 transaction then also somehow has to make it in the L2 block while not being part of the L2 block building process. It can also be problematic when a single preconfer is supposed to have the sole proposing rights.\nIf the L1 → L2 call requires a return value, then a proof also needs to be supplied in the transaction. Extra data needs to be supplied that the user has to sign as part of their transaction, and this data may also get outdated before the transaction ends up onchain. This results in bad UX. If there are many L1 → L2 interactions that would also mean many proofs would have to be verified onchain which makes things very inefficient.\nGwyneth allows L1 work to be initiated on L2. However, it is impossible to do certain operations like transferring ETH out of an account without a real L1 transaction signed by the owner. Account abstraction solves this problem because now all account modifications can actually be supported both using L1 transactions and L2 transactions.\nimage1399×776 24 KB\nL1 functionality can be extended by putting extra functionality behind an external call (potentially similar to how native rollups will be extendable). In the case of Gwyneth, this call is a cross chain call into L2. When building and proving the block, the block is created as if this extra functionality is also available on L1. The outputs of these calls are collected and sent onchain as part of the ULTRA TX:\nThe outputs generated by the extensions called on L1 are stored in the ExtensionOracle contract. These values are set before the call is done as part of the ULTRA TX. The ExtensionOracle is a simple contract that provides the output for each call that is not actually supported by L1. This data is stored in transient storage.\nNow we can actually do the call. Each call to extended functionality checks if the call is supported in the environment it’s currently executing:\n\nIf it’s supported, then the call happens as normal. e.g. the call is actually done to the target contract. This is the path that is followed in the builder/prover.\nIf it’s not supported, it means the call instead should be redirected to the ExtensionOracle smart contract where the call output generated offchain will be read instead. This is the path that is followed on L1.\n\n\nIf it’s supported, then the call happens as normal. e.g. the call is actually done to the target contract. This is the path that is followed in the builder/prover.\nIf it’s not supported, it means the call instead should be redirected to the ExtensionOracle smart contract where the call output generated offchain will be read instead. This is the path that is followed on L1.\nFinally the proof is verified showing that everything was done as expected.\nThis extra data is generated and provided by the master builder, not by the user. The user doesn’t have to sign any additional data or verify expensive proofs. The user can interact with smart contracts using extended functionality exactly the same way the user interacts with native functionality.\nDevelopers using the extended functionality in their smart contracts also do not have to know what is actually happening behind the scenes.\nNote that this exact approach only works because Gwyneth can “simulate” the execution of L1 transactions to glue everything together. L1 transactions are executed in the prover the same way as they will be on L1 when the ULTRA TX is proposed. This is important to make sure that the correct inputs are used to generate the output.\nimage1352×733 15.3 KB\nI will again be using Gwyneth’s approach to synchronous composability as an example (you can read up on it quickly here, but also here, here, and here). In short, an additional precompile is added on L2 that allows switching chains for external calls. Gwyneth can also simulate all L1 transactions and afterwards just apply the state updates back to L1.\nThe assumption I’m going to make here is that each L1 account is a smart contract account and that all L2s are based (such nice assumptions!).\nBuilding blocks can now easily be done as follows:\nWe start with the post state for each chain (including for L1). There is no difference between L1 and L2 transactions (except that L1 transactions should be meta transactions, if not they are added to L1 block after the ULTRA TX).\nThe L1/L2 transactions are executed in any order the builder wants.\nFor L1 transactions, the EVM execution is modified so that the XCALLOPTIONS precompile works exactly the same as on L2 (i.e. it actually executes the call on the target L2) as described above in the Extending L1 section. This allows L1 transactions to call into L2 which is something we need to support for true synchronous composability.\nFor any L1 → L2 call, we record the corresponding output of the call.\nOnce all required transactions are executed locally by the builder, we can seal the blocks:\n\nFor the L2s, either the transactions or the state delta is put on L1 for data availability. This is done for each L2 independently so that they don’t have any interdependencies. The block hashes can be put onchain aggregated to save gas.\nFor L1, we need to apply all state changes in the expected order onchain as they happened in the builder. For Gwyneth, this means applying the L1 transactions and the L1 state deltas (for L1 state changes done from L2) in the correct order.\n\n\nFor the L2s, either the transactions or the state delta is put on L1 for data availability. This is done for each L2 independently so that they don’t have any interdependencies. The block hashes can be put onchain aggregated to save gas.\nFor L1, we need to apply all state changes in the expected order onchain as they happened in the builder. For Gwyneth, this means applying the L1 transactions and the L1 state deltas (for L1 state changes done from L2) in the correct order.\nThe building process can be repeated as many times as needed to produce any number of blocks. This can be important to support execution preconfs for the transactions that are being included at faster than L1 block times. It is also important to parallelize the block building (see below).\nFinally, a single proof is generated of the whole process (note that this may contain sub-proofs, see the section on parallelization below).\nThe ULTRA TX is then finally proposed onchain. All inputs to the transaction are used as the input to the proof and the proof is verified. If the proof is not valid, the whole transaction reverts.\nNote that any additional requirements a builder has to support to be able to correctly include a transaction can be made part of the meta data of the transactions. This way builders can easily see if they are able to include the transaction without having to execute it. These rules can be enforced onchain. For example, for a cross chain transaction the meta tx would contain a list of chains that are allowed to be accessed. If this list is incomplete, the transaction is allowed to revert with the builder getting the transaction fee.\nThe simplified process above is strictly sequential to allow all chains to interact with each other freely and synchronously in the easiest way. It is possible to build blocks for any set of chains in parallel as well if they do not require synchrony with each other. Multiple blocks can be submitted with practically the same efficiency. This allows breaking the sequential bottleneck and allows achieving greater throughput.\nEven if, for example, a chain has an L1 state dependency, it is also still possible to build blocks in parallel. Only the subset of the state used in the block is important to be the actual latest values for the block to be valid.\nThere can be an additional layer on top of these blocks tracking the global state, while each block only depends directly on this sub state. Each block is proven individually, and then afterwards aggregated together with the additional state checks. The aggregation proof will track the global latest state across blocks and will check that the local state used in the block matches the current latest global state. The builder just has to ensure that these assumptions hold while building the blocks in parallel.\nThe generalization of how this (and more) can be used for all rollups (not just Gwyneth ones) will be coming in part 2. This framework will be called GLUE. A previous sketch was done here. It will contain, in reasonable depth, the interfaces necessary both offchain and onchain to make it possible for all L1 extensions to make use of the proposed design.\nSome code to make things more concrete and fun. Some details were omitted for brevity.\nWhat the ULTRA TX would look like onchain:\nHow it looks for developers that want to take advantage of extended functionality:\nHow extensions can be exposed to developers:\nWhat the Extension Oracle looks like:\n",
        "category": [
            "Layer 2"
        ],
        "discourse": []
    },
    {
        "title": "Delayed Execution And Skipped Transactions",
        "link": "https://ethresear.ch/t/delayed-execution-and-skipped-transactions/21677",
        "article": "Many thanks to Francesco for feedback, review an collaboration on this!\nEthereum requires every block to be fully executed before it’s considered valid. Each block header commits to a set of execution outputs—like the new state root, receipts, and logs—that result from processing every transaction within that block. This tight coupling means that validators must run every transaction as soon as they see a new block, making execution an inherent part of the critical path.\nA proposed solution, known as Delayed Execution (spec’ed by Francesco, here), offers an elegant approach by decoupling block validation from immediate transaction execution. In this post, I’ll go through how this mechanism works and what it could mean for scaling.\nAlso, check out Francesco’s post on delayed execution that explores another potential approach besides the one described in this post.\nBlockchain 1x1\nIn Ethereum, each block links to its predecessor by including a cryptographic commitment not only to the previous block’s header but also to the state resulting from all transactions in that block.\nHere’s what happens:\nExecution-Dependent Headers: The block header contains fields such as state_root, receipt_root, and logs bloom. These fields are generated only after the transactions have been executed.\nFull Execution: Every validator, upon receiving a new block, must execute all transactions to verify that the header’s commitments are correct. This ensures the block is consistent with the current state but forces nodes to do potentially heavy computation immediately.\nstate transition now1241×431 25.1 KB\nThe Concept of Delayed Execution\nDelayed Execution challenges this paradigm by splitting the block processing into two distinct stages:\nStatic Validation (Pre-Execution): Validators perform minimal checks using only the previous state. Instead of committing to a freshly computed state_root and related fields, the block header defers these execution outputs by referencing values from the parent block.\nPost-Attestation Execution: The actual execution of transactions is delayed until after the block is initially validated and attested to by the network.\nThis decoupling means that validators can quickly agree on the block’s validity without having to execute every transaction upfront. In essence, the block is “chained” to its predecessor using minimal data that does not require full execution.\nstate transition with delayed execution1142×350 24.6 KB\nInstead of fully executing the block before attesting, we can already attest to the block as soon as minimal static validation is done. This relieves stress from the critical path. The following is a simplified illustration of the efficiency gains. It compares the current situation (top) with delayed execution (bottom):\novertime1250×739 162 KB\nThe following graph has a (incomplete) list of things we do during a state transition. Only the initial, static validation phase (which relies solely on the previous state) needs to be executed immediately, while the more complex, state-changing operations can be safely deferred until after attestation.\nstate transition function840×690 105 KB\nIn theory, this change could boost efficiency by as much as 8x, assuming blocks arrive at second 3 of the slot, the attestation deadline stays at second 4, and the worst-case execution time is 1 second (based on the 99.9999th percentile). Special thanks to Marek, Ben, and Łukasz for providing this figure.\nThat said, take this estimate with a grain of salt—you might more realistically expect around a 5x improvement.\nThe Role of Skipped Transactions\nA novel concept in the delayed execution mechanism is the allowance for skipped transactions. Under the current protocol, a single invalid or underfunded transaction can invalidate an entire block. Delayed execution introduces a more resilient approach:\nInclusion Without Execution: Transactions are still included in the block’s transaction list but might be marked as “skipped” during execution if they fail certain conditions (e.g., insufficient funds, underpricing, incorrect nonce, or other execution-dependent checks).\nUpfront Fee Payment by Coinbase: To protect the network against the cost of including these transactions, the block proposer’s account (known as the COINBASE) pays an upfront “inclusion cost.” This cost covers basic expenses like the base transaction cost and calldata fees.\nNetwork Compensation: Even if a transaction is skipped, the network is compensated because the inclusion cost has been pre-paid by the COINBASE. This mechanism eliminates the risk of having transactions that consume resources without paying.\ndelayed execution flow1181×331 20.9 KB\nBy allowing invalid transactions to be skipped without invalidating the whole block, the proposal shifts the burden of heavy execution away from the immediate validation process. Similar to EIP-7732 (ePBS), we relieve the critical path from the heavy load of execution and state root validation.\nThe Role of COINBASE and Direct Sponsorship (optional)\nDelayed execution could create new opportunities for direct sponsorship. Since the coinbase already covers the inclusion cost upfront, it might be reasonable to extend this responsibility to the base fee as well.\nHere’s how it works:\nCOINBASE’s Signature Commitment: The block header comes with a signature from the COINBASE address. This signature is a commitment that the COINBASE is responsible for paying all inclusion costs upfront. In effect, the COINBASE sponsors the execution of transactions that might otherwise be underfunded (=not able to pay for the basefee).\nFlexible Fee Models: With the COINBASE on the hook for initial fees, the protocol can allow transactions that don’t strictly meet the minimum fee requirements. This opens the door to new possibilities such as gasless or sponsored transactions, where the sender might not have enough ETH to pay upfront but is later reimbursed—or the COINBASE recoups the cost—once execution is successful.\nDelayed execution and block-level base fee mechanisms are separate topics that should be addressed independently. However, the COINBASE’s commitment to covering the inclusion cost could be extended to also committing to sponsoring the base fee.\nFor additional details on why block-level markets have the potential to contribute to more efficient resource allocation, check out Barnabé’s post on Block-level Markets.\nUnder The Hood\nUnder delayed execution, the way blocks are chained together undergoes a small transformation:\nDeferred Execution Outputs: Header fields such as the state_root, receipt_root, and logs bloom are deferred. Instead of reflecting the immediate execution of the block, these fields hold values from the parent block. This means that the block’s validity can be confirmed without performing all of its computational work. Invalid transactions can be included in blocks.\nInclusion Cost Calculation: For every transaction, an inclusion cost is computed that typically includes a base cost (e.g., 21,000 gas), calldata fees, and any blob gas fees. This cost is deducted from the COINBASE’s balance before the transactions are executed. If the transaction is skipped, the COINBASE loses the inclusion cost fronted for the transaction. If the transaction executes successfully, the inclusion cost fronted by the COINBASE is refunded by the sender of the transaction.\nTwo-Phase Validation: The validation process is split into an initial static check—ensuring that the block is structurally sound and that the COINBASE can cover inclusion costs—and a later execution phase, where transactions are processed or skipped as appropriate.\nThis design relieves the critical path of execution, allowing blocks to be validated and attested more quickly. It ultimately results in a more scalable and flexible protocol, as the heavy lifting of transaction execution can be handled asynchronously relative to block attestation.\nAdvantages and Trade-Offs\nAdvantages:\nIncreased Throughput: By taking transaction execution out of the immediate validation path, blocks can be attested to more rapidly.\nEnhanced Flexibility: The model simplifies introducing new fee mechanisms, such as sponsored and gasless transactions, which can make Ethereum more accessible to users.\nTrade-Offs:\nLiquidity Requirements for Proposer/Builder: The COINBASE address must be sufficiently funded to cover the maximum possible inclusion costs for a block, which may introduce liquidity constraints, especially during periods of high base fee conditions. The maximum inclusion fee equals gas_limit * base_fee, so, with a base_fee of 100 GWEI, we’re at 3 ETH.\nProtocol Complexity: Introducing delayed execution involves substantial changes to Ethereum’s execution layer. However, unlike other delayed execution proposals, this approach avoids modifying the fork-choice function, which keeps the complexity lower. For a closer look at what these changes entail—especially if you’re interested in adding base fee sponsoring—check out the flow chart in the appendix and the EELS specs here.\nAppendix\nFor flowchart enthusiasts, here is how the described mechanism is currently spec’ed; this includes the block-basefee feature and skipped transactions, going through the EELS implementation here:\nflow chart of complete flow with skipped transactions adn sponsoring891×3640 285 KB\n^find the uncompressed version of this diagram here.\n",
        "category": [
            "Execution Layer Research"
        ],
        "discourse": []
    },
    {
        "title": "Proof of Validator: A simple anonymous credential scheme for Ethereum's DHT",
        "link": "https://ethresear.ch/t/proof-of-validator-a-simple-anonymous-credential-scheme-for-ethereums-dht/16454",
        "article": "Authors: George Kadianakis, Mary Maller, Andrija Novakovic, Suphanat Chunhapanya\nIntroduction\nEthereum’s roadmap incorporates a scaling tech called Data Availability Sampling (DAS). DAS introduces new requirements to Ethereum’s networking stack, necessitating the implementation of specialized networking protocols. One prominent protocol proposal uses a Distributed Hash Table (DHT) based on Kademlia to store and retrieve the samples of the data.\nHowever, DHTs are susceptible to Sybil attacks: An attacker who controls a large number of DHT nodes can make DAS samples unavailable. To counteract this threat, a high-trust networking layer can be established, consisting solely of beacon chain validators. Such a security measure significantly raises the barrier for attackers, as they must now stake their own ETH to attack the DHT.\nIn this post, we introduce a proof of validator protocol, which enables DHT participants to demonstrate, in zero-knowledge, that they are an Ethereum validator.\nMotivation: “Sample hiding” attack on DAS\nIn this section, we motivate further the proof of validator protocol by describing a Sybil attack against Data Availability Sampling.\nThe DAS protocol revolves around the block builder ensuring that block data is made available so that clients can fetch them. Present approaches involve partitioning data into samples, and network participants only fetch samples that pertain to their interests.\n\nConsider a scenario where a Sybil attacker wants to prevent network participants from fetching samples from a victim node, which is resposible for providing the sample. As depicted in the figure above, the attacker generates many node IDs which are close to the victim’s node ID. By surrounding the victim’s node with their own nodes, the attacker hinders clients from discovering the victim node, as evil nodes will deliberately withhold information about the victim’s existence.\nFor more information about such Sybil attacks, see this recent research paper on DHT Eclipse attacks. Furthermore, Dankrad’s DAS networking protocol proposal describes how the S/Kademlia DHT protocol suffers from such attacks and shows the need for a proof of validator protocol.\nProof of Validator\nThe above attack motivates the need for a proof of validator protocol: If only validators can join the DHT, then an attacker who wants to launch a Sybil attack must also stake a large amount of ETH.\nUsing our proof of validator protocol we ensure that only beacon chain validators can join the DHT and that each validator gets a unique DHT identity.\nFurthermore, for validator DoS resilience, we also aim to hide the identity of the validators on the networking layer. That is, we don’t want attackers to be able to tell which DHT node corresponds to which validator.\nTo fulfill these objectives, the proof of validator protocol must meet the following requirements:\n\nUniqueness: Each beacon chain validator must be able to derive a single, unique keypair. This property not only restricts the number of nodes a Sybil attacker can generate, but also enables network participants to locally punish misbehaving nodes by blocklisting their derived keypair\n\nPrivacy: Adversaries must be unable to learn which validator corresponds to a particular derived public key\n\nVerification Time: The protocol’s verification process must be efficient, taking less than 200ms per node, enabling each node to learn at least five new nodes per second\nSuch a proof of validator protocol would be used by Bob during connection establishment in the DHT layer, so that Alice knows she is speaking to a validator.\nProof of Validator protocol\nOur proof of validator protocol is effectively a simple anonymous credential scheme. Its objective is to enable Alice to generate a unique derived key, denoted as D, if and only if she is a validator. Subsequently, Alice uses this derived key D within the networking layer.\nIn designing this protocol, our objective was to create a solution that was both straightforward to implement and analyze, ensuring it meets the outlined requirements in an efficient way.\nProtocol overview\nThe protocol employs a membership proof subprotocol, wherein Alice proves she is a validator by demonstrating knowledge of a secret hash preimage using ZK proofs. Alice then constructs a unique keypair derived from that secret hash preimage.\nThe membership proof subprotocol can be instantiated through different methods. In this post, we show a protocol using Merkle trees and a second protocol using lookups.\nWhile both approaches demonstrate acceptable efficiency, they feature distinct tradeoffs. Merkle trees rely on SNARK-friendly hash functions like Poseidon (which may be considered experimental). On the other hand, efficient lookup protocols rely on a powers-of-tau trusted setup of size equal to the size of the validator set (currently 700k validators but growing).\nNow let’s dive into the protocols:\nApproach #1: Merkle Trees\nMerkle trees have seen widespread use for membership proofs (e.g. see Semaphore). Here is the tradeoff space when designing a membership proof using Merkle trees:\n\nPositive: No need for trusted setup\n\nPositive: Simple to understand\n\nNegative: Relies on SNARK-friendly hash functions like Poseidon\n\nNegative: Slower proof creation\nBelow we describe the proof of validator protocol based on Merkle trees:\nEvery validator i registers a value p_i on the blockchain, such that p_i = Hash(s_i). Hence, the blockchain contains a list \\{p_i\\} such that:\nwhere the p_i are public and the s_i are secret. The blockchain creates and maintains a public Merkle root R for the list of public p_i values.\nSuppose Alice is a validator. Here is how she can compute and reveal her derived key D given her secret value s_i:\n\nSet derived key D to equal D = s_i  G\n\n\nProve in zero-knowledge with a general purpose zkSNARK that there is a valid Merkle path from p_i to the merkle root R, plus the following statement:\n\nSet derived key D to equal D = s_i  G\nProve in zero-knowledge with a general purpose zkSNARK that there is a valid Merkle path from p_i to the merkle root R, plus the following statement:\nAt the end of the protocol, Alice can use D in the DHT to sign messages and derive her unique DHT node identity.\nNow let’s look at a slightly more complicated, but much more efficient, solution using lookups.\nApproach #2: Lookups\nHere is the tradeoff space of using lookup protocols like Caulk:\n\nPositive: Extremely efficient proof creation (using a preprocessing phase)\n\nPositive: Protocol can be adapted to use a regular hash function instead of Poseidon\n\nNegative: Requires a trusted setup of big size (ideally equal to the size of validators)\nBelow we describe a concrete proof of validator protocol:\nExactly like in the Merkle approach, every validator i registers a new value p_i on the blockchain such that:\nwhere the p_i are public and the s_i are secret. The blockchain creates and maintains a KZG commitment R to the vector of all p_i values.\nSuppose Alice is a validator. Here is how she can compute and reveal her derived key D given her secret value s_i:\n\nSet derived key D to equal D = s_i  G\n\n\nReveal commitment C = p_i G + s_i H where H is a second group generator.\nYou can view (D, C) = (s_i G, p_i G + s_i H) as an El-Gamal encryption of p_i under randomness s_i.  Assuming Hash is a random oracle, s_i is not in any way revealed so this is a valid encryption of s_i.\n\n\nProve using a Caulk+ proof that C is a commitment to a value in the set \\{p_i\\} represented by commitment R.\n\n\nProve with a Sigma protocol that s_i is consistent between D and C.\n\n\nProve with a general purpose zkSNARK that p_i = Hash(s_i) and that C = p_iG + s_iH.\n\nSet derived key D to equal D = s_i  G\nReveal commitment C = p_i G + s_i H where H is a second group generator.\nYou can view (D, C) = (s_i G, p_i G + s_i H) as an El-Gamal encryption of p_i under randomness s_i.  Assuming Hash is a random oracle, s_i is not in any way revealed so this is a valid encryption of s_i.\nProve using a Caulk+ proof that C is a commitment to a value in the set \\{p_i\\} represented by commitment R.\nProve with a Sigma protocol that s_i is consistent between D and C.\nProve with a general purpose zkSNARK that p_i = Hash(s_i) and that C = p_iG + s_iH.\nAt the end of the protocol, the validator uses D as her derived key on the networking layer.\nEfficiency\nWe benchmarked the runtime of our membership proof protocol (link to the benchmark code) in terms of proof creation and verification. Note that while the membership proof is just one part of our proof of validator protocol, we expect it to dominate the overall running time.\nBelow we provide benchmark results for a merkle tree membership proof using the Halo2 proof system with IPA as the polynomial commitment scheme. IPA is a slower scheme than KZG but it doesn’t require a trusted setup maximizing the advantages of the merkle tree approach.\nWe observe that both the prover and verifier times align well with our efficiency requirements.  For this reason, we decided against benchmarking the Caulk-based approach, as its performance is expected to be significantly better in all categories (especially prover time and proof size).\nBenchmarks were collected on a laptop running on an Intel i7-8550U (five years old CPU).\nThe uniqueness property of the proof of validator protocol ensures that each network participant possesses a distinct derived keypair. However, for certain networking protocols, it might be advantageous to allow validators to have rotating identities, where their derived keys change periodically, perhaps daily.\nTo implement this, we can adapt the protocol to generate rotating derived keys based on a variable string, such as a daily changing value. For instance, let D = r_iG: to prove the validity of this rotated key, we can utilize a SNARK proving that r_i = Hash(\ns_i || \\mathsf{daily string}). Additionally, the SNARK must prove that p_i = Hash(s_i) and conduct a membership proof on p_i.\nIn such a scenario, if Eve misbehaves on a particular day, Alice can blocklist her for that day. However, on the next day, Eve can generate a new derived key, which is not blocklisted. If we wanted to be able to permanently blocklist validators based on their rotating identity we would need a more advanced anonymous credentials scheme like SNARKBlock.\nAn alternative (perhaps simpler) approach would be to build a commitment out of all validator identity BLS12-381 keys and do a membership proof on that commitment.\nHowever, this approach would require validators to insert their identity private key into the ZK proof system to create a valid membership proof and compute the unique derived key.\nWe decided to not take this approach because it’s not good practice to insert sensitive identity keys into complicated cryptographic protocol, and it would also make it harder for validators to keep their main identity key offline.\nFuture research directions\nCan we avoid SNARK circuits entirely and perform the membership proof and key derivation in a purely algebraic way?\nRelated: Can we have an efficient proof of membership protocol without a trusted setup and without relying on SNARK-friendly hash functions?\nAcknowledgements\nThanks to Enrico Bottazzi, Cedoor, Vivian Plasencia and Wanseob for the help in navigating the web of membership proof codebases.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": [
            "data-availability"
        ]
    },
    {
        "title": "zkFOCIL: Inclusion List Privacy using Linkable Ring Signatures",
        "link": "https://ethresear.ch/t/zkfocil-inclusion-list-privacy-using-linkable-ring-signatures/21688",
        "article": "by George, Benedikt and Thomas.\nThanks to Francesco for discussions and comments.\nMotivation\nWe propose an alternative approach to improving FOCIL (EIP-7805) privacy. Inspired by the work on Anonymous Inclusion Lists and focusing on includer privacy, we propose a simple protocol based on Linkable Ring Signatures (LRS) that allows validators to privately publish inclusion lists. We also propose an LRS scheme based on general-purpose SNARKs.\nOur goal with this document is to spark research into specialized constructions that instantiate this LRS primitive more efficiently and elegantly, compared to our SNARK construction.\nOverview\nIn FOCIL, the beacon chain uses RANDAO to select a committee of includers, who then publish inclusion lists.\nHere, we aim to hide the identity of these includers in two steps:\nA private lottery protocol picks the committee of includers from the validator set.\nA linkable ring signature scheme anonymizes includers’ messages.\nWe show how to achieve these steps using an LRS scheme and propose a SNARK-based instantiation.\nLinkable Ring Signatures\nAbstractly, linkable ring signatures (LRS) let a signer prove membership in a group without revealing which member signed, while also detecting repeated use of the same key. Each signature includes a “key image” that is derived from the secret key but does not reveal the signer’s identity. If the same secret key is used again, the resulting key image is the same, allowing verifiers to link multiple signatures from the same (anonymous) signer without exposing who they are.\nLinkable ring signatures are well studied cryptographic primitives (e.g. see this construction). For this proposal, we assume the following minimal API:\nKeyGen() → (SigningKey, PublicKey)\nGenerates a key pair.\nSign(Ring, SigningKey, message) → (Signature, KeyImage)\nProduces a ring signature on message over the given Ring of public keys, returning both the signature and its associated KeyImage.\nVerify(Ring, message, (Signature, KeyImage)) → bool\nVerifies that the signature was validly produced by one of the private keys in Ring.\nThis mirrors a standard linkable ring signature interface, except we expose the key image directly rather than providing a separate Link() function. Looking ahead, making the key image explicit allows us to use it in private lotteries. Verifiers can then detect multiple signatures from the same secret key simply by comparing key images. We also assume that the key images are indistinguishable from random strings for anyone not holding the corresponding signing key.\nProtocol overview\nAt every slot, Alice computes her ephemeral ring signing key (i.e. the key image) and checks it against a predicate to see if she won the lottery. Because the key image is tied to the validator’s identity public key, the key image is unique for each validator, and hence Alice cannot grind lottery tickets to bias the election. Furthermore, because the key image looks random for everyone except Alice, only Alice herself will know that she won the lottery, until she decides to reveal herself.\nIf Alice won the lottery, she is an includer for that slot, and she should build and broadcast an inclusion list and sign it with her LRS signing key.\nProtocol Spec\nWe now provide a protocol spec of our “Private Proof Of Includer” protocol using the API of the LRS scheme above:\nKey generation\nEvery validator generates a linkable ring signature keypair and registers its public key to the blockchain:\nThis only needs to be done once, but there is currently no such channel for key registration in the beacon chain. There are approaches (like the SNARK-based one we describe below) where the validator BLS12-381 signing key can be used for this scheme, at the cost of exposing the validator private key to this new primitive.\nLottery\nA validator is selected to be an includer based upon the return value of is_includer().\nThis approach satisfies the secrecy requirement from the FOCIL IL Committee Selection document, since only Alice can compute her own key image.\nNote that the probability of winning is the same for all validators and it’s not weighted by stake.\nInclusion list verification\nWhen Alice, the proposer, receives ILs from the P2P network, she checks the following for every IL:\nthat the signature is valid\nthat the sender is a valid includer\nwhether she has received more than one IL from the same signer\nSimilar rules would be introduced for nodes on the P2P network as suggested by the EIP.\nLinkable Ring Signature schemes\nThe above spec can be realized with various LRS protocols from the literature. Below, we propose an LRS construction using general-purpose SNARKs, since existing published schemes are not efficient enough for our use case. Afterwards, we provide performance requirements for an LRS scheme to be compatible with our use case.\nLRS based on zk-SNARKs\nIn this section we provide an instantiation of an LRS scheme based on general purpose zk-SNARKs:\nThe following SNARK-based approach is inspired by the Proof Of Validator post (also see this recent paper from ESORICS 2024). In this approach, the lottery protocol is implemented using the derived ephemeral BLS public key (similar to how beacon chain aggregators are currently selected using a BLS-based VRF).\nIn particular, consider a validator with (Sk, Pk) as her BLS identity keypair:\nwhere validators_pks is a list of BLS public keys for all validators, and validators_pks_commitment is a Merkle commitment to validators_pks.\nSNARK.Prove(w, x) is the prove function of an abstract zk-SNARK scheme where w represents the witness and x the statement. Similarly, SNARK.Verify(proof, x) verifies the statement x using the proof.\nThis approach is efficient and practical and with the right SNARK proof system (e.g. Halo2+IPA) it does not require a trusted setup.\nHowever, the complexity of specifying and implementing a SNARK is considerable. This is due to SNARKs having multiple complex underlying parts (polynomial commitment scheme, proof system, arithmetization) and each of them being critical to security and performance.\nIt’s worth noting that the approach above avoids key registration by inherently using the BLS identity keypair of the validators. This is generally not recommended practice: you are not supposed to use long-term private keys in cryptosystems they were not intended for. An alternative would be to use the “Approach 1” from the Proof Of Validator post, which would add an explicit key registration step to the protocol. Key registration is particularly hard in Ethereum because there is no standard way for validators to register new cryptographic keys, and because it bloats the size of the BeaconState considerably (given that we have 1 million validators).\nPerformance Requirements\nAny LRS that satisfies the following requirements would work here:\nInclusion lists are 8kb, so signature size should be less than 1.5 kb (this likely translates to O(1) or O(logn) constructions, where n is the total number of validators).\nSigning time must be fast (on the order of milliseconds).\nVerification time must be fast (on the order of milliseconds).\nPublic keys should be tiny (ideally one group element).\nKey image computation can be slow (on the order of seconds). It’s likely that we can precompute it ahead of time since it does not depend on the block or transactions.\nIdeally should be able to work with already existing BLS keys. Require no key registration step.\nNo trusted setup. No RSA accumulators.\nInformal Security Properties\nHere is a list of informal security properties that our private FOCIL scheme should cover:\nIndependence and Fairness of the Lottery\nEach validator has the same probability of winning in a given slot, and one validator’s probability of winning does not depend on whether others have won. This security property depends on an LRS scheme which produces unique key images given a public key, and also by feeding the key image into the a random oracle before applying the predicate.\nHiding of Winners Until Reveal\nNo one can determine whether a validator has won unless and until that validator publishes its key image and signature. In other words, a validator’s winning status remains secret until it explicitly reveals proof. This security property depends on an LRS scheme where key images are pseudorandom and the pair (key_image, signature) is unforgeable. Hence, Bob cannot derive the key image of Alice to check if she won the lottery.\nAnonymity Upon Reveal\nEven when a validator does reveal its key image and signature, external observers only learn a pseudonym (i.e., the key image) and cannot link this ephemeral identity to the validator’s long-term public key.\nUnforgeability of Signatures\nAn adversary cannot create a valid signature or key image on behalf of a winning validator without knowing that validator’s secret key.\n",
        "category": [
            "Proof-of-Stake",
            "Block proposer"
        ],
        "discourse": [
            "censorship-resistance"
        ]
    },
    {
        "title": "LazyTower: An O(1) Replacement for Incremental Merkle Trees",
        "link": "https://ethresear.ch/t/lazytower-an-o-1-replacement-for-incremental-merkle-trees/21683",
        "article": "LazyTower is a data structure. Its purpose is the same as an Incremental Merkle Tree (IMT): to allow users to incrementally append items, and to support zero-knowledge proofs of membership.\nAppending an item to LazyTower has an amortized cost of O(1).\nThe circuit complexity of the proof is O(log N). The verification cost is O(1).\nCore Concepts\nItems are appended starting from the bottom-most level. When a level is full, its entire contents are digested and appended to the level above, freeing space for new items on the lower level. For example, consider a tower with a width of 4; please watch the video:\nFor instance, by storing\nwe can later prove that items 0, 1, 2, and 3 have been appended.\nBy storing\nwe can later prove that items 0 through 15 have been appended.\nWe can observe that each cell in the tower is a Merkle root, which fixes 4^0, 4^1, 4^2, … items respectively.\nThis is explained more clearly in the following video:\nCost\nOn average, the lowest level is modified once per append.\nThe next level is modified once for every 4 appends.\nThe level above that is modified once for every 16 appends.\nWe can find a constant C such that the cost of any single-level modification does not exceed C.\nThus, on average, the cost of a single append is no more than:\nPrivacy\nWhen we want to prove that an item has been appended without revealing the item itself, we can use zero-knowledge proofs to demonstrate its membership.\nWe can use a traditional Merkle proof to prove that an item belongs to a particular root in the tower. For example: “My item is hidden in the second root of level 10, which covers 4^10 items.”\nIf we wish to maintain complete privacy, we would need to load all the roots in the tower to prove membership, which incurs an O(log N) cost – not ideal.\nIs it possible to use a single value to fix these roots so that only one value needs to be loaded when proving membership?\nWe improve this in two stages: horizontal and vertical.\nImprovement 1: Level Digests (Horizontal)\nAs mentioned earlier, when a level is full, we digest it and store the result in the upper level. If we choose a digest function that can be computed incrementally, we only need to store the latest digest instead of an array of roots.\nFor example, we can use a Merkle-Damgård construction combined with a ZK-friendly hash function (such as Poseidon hash):\nSince each level only appends data at the end, every operation requires a fixed set of steps — loading, hashing, saving, and updating the length — without the need to recompute everything from scratch.\nlevel_digests.png1724×1684 47.8 KB\nThus, during the proof, each level only needs to load a single level digest.\nHowever, this still requires O(log N) values to be loaded into the circuit. Can this be further improved?\nImprovement 2: Digest of Digests (Vertical)\nFor the digests of each level, we can compute a vertical digest of digests from top to bottom, thereby fixing all the digests.\nBy placing the frequently changing lower levels at the end, we can perform localized updates by storing the prefix result, without recomputing from scratch.\nThat is, we store the following values:\ndigest_of_digests.png1400×1684 58.4 KB\nWhen the lowest level d0 changes, we can load H(H(H(d4, d3), d2), d1) and update d0 locally.\nWhen the lower levels d1 and d0 change, we can load H(H(d4, d3), d2) and update d1 and d0 locally.\nThe cost of updating a single level remains constant (load/hash/save).\nOne to Rule Them All\nThus, we obtain a digest of digests that fixes each level’s digest. Each level digest, in turn, fixes the roots of that level; and each root fixes the leaves of that tree, i.e., the items that were initially appended.\nThis means that during the proof, we only need to load O(1) data.\nproof.png3004×1704 62.5 KB\nBecause:\nThe cost at each level remains bounded by a constant.\nThe frequency of modifications decreases exponentially at higher levels.\nThe amortized cost for appending an item remains O(1).\nImplementation\nBelow, we observe the results of the implementation.\nWe can see that the average gas cost for appending an item quickly converges to a constant (21000 included).\nMoreover, a tower with a larger width results in a lower average gas usage, though with limits.\naverage_gas_usage.png2912×1804 298 KB\nCompared to the Incremental Merkle Tree, even when accommodating a large number of items, users do not have to worry about rising gas costs, nor do they need to determine a capacity limit from the start.\nIMT_LazyTower.png2500×1545 290 KB\nAlthough increasing the width can slightly reduce gas usage, it also increases circuit complexity during the proof.\ncircuit_complexity.png2500×1545 335 KB\nDuring deployment, both gas cost and circuit complexity are considered. A width of 4 to 7 is a reasonable choice.\nConclusion\nLazyTower has an average gas cost of O(1), a circuit complexity of O(log N), and does not require determining a capacity limit upfront. Projects that use Merkle Trees may consider using LazyTower.\nAcknowledgement\nI conceived this idea in early 2023. Many thanks to the Ethereum Foundation for its grant, which enabled the implementation, and to the reviewers for their invaluable help!\nThe implementation is currently maintained by the PSE team and released under an open source license.\nJavascript: GitHub - privacy-scaling-explorations/zk-kit: A monorepo of reusable libraries for zero-knowledge technologies.\nSolidity: GitHub - privacy-scaling-explorations/zk-kit.solidity: A monorepo of reusable contracts for zero-knowledge technologies.\nCircom: GitHub - privacy-scaling-explorations/zk-kit.circom: A monorepo of reusable Circom circuits.\n",
        "category": [
            "Data Structure"
        ],
        "discourse": []
    },
    {
        "title": "Fabric - Fabric to Accelerate Based Rollup Infrastructure & Connectivity",
        "link": "https://ethresear.ch/t/fabric-fabric-to-accelerate-based-rollup-infrastructure-connectivity/21640",
        "article": "Over the last few weeks many teams and individuals across Ethereum provided feedback and input on this idea and post. Some of these teams were able to reflect their support in the recent Sequencing Call hosted by Justin Drake, but this support stretches beyond those teams. We also want to note that Fabric is a continuation of many efforts and is a Schelling point to help coordinate and accelerate the benefits of based sequencing.\nPersonal Note: Since last year I have been focused on an effort called Commit-Boost. Fabric in no way is a shift away from Commit-Boost, rather leaning in unlocking more resources to help push Ethereum forward.\nFabric850×855 89.2 KB\nTL;DR\nThe Ethereum rollup ecosystem has experienced remarkable growth, with multiple L2s driving scalability, innovation, and adoption. Years of progress have provided the community with valuable insights into design trade-offs and opportunities for collaboration.\nOver the past year, based rollup research and development contributions and efforts have progressed significantly with the goal of offering an alternative way to sequence rollups that aims to inherit as much of Ethereum’s core properties—liveness, decentralization, and censorship resistance—as possible, while helping address fragmentation within the L2 ecosystem, improving UX, and enabling new forms of composability with the L1.\nWith two based rollups (Taiko, Facet) now live in production and several preconfirmation (preconf) protocols in development/testing on Holesky, we believe based rollups have matured to a point where the community has a clearer understanding of design trade-offs and a path forward. Conversations with over a dozen teams indicate strong support for a coordinated effort around based rollups to:\nBackground\nThe Ethereum ecosystem has been experiencing rapid growth, with multiple L2s flourishing and bringing in users and applications at an accelerated pace. This growth has driven scalability, user adoption, and innovation, showcasing the strength and diversity of Ethereum’s developer and user communities. Along with this, based rollup efforts emerged as a solution to help address L2 ecosystem fragmentation. Following an ETH Research post nearly two years ago and a year-long push throughout 2024, there is now general consensus among an ever-growing part of Ethereum that we should pursue the development of based rollups (timeline of based rollup talks and developments).\nThere is a vast diversity of based rollups and based rollup stacks being built (these are just the ones we’re aware of), each innovating in its own ways. They all share the challenge of re-implementing common infrastructure, such as modifications to the PBS pipeline, highlighting the importance of collaboration and standardization within the based ecosystem.\nBased Ecosystem as of Sequencing Call 16 - Justin Drake1920×596 81.8 KB\nWe want to note that the “How It Is Going” is does not capture all the teams looking to support or be based.\nProposal\nWe propose Fabric, an effort to coordinate and standardize various components needed for based rollups. The goal is to coordinate, help build, shepherd adoption, and then sustain a collection of common standards and components that anyone can use or develop towards in their own based or non-based rollup stack. As part of the effort, we will encourage and help develop reference implementations for anyone to deploy a based rollup or use as a guide to adapt and evolve their stack.\nDesign principles\nOpen-source/Open Development: Everything will be developed in the open and released under open-source licenses (MIT/Apache-2.0).\nLimit Stack Lock-In: We aim to develop the minimal components necessary to integrate with a variety of existing rollups ensuring they are not purpose-built for any single stack.\nModularity: Fabric will establish a minimal set of standards that rollup developers can adopt to expedite the creation of based rollups while leaving room for competition, such as developers creating custom stacks or SDKs.\nGovernance Minimized: This effort will not be venture-backed, and there are no plans to launch a token or monetize. It is a public good and will involve contributions from teams across the Ethereum ecosystem.\nOverview of Fabric\nFabric is designed to facilitate the adoption of based sequencing by providing the tools necessary to decentralize rollup sequencers and efficiently interface with Ethereum validators. Rather than being a monolithic stack, Fabric offers a minimal set of modular components built on a set of standards that rollups can use to transition towards being based. This helps ensure rollups have a path to adopt based sequencing efficiently while maintaining flexibility to innovate and observe the based rollup infrastructure mature.\nBased rollups exist on a spectrum of decentralization, and Fabric seeks to avoid imposing rigid design choices. For example, this can range from a straightforward “total anarchy mode,” where anyone can propose the next block, to more structured models that restrict proposers to those providing preconfs—credible commitments about transaction inclusion and execution that enhance the user experience.\nAs part of this effort, Fabric will include open-source reference implementations for components that are not yet built or are built but need modifications, ensuring developers have a clear starting point for adoption.\nWe view this effort as an initial foundation and are eager to collaborate across the ecosystem to refine and advance its development. Fabric tentatively includes:\nL1 components for based block construction:\nCommitments API: Enables consumers to request and verify preconfs (needs coordination across preconf teams which will build on the Constraints API efforts).\nConstraints API: Ensures proposers build L1/L2 blocks that satisfy preconfs (in development).\nUniversal Registry Contract: Allows proposers to register and be discovered (in development).\nL2 components for based sequencing:\nProposing Layer: Multiple components needed to make rollup contracts aware of based preconfers (needs development and coordination across teams).\nSequencing Layer: Multiple components to delegate L2 proposing rights, price transactions, and distribute fees i.e., to proposers/gateways/PGF/etc (needs development and coordination across teams and in particular coordination and thoughts around PGF—see Vitalik’s recent talk here).\nBlob Sharing: Optimizes shared blob space across rollups (needs development and coordination across teams).\nShared Bridging: Enables shared settlement helping interoperability (needs development and coordination across teams).\nFabric is unopinionated on the VM or proof system so the following layers are left open for custom implementations:\nDerivation Layer: Facilitates L2 state reconstruction using an L1 node (exists in multiple rollup stacks, but needs to be standardized/developed across different stacks).\nExecution Layer: The goal is to support multiple virtual machines (needs development and coordination to potentially support different EVM implementations).\nSettlement Layer: Accommodates diverse proving systems (needs coordination and development around the proving system i.e., validity/fraud/multi-proof).\nFabric Overview856×671 363 KB\nNear-Term Roadmap\nWe have already begun work at the proposer/PBS pipeline level and are now focused on expanding efforts to cover the full scope. In the coming days, weeks, and months we plan to help drive consensus and coordination around the Fabric’s design, secure funding to support development, and hire developers to accelerate progress. Collaboration with the community will be a key focus, ensuring that specifications are refined and aligned with broader ecosystem needs. We expect to have a more detailed plan, finalize specs, and start shipping components of the stack within the next few days to weeks.\nFAQ:\nWhy take this approach for Fabric?\nWe aim to standardize based sequencing, which doesn’t fit neatly into a single “rollup stack.” For example, efficient based sequencing (such as with preconfs) involves components that impact the L1 block production supply chain which is out of scope of a rollup stack. Fabric aims to deliver a complete set of tools—such as standards, APIs, contracts, and other essential components—required to reduce frictions with choosing to go based. Some of these components are public goods that require coordination and sustained development.\nDoes this compete with L2s already being developed and flourishing or based rollup efforts already underway?\nNo, Fabric does not compete with existing L2s or based rollup efforts. Instead, it seeks to identify common infrastructure, standardize it, and sustain it as public goods. Rather than compete, Fabric aims to reduce development friction for upcoming based rollups and existing rollups to become based.\nGiven the significant resources invested, why hasn’t a coordinated effort like this emerged earlier for L2s? What lessons can we apply now to ensure the success of this initiative?\nWhen L2 teams first began building, they faced the challenge of pioneering new technologies without the benefit of established frameworks or insights into the long-term implications of their design choices. Each team had to operate independently and innovate from the ground up. Now, in 2025, the ecosystem has matured significantly. The community has a clearer understanding of best practices, trade-offs, and the impact of various implementations. This perspective allows us to rethink the process and design an approach that is more aligned with Ethereum’s core principles from the outset.\nIs this just another standards working group?\nThis effort is about coordinating with stakeholders to agree and align on standards and then ship the necessary components. This will require cross-team discussions and coordination to develop minimum viable implementations.\nIs the team developing this plan to launch their own rollup?\nNo, the team will not launch its own rollup. The focus is on developing and maintaining minimal open-source components that serve the Ethereum ecosystem as a whole. Any reference implementation stacks are solely meant to be references for the community and ideally are developed by various rollup teams.\nWill the team launch a token or monetize?\nNo, the Fabric initiative will not launch a token or engage in monetization. The effort is designed to be governance-minimized with no venture backing or business model, ensuring it remains a public good for the Ethereum community.\nIs Fabric finalized?\nNo, Fabric is still under development. While the team has begun work on key components and initial conversations with various teams, the exhaustive list of components and their designs has not yet been finalized. We are working with the community to identify, define, and refine the specs, and we expect to ship parts of Fabric in the coming weeks to months. Please reach out if you’re interested in helping!\nConclusion:\nWe are excited to continue this effort, promote collaboration, and advance the coordination and development of Ethereum’s ecosystem. If you would like to help, please reach out.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "based-sequencing"
        ]
    },
    {
        "title": "Dynamic Blob Targets for Better Blob Pricing",
        "link": "https://ethresear.ch/t/dynamic-blob-targets-for-better-blob-pricing/20687",
        "article": "Abstract\nThis proposal introduces a dynamic pricing mechanism for blobs in Ethereum, using a PID (Proportional-Integral-Derivative) controller to adjust the target number of blobs. The goal is to maintain baseline security and assumes Data Availability Sampling (DAS) while optimizing blob usage and burn rates, ensuring economic stability and predictability for network participants.\nWithin Target Bounds:\n-Price adjusts linearly as actual blob count varies from the target.\n-Increases when above target, decreases when below.\nOutside Target Bounds:\n-Existing blob pricing mechanisms take over, causing exponential price changes.\n-This continues until actual blob count returns within the target bounds.\nBlob Target Adjustment Effects:\n-When target increases: Price per blob decreases, but overall burn amount increases.\n-When target decreases: Price per blob increases, but overall burn amount decreases.\nThis proposal aims to create a more dynamic and responsive blob pricing system for Ethereum, enhancing network efficiency and economic stability while maintaining crucial security guarantees. The use of a PID controller for target adjustment provides a well-established control mechanism, potentially offering more predictable, sustainable, and stable long-term behavior compared to other algorithmic approaches. It is my belief that a pricing mechanism with similar behavior to what I’ve described would better match the inelastic demand common to L2s utilizing blobs for DA.\n",
        "category": [
            "Economics"
        ],
        "discourse": [
            "data-availability",
            "rollup"
        ]
    },
    {
        "title": "ETH Issuance Discovery Research: Issuance Debate & Case Studies By Staking Cohort",
        "link": "https://ethresear.ch/t/eth-issuance-discovery-research-issuance-debate-case-studies-by-staking-cohort/21664",
        "article": "Vivian Zhu, Otakar Korinek, Alex Duckworth - February 3, 2025\nThis research was funded by cyber•Fund and conducted by members of FranklinDAO, a blockchain organization run by students from the University of Pennsylvania. The focus of the research was to analyze the proposal focused on reducing the issuance of ETH by Ethereum to Ethereum validators. The effects of this proposal would directly impact stakers and market participants who benefit from the staking yield. Therefore, we explored the literature in depth and also conducted interviews with participants at different levels of staking sophistication and market participation. The breakdown of interviews consists of 1-4 interviewees from largest players in each of the key actors in eight categories we have broken down – Solo Stakers, Large Crypto holders / Digitally Native Individuals, Retail Investors, Institutions / High Net Worth Individuals, SSPs, Staking Pools, Centralized Actors, and Distributed Validator Technology.\nThe transition of Ethereum from a Proof-of-Work (PoW) to a Proof-of-Stake (PoS) consensus mechanism marked a significant evolution in Ethereum’s history. This shift was driven by the need for enhanced security, greater decentralization, and more efficient energy usage. In PoS, validators replace miners, securing the network by staking their ETH. Validators are chosen to propose and attest to blocks, earning rewards or facing penalties based on their performance. The requirement to stake 32 ETH to become a validator ensures a robust commitment to the network’s security and functionality, while the separation of layers—the consensus layer (CL) and execution layer (EL)—facilitates efficient communication and coordination within the blockchain.\nDeposits and withdrawals play a crucial role in maintaining the PoS consensus. Deposits are used to create or top up validators, ensuring they can participate effectively in securing the network. Withdrawals, whether partial or full, allow validators to reclaim their staked ETH, providing flexibility and incentivizing continued participation. The introduction of withdrawal mechanisms, initially met with skepticism, has proven effective in enhancing Ethereum’s resiliency and encouraging validator diversity. This flexibility has increased the number of validators and integrated staking into the broader DeFi ecosystem, leading to innovations like Liquid Staking Tokens (LSTs) and restaking.\n|1178.223642172524x3761600×511 94.2 KB\n|1163.7299035369774x3711600×511 94.2 KB\nThe proposal suggests a new reward curve that gradually reduces issuance as the amount of staked ETH increases. This approach aims to balance the incentives for staking with the need to keep the network secure without over-rewarding validators. By tempering issuance, Ethereum can ensure that staking remains attractive while preventing excessive accumulation of control by a few entities. The paper asserts that the adjustment is crucial for the long-term sustainability and robustness of the Ethereum network. For a more detailed discussion of the proposal, please refer to Caspar’s writeup here.\nIn the paper, we will reference interviews conducted with each stakeholder listed. Our interviewers have requested to remain anonymous, so we have created the descriptions accordingly:\n[1] Interview conducted with Nixo from the EthStaker community\n[2] Interview conducted with P2P\n[3] Interview conducted with a large crypto holder\n[4] Interview conducted with a founder of liquid staking derivative startup\n[5] Interview conducted with Coinbase Investor Relations team\n[6] Four interviews conducted with university students in blockchain space\n[7] Interview conducted with a hedge fund\n[8] Interview conducted with Chorus One\n[9] Interview conducted with Lido\n[10] Interview conducted with Obol\nAreas of investigation\nOur research has focused on key actors and firms across the Ethereum staking landscape. We have conducted interviews and analyzed data to understand the potential effects of the proposed issuance changes on various stakeholders. Definitions of each stakeholder will be discussed in their individual sections.\nEffects on stakers\nSolo Stakers:\nMost vulnerable to changes in the issuance curve\nSupportive of adjustments that favor decentralized staking\nConcerned about potential negative issuance and declining real yields\nFace challenges with tax implications, especially when nominal yields don’t reflect real economic gains\nLarge Crypto Holders / Digitally Native Individuals:\nPrimarily interested in ETH for price appreciation\nView staking as a secondary benefit\nPrefer liquid staking tokens (LSTs) for DeFi utility\nRelatively insensitive to small yield changes due to convenience and gas fee considerations\nRetail Investors:\nGenerally “sticky” customers, prioritizing convenience and brand trust\nOften indifferent to short-term price fluctuations or minor yield differences\nWilling to stake on platforms like Coinbase despite potentially lower yields\nInstitutions/High Net Worth Individuals:\nPrioritize security, compliance, and dilution protection\nDemand for staking is relatively inelastic\nWilling to accept lower yields (even as low as 1%) for the benefits of staking\nChanges in distribution amongst staking entities\nStaking Service Providers (SSPs):\nLarger SSPs may be better positioned to weather reduced yields due to economies of scale\nSmaller SSPs likely to face more significant challenges, potentially leading to industry consolidation\nMay need to cut costs, particularly in engineering and infrastructure\nStaking Pools (e.g., Lido):\nMay face some compression in fees proportionate to the decline in ETH yield\nExpected to continue innovation efforts, supported by substantial treasury reserves\nCentralized Exchanges (e.g., Coinbase):\nStaking flows expected to remain steady due to prioritization of security and ease of use by customers\nPotential for increased centralization raises concerns about network security (e.g., 51% attack risk)\nPlans to continue operations, subsidized by profits from other parts of the business\nDistributed Validator Technology (DVT):\nFaces significant challenges with reduced issuance rates\nMay struggle to maintain profitability, potentially undermining decentralization goals\nExploring strategies to remain viable, including fee structure changes and efficiency improvements\nIn simplest terms, the staking landscape can be divided into those who supply ETH and those who take in ETH “deposits” and use them to participate in validating the network. We detail both segments below.\nSolo Stakers\nSolo stakers supply ETH into their own validators, which are participants in the Ethereum network that work with a group of validators to verify transactions together. They are necessarily tech-savvy, allowing them to run their own infrastructure. They need to setup the validator nodes, install all necessary software for transaction processing and validation rules, and maintain their infrastructure.\nIndividual solo stakers are intermediate in size amongst the “ETH suppliers.” Based on data from Rated Network, the average solo staker has 4.5 validator nodes running and the largest solo staker has 201 validator nodes. The median, though, sits at 1 validator node. That means most solo stakers have ~32 ETH staked, translating to roughly $125,000 – a sizable investment.\n722×434 8.69 KB\nIn terms of solo stakers share of the ETH supplied, it’s fairly low. Rated estimates that they form 6.5% of the network. Nevertheless, solo stakers form a larger portion of the “voice” within the community because they contribute most to decentralization of the network.\nThe most pressing concern that solo stakers face is that they are finding themselves becoming more irrelevant to ongoing protocol changes in the rapid expansion of staking protocols and centralized exchanges. Since the minority of validators belong to solo stakers, they are put into a challenging position with less representation as most other validators in the network belong to professionals [1]. Researchers and other actors have attempted to represent or advocate for solo stakers, however they have not been able to make a concrete impact with actionable results.\nPreferences\nIn addition to the ETH, solo stakers must purchase the hardware capable of validating the network. That cost typically amounts to $1,000. The operating costs of solo-staking are thus minimal. The only substantial costs, both monetary and time-wise, are the startup costs. The startup costs are sunk for existing solo stakers. Running the validator itself requires little to no cost and time commitment.\nSolo staking offers lower ROI than liquid staking due to startup infrastructure costs, their security setup, and the inability to use a liquid token. The tradeoff is added security – solo stakers don’t need to worry about the integrity of the liquid staking protocol. Perhaps a more important benefit of solo staking is the “altruistic” gain from securing the network as was originally intended and contributing to decentralization.\nFrom an economic perspective, the primary concern for solo stakers is negative issuance, where the total supply of tokens actively decreases over time, as they are the most significantly affected group. Solo stakers are particularly vulnerable because they lack the resilience and buffer that professional staking services possess, making it harder for them to endure prolonged periods of financial strain.\nInsights\nSolo stakers are receptive to changes in the issuance curve.\nSolo stakers are particularly sensitive to changes in the issuance curve. A significant portion of solo stakers dislike issuance reduction in general, but expressed support for adjustments to the issuance curve if such changes would favor decentralized forms of staking. This would be more advantageous for solo stakers, however, is unlikely to be implemented, as it may take approximately four to five years of research and development to complete [1]. Additionally, implementing such changes will be challenging if those who benefit the most from the current issuance curves, such as venture capitalists with significant investments, oppose them.\nThe current risk for solo stakers lies in the potential for the issuance curve to reach 100% staked, resulting in a yield of just 1.8%. This yield is considerably low, making it difficult for solo stakers to remain competitive unless they have access to advanced technology. As it stands, the current issuance curve is particularly challenging for solo stakers, who lack the resources and buffers available to professional staking services.\nPrice elasticity\nA decision-making that’s not purely economic combined with the sunk costs makes solo stakers relatively sticky. That’s corroborated by our interviews with Nixo from the EthStaker community. However, we question how sensitive are “inflows” – e.g. does the number of new stakers depend on the yield they can get.\nAccording to P2P, most solo stakers invested in Ethereum a long time ago and are not primarily motivated by financial gains. Their participation is often rooted in a deep belief in the Ethereum network and its potential. The cost of running the necessary infrastructure for staking is significant, with fixed costs for setting up a node amounting to approximately $1,500 [2]. Given the current price of ETH, solo staking is frequently not profitable, underscoring that these individuals are driven by idealistic rather than economic motivations.\nTaxes\nOne of the most significant pain points for solo stakers is the disparity between real and nominal yields. When solo stakers receive nominal yields that do not reflect the true economic gains due to inflation or other factors, they still face tax liabilities based on these nominal figures. This situation becomes particularly painful when stakers are taxed on nominal yields without realizing any real financial benefit. Essentially, they end up paying taxes on “nothing,” as the nominal gains do not translate into actual income.\nTaxes represent the second biggest issue for solo stakers, who often face unfair or unclear tax treatment depending on their country of residence. In Germany, the tax situation for staking is favorable, providing a supportive environment for solo stakers. In contrast, Austria has a less favorable tax regime, making it more challenging for stakers. In the USA, the tax treatment of staking is inconsistent and often leaves participants guessing, with some instances being advantageous and others not.\nAs the staking ratio increases over time, this problem is exacerbated. The real yield for stakers declines faster than the nominal yield, further intensifying the tax burden on solo stakers. This declining real yield, combined with unfavorable tax treatment, creates a challenging financial environment for solo stakers, making it difficult for them to sustain their participation in the staking ecosystem. Addressing these tax issues is crucial for creating a more equitable and supportive environment for solo stakers.\nAttitudes on existing advocacy for solo stakers\nIn the Staking Survey 2024 conducted by EthStaker with 1024 responses, of which 868 respondents consist of stakers who claim to control their node’s configuration, a notable 19% of respondents believe researchers overlook solo Ethereum stakers, possibly due to the topic’s limited discussion and the prevalence of professional validators. Reducing returns to scale seems impractical, especially since larger entities can masquerade as solo stakers, further complicating the issue and potentially skewing perceptions of network participation.\nLarge Crypto holders / Digitally Native Individuals\nThis segment largely supplies their ETH into decentralized pools like Lido and RocketPool. By definition, these people are digitally native, have their own wallets, and feel comfortable navigating the blockchain. That removes perhaps the largest barrier to entry to decentralized pools for retail: knowing how to get your money there.\nPreferences\nStaking is a secondary concern for this segment, just like for most. Based on our interviews, techies are primarily interested in ETH due to the price appreciation component. Staking adds a little extra return with little downside.\nTechies prefer staking pools due to the ability to use the liquid staking tokens (LSTs) in DeFi. In their words, they “most likely wouldn’t stake if they wouldn’t get an LST, as the yield is lower than they can get on Aave.” [3] That allows them to maximize the utility of their staked tokens. They are also wary of the time commitment solo staking requires / do not have the requisite amount of ETH.\nInsights\nFor this segment, price elasticity plays a crucial role in their staking behavior. While staking is a secondary concern, large crypto holders and digitally native individuals are motivated by the additional returns it provides. They are not devoted to staking, but they don’t want to unstake primarily due to the hassle and gas fees associated with moving their funds elsewhere. The ease of use and liquidity offered by platforms like Lido, which has integration with DeFi applications, further reinforce their decision to remain staked, even in the face of lower yields.\nThe primary motivators for these individuals include potential airdrops associated with staking and the opportunity to capitalize on higher APY opportunities through staking or restaking. These incentives drive their participation in staking, despite the relatively low yields compared to other DeFi opportunities.\nThe sentiment expressed by these individuals highlights their pragmatic approach to staking. They are primarily motivated by convenience and the added utility of LSTs in DeFi, rather than the staking yield itself. As one interviewee mentioned, “If yield falls, I would keep it there—hassle not worth it plus gas” [4]. This reflects a general preference for maintaining the status quo due to the complexities and costs associated with moving staked assets. Lido’s extensive liquidity and broader DeFi applications further solidify this preference, making it the platform of choice for digitally native individuals looking to maximize the utility of their staked ETH.\nRetail Investors\nRetail investors constitute a significant portion of Ethereum stakeholders, though their ETH holdings typically form a minimal part of their overall investment portfolio. According to ByBit May 2024 data, only about 9% of retail crypto assets are invested in ETH. This suggests a diverse approach to cryptocurrency investment among retail participants, with Ethereum being just one component of a broader strategy.\nPreferences\nThe preferences of retail investors in the Ethereum ecosystem are noteworthy. As observed by Coinbase’s Investor Relations team, “by and large, simple customers tend to be more of a staker” [5]. In addition to interviews conducted with university students active in the blockchain space, we conclude that retail investors are often comfortable with long-term holding strategies and don’t typically face immediate liquidity constraints [6]. Their willingness to stake suggests a belief in Ethereum’s long-term potential and a desire for passive income through staking rewards.\nInsights\nWhen it comes to price elasticity, retail investors in Ethereum demonstrate interesting behaviors. Many seem relatively indifferent to short-term price fluctuations or minor differences in staking yields. Their primary motivation appears to be holding ETH for long-term price appreciation rather than maximizing short-term gains. This is evidenced by the fact that many retail investors stake on platforms like Coinbase, which may offer up to 25% lower yields compared to alternatives [5]. The reasons for this preference include ease of use, perceived trustworthiness, and possibly a lack of awareness or concern about yield differences. This behavior suggests that retail ETH investors are generally “sticky” customers, prioritizing convenience and brand trust over marginal gains in staking rewards.\nInstitutions/High Net Worth Individuals\nPreferences\nInstitutions and high-net-worth individuals prioritize security above all else, as losing clients’ money would have significant repercussions for their entire operation [7]. Compliance is also critical, ensuring accurate tax reporting and adhering to regulations when allocating client funds. Another key concern is dilution protection, as holding plain ETH is inherently dilutive—staking offers a way to mitigate this. A paper by Steakhouse Financial highlights that stakers, including institutions, face various barriers, but staking is essential to protect against the dilution of their ETH holdings. Due to these preferences, institutions primarily rely on centralized solutions for staking.\nInstitutional staking is substantial. According to Coinbase’s Investor Relations reports, over $10.9 billion worth of assets were staked by institutional customers (we use FY’23 disclosure and raise it by change in ETHs price). By applying the percentage of ETH assets on Coinbase’s platform, this translates to roughly $3.4 billion of ETH staked, or about 0.9 million ETH. The allocation of ETH by institutions continues to rise.\n\nSource: ByBit\nInsights\nInstitutional demand for staking is inherently inelastic. Yield is a secondary concern, with the primary focus on allocating ETH for clients. Institutional ETH holdings on platforms like Coinbase are significant, with only 24% of ETH currently staked. Based on insights from Coinbase Investor Relations, the percentage of institutional ETH staked is likely lower than the aggregate. Assuming that 20% of institutional ETH on Coinbase is staked (vs. 24% platform-wide average), yields approximately $15 billion of institutional ETH staked on the Coinbase platform.\nFrom our conversation with P2P, institutions would value even a 1% yield, indicating that demand for staking will persist [2]. The competition between different staking providers is not primarily based on yield but on the additional services they offer.\nStaking Providers\nSolo stakers also fit into this category, but they have been discussed separately.\nSSPs\nStaking Service Providers (SSPs) are actors who provide custodial or non-custodial staking services, which lower barriers to entry by handling technical complexities, provide more rewards through strategies and optimal infrastructure, protect against slashing, and data analytics. Specifically, in this paper, we are referring to companies such Chorus One, which provides quick staking integrations for institutions and investors, research, and protocol support.\nCost Structure\nFor SSPs, costs increase marginally with each additional validator until they reach around 500 validators, after which they benefit from economies of scale. The primary costs are infrastructure-related, followed by personnel expenses. With MVI, SSPs earn lower margins but remain profitable.\nTaking Chorus One as a unit economics example, about 80-90% of their costs are fixed, with the rest being variable. The fixed costs mainly involve backend operations, particularly engineering staff, which make up the largest portion. Engineers are needed to manage a certain number of servers, and as the number of machines grows, so does the need for “engineer hours,” making these costs semi-fixed. Node operators/SSPs can reduce some costs by cutting redundancies and administrative expenses, but these savings are marginal. Additional fixed costs include self-hosting, OVH, and maintaining a research team [8].\nVariable costs primarily involve servers, where some economies of scale are present. However, some costs, like accounting and finance, remain constant no matter the activity level, while most engineering salaries are semi-variable. They remain mostly constant, but, at a certain point, the SSP needs to invest incremental resources in them. The variable costs are more directly tied to the number of servers needed [8].\nSources of Staked ETH\nThere are two primary sources of staked ETH, each with different customer bases but similar motivations—price upside is the main concern, with staking as a secondary objective [8].\nCategory 1: Lido and Liquid Staking\nThe component of ETH staked through Lido and liquid staking solutions is relatively insensitive to staking yield due to the lower opportunity cost and strong network effects. Many users opt to stake simply because they can, regardless of whether the yield is 1%, 2%, or 3%. This side of the business has seen significant growth, with liquid staking tokens (LSTs) increasing in popularity. However, the share of LSTs in the market has been decreasing as SSPs ramp up direct sales.\nCategory 2: Direct Staking by High-Net-Worth Individuals (HNW), Institutions, and Intermediaries\nThis category includes those who hold ETH primarily for price appreciation and choose to stake it, often using an SSP’s API or DIEP (a smart contract for staking) to capture a fraction of the rewards. Direct stakers prefer SSPs due to lower risks and compliance benefits. These stakers are typically long-term holders, parking their capital to earn additional yield without taking on smart contract risks associated with liquid staking providers. They view direct staking as a relatively risk-free way to earn some yield on their crypto holdings.\nIn general, direct stakers are fairly insensitive to yield fluctuations. They purchase ETH for price exposure and stake it as a secondary benefit, provided the price upside remains attractive.\nOverall, sources of staked ETH are inelastic. Most users continue to stake unless there is a significant price spike, which is more common in less established networks like NEAR. In such cases of high volatility, users may prefer to realize their profits before deciding whether to continue staking.\nProfitability\nThe SSP operators we spoke to don’t expect the profitability change for them to be dramatic. However, we note that they were at the larger side of the spectrum. That means their fixed costs were lower as percentage of their current revenue. Subscale operators might find themselves unable to leverage these fixed costs.\nWe expect this will manifest in cutting costs. Our operators mentioned that reducing headcount, specifically in the engineers managing staking set-up, is the most likely first line of defense against falling revenues.\nOperators further mentioned that the cost take-out could continue on the infrastructure side, though we view that opportunity set to be limited, as SSPs already run fairly efficient infrastructures. Here we again note the pernicious effect on small operators: smaller SSPs have less space to cut on the infrastructure side. Though there is some wiggle room, our conversations have revealed that cutting infrastructure costs more would cut into the necessary costs that would significantly hamper the quality of their service. On the other hand, larger SSPs generally said that they could reduce “nice-to-have” infrastructure costs.\nThe last cost-reduction avenue are administrative expenses, however those are marginal and unlikely to move the needle.\nHence, we expect the profitability of small operators to suffer disproportionately more.\nETH Inflows\nLido\nThe Lido portion depends on the growth of (i) ETH staked on Lido and (ii) the allocation towards SSPs. Lido’s base is retail heavy and doesn’t look at yield as the primary variable when staking. Hence, we expect the number of ETH staked on Lido to be relatively stable. However, if it decreases on the margin, it’s possible that Lido would consider changing the allocations to transfer the impact onto SSPs. Thus, we are cautious about the growth in Lido inflows.\nInstitutions\nWe think that institutions will have an incrementally lower appetite for staking. The decrease in yield will slim down the margin between staking and their opportunity cost. Nevertheless, institutions are yield starved today and we don’t expect large-scale scaling down of staking.\nOverall, SSPs inflows should suffer, though even the effect on them won’t be catastrophic. Nevertheless, an increasing centralization among SSPs appears imminent. These platforms compete on added services, particularly those most valuable to institutions. However, these services introduce a layer of fixed costs that create challenges for smaller providers. When staking yields decrease, smaller direct service providers may struggle to maintain these services.\nAs a result, any decrease in the inflows of ETH will be felt by the smaller SSPs, further reducing their ability to remain competitive against larger SSPs with a greater resource base. We think this will lead to a concerning level of industry centralization.\nLido’s strategy of diversifying its operator base serves as a potential countermeasure to centralization. Per its most recent Q3’24 report, Lido added 130 new node operators, bringing its total on DVT platform to 236 unique operators, slowing the consolidation trend by preventing larger players from disproportionately dominating the staking landscape.\nStaking Pools\nA staking pool lets people combine their cryptocurrency to collectively validate blockchain transactions and earn rewards, making it possible for smaller holders to participate in Proof-of-Stake networks when they couldn’t meet minimum requirements alone. In this paper, we are referring to actors such as Lido, which distributes stake uniformly across thirty-eight node operators.\nLido’s main module distributes stake uniformly among 38 node operators, and include additional modules include Simple Distributed Validator Technology (DVT) and Community Staking Model (CSM) [9].\nThe Simple DVT module tests distributed validator technology on Ethereum mainnet via Obol and SSV Network, enabling new stakers to join as node operators while improving security and decentralization through enhanced resilience.\nThe Community Staking Module enables anyone to become a Lido node operator with just an ETH deposit, offering smoothed rewards and a user-friendly experience to make staking more accessible than traditional solo staking while increasing protocol decentralization.\nSize and Sources of Staked ETH\nLido is the largest staking pool, commanding 28% of all ETH staked. Lido’s staked ETH comes from sophisticated on-chain individuals and crypto native institutions. The average staker puts in around $80,000 worth of ETH.\nCost Structure\nPer our conversations with Lido, the cost structure is mostly fixed. The main cost items are servers and engineers. We think that the cost distribution is similar to SSPs, where the bulk of the costs are eaten up by the people working on the infrastructure. The fixed cost curve is flat, but rises in a stepwise fashion for infrastructure expansion – e.g. a move from 100 to 150 ETH staked has minimal effect on costs, but going from 100 ETH to 200 ETH necessitates a large infrastructure investment.\nInnovation Roadmap\nLido plans to integrate the Distributed Validator Technology (DVT). DVT should decentralize Lido’s validator set. By distributing validator responsibilities across multiple operators, Lido is reducing the risk of any single point of failure. That should ensure a more resilient and secure staking infrastructure. Implementation will begin in 2024. The roll out is expected to be gradual, as they test and refine the system.\nLido is also planning to introduce staking on new networks, expanding beyond Ethereum. This diversification gives users access to broader yield opportunities while spreading risk across multiple blockchain ecosystems. It allows Lido to position itself as a multi-chain staking solution, attracting a wider base of users. The timeline for adding these networks is flexible, but Lido aims to launch initial expansions throughout 2024 and 2025.\nIn parallel, Lido will enhance node operator participation with permissionless validation, opening up validator operations to more participants. This promotes decentralization by making it easier for smaller or independent operators to join and secure the network. The added diversity of node operators strengthens the overall network and reduces centralization risks. Lido plans to fully deploy this feature by mid-2024. The innovation is now live on testnests.\nInherently decentralizing\nLido and other pools distribute ETH across various staking providers\nSSPs for example partner w/ Lido and wait for allocation\n\nReceive some part every time someone stakes with Lido\n\n\nReceive some part every time someone stakes with Lido\nLimits growth of operators by allocating new ETH staked disproportionately to smaller operators\n\nE.g. If there are 100 ETH staked on the platform, with 80 managed by Operator1 and 20 by Operator2, when new 10 ETH comes on the platform, both operators will receive 5 ETH (not actual figures)\n\n\nE.g. If there are 100 ETH staked on the platform, with 80 managed by Operator1 and 20 by Operator2, when new 10 ETH comes on the platform, both operators will receive 5 ETH (not actual figures)\nProfitability\nWe think that Lido would be less affected by the proposal than solo stakers and SSPs. Lido’s staking base skews retail heavy compared to SSPs. The retail customers that we spoke to don’t consider yield to be their primary variable in staking with Lido. They regard Lido as better than the opportunity cost: letting their ETH lay unproductive. As long as that holds, Lido should maintain the vast majority of its staked ETH [6].\nNevertheless, Lido’s fees would compress, similarly to Coinbase, proportionately to the decline in yield on ETH.\nETH Inflows\nLido’s inflows are fairly hard to forecast, However, we believe that Lido should expand its market share in the upcoming years, being an incremental share taker thanks to its customer base. The customer base skews towards retail and is less price elastic than larger institutions.\nWe do note though that Lido should lose more of the staked ETH than Coinbase should the proposal pass. That’s because Lido’s staking base is larger (~$80k ETH staked on average) and is more acquainted with on-chain alternatives, compared to Coinbase’s retail-heavy stakers.\nInnovation Roadmap\nLido’s innovation roadmap would likely not be that affected. Lido has one of the largest corporate treasuries in DeFi. Its leadership has proven that it is forward thinking and emphasizes innovation. One thing emphasized by Lido representatives when we spoke to them is that they realize the only way to survive in staking is to offer the best product. They understand that cutting innovation amidst the decrease in profits sets them up to be uncompetitive with other offerings in the future [9]. Thus, bare a dramatic decrease in profitability, we think that Lido should be able to maintain its innovation roadmap.\nCentralized Actors\nCoinbase currently has around 4.3 million ETH staked, valued at approximately $16 billion, with control over 20,000 validators. The majority of Coinbase’s staked ETH comes from retail customers, who tend to buy and hold crypto, viewing staking as a simple way to earn additional returns. Institutional staking, although a smaller share today, is expected to grow significantly due to larger capital allocations.\nLooking ahead, Coinbase expects growth in both retail and institutional staking, with institutions likely to play a larger role due to their greater financial resources. Coinbase’s staking cost structure involves taking a 25% cut from staking rewards (reportedly lower for institutions, though Investor Relations didn’t disclose the number to us), which mainly covers fixed costs. This fee is justified by the platform’s strong brand and reliability, allowing users to earn staking rewards they otherwise wouldn’t participate in, given the onboarding barrier for on-chain staking solutions.\nImpact of the Change\nDespite the potential impact of these changes, staking flows into Coinbase are expected to remain steady and largely inelastic. Customers prioritizing security, compliance, and ease of use are likely to continue choosing Coinbase, with staking returns being a secondary concern. However, the increasing percentage of ETH on Coinbase raises concerns about the potential for a 51% attack. While Coinbase’s Investor Relations (IR) team claims they will work to avoid this risk, there is skepticism about their commitment given the company’s fiduciary duty to shareholders. IR mentioned that Coinbase could mitigate this risk by staking through third-party validators via Coinbase Cloud [5].\nNumber of ETH on Coinbase’s platform at the end of each quarter\n|448.4048802530502x269.5735697018534722×434 8.23 KB\nPercentage of ETH on Coinbase’s platform staked|450.7969383160738x267.8798291109656724×430 8.27 KB\nDistributed Validator Technology (DVT)\nDistributed Validator Technology (DVT) represents a significant advancement in blockchain validator infrastructure decentralization. Unlike traditional setups where a single entity operates nodes, DVT enables validators to be run by a distributed network of nodes. This approach changes how rewards are distributed and infrastructure is managed. Companies like Lido are integrating with DVT to reduce reliance on single entities, aiming for a network of 14 operators collaborating to run validators [9].\nDVT introduces challenges in reward distribution. Traditional staking pools typically charge 10% of rewards, but with DVT, this share must be divided among multiple operators. This fragmentation raises questions about DVT operation viability and optimal reward structures to ensure profitability for all participants.\nObol is a key player in this landscape, providing DVT solutions that enhance validator key security and management. Their approach is particularly crucial for liquid staking protocols, which view DVT as essential for achieving truly decentralized node operations [10]. While liquid staking protocols are early adopters, institutional adoption is still in exploratory phases.\nDVT is increasingly seen as critical infrastructure for Ethereum’s consensus layer, essential for realizing blockchain decentralization. It enables validator participation from diverse geographic locations, enhancing network resilience and global representation.\nImpact of the Change\nThe proposed reduction in Ethereum’s issuance rate poses significant challenges for the DVT ecosystem. This change disproportionately affects DVT operators who must split already reduced rewards among multiple entities. Current profitability estimates for DVT operations are concerning, with reports suggesting low returns for operator sets.\nThe impact extends beyond individual operators to the broader ecosystem. Liquid staking protocols may need to reassess the number of validators they can profitably support. There’s a risk that financial pressures could inadvertently favor larger, more centralized operators, potentially undermining DVT’s decentralization goals.\nGeographic diversity in validator participation could also be at risk, as lower rewards might make it economically unfeasible for operators in developing regions to participate. This potential contraction in the validator pool affects both network security and cultural diversity.\nTo address these challenges, several strategies are being explored [10]:\nReconsidering the traditional fee structure, potentially increasing operator fees.\nDeveloping more efficient infrastructure and implementing cost reduction strategies.\nExploring alternative reward mechanisms or supplementary income streams for DVT operators.\nWhile the issuance change presents significant challenges, it also catalyzes innovation and efficiency improvements. The coming months will be crucial in determining how the DVT model evolves to maintain decentralization while ensuring economic viability. The Ethereum community’s resilience suggests that creative solutions will emerge, potentially strengthening both DVT and the Ethereum network long-term.\nCentralization\nUnder the new proposal, staking will become far less attractive than traditional finance (TradFi) yields, compounding the current challenges. That lowers the incentives for new entrants. Consequently, the consolidation trend is unlikely to reverse.\nA possible countermeasure is incentivizing solo staking. Unfortunately, the solo staking community itself thinks that these measures will take 4-5 years to develop and implement. However, it is plausible that, by then, 70-80% of ETH may already be staked by large holders and venture capitalists. Implementing changes at that point will be difficult, as those benefiting most will resist.\nFor example, to reduce staking from 90% to 30%, drastic measures would be required to remove the 60% and will cause significant disruption.\nThe issuance reduction proposal, which would cut yields across the board to slow new staking, could make staking less appealing. Based on the proposal, at current rate of ETH invested, the real yield would fall from 2.5% to 1 % – a 60% reduction. This approach discourages incremental stakers from entering the market, but does nothing to counteract the underlying forces that lead to consolidation – cost and distribution advantages of large players that allow them to tap into price/yield-insensitive staker base.\nOur analysis of the current Ethereum staking model suggests that while the intentions behind the issuance curve are well-meaning, they might be addressing the wrong issues. The original issuance curve was somewhat arbitrary, a design the Ethereum Foundation adopted during the transition to Proof of Stake (PoS). As Ethereum’s issuance has become more modest and is approaching an equilibrium, profitability for staking has become increasingly challenging. The applications built on Ethereum make it risky to alter the issuance curve significantly. For example, Eigenlayer’s rapid growth can be attributed to the low profitability of staking, which has also led to greater centralization in staking activities.\nCore Problem: Centralization in Staking\nThe primary issue lies in the fact that staking inherently drives centralization due to the PoS model and its economies of scale. Staking pools are currently the only mechanism countering this force, and they can rate limit the growth of individual operators. The measurable concentration of staking is a concern, especially when considering entities like Binance and Coinbase that do not disclose their on-chain activities. By decreasing the issuance yield, smaller players suffer more, while entities like Coinbase, which have less sensitivity to yield fluctuations and larger scales, can absorb lower profits. This scenario exacerbates the centralizing tendency.\nFuture Development Concerns\nEthereum is not yet prepared for the influx of institutional money that could fundamentally change its landscape. Currently, ETH remains small compared to other asset classes, but when institutions start allocating significant resources to ETH, entities like Coinbase and other centralized actors will be the primary beneficiaries. This poses a legitimate threat of a 51% attack, with Coinbase being poised to gain the most due to its compliance with regulatory standards.\nCoinbase, as a public company focused on maximizing shareholder value, is unlikely to sacrifice revenue for the sake of decentralization. A concerning sign is that in Q3 2023, Coinbase stopped reporting the balance of ETH staked from institutional clients, which raises red flags about transparency. As of Q3 2024, Coinbase has also stopped reporting retail staked ETH. Without this information, the community has no visibility into the concentration of staking power. Staking pools, particularly Lido, have attempted to address these concerns in their core through distributing stake uniformly amongst node operators whitelisted by Lido DAO to limit the growth of individual operators.\nThe Role of Liquid Staking Tokens (LSTs)\nThe Ethereum community harbors concerns about LSTs because anything that exerts significant control over the network is viewed as a potential threat, leading to fears of cartelization and monopolistic practices (Ethereum Notes: Magnitude and Direction of Lido Attack Vectors). LSTs could potentially compete with ETH directly as a token, introducing use in transactions. However, while LSTs pose some risks, they also represent one of the most credible defenses against more significant threats, such as centralization by entities like Coinbase.\nLido has reached similar conclusions. The team doesn’t support changes in ETHs monetary policy, viewing these changes as unlikely to help counteract increasing consolidation. Instead, the protocol tries to leverage its market share to mitigate concerns through mechanism design. For example, by limiting the growth of node operators, Lido introduces checks that balance individual node operators market power.\nSupporting Solo Stakers\nSolo stakers are currently at a disadvantage in the staking landscape, with most new validators belonging to large liquid staking or restaking operations. There is a growing concern that solo stakers will not benefit from future proposals or be considered in the network’s evolution.\nStaking exhibits strong returns to scale: as a platform gets larger, its returns on capital increase. Reducing returns to scale in staking is not a practical solution, as it could be easily gamed to appear as if more solo stakers are participating. Potential solutions include implementing offline correlation penalties and initial slashing penalties. These would penalize operators with correlated validator behavior, such as when multiple validators go offline simultaneously, which often indicates they are managed by the same operator. Although this is an imperfect solution, it encourages operators to diversify their setups, reducing the risk of correlated offline events and promoting a more decentralized network.\n",
        "category": [
            "Economics"
        ],
        "discourse": []
    },
    {
        "title": "Faster block/blob propagation in Ethereum",
        "link": "https://ethresear.ch/t/faster-block-blob-propagation-in-ethereum/21370",
        "article": "Acknowledgements to @n1shantd, @ppopth and Ben Berger for discussions and feedback on this writeup and @dankrad for many useful discussions.\nAbstract\nWe propose a change on the way we broadcast and transfer blocks and blobs in the P2P network, by using random linear network coding. We show that we can theoretically distribute the block consuming 5% of the bandwidth and with 57% of the number of network hops (thus half the latency per message) of the time it takes on the current gossipsub implementation. We provide specific benchmarks to the computational overhead.\nIntroduction\nThe current gossipsub mechanism for distribution of blocks roughly works as follows. The proposer picks a random subset (called its Mesh) of D=8 peers among all of its peers and broadcasts its block to them. Each peer receiving a block performs some very fast preliminary validation: mostly signature verification, but most importantly not including state transition nor execution of transactions. After this fast validation, the peer rebroadcasts its block to another D peers. There are two immediate consequences from such a design:\nEach hop adds at least the following delay: one full block transfer from one peer to the next one (including both network ping latency, essentially bandwidth independent, plus transfer of the full block, bound by bandwidth).\nPeers broadcast unnecessarily a full block to other peers that have already received the full block.\nWe propose to use random linear network coding (RLNC) at the broadcast level. With this coding, the proposer would split the block in N chunks (eg. N=10 for all simulations below) and instead of sending a full block to ~8 peers, it will send a single chunk to ~40 peers (not one of the original chunks, but rather a random linear combination of them, see below for privacy considerations). Peers still need to download a full block, or rather N chunks, but they can get them in parallel from different peers. After they have received these N chunks that each is a random linear combination of the original chunks composing the original block, peers need to solve a linear system of equations to recover the full block.\nA proof of concept implementation highlights the following numbers\nProposing a block takes extra 26ms that are CPU bound and can be fully parallelized to less than 2ms on a modern laptop (Apple M4 Pro)\nVerifying each chunk takes 2.6ms.\nDecoding the full block takes 1.4ms.\nWith 10 chunks and D=40, each node sends half the data than with current gossipsub and the network broadcasts a 100KB block in half the time with benefits increasing with block size.\nThe protocol\nFor an in-depth introduction to network coding we refer the reader op. cit. and this textbook. We here mention minimal implementation details for the proof of concepts benchmarks cited above. In the case of block propagation (~110KB), latency or number of network hops dominate the propagation time, while in the case of large messages like blobs in full DAS, bandwidth dominates the propagation time.\nWe consider a finite field \\mathbb{F}_p of prime characteristic. In the example above we choose the Ristretto scalar base field as implemented by the curve25519-dalek rust crate. The proposer takes a block, which is an opaque byte slice, and interprets it as a vector of elements in \\mathbb{F}_p. A typical ethereum block is about B = 110KB at the time of writing, given that each Ristretto scalar takes a little less than 32 bytes to encode, a block takes about B/32 = 3520 elements of \\mathbb{F}_p. Dividing into N=10 chunks, each chunk can be viewed as a vector in \\mathbb{F}_p^{M}, where M \\sim 352. The block is thus viewed as N vectors v_i \\in \\mathbb{F}^M_p, i=1,...,N. The proposer chooses a subset of D\\sim 40 peers at random. To each such peer it will send one vector of \\mathbb{F}_p^M together with some extra information to validate the messages and prevent DOS on the network. We explain the proposer\nThe Proposer\nWe will use Pedersen commitments to the Ristretto elliptic curve E as implemented by the above mentioned rust crate. We assume that we have already chosen at random a trusted setup of enough elements G_j \\in E, j = 1, ..., K with K \\gg M. We choose a standard basis \\{e_j\\}_{j=1}^M for \\mathbb{F}_p^M. So each vector v_i can be written uniquely as\nv_i = \\sum_{j=1}^M a_{ij} e_j,\nfor some scalars a_{ij} \\in \\mathbb{F}_p. To each vector v_i we have a Pedersen commitment\n C_i = \\sum_{j=1}^M a_{ij}G_j \\in E. \nFinally for each peer in the subset of size D \\sim 40 the proposer chooses uniformly random a collection of scalars b_i, i=1, ...,N and sends the following information to the peer\nThe vector v = \\sum_{i=1}^N b_i v_i \\in \\mathbb{F}_p^M. This is of size 32M bytes and it’s the content of the message.\nThe N commitments C_i, i=1,...,N. This is 32N bytes.\nThe N coefficients b_i, i=1, ...,N. This is 32N bytes.\nA BLS signature to the hash of the N commitments C_1 || C_2 || ... || C_N, this is 96 bytes.\nA signed message is the collection of elements 1–4 above. We see that there are 64N \\sim 640 extra bytes sent on each message as a sidecar.\nReceiving Peers\nWhen a peer receives a message as in the previous section, the verification goes as follows\nIt verifies that the signature is valid for the proposer and the hash of the receiving commitments.\nIt writes the receiving vector v = \\sum_{j=1}^M a_j e_j and then computes the Pedersen commitment C = \\sum_{j=1}^M a_j G_j.\nThe received coefficients b_i are a claim that v = \\sum_{i=1}^N b_i v_i. The peer computes C'= \\sum_{i=1}^N b_i C_i, and then verifies that C = C'.\nPeers keep track of the messages that they have received, say they are the vectors w_i, i = 1,...,L for L < N. They generate a subspace W \\subset \\mathbb{F}_p^M. When they receive v, they first check that if this vector is in W. If it is, then they discard it as this vector is already a linear combination of the previous ones. The key of the protocol is that this is very unlikely to happen (for the numbers above the probability of this happening is much less than 2^{-256}). As a corollary of this, when the node has received N messages, then it knows that it can recover the original v_i, and thus the block, from the messages w_i, i=1,...,N.\nNotice also that there is only one signature verification that is needed, all incoming messages have the same commitments C_i and the same signature over the same set of commitments, thus the peer may cache the result of the first valid verification.\nSending Peers\nPeers can send chunks to other peers as soon as they receive one chunk. Suppose a node holds w_i, i=1,...,L with L \\leq N as in the previous section. A node also keeps track of the scalar coefficients they received, thus they know the chunks they hold satisfy\n w_i = \\sum_{j=1}^N b_{ij} v_j \\quad \\forall i,\nfor some scalars b_{ij} \\in \\mathbb{F}_p they save in their internal state. Finally, nodes also keep the full commitments C_i and the signature from the proposer that they have validated when they validated the first chunk they received.\nThe procedure by which a node sends a message is as follows.\nThey choose randomly L scalars \\alpha_i \\in \\mathbb{F}_p, i=1,...,L.\nThey form the chunk w = \\sum_{i=1}^L \\alpha_i w_i.\nThey form the N scalars a_j, i=1,...,N by\n a_j = \\sum_{i=1}^L \\alpha_i b_{ij}, \\quad \\forall j=1,...,N. \nThe message they send consists of the chunk w, the coefficients a_j and the commitments C_i with the signature from the proposer.\nBenchmarks\nThe protocol has some components that are in common with gossipsub, for example the proposer needs to make one BLS signature and the verifier has to check one BLS signature. We record here the benchmarks of the operations that need to be carried in addition to the usual gossipsub operations. These are the CPU overhead that the protocol has on nodes. Benchmarks have been carried on a Macbook M4 Pro laptop and on an Intel i7-8550U CPU @ 1.80GHz.\nParameters for these benchmarks were N=10 for the number of chunks and the total block size was considered to be 118.75KB. All benchmarks are single threaded and all can be parallelized\nProposer\nThe proposer needs to perform N Pedersen commitments. This was benchmarked to be\nNodes\nA receiving node needs to compute 1 Pedersen commitment per chunk and perform a corresponding linear combination of the commitments supplied by the proposer. The timing for these were as follows\nWhen sending a new chunk, the node needs to perform a linear combination of the chunks it has available. Timing for these were as follows\nWhen decoding the full block after receiving N chunks, the node needs to solve a linear system of equations. Timings were as follows\nOverall CPU overhead.\nThe overall overhead for the proposer on the Apple M4 is 26ms single threaded while for the receiving nodes it is 29.6ms single threaded. Both processes are fully parallelizable. In the case of the proposer, it can compute each commitment in parallel, and in the case of the receiving node these are naturally parallel events since the node is receiving the chunks in parallel from different peers. Running these process in parallel on the Apple M4 leads to 2.6ms in the proposer side and 2.7ms in the receiving peer. For real life applications it is reasonable to consider these overheads as zero compared to the network latencies involved.\nOptimizations\nSome premature optimizations that were not implemented consist on inverting the linear system as the chunks come, although the proof of concept cited above does keep the incoming coefficient matrix in Echelon form. Most importantly, the random coefficients for messages do not need to be in such a large field as the Ristretto field. A small prime field like \\mathbb{F}_{257} suffices. However, since the Pedersen commitments take place in the Ristretto curve, we are forced to perform the scalar operations in the larger field. The implementation of these benchmarks chooses small coefficients for the linear combinations, and these coefficients grow on each hop. By controlling and choosing the random coefficients correctly, we may be able to bound the coefficients of the linear system (and thus the bandwidth overhead in sending the blocks) to be encoded with say 4 bytes instead of 32.\nThe simplest way to perform such optimization would be to work over an elliptic curve defined over \\mathbb{F}_q with q = p^r for some small prime p. This way the coefficients can be chosen over the subfield \\mathbb{F}_p \\subset \\mathbb{F}_q.\nPrivacy considerations the implementation in the PoC linked above considers that each node, including the proposer, picks small coefficients to compound its linear transformation. This allows a peer receiving a chunk with small coefficients to recognize the proposer of the block. Either the optimization above is employed to keep all coefficients small by performing an algorithm like Bareiss’ expansions or we should allow the proposer to choose random coefficients from the field \\mathbb{F}_p.\nSimulations\nWe performed simulations of block propagation under some simplifying assumptions as follows.\nWe choose a random network modeled as a directed graph with 10000 nodes and each node having D peers to send messages to. D is called the Mesh size in this note and was chosen varying on a large range from 3 to 80.\nPeers where chosen randomly and uniformly on the full node set.\nEach connection was chosen with the same bandwidth of X MBps (this is typically assumed to be X=20 in Ethereum but we can leave this number as a parameter)\nEach network hop, incurs in an extra constant latency of L milliseconds (this is typically measured as L=70 but we can leave this number as a parameter)\nThe message size is assumed to be B KB in total size.\nFor the simulation with RLNC, we used N=10 chunks to divide the block.\nEach time a node would send a message to a peer that would drop it because of being redundant (for example the peer already had the full block), we record the size of the message as wasted bandwidth.\nGossipsub\nWe used the number of peers to send messages D=6. We obtain that the network takes 7 hops in average to propagate the full block to 99% of the network, leading to a total propagation time of\n T_{\\mathrm{gossipsub, D=6}} = 7 \\cdot (L + B/X), \nin milliseconds.\ngossipsub-total-theorical1712×982 70.6 KB\nWith D=8 the result is similar\nT_{\\mathrm{gossipsub, D=8}} = 6 \\cdot (L + B/X), \nThe wasted bandwidth is 94,060 \\cdot B for D=6 and 100,297 \\cdot B for D=8.\nFor low values of B, like the current Ethereum blocks, latency dominates the propagation, while for larger values, for example propagating blobs after peer-DAS, bandwidth becomes the main factor.\nWith random linear network coding we can use different strategies. We simulated a system in which each node will only send a single chunk to all of the peers in their mesh of size D, this way we guarantee that the latency incurred is the same as in gossipsub: a single latency cost of L milliseconds per hop. This requires the mesh size to be considerably larger than N, the number of chunks. Notice that for a gossipsub mesh size of D_{gossipsub} (for example 8 in current Ethereum), we would need to set D_{RLNC} = D_{gossipsub} \\cdot N to consume the same bandwidth per node, this would be 80 with the current values.\nWith a much more conservative value of half this bandwidth, that is D=40 we obtain\nT_{RLNC, D=40} = 4 \\cdot \\left(L + \\frac{B}{10 X} \\right), \nwith a wasted bandwidth of 29,917\\cdot B. Assuming the same bandwidth as today we obtain with D=80 we get the impressive\nT_{RLNC, D=80} = 3 \\cdot \\left(L + \\frac{B}{10 X} \\right), \nwith a wasted bandwidth of 28,124\\cdot B, which is 28% of the corresponding wasted bandwidth in gossipsub.\nFor the same bandwidth sent per node, we see that the propagation time differs both by dividing the latency in two (there are 3 hops vs 6) and by propagating the block faster consuming a tenth of the bandwidth per unit of time. In addition the wasted bandwidth by superfluous messages gets slashed to 28% of the gossipsub wasted messages. Similar results are obtained for propagation time and wasted bandwidth but reducing the bandwidth sent per node by a half.\nIn the lower block size end, latency is dominant and the 3 hops vs 6 on gossipsub make most of the difference, in the higher block size end, bandwidth performance is dominant. For much larger blocksizes CPU overhead in RLNC gets worse, but given the order of magnitude of the transmission times, these are negligible.\nrlnc-gossipsub1712×982 87.7 KB\nIn the single chunk per peer approach in production, nodes with higher bandwidth could choose to broadcast to more peers. At the node level this can be implemented by simply broadcasting to all the current peers and node operators would simply chose the number of peers via a configuration flag. Another approach is to allow nodes to send multiple chunks to a single peer, sequentially. The results of these simulations are exactly the same as the above, but with much lower D as expected. For example with D=6, which would never broadcast a full block in the case of a single chunk sent per peer. The simulation takes 10 hops to broadcast the full block. With D=10 the number of hops is reduced to 9.\nConclusions, omissions and further work\nOur results show that one expects considerable improvement in both block propagation time and bandwidth usage per node if we were to use RLNC over the current routing protocol. These benefits become more apparent the larger the block/blob size or the shorter the latency cost per hop. Implementation of this protocol requires substantial changes to the current architecture and it may entail a new pubsub mechanism altogether. In order to justify this we may want to implement the full networking stack to simulate under Shadow. An alternative would be to implement Reed-Solomon erasure coding and routing, similar to what we do with Peer-DAS. It should be simple to extend the above simulations to this situation, but op. cit already includes many such comparisons.\n",
        "category": [
            "Networking"
        ],
        "discourse": []
    },
    {
        "title": "ZK-EVM Prover Input Standardization",
        "link": "https://ethresear.ch/t/zk-evm-prover-input-standardization/21626",
        "article": "Special thanks to Justin Drake, Elias Tazartes, Clément Walter, Rami Khalil for review and feedback.\nOn the path towards enabling real-time block proving, this post proposes a unified data format for the input that ZK-EVM prover engines should accept to prove an Ethereum Execution Layer (or EVM) block (see article by Vitalik for context about Validity proofs of EVM execution).\nNote: This article focuses only on the input for ZK-EVM to generate Execution Layer proofs and does not address the Consensus Layer, which is considered an independent problem.\nThe ZK-EVM block proving ecosystem is rapidly expanding, with various teams implementing their own ZK-EVM engines, such as Keth (Cairo) by Kakarot, Zeth (RiscV) by RiscZero, and RSP (RiscV) by Succinct.\nWhile all these ZK-EVMs aim to prove the same EVM blocks, they currently accept unique, vendor-specific inputs. This hinders operators who need to work with multiple vendors and does not allow to achieve a neutral and open supply chain for real-time block proving.\nThis post proposes a standardized EVM data format for ZK-EVM Prover Input that:\nEL clients can generate for every block.\nZK-EVM provers accept to generate EL proofs.\nThe Prover Input format aims to facilitate the creation of a neutral supply chain for real-time block proving by:\nFacilitating multi-proof proving infrastructure: Allowing operators to work seamlessly with multiple ZK-EVM prover vendors by always passing the same input to every ZK-EVM prover engine.\nEnabling the creation of a standard interface: Between EL clients (Execution Layer full clients executing block) and the multiple proving infrastructures running ZK-EVM(s) prover (proving the blocks).\nWorkflow\nZK-EVM Prover Inputs2363×379 16.3 KB\nWhen dealing with ZK-EVMs, we can broadly distinguish three types of data for generating the block’s proof:\nThe Prover Input: The EVM data required by the ZK-EVM engine to execute the block and generate the Execution Trace. This is vendor-agnostic.\nThe Execution Trace: An intermediary input generated by the ZK-EVM engine for each block, required by the proving engine to generate the final proof. This is vendor-specific.\nThe Proof: The final output of the prover, the proofs execution and state transition. This is the vendor-specific\nStatelessness\nZK-EVM proving engines, when proving a block, operate in a stateless environment without direct access to a full node. Therefore, the Prover Input must include the block witness, which consists of the minimal state and chain data required for EVM block execution (executing all transactions in the block, applying fee rewards, and deriving the post-state root).\nProver Input Data\nWe propose the following format for Prover Input data:\nNote: This does not include the total_difficulty accrued as of the parent block, which is necessary for fork selection in a pre-merge context. Assuming proving pre-merge blocks is not absolutely necessary, we keep this open for discussion.\nIn JSON format, it gives:\nWe are currently evaluating various options for Prover Input formats (e.g., Protobuf, SSZ, RLP) and compression (e.g., gzip). The primary goals are to:\nReduce Prover Input size: Speed up network communication, which is crucial for real-time block proving.\nEnsure compatibility: Support all major programming languages and existing blockchain tooling.\nTo enable batching and optimize proving costs, we propose a Prover Input containing a list of blocks (instead of a single block).\nThis allows setting the witness for all the blocks at once, so there is no duplication when multiple blocks access the same account, storage, headers, or codes.\nIn the context of real-time block proving, which requires a proof per block, the prover input will contain only a single block, but other use cases may benefit from a list.\nBeyond posting Prover Input to proving infrastructure for real-time block proving, Prover Input can be reused for other use cases by any agents. We are considering options to also store Prover Inputs on:\nData Availability layers (e.g., Avail, Celestia): Ideal for making input available temporarily, allowing all provers to generate proofs before purging the data.\nA shared data bucket (e.g., S3): Public for reads, with policies in place to freeze older data.\nThe Prover Input format can be easily extended to other EVM chains by using a specific version. For example, in the case of Optimism, the OP-specific chain configuration fields would also be included.\n",
        "category": [],
        "discourse": [
            "real-time-block-proving"
        ]
    },
    {
        "title": "Impact of Consensus Issuance Yield Curve Changes on Competitive Dynamics in the Ethereum Validator Ecosystem",
        "link": "https://ethresear.ch/t/impact-of-consensus-issuance-yield-curve-changes-on-competitive-dynamics-in-the-ethereum-validator-ecosystem/21617",
        "article": "Antero Eloranta, Santeri Helminen  - January 28th, 2025\nLink to full study\nThis research has received funding from cyber•Fund. Any opinions in this post are our own.\nTL;DR\nThis study reveals key dynamics in Ethereum’s validator ecosystem. Findings highlight the need to balance execution rewards and staking yields across validator segments. Implementing MEV-Burn with gradual issuance adjustments could promote a decentralized, diverse validator set.\nKey Findings:\n\nThe study reveals notable competitive disparities among Ethereum validators, with large pools currently enjoying a 12% higher mean return compared to single-validator pools. This advantage could potentially increase to 13-15% under proposed issuance curve reductions without MEV-Burn implementation.\n\n\nEconomically capped curves might create scenarios where large pools remain profitable while smaller ones incur losses.\n\n\nElasticity analysis shows solo stakers are particularly sensitive to relative yield decreases, emphasizing the importance of maintaining a balanced competitive landscape. This underscores the potential vulnerability of individual stakers to changes in the ecosystem’s dynamics.\n\n\nPositive market events like the Ethereum Shanghai/Capella upgrade or the Bitcoin ETF launch had significantly more positive net effects on the validator counts of larger stakers compared to smaller ones.\n\nThe study reveals notable competitive disparities among Ethereum validators, with large pools currently enjoying a 12% higher mean return compared to single-validator pools. This advantage could potentially increase to 13-15% under proposed issuance curve reductions without MEV-Burn implementation.\nEconomically capped curves might create scenarios where large pools remain profitable while smaller ones incur losses.\nElasticity analysis shows solo stakers are particularly sensitive to relative yield decreases, emphasizing the importance of maintaining a balanced competitive landscape. This underscores the potential vulnerability of individual stakers to changes in the ecosystem’s dynamics.\nPositive market events like the Ethereum Shanghai/Capella upgrade or the Bitcoin ETF launch had significantly more positive net effects on the validator counts of larger stakers compared to smaller ones.\nIntroduction\nThe current consensus issuance of Ethereum rewards is proportional to the square root of the deposit size. On top of these rewards, the block proposer is rewarded with execution rewards that depend on how much block builders are paying the proposer to include the block they propose as the next block.\nThe Ethereum community has discussed changing the issuance curve, e.g. [1, 2, 3, 4]. Some of the most widely discussed curve candidates include a reward curve with gradual reward reduction proposed by Anders, a reward curve with substantial reward reduction proposed by Anders [4] and a reward curve with an economically capped reward issuance proposed by Vitalik [5].\n872×547 42.5 KB\nFigure 1: Mean validator rewards under different reward curves\n872×547 32.3 KB\nFigure 2: Mean validator rewards under different current and economically capped reward curves\nThis study analyzes competitive dynamics, elasticities, and event responses within the Ethereum validator ecosystem. Using historical blockchain data from the Beacon Chain genesis to May 2024, we examine differences in competitive advantages between validator subgroups under current and proposed issuance curves. We find that large validator pools (100+ validators) currently have 12-15% higher mean returns compared to solo validators, with proposed issuance reductions potentially exacerbating this advantage.\nElasticity analysis reveals solo stakers are highly sensitive to relative yield decreases compared to other staking categories, while being less responsive to absolute yield changes or DeFi yield fluctuations. Event studies show significant validator behavior changes in response to major ecosystem developments like withdrawal enabling and Bitcoin ETF launches, but limited impact from new staking protocols or macroeconomic events.\nOur findings highlight the importance of addressing execution reward imbalances and maintaining relative yield competitiveness across validator segments to preserve a decentralized validator set. We conclude that a balanced approach combining MEV-Burn implementation with gradual issuance adjustments may best maintain a healthy, diverse validator ecosystem. Future research directions are proposed to further explore technological and alternative staking model impacts on validator behavior.\nCompetitive Advantages Analysis\nCurrent Competitive Landscape\nThere are differences in the mean consensus and the mean execution rewards as well as the combination of the two between different groups formed based on pool size, category, and entity. The consensus rewards increase as the pool size increases. The mean execution rewards for pools with 1 validator are lower than the mean execution rewards of pools with 100+ validators. The median of execution rewards increases as the pool size increases. Combining the consensus and execution rewards results, pools with size 1 have lower total rewards compared to pools with size of 100+ with the mean rewards for pools with size of 100+ being 12% higher.\nWhen grouped by category liquid restaking and solo stakers have lower than mean consensus rewards while CEX, liquid staking, and staking pools have higher than mean consensus rewards. Considering execution rewards no category’s execution rewards statistically significantly differ from the sample mean. Combining both consensus and execution rewards, no category has statistically significantly different rewards from the mean.\nObserving the 10 biggest individual entities. Out of the entities Lido, Binance, Kraken, and OKX have higher than mean consensus rewards while Coinbase, Rocketpool, Bitcoin Suisse, and Ether.Fi have lower than mean consensus rewards. No individual entity has execution rewards statistically significantly different from the mean. Combining consensus and execution rewards Lido has higher than average rewards while Rockerpool has lower than average rewards.\nGradual Reward Curve Reduction\nAnders proposed a reward curve with gradual reward reduction where the yield of consensus rewards follows a formula y_i = \\frac{cF}{\\sqrt{D}(1 + \\frac{D}{k})} where k = 2^{26}. Based on the analyzed sample if the issuance curve was to be changed and MEV-burn was not implemented this would result in a situation where staking pools with the size of 1 validator would have a mean APY of 3.13% while the pools with the size of 100+ validators would have a mean APY of 3.55% when considering the execution rewards. This means rewards for pools with a size of 100+ would be 13% higher than that of pools with a size of 1.\n872×547 49.5 KB\nFigure 3: Validator rewards for different size of validators under gradual reduction curve\nSubstantial Reward Curve Reduction\nAnders also proposed an alternative reward curve with substantial reward reduction where the yield of consensus rewards follows a formula y_i = \\frac{cF}{\\sqrt{D}(1 + \\frac{D}{k})} where k = 2^{25}. Based on the analyzed sample if the issuance curve was to be changed and MEV-burn was not implemented this would result in a situation where staking pools with the size of 1 validator would have a mean APY of 2.63% while the pools with the size of 100+ validators would have a mean APY of 3.02% when considering the execution rewards. This means rewards for pools with a size of 100+ would be 15% higher than that of pools with a size of 1.\n872×547 49.4 KB\nFigure 4: Validator rewards for different size of validators under substantial reduction curve\nEconomically Capped Curve\nVitalik proposed an economically capped reward issuance curve in his blog post. Under this curve, staking yield would follow y_i = cF(\\frac{1}{\\sqrt{D}} - \\frac{0.5}{\\sqrt{2^{25} - D}}) and become negative after a certain threshold staking amount is met. Based on the analyzed sample if the issuance curve were to be changed and MEV-burn was not implemented this would result in a situation where staking pools with the size of 1 validator would have a mean APY of 1.65% while the pools with the size of 100+ validators would have a mean APY of 1.97% when considering the execution rewards. Adopting such curve might create a scenario where the biggest pools could operate profitably while smaller pools would be losing money.\n873×547 43 KB\nFigure 5: Validator rewards for different size of validators under economically capped curve\nElasticity Analysis\nStaker behavior and elasticity were studied through validator yield and external market factors, examining deposits, withdrawals, and changes in total validator count. Validator yield was analyzed both in absolute terms and relative to other stakers. External market factors included Ethereum market price and DeFi yields from liquid staking tokens on Aave lending and Curve liquidity.\nValidator APY\nSolo stakers show negative elasticity in deposits when their yields fall relative to other staking options. They’re less responsive to changes in total staking yield or DeFi yields. This could suggest solo stakers prioritize their competitive position over general market conditions when making staking decisions.\n1189×593 42.6 KB\nFigure 6: Staking APY difference elasticity\nExternal Market Factors\nOur analysis shows distinct ETH price elasticity patterns across validator groups. Solo stakers and major entities like Lido and Binance are less likely to exit during price increases, showing negative elasticity (-0.4031 to -0.7476). Conversely, staking pools (1.1772) and entities like Ledger Live (4.1524) demonstrate higher exit rates as prices rise. Liquid restaking services show notably strong negative elasticity (-6.2982).\nDeFi yields from Aave and Curve protocols showed no significant impact on validator behavior, suggesting stakers’ decisions are primarily influenced by ETH price movements rather than DeFi opportunities.\nEvents Study\nThe study also investigates staker reactions to major events. The Rocket Pool mainnet launch and the FED’s first major interest rate hike did not significantly impact validator deposits, indicating limited immediate influence on staking behavior.\nThe Shanghai/Capella upgrade, which enabled staking withdrawals, led to increased deposits for larger pools and more exits from centralized exchanges and liquid staking providers.\n1189×378 30.2 KB\nFigure 7: Validator deposits one week before and after the Ethereum Shanghai/Capella upgrade that allowed staker withdrawals. The deposits have been normalized by the net deposits of the subgroup.\n1010×547 42.1 KB\nFigure 8: Cumulative validator exits by pool size for one month after the Shanghai/Capella upgrade. The exits have been normalized by the subgroup validator count at slot 6202800.\nThe launch of the first Bitcoin ETF significantly increased validator exits across most pool sizes and categories, except for the largest pools and staking pools. This behavior, prevalent in the retail-heavy CEX category, may reflect a “sell the news” phenomenon where stakers capitalized on positive news by exiting their positions in anticipation of a price correction.\n1189×378 30.3 KB\nFigure 9: Validator exits one week before and after the launch of the first Bitcoin ETF. The exits have been normalized by the active validator count of the subgroup.\nConclusion\nIn conclusion, this study provides insights into the competitive dynamics, elasticities, and event responses within the Ethereum validator ecosystem. The findings underscore the importance of addressing execution reward imbalances and balancing relative staking yield differences among validator segments. A balanced approach involving MEV-Burn and gradual issuance adjustments may better maintain a healthy, decentralized validator set, preserving incentives for diverse participation. Future research could explore the impact of regulatory changes, technological advancements, and alternative staking models on validator behavior, accounting for potential time lags and non-linear effects.\nLink to full study\n",
        "category": [
            "Economics"
        ],
        "discourse": []
    },
    {
        "title": "Native rollups—superpowers from L1 execution",
        "link": "https://ethresear.ch/t/native-rollups-superpowers-from-l1-execution/21517",
        "article": "TLDR: This post suggests a pathway for EVM-equivalent rollups to rid themselves of security councils and other attack vectors, unlocking full Ethereum L1 security.\nCredit for this post goes to the wider Ethereum R&D community. Key contributions originated from 2017, with significant incremental design unlocks over the years. A thorough design space exploration was instigated by recent zkVM engineering breakthroughs. This write-up is merely a best effort attempt to piece together a coherent design for a big idea whose time may have finally come.\nAbstract: We propose an elegant and powerful EXECUTE precompile exposing the native L1 EVM execution engine to the application layer. A native execution rollup, or “native rollup” for short, is a rollup which uses EXECUTE to verify EVM state transitions for batches of user transactions. One can think of native rollups as “programmable execution shards” that wrap the precompile within a derivation function to handle extra-EVM system logic, e.g. sequencing, bridging, forced inclusion, governance.\nBecause the EXECUTE precompile is directly enforced by validators it enjoys (zk)EL client diversity and provides EVM equivalence which is by construction bug-free and forward-compatible with EVM upgrades through L1 hard forks. A form of EVM introspection like the EXECUTE precompile is necessary for EVM-equivalent rollups that wish to fully inherit Ethereum security. We call rollups that fully inherit Ethereum security “trustless rollups”.\nThe EXECUTE precompile significantly simplifies development of EVM-equivalent rollups by removing the need for complex infrastructure—such as fraud proof games, SNARK circuits, security councils—for EVM emulation and maintenance. With EXECUTE one can deploy minimal native and based rollups in just a few lines of Solidity code with a simple derivation function that obviates the need for special handling of sequencing, forced inclusion, or governance.\nAs a cherry on top native rollups can enjoy real-time settlement without needing to worry about real-time proving, dramatically simplifying synchronous composability.\nThis write-up is structured in two parts, starting with a description of the proposed precompile and ending with a discussion of native rollups.\nPart 1—the EXECUTE precompile\nconstruction\nThe EXECUTE precompile takes inputs pre_state_root, post_state_root, trace, and gas_used. It returns true if and only if:\ntrace is a well-formatted execution trace (e.g. a list of L2 transactions and corresponding state access proofs)\nthe stateless execution of trace starting from pre_state_root ends at post_state_root\nthe stateless execution of trace consumes exactly gas_used gas\nThere is an EIP-1559-style mechanism to meter and price cumulative gas consumed across all EXECUTE calls in an L1 block. In particular, there is a cumulative gas limit EXECUTE_CUMULATIVE_GAS_LIMIT, and a cumulative gas target EXECUTE_CUMULATIVE_GAS_TARGET. (The cumulative limit and target could be merged with the L1 EIP-1559 mechanism when the L1 EVM is statelessly enforceable by validators.)\nCalling the precompile costs a fixed amount of L1 gas, EXECUTE_GAS_COST, plus gas_used * gas_price where gas_price (denominated in ETH/gas) is set by the EIP-1559-style mechanism. Full upfront payment is drawn, even when the precompile returns false.\nThe trace must point to available Ethereum data from calldata, blobs, state, or memory.\nenforcement by re-execution\nIf EXECUTE_CUMULATIVE_GAS_LIMIT is small enough validators can naively re-execute traces to enforce correctness of EXECUTE calls. An initial deployment of the precompile based on re-execution could serve as a stepping stone, similar to what proto-danksharding with naive re-downloading of blobs is to full danksharding. Notice that naive re-execution imposes no state growth or bandwidth overhead for validators, and any execution overhead is parallelisable across CPU cores.\nValidators must hold an explicit copy of trace for re-execution, preventing the use of pointers to blob data which are sampled (not downloaded) via DAS. Notice that optimistic native rollups may still post rollup data in blobs, falling back to calldata only in the fraud proof game. Notice also that optimistic native rollups can have a gas limit that far surpasses EXECUTE_CUMULATIVE_GAS_LIMIT because the EXECUTE precompile only needs to be called once on a small EVM segment to settle a fraud proof challenge.\nAs a historical note, in 2017 Vitalik suggested a similar “EVM inside EVM” precompile called EXECTX.\nenforcement by SNARKs\nTo unlock a large EXECUTE_CUMULATIVE_GAS_LIMIT it is natural to have validators optionally verify SNARK proofs. From now on we assume one-slot delayed execution where invalid blocks (or invalid transactions) are treated as no-ops. (For more information about delayed execution see this ethresearch post, this EIP, and this design by Francesco.) One-slot delayed execution yield several seconds—a whole slot—for proving. They also avoid incentivising MEV-driven proof racing which would introduce a centralisation vector.\nNote that even when EXECUTE is enforced by SNARKs no explicit proof system or circuit is enshrined in consensus. (Notice the EXECUTE precompile does not take any explicitly proof as input.) Instead, each staking operator is free to choose their favourite zkEL verifier client(s) similar to how EL clients are subjectively chosen today. The benefits of this design decision are explained in the next section titled “offchain proofs”.\nFrom now on we assume that execution proposers are sophisticated in the context of Attester-Proposer Separation (APS) with alternating execution and consensus slots. To incentivise rational execution proposers to generate proofs in a timely fashion (within 1 slot) we mandate attesters only attest to execution block n+1 if proofs for execution block n are available. (We suggest bundling block n+1 with EXECUTE proofs for block n at the p2p layer.) An execution proposer that skips proving may miss their slot, leading to missed fees and MEV. We further apply a fixed penalty for missed execution slots, setting it high enough (e.g. 1 ETH) to always surpass the cost of proving.\nNote that in the context of APS the production of consensus blocks is not blocked by missed execution slots. Timely generation of proofs is however relevant for light clients to easily read state at the chain tip, without stateless re-execution. To ensure proofs are generated in a timely fashion for light clients, even in the exceptional case where the next execution proposer misses their slot, we rely on an altruistic-minority prover assumption. A single altruistic prover is sufficient to generate proofs within 1 slot. To avoid unnecessary redundant proving, most altruistic provers can wait on standby and kick in only when no proof arrives within 1 slot, thereby acting as a fail safe with at most 2-slot latency.\nNote that EXECUTE_CUMULATIVE_GAS_LIMIT needs to be set low enough for the altruistic-minority prover assumption to be credible (as well as for execution proposing to not be unrealistically sophisticated). A conservative policy could be to set EXECUTE_CUMULATIVE_GAS_LIMIT so that single-slot proving is accessible to laptops, e.g. high-end MacBook Pros. A more pragmatic and aggressive policy could be to target a small cluster of GPUs, and maybe eventually SNARK ASIC provers once those are sufficiently commoditised.\noffchain proofs\nTo reiterate, we suggest that zkEL EXECUTE proofs not go onchain and instead be shared offchain. Not enshrining proofs is a beautiful idea first suggested by Vitalik which comes with several advantages:\ndiversity: Validators are free to choose proof verifiers (including proof systems and circuits) from dev teams they trust, similar to how validators choose EL clients they trust. This provides robustness through diversity. zkEL verifier clients (and the zkVMs that would underlie some) are complex pieces of cryptographic software. A bug in any one client should not take down Ethereum.\nneutrality: Having a market of zkEL verifier clients allows for the consensus layer to not pick technology winners. For example, the zkVM market is highly competitive and picking a winning vendor such as Risc0, Succinct, or the many other vendors may not be perceived as neutral.\nsimplicity: The consensus layer need not enshrine a specific SNARK verifier, dramatically simplifying the specification of the consensus layer. It is sufficient to enshrine a format for state access proofs, not specific proof verifier implementation details.\nflexibility: Should a bug or optimisation be found, affected validators can update their client without the need for a hard fork.\nHaving offchain proofs does introduce a couple manageable complications:\nprover load and p2p fragmentation: Because there isn’t a single canonical proof, multiple proofs (at least one per zkEL client) needs to be generated. Every zkEL client customisation (e.g. swapping one RISC-V zkVM for another) requires a different proof. Likewise, every zkEL version bump requires a different proof. This will lead to increased proving load. It will additionally fragment the p2p network if there’s a separate gossip channel per proof type.\nminority zkELs: It’s hard to incentivise proof generation for minority zkELs. Rational execution proposers may only generate sufficient proofs so as to reach a super-majority of attesters and not miss their slot. To combat this, staking operators could be socially encouraged to run multiple zkEL clients in parallel similar to Vouch operators today. Running a k-of-n setup has the additional benefit of boosting security, in particular hedging against soundness bugs that allow an attacker to craft proofs for arbitrary EXECUTE calls (a situation that would be unusual for a traditional EL client).\nOffchain proofs also introduces inefficiencies for real-time settled L2s:\nno alt DA: Because the trace input to EXECUTE needs to have been made available to L1 validators, real-time settled L2 (i.e. L2s that immediately update their canonical state root) must consume L1 DA, i.e. be rollups. Notice that optimistic L2s that delay settlement via a fraud proof game do not have this limitation, i.e. can be validiums.\nstate access overhead: Because the trace must be statelessly executable it must include state trie leaves that are read or written, introducing a small DA overhead over a typical L2 block. Notice that optimistic L2s do not have this limitation because state trie leaves are only required in fraud proof challenges and the challenger can recompute trie leaves.\nno state diffing: Because proving should be permissionless given the trace, rollup state diffing is not possible. It would however be possible to compress stateless access proofs or EVM transaction signatures if corresponding specialised proofs were enshrined in consensus.\nRISC-V native execution\nGiven today’s de facto convergence towards RISC-V zkVMs there may be an opportunity to expose RISC-V state transitions natively to the EVM (similarly to WASM in the context of Arbitrum Stylus) and remain SNARK-friendly.\nPart 2—native rollups\nnaming\nWe start by discussing the naming of native rollups to address several sources of confusion:\nalternative names: Native rollups were previously referred to as enshrined rollups, see for example this writeup and this writeup. (The term “canonical rollup” was also briefly used by Polynya.) The term “enshrined” was later abandoned in favour of “native” to signal that existing that EVM-equivalent rollups have the option to upgrade to become native. The name “native” was independently suggested in November 2022 by Dan Robinson and a Lido contributor which wishes to remain anonymous.\nbased rollups: Based rollups and native rollups are orthogonal concepts: “based” relates to L1 sequencing whereas “native” relates to L1 execution. A rollup that is simultaneously based and native is whimsically called an “ultra sound rollup”.\nexecution shards: Execution shards (i.e. enshrined copies of L1 EVM chains) is a different but related concept related to native rollups, predating native rollups by several years. (Execution sharding was previously “phase 2” of the Ethereum 2.0 roadmap.) Unlike native rollups, execution shards are non-programmable, i.e. without the option for custom governance, custom sequencing, custom gas token, etc. Execution shards are also typically instantiated in a fixed quantity (e.g. 64 or 1,024 shards). Unfortunately Martin Köppelmann used the term “native L2” in his 2024 Devcon talk about execution shards.\nbenefits\nNative rollups have several benefits which we detail below:\nsimplicity: Most of the sophistication of a native rollup VM can be encapsulated by the precompile. Today’s EVM-equivalent optimistic and zk-rollups have thousands of lines of code for their fraud proof game or SNARK verifier that could collapse to a single line of code. Native rollups also don’t require ancillary infrastructure like proving networks, watchtowers, and security councils.\nsecurity: Building a bug-free EVM fraud proof game or SNARK verifier is a remarkably difficult engineering task that likely requires deep formal verification. Every optimistic and zk EVM rollup most likely has critical vulnerabilities today in their EVM state transition function. To defend against vulnerabilities, centralised sequencing is often used as a crutch to gate the production of adversarially-crafted blocks. The native execution precompile allows for the safe deployment of permissionless sequencing. Trustless rollups that fully inherit L1 security additionally fully inherit L1 asset fungibility.\nEVM equivalence: Today the only way for a rollup to remain in sync with L1 EVM rules is to have governance (typically a security council and/or a governance token) to mirror L1 EVM upgrades. (EVM updates still happen regularly via hard forks roughly once per year.) Not only is governance an attack vector, it is strictly-speaking a departure from the L1 EVM and prevents any rollup from achieving true long-term EVM equivalence. Native rollups on the other hand can upgrade in unison with the L1, governance-free.\nSNARK gas cost: Verifying SNARKs onchain is expensive. As a result many zk-rollups settle infrequently to minimise costs. Since SNARKs are not verified onchain the EXECUTE precompile could be used as a way to lower the cost of verification. If EXECUTE proofs across multiple calls in a block are batched using SNARK recursion EXECUTE_GAS_COST could be set relatively low.\nsynchronous composability: Today synchronous composability with the L1 requires same-slot real-time proving. For zk rollups achieving ultra-low-latency proving, e.g. on the order of 100ms, is an especially challenging engineering task. With one-slot delayed state root the proving latency underlying the native execution precompile can be relaxed to one full slot.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "stateless"
        ]
    },
    {
        "title": "Proposal: Delay stateRoot Reference to Increase Throughput and Reduce Latency",
        "link": "https://ethresear.ch/t/proposal-delay-stateroot-reference-to-increase-throughput-and-reduce-latency/20490",
        "article": "By: Charlie Noyes, Max Resnick\nIntroduction\nRight now, each block header includes a stateRoot that represents the state after executing all transactions within that block. This design requires block builders and intermediaries (like MEV-Boost relays) to compute the stateRoot, which is computationally intensive and adds significant latency during block production.\nThis proposal suggests modifying the Ethereum block structure so that the stateRoot in block n references the state at the beginning of the block (i.e., after executing the transactions in block n - 1, rather than the state at the end of the block).\nBy delaying the stateRoot reference by one block, we aim to remove the stateRoot calculation from the critical path of block verification at the chain tip, thereby reducing L1 latency and freeing up capacity to increase L1 throughput.\nTechnical Specification (High-Level)\nWhen validating block n, nodes ensure that the stateRoot matches the state resulting from executing block n-1 (i.e., the pre-state root of block n).\nTo be clear, there is no change to exeuction ordering. Transactions in block n are still applied to the state resulting from block n-1.\nMotivation\nstateRoot calculation and verification is unnecessary work on the critical path of block production. A builder cannot propose a block on MEV boost without first calculating the stateRoot and the attestation committee cannot verify a block without computing the stateRoot to compare with the proposed stateRoot. stateRoot calculation itself accounts for approximately half of time spent by all consensus participants working at the tip. Moreover, whatever latency implications the stateRoot calculation imposes are paid twice on the critical path: once at the block building stage and then again during verification.\n\n\nWhen block builders submit blocks to relays, they are required to provide the calculated stateRoot. From surveying three of the four largest builders, each spends on average only 40%-50% of their time actually building each block, and the rest on stateRoot calculation.\n\n\nWhen block builders submit blocks to relays, they are required to provide the calculated stateRoot. From surveying three of the four largest builders, each spends on average only 40%-50% of their time actually building each block, and the rest on stateRoot calculation.\n\n\nWhen MEV-Boost relays recieve blocks from builders, they are supposed to verify their correctness. In Flashbots’ relay, also approximately half of the ~100ms (p90) verification time is spent on the stateRoot calculation.\n\n\nWhen MEV-Boost relays recieve blocks from builders, they are supposed to verify their correctness. In Flashbots’ relay, also approximately half of the ~100ms (p90) verification time is spent on the stateRoot calculation.\n\n\nWhen validators receive a new block, or when non-MEV-Boost validators (“home builders”) produce a block, they are also required to re-verify its execution and its stateRoot. Commodity hardware Reth nodes spend approximately 70% of its time in live-sync on the stateRoot (remainder on execution).\n\n\nWhen validators receive a new block, or when non-MEV-Boost validators (“home builders”) produce a block, they are also required to re-verify its execution and its stateRoot. Commodity hardware Reth nodes spend approximately 70% of its time in live-sync on the stateRoot (remainder on execution).\nThese participants - builders, relays, and validators - are highly latency sensitive. They operate under tight timing constraints around slot boundaries (particularly with the incresaing prevalence of timing games).\nThe latency introduced by stateRoot verification at the tip is unnecessary and removing it could allow us to improve the health of the block production pipeline, and network stability.\nBenefits of Delaying the stateRoot\nHigher L1 throughput, because the time currently spent verifying the stateRoot can be re-allocated to execution. stateRoot verification would be pipelined to occur in parallel with the next slot (i.e. during time that nodes are currently idle). Bandwidth requirement increases and state growth would also need to be acceptable before activating a throughput increase.\nTime saved by pipelining the stateRoot could also be allocated towards lowering slot times - improving L1 Ethereum UX, and likely resulting in tighter spreads for users of decentralized exchanges.\nBuilders and relays avoid an unnecessary latency speedbump. Both are highly latency-sensitive actors. We want to minimize the sophistication it takes to be a relay or validator. Removing stateRoot latency from the critical path of block verification means they will no longer have to worry about optimizing it, improving the health and efficiency of the block production pipeline.\nRequest for Feedback\nWe invite the community to provide feedback on this proposal, particularly:\nFeasibility: Are there technical challenges that might impede the implementation of this change?\nUpside: How much throughput will we be able to eke out from pipelining stateRoot calculation, and reallocating the time to exeuction?\nAffected Applications: We don’t obviously see a class of widely used applications which would be affected. We hope any developers whose applications do depend on same-block stateRoot will let us know.\nNext Steps\nWe plan to formalize this proposal into an EIP for potential inclusion in Pectra B.\nAcknowledgements\nThanks to Dan Robinson, Frankie, Robert Miller, and Roman Krasiuk for feedback and input on this proposal.\n",
        "category": [],
        "discourse": []
    },
    {
        "title": "Empirical Analysis of Cross Domain CEX <> DEX Arbitrage on Ethereum",
        "link": "https://ethresear.ch/t/empirical-analysis-of-cross-domain-cex-dex-arbitrage-on-ethereum/17620",
        "article": "Colin, in collaboration with Thomas, Julian and Barnabé as a RIG Open Problems (ROP) project - December 6th, 2023\n\nIntroduction\nFollowing the Merge on September 15, 2022, 91.8% of blocks (mevboost.pics, n.d) on Ethereum are built via MEV-Boost under the Proposer-Builder Separation (PBS) design. This aimed to minimise the computing power for validators and reduce the centralizing effects of MEV extraction (e.g. exclusive orderflows) by splitting the block construction role from the block proposal role (Barnabe, 2022). Today, sophisticated entities known as searchers look for MEV opportunities, bundle multiple profitable transactions, and send them to builders. Block builders are then in charge of packing blocks efficiently and participate in MEV Boost auctions by bidding for their blocks to be chosen by blocks proposer via relays. Relays are trusted parties which validate these blocks along with their bids, and make them available for proposers to choose from before proposing the block to the rest of the network.\nTo date, research on MEV has been largely confined within the on-chain space - liquidations, front-running and sandwich attacks (Qin et al, 2021). However, it is important to recognise that large amounts of price discrepancies also exist when compared with the off-chain environment on centralised exchanges (CEX). In fact, cross-domain arbitrages remain a relatively nascent space with limited research (Gupta et al, 2023). Obadia et al (2021) formalized cross-domain MEV through the ordering of transactions in two or more domains; Chiplunkar and Gosselin, (2023) highlighted the phenomenon where certain block builders dominate the market during periods of volatility; Milionis et al (2023) provided a theoretic analysis of the impact of certain cross-domain arbitrages on liquidity provider’s profits and formalized a model, known as “loss versus rebalancing” in the presence of fees; Thiery (2023) had also provided an empirical analysis into the behavioral profiles of block builders to elucidate unique features and strategies in this process. Given the opaqueness of the CEX part in this trade, the exploration of this field is still in its infant stages. Yet, these opportunities have grown in dominance with the rising adoption and maturity of the markets.\nIn this post, we conduct an empirical analysis of CEX <> DEX arbitrages by studying on-chain daa to infer the relationships between builders and searchers, estimate MEV profits and reverse engineer the strategies used by CEX <> DEX arbitrageurs.\n\nInsights on CEX <> DEX Arbitrages\nThe following heuristics was applied to identify potential successful CEX <> DEX arbitrages based on the on-chain transactions from the AMM trades. These either contained a single swap followed by a direct builder payment (coinbase transfer) or two consecutive transactions where the first is a single swap while the second is the coinbase transfer. The time period for this data collection started from May 5, 2023 and ended on July 16, 2023, returning a total of 157, 205 CEX <> DEX arbitrages amongst 101, 022 blocks.\nOrder of MEV Opportunity\nWe note that nearly all of these arbitrages are top of the block opportunities, suggesting that these searchers vie to be at the front. This is supported by Gupta et al’s (2023) observation that these arbitrages “required priority access” to exploit the price divergence.\n1462×550 83.6 KB\nFigure 1. Index of CEX <> DEX arbitrages within the block. A. consists of transactions with both a swap and coinbase transfer. B. represents the arbitrage where there are 2 separate transactions - 1 swap (DEX index) and 1 coinbase transfer (Builder index). The y axis indicates the number of arbitrages while the x-axis is the index of the transaction within the block.\nSymbols Traded and Venues\nNext, we calculated the average number of symbols traded to understand the general preference amongst arbitrageurs. In general, WETH topped the list by appearing in 45.0% of transactions, while USDC and USDT were in 11.5% and 5.3% of the time.\nAs for the pools, we note that Uniswap v3 was the venue which had the most CEX <> DEX arbitrages (74.65%).\n1600×517 111 KB\nFigure 2. A. Types of token symbols traded. B. Venue where CEX <> DEX arbitrages occur\nSearcher - Builder Landscape\nTo shed light on the distribution of the searchers and builders involved in these arbitrage opportunities, our findings indicated a relatively concentrated market where 1 to 2 entities dominated the CEX <> DEX landscape. Searcher 0xa69 has consistently represented 55.7% of market share while 0x98 had 20.23% of these arbitrages. In the meanwhile, beaverbuild continued to lead in this space with 41.77% of all related blocks and 52.91% of these CEX <> DEX arbitrages.\n1600×879 226 KB\nFigure 3. Distribution of CEX DEX Arbitrage amongst searchers and block builders, builder payments. A. Total Transaction Count per Searcher. B. Daily Distribution of arbitrages made by the Top 10 searchers, with the remaining labelled as ‘Others’. C, D: Similar to A, B but distribution for block builders.\nBuilder Payments\n1600×395 82.3 KB\nFigure 4. Amount of ETH related to builder payments. A. Amount of ETH given to block builders by searchers. B. Amount of ETH earned by block builders from searchers.\nTypes of Transactions Made\nWe then classified the transactions based on the type of asset pairs traded. These conditions were used in the classification process, referenced from Coingecko.\n\nMarket capitalizations. We note that BTC and ETH are leading cryptocurrencies with significantly higher market capitalizations relative to the other digital currencies and thus, classified them as the majors.\n\n\nNature of asset. This was based on the inherent stability / volatility of the asset since these influence the potential price movements during the trading window. As such, we further segmented the remaining assets into stablecoins and memecoins (based on Coingecko’s definitions).\n\nMarket capitalizations. We note that BTC and ETH are leading cryptocurrencies with significantly higher market capitalizations relative to the other digital currencies and thus, classified them as the majors.\nNature of asset. This was based on the inherent stability / volatility of the asset since these influence the potential price movements during the trading window. As such, we further segmented the remaining assets into stablecoins and memecoins (based on Coingecko’s definitions).\nTherefore, we derived these categories for the assets - majors (BTC/ETH), Stablecoins (USDC, USDT, BUSD, TUSD, DAI), Memecoins (PEPE, DOGE, SHIB, FLOKI, ELON) and Altcoins (All remaining types of cryptocurrencies). Table 2 highlights the distribution of trades for each category with ‘major-alt’ type representing 43.87% and meme-alts as the least popular token pair.\nAfter which, we determined the average revenue from the arbitrage by collecting price data from Binance at the 1s interval. An example to calculate the revenue of a CEX <> DEX arbitrage can be seen below:\n\nStep 1: In this identified CEX <> DEX arbitrage (0xc4322), the arbitrageur swapped 175,070 USDC for 92.70 ETH.\n\n\nStep 2: At the time of trade, it can be interpreted that the DEX exchange rate was at 1,888.57 USDC/ETH. On Binance, the approximated rate was at 1,896.68 USDC/ETH\n\n\nStep 3: Revenue = Difference between Binance price and dex price * Tokens transacted. Since the arbitrageur sold USDC on-chain, it will purchase the same amount of using its ETH on Binance, to form a delta neutral position. Thus, receiving 92.70 * 1,896.68 = 175,822.24 USDC on Binance. The revenue will be 175,822.24 - 175,070 = 752.24 USDC.\n\nStep 1: In this identified CEX <> DEX arbitrage (0xc4322), the arbitrageur swapped 175,070 USDC for 92.70 ETH.\nStep 2: At the time of trade, it can be interpreted that the DEX exchange rate was at 1,888.57 USDC/ETH. On Binance, the approximated rate was at 1,896.68 USDC/ETH\nStep 3: Revenue = Difference between Binance price and dex price * Tokens transacted. Since the arbitrageur sold USDC on-chain, it will purchase the same amount of using its ETH on Binance, to form a delta neutral position. Thus, receiving 92.70 * 1,896.68 = 175,822.24 USDC on Binance. The revenue will be 175,822.24 - 175,070 = 752.24 USDC.\n1600×777 63.5 KB\nFigure 5. Illustration of the convergence of prices on Binance and on Uniswap across the sampled 25s trading window. Price on DEX remains the same between T - 11 and T which is equivalent to block n - 1 to block n\nTable 1. Number of CEX DEX Arbitrages, average absolute and relative profit levels, segmented by type of asset pairs traded\nMeme-alt trading strategies yielded the greatest revenue given that both are relatively volatile assets and thus, reaped the greatest rewards. Conversely, Stable-stable coin pairs had the lowest rewards given the inherent stability compared to the data set.\nMinimising Risks\nWe then computed the distribution in revenue over the window, before and after block time. Given that blocks are created in 12s intervals, this means that the searcher will be potentially vulnerable to risks from changes in market prices. Therefore, we aimed to highlight the distribution and relative comparative advantage by computing the marginal change in revenue earned per second, over the window.\n1600×1003 87 KB\nFigure 6. Marginal difference in revenue before and after block time, calculated by taking the difference in average revenue per second.\nIn general, the average revenue for the strategies continues to increase just before block time (at t = 0s) before tapering off This can be seen that latency is important in maximising the revenue extracted nearer to the actual block confirmation. The arbitrage opportunity closes out thereafter as the price on-chain gets updated and the differential with the off-chain price (on Binance) narrows. As a result, the average difference in prices decreased and thus, revenue flattened out which remained relatively constant.\nWe then determined the market risks borne by these arbitrageurs over the period by referencing the revenues at each juncture. This is because they will be holding onto inventory on either CEX or DEX depending which leg gets executed first. It aims to provide insights on the uncertainty of their revenues in this arbitrage by optimising for latency and executing their trades. We visualized the spread of the profitability by taking the 25th, 50th, and 75th percentiles for each asset pair.With the exception of meme-alt pairs (due to the small sample size), the findings indicated that -2s to +2s intervals will be generally preferred to minimise the uncertainties involved in trading. In fact, we noted that stablecoin pairs exhibited the least deviation while meme-stables showed the greatest change in expected rates of return. This is largely aligned with the intuition that volatile assets will show a greater difference.\n1600×924 182 KB\nFigure 7. Market Risk that arbitrageurs bear from fluctuations in prices throughout block time. This is measured by taking the percentage difference between the profitability of the transaction at time t, and comparing it to the profits at block time. The average for these differences were derived then derived and plotted. A sample of the boxplot was taken, which represents the distribution in revenue over the trading window for major-stable asset pairs.\n\nCost and Revenue Analysis\nTo further analyze the profitability of these strategies, we segregated the dataset into arbitrageurs which interacted with Flashbots builder against those which did not interact with it. This is because Flashbots publicly stated that they are not for profit builders and will not take part in strategic or integrated searcher-builder behaviors. In addition, based on searcherbuilder.pics, we extracted the searcher-builder entities which consist of:\n\nSymbolic Capital Partners <> beaverbuild\n\n\nWintermute <> rsync builder\n\nSymbolic Capital Partners <> beaverbuild\nWintermute <> rsync builder\nThe addresses of these searchers and block builders are based on the raw data processed by searcherbuilder.pics team. The list may not be exhaustive.\nThese entities are likely to shown forms of vertical integration across the MEV Supply Chain, where the searcher enjoys preferential access to blockspace and increased certainty of their transaction by being associated with a builder downstream.\nIn all, there were 46.24% of CEX <> DEX arbitrages by searcher-builder entities, 7.77% by searchers which interacted with Flashbots and 46.00% which did not interact with Flashbots.\n1600×559 99.4 KB\nTable 2. Descriptive Statistics on costs for arbitrageurs, split into those which interacted with a Flashbots builder vs a Non-Flashbots builder vs the searcher builder entities. Builder Payments (ETH) represents the amount of ETH the arbitrageur sends the block builder for each segment. Cost as Percentage of Transaction Amount = Total Cost / Transaction Amount. * Revenue (%) measures the revenue earned by arbitrageurs from the CEX DEX Arbitrage.\nOn average, searchers which interacted with non-Flashbots block builders paid lower amounts of builder payments and appear to have a higher level of revenue compared to the others which interacted with Flashbots builders and for searcher builder entities. This could be explained by the relatively higher proportion of CEX-DEX arbitrages where over 46% of these arbitrages are made by the SCP <> beaverbuild entity and they represent nearly 100% of all arbitrages by the searcher-builders identified above. Furthermore, given that this is only over a period of slightly over 2 months, there are possible limitations to the dataset with certain skews, contrary to the general perception that searcher-builder entities enjoy a significant advantage. Nonetheless, this can be offsetted by the relatively large number of arbitrages the searcher builder entities contribute and hence, cumulative profits will likely be the highest.\n\nEmpirical vs Theoretical Arbitrage\nBased on the empirical revenue calculated from the price difference between Binance and DEXs, we can determine if these searchers were rational by comparing with the theoretical revenue that can be yielded based on the AMM formula. Anthony et al (2022) introduced the arbitrageur’s optimization problem based on the pool reserves, where a rational profit-maximising user will be able to earn:\n732×104 3.49 KB\nwhere L is the invariant, P is the price of the pair on the CEX, x and y are the reserves in the pool.\nFigure 8. Formula to determine the theoretical profits from the Uniswap V2 AMM Model (Adapted from Anthony et al (2022) - Automated Market Making and Loss-Versus-Rebalancing). With courtesy of Julian.\nTo obtain the relevant data, we extracted the reserves at the time of trade from Dune Analytics based on the Uniswap sync function when the transaction occurs. As an initial guide, we have started with Uniswap V2’s AMM model. This returned a total of 20,123 transactions. The number of transactions per type of asset pair can be found below:\nTable 3. Number of CEX DEX Arbitrages on Uniswap V2, segmented by type of asset pairs traded.\nIn general, the formula held true, presenting the upper bound of revenue that can be potentially earned. As seen in Figure 8, we extracted the relevant transactions with ‘ETH’ and ‘USDC’ to plot the difference between the theoretical and empirical profits.\n1600×833 154 KB\nFigure 9. Scatterplot of the theoretical profit (orange) vs empirical profit (blue) for all ETH-USDC and USDC-ETH transactions. The x axis simply represents the row number within the dataframe for plotting the data.\nThe numbers represent the difference between the theoretical revenue and empirical revenue earned by the arbitrageurs. In particular, based on the different types of asset pairs, we note that the major-meme pairs had the largest variation and difference across the percentiles. It is important to note that the theoretical upper bound of the profits did not hold based on the reserves pool for some of the asset pairs, as these could be due to risky / directional trading. In contrast, major-stable pairs such as ETH USDC and stable-stable pairs largely conformed to the model. This confirms the intuitive understanding the the volatility of the asset pair are more likely to influence the behavior of searchers in arbitraging the pool - exercise greater caution in the amount being swapped to manage the risks from large swings in prices.\n1600×562 73.7 KB\nFigure 10. Boxplot distribution of the difference between theoretical and empirical revenues for the different types of asset pair. Difference = Theoretical Revenue (based on the formula) - Empirical Revenue. A. Distribution for all pair types. B. Distribution for all pair types except for meme-stable pairs.\nWe then grouped the trades into different buckets based on their order sizes to determine the differences between theoretical and empirical profits once again.\n1598×566 48.7 KB\nFigure 11. Boxplot distribution of the difference between theoretical and empirical revenues for the different order sizes. Difference = Theoretical Revenue (based on the formula) - Empirical Revenue. A. Distribution for all order sizes. B. Distribution for all order sizes except for order size >$1M.\nInterestingly, the larger the transaction, the less likely the model held true. However, this could be due to the larger percentage of CEX <> DEX arbitrages that had alts and memes within the pair, which deviated from the model.\nMoving forward, the theoretical model can be improved by adding fees to the calculations which has been recently revisited by Milionis et al (2023).\n\nConclusion\nIn this post, we investigated the prevalence of CEX <> DEX arbitrages and shed light on the patterns and insights into these opportunities. By examining the interactions between searchers and builders, estimating the costs and potential revenues, and contrasting it with the theoretical profits using the reserves in the pool, we’ve delved deeper into the dynamics of this market.\nMoving forward, we hope that the community can further contribute to this study by exploring other factors such as bidding data and markout analysis over a longer period of time to provide a more comprehensive picture and a robust understanding of the value flow between the Ethereum blockchain and centralized exchanges.\n",
        "category": [
            "Economics"
        ],
        "discourse": []
    },
    {
        "title": "State tree preimages file generation",
        "link": "https://ethresear.ch/t/state-tree-preimages-file-generation/21651",
        "article": "Thanks to Guillaume Ballet and Tanishq Jasoria for their feedback and to Stateless Implementors Call participants for previous discussions on this topic.\nIn this article, we dig into the topic of state tree preimages generation topic, including:\nExplain enough context to understand where this topic fits the protocol evolution.\nA high-level explanation for EIP-7612 and EIP-7748 is required to understand the problem properly.\nPropose a potential file format for the preimages and analyze whether more complex formats might be worth it.\nProvide a new preimages tool using a non-archive reth to generate and verify preimage files and generate the data used in this article so it can be reproduced.\nIf you know why generating preimages for a future tree conversion is required, skip the Context section.\nThe Verge part of the roadmap proposes replacing the current Merkle Patricia Trie (MPT) with a new tree, which is more efficient in achieving statelessness and further SNARKifying L1.\nToday’s main tree candidates are Verkle Trees (EIP-6800) and Binary Tree (EIP-7864). If you want more about this dichotomy, read the EIP-7864 discussion thread. The topic of this article is independent of this decision, so you can assume any future path.\nSince a new tree is used, we need a strategy to move the existing data from the MPT to the new tree. Today’s proposed strategy is converting the data with an Overlay Tree (EIP-7612 + EIP-7748), which we’ll explore later.\nSay we want to move an account’s data (nonce, balance, code hash) from the MPT to the new tree. The tree key in the MPT is keccack(address), but in the new tree, it is new_key(address), where new_key isn’t necessarily keccak.  EL clients must have access to the underlying preimage (i.e., address in this case) for each key in the tree. If they only have keccack(address) it becomes impossible to calculate new_key(address). The same applies to accounts state tries.\nIn short, every node in the network that will undergo the state conversion from the MPT to the new target tree should be able to resolve every MPT key preimage for the accounts and storage tries.\nAren’t EL clients storing tree key preimages?\nUnfortunately, no. Depending on the current architecture and database design of EL clients, these preimages might be computed from existing data, but this only applies to a minority of clients.\nSome examples of the relationship between tree key preimages and client databases:\nGeth has a flatdb to access the state faster than using the trees. The keys in this database are the hashed values of the accounts or the concatenation of accounts hash and storage slots hash—i.e., hash-based keys. Using hashed versions of keys makes sense for syncing, and there has never been a real incentive to justify the extra storage of saving the preimages.\nErigon and Reth also have a flatdb but with unhashed tree keys as keys in this database, which is very convenient for this problem.\nNethermind doesn’t have a flatdb yet, so it relies on efficiently accessing the tree for state access, but they’re planning to introduce a (key-hashed) flatdb soon.\nThis means that unless you’re an Erigon or Reth node, you don’t have the actual preimages of tree keys. According to ethernodes.org, non-Erigon+Reth nodes comprise more than 80% of the network, meaning most of the network has this preimage problem. Even for Erigon or Reth nodes, being able to get the preimages doesn’t mean they have an efficient way of resolving them today. To understand this better, let’s dive deeper into why.\nUnderstanding how the overlay tree and state conversion EIPs work together can better answer this question. You can read the EIP, but here’s a compressed summary to continue with the preimages discussion.\nEIP-7612 TL;DR\nNote: this EIP references Verkle, but the idea is agnostic to the target tree, so it also works with a Binary Tree.\nEIP-7612 explains how a new tree is introduced into the protocol:\nThe MPT becomes read-only (i.e., frozen) at the fork block.\nA new empty tree is created (e.g., Verkle or Binary).\nState updates resulting from block execution are only written to the new tree.\nAs a side effect, reading the state only using trees means reading the new tree, and if the entry isn’t found, checking the MPT.\nIn summary, it’s a two-level (i.e., overlay) tree where the base tree (MPT) is frozen, and the top tree (Verkle or Binary) starts fresh, receiving the writes. Regarding the last bullet, EL clients have a flatdb to access the state, so it doesn’t necessarily mean you have to do double-tree-lookups.\nEIP-7748 TL;DR\nNow that we understand EIP-7612, this is when EIP-7748 comes into play:\nAt a defined block timestamp (CONVERSION_START_TIMESTAMP), the conversion process starts. Note that there must be enough time between EIP-7612 activation and CONVERSION_START_TIMESTAMP so we can guarantee the MPT is frozen (i.e., has chain finalization so no reorgs can change the fact)\nIn the next block, before the transactions are executed:\n\nDeterministically take CONVERSION_STRIDE tree entries from the MPT.\n\nThe concept of tree entries is more precisely defined in the EIP (i.e., named conversion unit) — we’re simplifying a bit here.\nThe CONVERSION_STRIDE value is still TBD since it’s a tradeoff between the block execution state conversion overhead and the full conversion duration. The proper value depends on which is the target tree since Verkle has a more costly re-hashing than Binary.\n\n\nWe copy it in the top tree if it’s not a stale value—since EIP-7612 was activated, block execution could have already written a new value for this MPT key, which must not be overridden.\n\n\nDeterministically take CONVERSION_STRIDE tree entries from the MPT.\n\nThe concept of tree entries is more precisely defined in the EIP (i.e., named conversion unit) — we’re simplifying a bit here.\nThe CONVERSION_STRIDE value is still TBD since it’s a tradeoff between the block execution state conversion overhead and the full conversion duration. The proper value depends on which is the target tree since Verkle has a more costly re-hashing than Binary.\n\n\nThe concept of tree entries is more precisely defined in the EIP (i.e., named conversion unit) — we’re simplifying a bit here.\nThe CONVERSION_STRIDE value is still TBD since it’s a tradeoff between the block execution state conversion overhead and the full conversion duration. The proper value depends on which is the target tree since Verkle has a more costly re-hashing than Binary.\nWe copy it in the top tree if it’s not a stale value—since EIP-7612 was activated, block execution could have already written a new value for this MPT key, which must not be overridden.\nContinue doing the above until we finish walking all MPT keys.\nThe design decision that the state conversion step happens before the transactions are executed is beneficial since EL clients can do the state conversion step for the next block before it arrives.\nThe primary fact about this process relevant to the preimage file is the order in which we iterate on MPT entries. The currently proposed ordering is to do a full depth-first walk (DFW) in the MPT(s): do a DFW in the accounts trie, and when reaching a leaf, do a DFW in the accounts storage tries before continuing the walk in the accounts trie. We first migrate the account storage slots and then the account’s data.\nLet’s look at an example to understand it better. Let’s assume CONVERSION_STRIDE is 5, and we’re in the first block of the state conversion. Here are what MPT entries we might see in the walk:\nAccounts trie key 0x000000abc... equals keccak(address_A). We note that address_A has a storage trie with two storage slots, so we DFW into it.\n\nStep #1: Account address_A storage trie with key 0x0000131... equals keccack(storage_slot_A), meaning we migrate storage_slot_A value for address_A to the new tree.\nStep #2: Account address_A storage trie with key 0x0003012... equals keccack(storage_slot_B), meaning we migrate storage_slot_B value for address_A to the new tree.\nStep #3: Since we migrated all storage slots, we now migrate address_A account nonce, balance, and code.\n\n\nStep #1: Account address_A storage trie with key 0x0000131... equals keccack(storage_slot_A), meaning we migrate storage_slot_A value for address_A to the new tree.\nStep #2: Account address_A storage trie with key 0x0003012... equals keccack(storage_slot_B), meaning we migrate storage_slot_B value for address_A to the new tree.\nStep #3: Since we migrated all storage slots, we now migrate address_A account nonce, balance, and code.\nAccounts trie key 0x000000ffa... equals keccak(address_B). address_B is an EOA\n\nStep #4: there are no storage slots, so no DFW is done in the storage trie. We migrate address_B account nonce, balance, and code.\n\n\nStep #4: there are no storage slots, so no DFW is done in the storage trie. We migrate address_B account nonce, balance, and code.\nAccounts trie key 0x000005630... equals keccak(address_C). We note that address_C has a storage trie with 1000 storage slots, so we  DFW it.\n\nStep #5: Account address_C storage trie with key 0x0000021... equals keccack(storage_slot_A), meaning we migrate storage_slot_A value for address_C to the new tree.\n\n\nStep #5: Account address_C storage trie with key 0x0000021... equals keccack(storage_slot_A), meaning we migrate storage_slot_A value for address_C to the new tree.\nNote the following:\nThe walking order in the account trie and potential tries is hash-based, not address or plain storage slot value. This is a consequence of DFW in the trie since the keys are hashes of addresses or storage slots.\nstorage_slot_A from address_A and address_C is a different number (e.g., 10 and 24, respectively). They mean the first storage slot found while walking DFW in their storage tries. As mentioned in the previous bullet, the storage slot walking order is hash-based and not “natural” ordering, so storage_slot_A can be greater than storage_slot_B.\nThe above point is also why we need the preimages—the first key has the value 0x000000abb, which you need to know corresponds to address address_A so you can re-hash to determine which key is in the new tree.\nThe last migrated entry is only the first storage slot of account_C, but we still have 99 remaining slots. We’ll continue migrating those in the next block since we have already reached the CONVERSION_STRIDE limit!\nYou can also find a talk about this EIP here, with some visuals to help complement this explanation.\nRecap\nThe above hopefully makes clear the following facts:\nThe preimages file must contain all existing address_X in the frozen MPT account trie.\nThe preimages file must contain all existing storage_slot_X in all frozen MPT storage tries.\nSince the MPT is frozen and the walk is deterministic, the order in which we need the preimages is fully determined upfront.\nNow we dive into different dimensions of this preimage file:\nDistribution\nVerifiability\nGeneration and encoding\nUsage\nThe main focus of this article is Generation and encoding, but we touch on the other dimensions for completeness. Until we reach the Generation section, please assume this file is magically generated and encoded.\nDistribution\nGiven that the preimage file is somehow generated, how does this file reach all nodes in the network? This topic was explored multiple times during Stateless Implementers Calls (SIC), L1 event workshops, and informal discussions.\nAfter talking with many (but overall small sample) core devs, the current consensus is that it’s probably OK to expect clients to download this file through multiple potential CDNs.This is compared to relying on the Portal network, in-protocol distribution, or including block preimages.\nOther discussed options are:\nHaving an in-protocol distribution mechanism.\nDistributing the required preimages packed on each block.\nThis topic is highly contentious since these options have different tradeoffs regarding complexity, required bandwidth in protocol hotpaths, and compression opportunities. Despite talking with many core developers, they’re still not representative enough to conclude that ACD would reach the same conclusion.\nThis article focuses on the preimage generation and encoding format, which we must do regardless of how this file is distributed.\nVerifiability\nAs mentioned above, full nodes will receive this file from somewhere that can be a potentially untrusted party or a hacked supply chain. If the file is corrupt or invalid, the full node will be blocked at some point in the conversion process.\nThe file is easily verifiable by doing the described tree walk, reading the expected preimage, calculating the keccak hash, and verifying that it matches the client’s expectations. After this file is verified, it can be safely used whenever the conversion starts, with the guarantee that the client can’t be blocked by resolving preimages — having this guarantee is critical for the stability of the network during the conversion period. This verification time must be accounted for in the time delay between EIP-7612 activation and EIP-7748 CONVERSION_START_TIMESTAMP timestamp*.*\nOf course, other ways to verify this file are faster but require more assumptions. For example, since anyone generating the file would get the same output, client teams could generate themselves and hardcode the file’s hash/checksum. When the file is downloaded/imported, the verification can compare the hash of the file with the hardcoded one.\nGeneration and encoding\nNow that we know which information the file must contain and in which order this information will be accessed, we can consider how to encode this data in a file. Ideally, we’d like an encoding that satisfies the following properties:\nOptimizes for the expected reading pattern: the state conversion is a task in the background while the main chain runs, so reading the required information should be efficient.\nOptimize for size: as mentioned before, the file has to be distributed somehow. Bandwidth is a precious resource; using less is better.\nLow complexity: this file will only be used once, so a simple encoding format is good. It doesn’t make sense to reinvent the wheel by creating new complex formats unless they offer exceptional benefits while taking longer to spec out and test.\nThere’s a very simple and obvious candidate encoding that can be described as follows following the example we explored before: [address_A][storage_slot_A][storage_slot_B][address_B][address_C][storage_slot_A].... We directly concatenate the raw preimages next to each other in the expected walking order.\nThis encoding has the following benefits:\nThe encoding format has zero overhead since no prefix bytes are required. Although preimage entries have different sizes (20 bytes for addresses and 32 bytes for storage slots), the EL client can know how many bytes to read next depending on whether they should resolve an address or storage slot in the tree walk.\nThe EL client always does a forward-linear read of the file, so there are no random accesses. The upcoming Usage section will expand on this.\nBefore diving deeper into the efficiency of this encoding, let’s try to generate this file for mainnet and get a sense of the size. To do it, we created a tool that uses a synced reth full node (i.e., not necessarily archive) to generate, verify, and do other analyses that will be presented later. A while ago, we created a geth tool, but it requires a node syncing from genesis with the --cache.preimages flag enabled.\nWe can create the preimage file by running preimages generate --datadir <...> --eip7748. Here are some facts about the generated file in a mid-end machine:\nTime to generate: ~1hr\nFile size uncompressed: 42GiB\nFile size compressed (zstd): 36GiB\nNote that anyone in the network running a Reth (or potentially Erigon) node can generate the file, and the output is always the same since, given a frozen MPT, the preimages are fixed.\nDiving deeper into encoding efficiency\nDespite the encoding format’s zero encoding overhead, the difference between the compressed and uncompressed size signals compression opportunities. Let’s explore why that’s the case.\nWe can put each preimage entry in the file in two buckets:\nAddresses preimages\n\nDeduping\n\nEvery [address_X] entry is unique in the file since the accounts tree contains unique information for each account, so there’s no opportunity for deduplication.\n\n\nCompression:\n\nAddresses are hashes, so there is no opportunity to compress them.\n\n\n\n\nDeduping\n\nEvery [address_X] entry is unique in the file since the accounts tree contains unique information for each account, so there’s no opportunity for deduplication.\n\n\nEvery [address_X] entry is unique in the file since the accounts tree contains unique information for each account, so there’s no opportunity for deduplication.\nCompression:\n\nAddresses are hashes, so there is no opportunity to compress them.\n\n\nAddresses are hashes, so there is no opportunity to compress them.\nStorage slots preimages\n\nDeduping\n\nThey repeat in the file since multiple contracts can (and do) overlap in storage slot usage. e.g., multiple contracts use storage slots 0, 1, and 2.\nThe biggest group of repeated storage slots are top-level slots since they share a 0x00000... prefix (e.g., the ones mentioned in the previous bullet).\nWhen contracts have arrays or hashmaps, the storage slots get more scattered in the storage slots space due to the nature of how hashing maps variables into storage slots.\n\n\nCompression\n\nThe compression opportunities for storage slots vary. For example, storage slots with 0x00000.. prefixes (i.e., the top contract variables) are very compressible since they have many zero-bytes. Other storage slots are mainly the result of hashes; they aren’t that compressible (but “dedupable”).\n\n\n\n\nDeduping\n\nThey repeat in the file since multiple contracts can (and do) overlap in storage slot usage. e.g., multiple contracts use storage slots 0, 1, and 2.\nThe biggest group of repeated storage slots are top-level slots since they share a 0x00000... prefix (e.g., the ones mentioned in the previous bullet).\nWhen contracts have arrays or hashmaps, the storage slots get more scattered in the storage slots space due to the nature of how hashing maps variables into storage slots.\n\n\nThey repeat in the file since multiple contracts can (and do) overlap in storage slot usage. e.g., multiple contracts use storage slots 0, 1, and 2.\nThe biggest group of repeated storage slots are top-level slots since they share a 0x00000... prefix (e.g., the ones mentioned in the previous bullet).\nWhen contracts have arrays or hashmaps, the storage slots get more scattered in the storage slots space due to the nature of how hashing maps variables into storage slots.\nCompression\n\nThe compression opportunities for storage slots vary. For example, storage slots with 0x00000.. prefixes (i.e., the top contract variables) are very compressible since they have many zero-bytes. Other storage slots are mainly the result of hashes; they aren’t that compressible (but “dedupable”).\n\n\nThe compression opportunities for storage slots vary. For example, storage slots with 0x00000.. prefixes (i.e., the top contract variables) are very compressible since they have many zero-bytes. Other storage slots are mainly the result of hashes; they aren’t that compressible (but “dedupable”).\nIn summary, the addresses can’t be deduped or compressed, but storage slots do have an opportunity to be deduped/compressed. However, it’s hard to know how big the impact would be and if it’s worth it.\nTo do a deeper analysis, you can run preimage storage-slot-freq analysis tool. Let’s look at the output first:\n(The full output is longer and can be found here)\nThe program counts the number of storage slots with the same 29-byte prefix, for example, for the prefix 00000....00:\nThere are ~57 million storage slots, which accounts for 4.65% of the total.\nIt maps to ~1580MiB of the preimage file size.\nThe cumm value is the sum of the sizes up to the current row. For example, the top 3 storage slot preimage prefixes account for 2219MiB of the preimage file.\nYou might be wondering what are those values in the second, third, and following rows:\nf3f7... is  keccak(0x000000..08)\n8a35... is keccak(0x000000..04)\nThese prefixes appear because many contracts have defined arrays in storage slots 8 and 4, respectively. This means that the value in the preimage file is the hash of those slot numbers. Since items in arrays are consecutive from this base value, these “top-level arrays” are top prefixes in the preimage file.\nThere’s nothing special about choosing 29 bytes for the prefix; the idea is to capture grouping top-level variables and arrays. For hashmaps, entries are distributed all over the storage slot space, so there are fewer opportunities for deduplication. The chances are even lower as more “nesting” happens in contract variables. That’s why top prefixes are top-level arrays, so it makes sense.\nIf you look at the full output link, the top 1000 prefixes account for ~4GiB, close to the ~6GiB compression we gained via zstd. Of course, the tail is very long, so the optimal size could probably be better.\nTrying to deduplicate as part of the file format would increase the complexity of the file format spec. There could be ways to preserve quasi-linear reads by separating the top N preimages to be kept in RAM. Still, it might not be worth it if the extra complexity compared to the simple format+zstd for a small relative reduction in size. Remember, this is a one-time download. It could be downloaded in non-critical parts of the slot duration or even through separate internet links (but that requires manual work).\nNote that although zstd is a heavily used compression tool, it’s adding a new dependency for EL clients. Also, extra-temporal space might be required to decompress the file (but there might be ways to avoid this). In any case, the main goal of this article is to provide real measurements regarding how the tail of storage-slot sizes behaves and invite more discussion around this topic.\nUsage\nIt is worth mentioning some facts about how EL clients can use this file:\nSince the file is read linearly, persisting a cursor indicating where to continue reading from at the start of the next block is useful.\nKeeping a list of cursor positions for the last X blocks helps handle reorgs. If a reorg occurs, it’s very easy to seek into the file to the corresponding place again.\nClients can also preload the next X blocks preimages in memory while the client is mostly idle in slots, avoiding extra IO in the block hot path execution.\nIf keeping the whole file on disk is too annoying, you can delete old values past the chain finalization point. We doubt this is worth the extra complexity, but it’s an implementation detail up to EL client teams.\nThis article explored the preimages file’s context, problem, and solution space in the context of a tree conversion protocol evolution. None of the ideas presented here are final or are expected to have obvious consensus in ACD.\nThe main goal is to provide a deep enough explanation to level up as many people as possible in the community, keep progressing, and hopefully serve as a resource to speed up future ACD discussions on this topic.\n",
        "category": [
            "Execution Layer Research"
        ],
        "discourse": [
            "stateless"
        ]
    },
    {
        "title": "Three-Tier staking (3TS) - Unbundling Attesters, Includers and Execution Proposers",
        "link": "https://ethresear.ch/t/three-tier-staking-3ts-unbundling-attesters-includers-and-execution-proposers/21648",
        "article": "stretched_image1500×476 1.21 MB\n^Attesters, Execution Proposers and Includers in the wild\nby Thomas Thiery, - January 31st, 2025\nThanks to Julian, Barnabé, Caspar, Anders, Justin and Connor for discussions, ideas, feedback and comments!\nDisclaimer: The goal of this post is to spark conversation about potential ideas, proposals, and mechanisms for the future of staking on Ethereum. It should not be considered an agreed-upon position by the EF or the broader Ethereum research community. While the general trend toward separating core protocol duties is relatively well-documented and supported, the specific mechanisms discussed here reflect my personal views on directions worth exploring.\nAlso, reviews ≠ endorsements. This post expresses opinions of the author, which may not be shared by reviewers.\nOn Ethereum, validators don’t “just validate”. They participate in consensus and cast votes, they build and propose beacon blocks and execution payloads, they participate in sync committees, they aggregate votes… Soon, they may also be asked to build and propagate inclusion lists, assess the amount of MEV that should be burned or the timeliness of an execution payload delivered by builders. Relying on a single tier of participation to perform all protocol duties ends up becoming impractical when asking single entities to meet conflicting demands like high performance and low hardware requirements to preserve decentralization. Today, we constantly face the fact that home stakers, while crucial for upholding the network’s censorship resistance and permissionless properties, also serve as a bottleneck to scaling (e.g., faster finality, increased gas limit, more blobs).\nIn recent years, a trend has emerged to unbundle roles and duties. Proposals like Proposer-Builder Separation (PBS, also see EIP-7732), Attester-Proposer Separation (APS), Attester-Includer Separation all reflect efforts in this direction. This post, inspired by Rainbow Staking and other posts on two-tier staking, aims to propose a holistic approach to safely match core protocol roles with core protocol duties by defining the desired properties for each tier of staker and proposing potential mechanisms for selection and reward.\nAll participants belong to a single, unified pool of stakers. Each slot, three different roles are chosen from this pool:\nAttesters\nIncluders\nExecution Proposers\nThese three roles are non-exhaustive: protocol participants can also be selected to be part of the sync committee, to aggregate attestations, or to be the beacon proposer (which was left out here because it’s still unclear to me where it best fits in this new unbundled world…). But in this post we choose to focus on attesters, includers and execution proposers given the potential impact of separating their core duties.\nEach staker must explicitly opt into the role(s) in which they wish to participate. They may choose to take on one, two, or all three roles, as long as they meet the associated capital requirements. Each protocol participant can mix and match these roles (e.g., serving as both an attester and an includer) and decide how to allocate their capital and efforts based on their preferences (such as contributing to the network’s censorship resistance), constraints (such as hardware requirements), and level of sophistication.\nBy unbundling responsibilities into three tiers, the protocol more closely aligns each role with its core duties. This structure allows each actor to specialize—enabling execution proposers to deliver more performance without compromising the decentralization of the attester set and the security they provide, or the censorship resistance properties maintained by includers. It also provides a robust “baked-in” failsafe mechanism by allowing participants to step in and fulfill other roles if needed. The goal is to achieve and maintain a balance between specialization and integration to preserve the overall system coherence.\nScreenshot 2025-01-31 at 09.27.561034×730 59.9 KB\nFigure 1. This Venn diagram illustrates the three tiers of participants—Attesters (yellow), Includers (green), and Execution Proposers (blue)—and shows how they can overlap. The outer circle denotes the broader set in which all participants operate, and the size of the squares represent the amount at stake for each protocol participant.\nIn this section, we outline the desired properties, capital requirements, the selection and reward mechanisms, as well as the failsafe mechanism for the three tiers of participation**.**\nNote: These proposed mechanisms and requirements represent promising avenues to explore, rather than serving as a definitive or agreed-upon choice.\nExecution Proposers\nExecution proposers are responsible for proposing a valid execution payload to the rest of the network. They have the final say over transaction inclusion and ordering, provided they include all required IL transactions. We expect these entities to maintain advanced infrastructure to run (or outsource) complex MEV extraction strategies, possibly manage pre-confirmations, and handle delegated stake (see selection mechanism section below). Explicitly relying on sophisticated parties to handle the execution payload gives us the potential to drastically scale and increase the chain’s performance (e.g., achieving higher throughput) without introducing more centralization for the already sophisticated (out-of-protocol) builder market. At the same time, we can implement checks and balances to (1) shield other roles from centralizing forces and (2) use clever mechanisms and incentives to avoid over-centralization of the execution proposer set.\nDesired properties\nPerformance and reliability: Ideally, execution proposers are able to reliably provide services requiring large capital and hardware requirements to potentially build and propose valuable blocks, provide pre-confirmations, download and propagate blobs across the network, etc.\nAvoid over-centralization: While we expect the execution proposer set to be somewhat centralized by nature, we want to prevent situations where a very small number of entities dominate execution-proposing rights. Such scenarios could lead to undesirable outcomes like monopolistic pricing and would be problematic from both a memetics and ethos perspective.\nSelection mechanism: Delegation-Based Tullock Contest (DBTC)\nIn a Tullock contest, participants expend resources to increase their probability of winning a valuable “prize.” Here, the prize is the right to propose execution blocks and capture MEV (Maximal Extractable Value) plus priority fees. Execution proposers compete to attract delegations from other parties called delegators (e.g., attesters and includers, or potentially any ETH holder) by sharing a portion of their earnings. The probability of being selected to produce a block is proportional to the total stake delegated to each proposer.\nCrowding-out effect and equilibrium\nWhen a proposer offers higher returns, it initially draws many delegators. This influx of stake can dilute per-delegator rewards, driving some delegators to switch to smaller proposers that offer a better yield on a per-unit basis. Over time, this dynamic encourages competition, reduces profit margins, and leads to an equilibrium reminiscent of a Tullock contest: multiple proposers coexist with comparable returns rather than a single dominant player.\nKey Assumption: Near-Constant Marginal Returns\nA crucial assumption in this delegation-based Tullock framework is that execution proposers’ profits exhibit constant (or near-constant) marginal returns with respect to their delegated stake. This ensures that once a proposer is “too big”, the marginal gain in per-delegator returns diminishes enough to nudge some delegators toward smaller or newer proposers.\nIf marginal returns increase (e.g., due to exclusive order-flow deals or powerful MEV synergies), being bigger becomes disproportionately more profitable—potentially causing a winner-takes-all outcome.\nIf marginal returns decrease heavily, the mechanism stops being incentive compatible, because participants could split their stake among multiple identities and earn more overall.\nMaintaining near-linear (constant marginal) returns is therefore key to balancing competition, avoiding over-centralization, and preventing splitting.\nInterestingly, it’s also worth considering the delegation part of the mechanism as an add-on to the Tullock contest, allowing delegators to direct delegations not solely on yield, but on broader considerations and preferences, including ethos and a commitment to decentralization across all levels of the Ethereum supply network. Unlike purely market-based mechanisms, the social and operational overhead of delegation makes it considerably harder for a proposer that has failed or misbehaved to regain lost delegations and trust.\nNote: While other selection ideas—like Execution Tickets or Execution Auctions—exist, the delegation mechanism is highlighted here for its novelty, and its potential to avoid over-centralization. Further research in this direction (h/t Conor and his Appointed Execution Proposers post) is needed to specify the details of this mechanism.\nReward mechanism: MEV and priority fees\nExecution proposers collect both MEV and priority fees from the blocks they build. They then share an fraction share of these rewards with their delegators, in proportion to each delegator’s share of the execution proposer’s total stake. By adjusting the share they offer, execution proposers can make themselves more or less attractive to new delegators.\nFailsafe mechanism\nRe-delegation: The delegation-based selection system incentivizes stakers to seek higher returns but could also prevent the excessive concentration of execution-proposing rights. If a single execution proposer accumulates too much power—or if delegators lose confidence in their performance or integrity—delegators can reassign their stake to other candidates, preserving the network’s neutrality. We can imagine out-of-protocol services that dynamically manage delegations, allowing delegators to respond quickly to changes in  performance or trustworthiness. Transparency and publicly available dashboards (e.g., à la L2beat) would also play a crucial role in ensuring accountability, providing real-time insights into exeuction proposer performance and enabling more informed decision-making by delegators.\nAttesters as fallback: ****In the event that no one opts in to be an execution proposer, attesters (presumably, with a large amount at stake) can temporarily assume the block-proposing role by including at least transactions from includers’ ILs and a vanilla block built by an attester to ensure the chain continues producing valid blocks despite a lack of dedicated proposers, preserving both liveness and censorship resistance. It is worth noting that falling back on attesters might lead to some trade-offs in terms of performance (e.g., less valuable blocks, less blobs included per slot) which should be evaluated. Another option is to allow attesters with sufficient stake to opt in as a fallback option in case no execution proposer is available, provided they meet the minimum requirements for fulfilling this role.\nAttesters\nAttesters secure the network by ensuring that both the consensus and execution information in proposed blocks are valid according to their view. The economic security they provide is determined by the amount of stake they put up, which exposes them to slashing if they misbehave. Their role is to vote for blocks that pass all validity checks, including being built on the correct head, containing valid transactions, and satisfying inclusion list (IL) conditions.\nDesired properties\nEconomic security**:** A substantial amount of stake (e.g., 20M ETH) should eventually secure blocks proposed to the network, making it prohibitively expensive to attack or control more than one-third of the total stake.\nFast finality: A sufficient amount of economic security should be attained shortly after a block is proposed (for instance, within 3 slots) to ensure fast, guaranteed settlement. Faster finality can be achieved using a combination of technical—such as an optimally secure consensus protocol, more efficient signature aggregation, and validator capping—and economic approaches like issuance capping and maxEB.\nDiverse stake distribution: Stake should be spread across multiple entities so that no single or small group can dominate the majority of attesting power. This safeguards against known attacks (e.g., 51% attacks) and reduces the risk of both correlated faults and commitment attacks. One way to preserve a diverse stake distribution is to ensure participants with smaller stake (e.g., solo stakers with 32 ETH, potentially down to 1 ETH in the future) can can still participate in securing the network under normal conditions and serve as a failsafe if larger-stake attesters are go offline.\nShielded from centralization forces like timing games: Attesters shouldn’t be incentivized to deviate from honest participation in consensus—for example, by strategically delaying their votes to collect issuance rewards.\nRobust to commitment attacks: Attesters should be well-protected against bribery or extortion attempts that might coerce them into deviating from their honest commitments, ensuring the integrity of consensus outcomes.\nVariable (e.g., from 1 ETH to 2048 ETH).\nHeavy — Capital is staked, locked upfront, and put at risk of being slashed (e.g., for attesting to different head blocks).\nSelection mechanism: Threshold selection (see Orbit SSF post for more details)\nThe Orbit threshold selection mechanism is used to attain high levels of economic security as quickly as possible while allowing inclusive participation to ensure a diverse stake distribution.\nDefine a stake threshold T_{\\text{att}} (e.g., 1024 ETH).\nParticipants who opt in are:\n\nWith S \\geq T_{\\text{att}}: Always selected as attesters, leveraging their substantial stake for network security.\nWith S < T_{\\text{att}}: Selected with probability S / T_{\\text{att}}, allowing smaller stakers to participate proportionally.\n\n\nWith S \\geq T_{\\text{att}}: Always selected as attesters, leveraging their substantial stake for network security.\nWith S < T_{\\text{att}}: Selected with probability S / T_{\\text{att}}, allowing smaller stakers to participate proportionally.\nThis selection mechanism ensures guaranteed participation by large-stake attesters with probabilistic inclusion of smaller-stake attesters. Interestingly, this selection mechanism for attesters and staking more generally can also be viewed as a Tullock contest, in which stakers pay capital and operating costs (“all pay”), the prize is issuance rewards (for attesting), and the winning probability is proportional to their stake.\nReward mechanism\nAttesters earn issuance-based rewards for securing consensus and ensuring economic finality, proportional to their stake (again, see Orbit SSF post). Importantly, the shape and properties of the issuance curve to reward attesters should be designed to interact synergistically with attester capping/rotation.\nFailsafe mechanism\nRewarding attesters via issuance provides robust guarantees to attract a “sufficient” number of participants to secure the protocol.\nBy allowing includers—or smaller-stake participants in general (e.g., with a balance of at least 1 ETH)—to opt into attesting, the protocol lowers barriers to entry and ensures more participants can step in to preserve liveness and safety if the set of large-stake validators is insufficient.\nIn extreme circumstances, execution proposers could also be relied upon to maintain block finality and secure the chain; however, this would be an undesirable outcome because it risks concentrating attesting power in the hands of a few already well-capitalized actors.\nIncluders\nIncluders are responsible for upholding the network’s censorship resistance by constructing inclusion lists (ILs) of transactions pending in the public mempool. In doing so, they constrain sophisticated execution proposers by specifying a set of transactions that must be included in blocks for those blocks to be considered valid.\nDesired properties\nGeographic decentralization: Ensure that the set of actively participating includers is distributed across multiple geographic regions, network topologies, or jurisdictions. This reduces the likelihood that any single region or entity can censor specific transactions, thereby preserving the network’s neutrality and permissionlessness.\nUnlinkability: Ideally, includers can participate in improving the network’s censorship-resistant properties and uphold chain neutrality without publicly revealing their preferences via the specific transactions included in their lists. This could be achieved using a combination of linkable ring signature schemes and anonymous broadcast protocols to protect their identities.\nSelection mechanism: Random\nThe protocol selects N includers (e.g., 16 based on the current FOCIL specifications but potentially more in the future) each slot using a weighted random mechanism. Each participant who has opted in to be an includer and meets the minimum stake threshold T_{\\text{incl}} (e.g., 0.1 ETH) is assigned a weight proportional to their staked amount above T_{\\text{incl}}.\nReward mechanism\nIncluders could be rewarded using:\nAn independent transaction fee mechanism (e.g., “inclusion fees”—see the Towards Attester-Includer Separation post for more details). These fees can be distributed among includers who add transactions to their inclusion lists, enabling the network to self-regulate based on the current level of censorship: If many transactions are being censored, users can raise inclusion fees, thereby increasing the cost of censorship. As these higher fees are distributed among includers, more individuals are incentivized to participate in creating inclusion lists, ultimately enhancing Ethereum’s censorship resistance.\nIssuance, to ensure the protocol rewards participants that contribute to the network’s censorship resistance. This would involve selecting the appropriate properties for the shape of the issuance curve and may include mechanisms like stake ratio targeting to get a fixed number of includers.\nNo rewards: Another alternative is to rely on the altruistic behavior of includers to build inclusion lists. While this might initially seem unappealing—since ideally, participants should be rewarded for honestly performing protocol duties—it becomes sensible when aiming to design a mechanism that preserves includers’ privacy. Rewarding includers based on their contributions inherently weakens the unlinkability property. Moreover, by allowing includers to remain private and participate only on an opt-in basis, we may eliminate the need for rewards altogether. This approach relies on a genuinely altruistic set of includers, enhancing privacy without the trade-offs associated with incentives.\nBy unbundling execution proposing and block building (execution proposers), participating in  consensus (attesters), and preserving censorship resistance (includers), 3TS gives each protocol participant the flexibility to choose a role—or multiple roles—that align with their preferences and requirements. This separation not only increases network resilience—by allowing different roles to step in if another group becomes underrepresented—but also unlocks more performance (e.g., by explicitly relying on sophisticated parties to build and propose execution payloads) and encourages participation from diverse stakeholders, from large-scale operators to small community members. The result is a mechanism designed to scale execution and achieve fast finality without having to trade-off security and censorship resistance even under adverse conditions. Although additional details and implementation research are still needed, I hope ideas from 3TS can serve as a stepping stone for the future development of Ethereum staking.\nDrawing participants from a unified set using specific selection mechanisms for each tier leads to greater complexity from a mechanism design perspective, as we would need to carefully consider how roles and incentives interact with one another (in addition to designing robust independent mechanisms). However, approaching core protocol design from first principles using a more holistic approach (e.g., matching core protocol roles to core protocol duties with baked-in failsafe mechanisms) can also serve as a forcing function, encouraging teams working on different aspects of the protocol to collaborate and coordinate their efforts. This could lead to new synergies and more elegant mechanisms than those attained through uncoordinated, specialized efforts for each role.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Blob Aggregation - Step Towards More Efficient Blobs",
        "link": "https://ethresear.ch/t/blob-aggregation-step-towards-more-efficient-blobs/21624",
        "article": "This research was conducted in collaboration with @artificialquant, @projectHodl\n and @Alpinex_.\nTL;DR:\nProblem\nNew rollups are created every day, but not all are as widely used as Arbitrum, Base, and Optimism. Through data availability (DA), rollups make transaction data available to all network participants so they can independently verify it. The data is posted to a DA layer, such as Ethereum (or other DA chains, e.g. Celestia), either using calldata (costly) or blobs (cheaper). These rollups compete for a limited 128KB blob space (6 blobs per block now, increasing to 9 after the Pectra upgrade in March 2025). As demand for blob space grows, fees can rise exponentially.\nSince number of blobs is limited, and each blob is paid in full, regardless of how much it is filled, it leads to a situation where smaller rollups (with empty blobs) pay the same as bigger rollups (with full blobs) for each blob tx. Consequently, the limited DA space is wasted and fees rise for all due to more blobs being included.\nFollowing the @hyeonleee research Potential impact of blob sharing for rollups, we see that blob under utilization is not only an issue for smaller rollups but also for larger ones (though they are less affected) through increased blob fees.\nrollup-blob-utilization552×808 68.1 KB\nSolution\nWe propose to expand the block-building process with “blob building” support by combining multiple rollups’ blobs into one. This would allow rollups to share blob space, lowering their costs, improving blob efficiency, reducing network congestion, and making blob space more affordable for all.\nThe proposed solution builds on top of existing infrastructure and concepts such as MEV-boost and Flashbots bundles. It’s meant to be permissionless so anyone can start providing blob aggregation services. Safety against blob manipulation is ensured via cryptography.\nTechnical Details\nThe specs of proposed implementation is stack-agnostic and can be used even for non-rollup blob data.\nWe also introduce some new terminology:\nBlob shard: signed blob which is included in aggregated blob.\nAggregated blob: group of blob shards, prefixed by a header.\nBlob Aggregator: service provider that collects blob shards, groups them into Aggregated blobs, and posts them via flashbots bundles.\nShared Blob Registry: contract on the ETH mainnet to which the blob txs are posted. It contains blob shard validation logic and fee collection mechanism.\nGeneral flow\nblob-sharing-flow2899×1379 150 KB\nRollup generates blob shard and sends it to Blob Aggregator RPC.\nBlob Aggregator connects to multiple Blob Aggregator RPC streams to receive blob shards.\nBlob Aggregator combines blob shards into a single blob and sends it as a blob transaction that interacts with the Shared Blob Registry on the execution layer.\nShared Blob Registry contract verifies blob shards, calculates fees, and rewards Blob Aggregator.\nBlock builder includes the blob in the next L1 block.\nBlob Shards\nEach blob shard has a specific structure. The actual blob data is prefixed with a header before being included in the aggregated blob. This is done by the aggregator.\nThe header contains:\nChain ID: blob source chain\nSignature: used to derive blob sender. Together with Chain ID it represents a tuple to uniquely identify to whom the blob shard belongs to.\nBlob Data Length: the number of blob data bytes in this shard.\nBlob Aggregator RPC\nSimilar to how eth_sendBundle is used by MEV searchers to send one or more transactions as a bundle, we propose extending this concept to blobs.\nBy using eth_sendBlob, rollups can send their rollup blobs to the aggregator RPC, which will try to combine them into a single blob and post it to L1. The ordering of the blobs is determined similarly to transactions – by gas price per blob byte, and in cases of equal gas prices, by the time of submission.\nThe structure of the blob bundle request is (encoded in JSON):\nThe Blob Shard struct provides metadata for a given blob shard, allowing it to be uniquely identified, preventing double posting, and calculating fees based on the per-shard utilization of available blob space. The proposed structure does not restrict or tie users to a specific blob data layout within their shards. This makes it possible for users to optimize their blob data structure for their specific use cases.\nThe Signature field ties a blob shard to a specific user, ensuring the request has not been tampered with. It’s also used as an on-chain authentication mechanism to charge fees for the blob and making sure the aggregator takes no more than the amount of fees paid by the blob shard.\nBlock Deadline serves a similar purpose as the target block parameter in flashbots bundles. This ensures that a shard can be included up to the deadline block. Shards included in blobs that land later than the block deadline are invalid. This makes the blob shard inclusion more predictable and simplifies fallback mechanisms of e.g. rollup batches where they can then post the blob tx themselves.\nBlob Aggregator\nBlob Aggregator is responsible for gathering blob shards from different users, combining them into a single blob, and submitting it to DA layer to the Blob Shard Registry, preferably in a flashbots bundle that reverts if the TX fails. The aggregator is a permissionless entity that anyone can run.\nThey are incentivized to run their service by taking a small fee on the realized blob cost savings of each shard. Thus, their goal is to fill the blob with as much data as possible.\nThe aggregated blob layout contains a blob header, followed by a blob body which includes the shards. The layout of the body can vary across aggregated blob versions (stored in header), and can even be additionally compressed.\nThe header additionally contains a lookup table to speed up processing by the shard senders, with the following layout: chain ID → shard sender → [shard1 idx, shard2 idx, …]. This makes it possible to jump straight to the start of blob shard data of specific shard sender, without having to process bytes one by one. The lookup table could potentially be moved from the blob header to Blob Shard Registry contract calldata or even emitted via events, but that’s TBD.\nFull aggregated blob structure:\nOnce the blob is aggregated, the aggregator prepares the blob tx which calls the recordSharedBlob() function on the SharedBlobRegistry contract to register the blob and correctly collect fees and rewards.\nShared Blob Registry\nShared Blob Registry is a contract responsible for verifying blob shards and compensating the blob aggregator for their service.\nWe propose the following structure for the Shared Blob Registry contract:\nConclusion\nThe proposed Blob Sharing concept aims to enhance rollup efficiency by allowing them to share blob space. This approach is expected to lower costs for rollups, improve blob space utilization, and make it more affordable. It also provides an opportunity for Blob Aggregators to experiment with blob shard composition and prepare to evolve into Based Sequencers.\nOther Resources\nSpire Labs: Shared Blob Compression\nSuhyeon - Tokamak Network: Potential impact of blob sharing for rollups\nFeedback Invitation\nThis proposal is meant to gather feedback from other community members before diving into development. We invite everyone to share their thoughts, suggestions, and potential concerns to refine and improve this concept.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": []
    },
    {
        "title": "Canonical cross-chain swap: fast and decentralized settlement for cross-chain swap using canonical/native L1->L2 messaging",
        "link": "https://ethresear.ch/t/canonical-cross-chain-swap-fast-and-decentralized-settlement-for-cross-chain-swap-using-canonical-native-l1-l2-messaging/21638",
        "article": "Written by Suah Kim & George (Tokamak Network)\nCanonical cross-chain swaps uses canonical/native L1→L2 messaging to improve decentralization and speed up settlements for liquidity providers. This improves capital efficiency and increases available liquidity to help address L2 fragmentation.\nPopular cross-chain swap platforms like Across, 1inch, UniswapX, and 0x have improved user experience with faster, cheaper, and more accessible swaps, helping reduce L2 liquidity fragmentation.\nHowever, these improvements come with increased centralization, as these platforms rely on permissioned or/and third-party consensus algorithms for cross-chain messaging. This creates heightened risks for liquidity providers such as loss of fund and longer settlement times, reducing the capital efficiency.\nUsing the canonical L1→L2 messaging (ex: Optimism’s Portal, Arbitrum’s Inbox) for cross-chain communication provides a simpler and more decentralized cross-chain swap. This approach enables liquidity providers to conduct their own due diligence without relying on third parties. While liquidity providers still need to trust the L1 and L2 networks, this method improves capital efficiency and increases available liquidity by lowering barriers to entry for liquidity providers.\nFor clarity, we define the following terms:\nRequester: A user who requests a cross-chain swap\nProvider: A liquidity provider who facillitates the cross-chain swap request\nWhile existing cross-chain swap services provide a seamless experience for Requesters, both Providers and Requesters face risks from systemic dependencies:\nDependency on centralized cross-chain messaging: Relying on centralized cross-chain messaging (such as permissioned or third party consensus protocol) introduces risks like loss of funds and settlement delays due to service outage, errors and failures. If the service Provider decides to stop the service, it could also affect the user experience for Requesters.\nReliance on L2: Providers must trust L2 networks to operate in good faith and prevent issues like service outages, censorship, or excessive gas prices.\nOur protocol, canonical cross-chain swap, leverages “canonical L1→L2 messaging” to enable seamless cross-chain communication, eliminating reliance on permissioned or third-party protocols. It is important to note that the current version is specifically designed to support L2-to-L1 cross-chain swaps and does not yet accommodate efficient L2-to-L2 cross-chain swaps (will be updated later).\n“Canonical L1→L2  messaging” refers to native messaging system used by L2s, for example, OP stack uses Portal contracts to send message from L1 to L2. This enhances both security and user experience for Providers by reducing the number of trusted parties.\nKey Features\nReduces dependency on permissioned or third-party cross-chain messaging\nFaster settlement times for Providers\nMinimizes Provider risks during request changes or cancellations\nSafeguards against timelock vulnerabilities in cases of service outages or L2 censorship\nMinimize transaction fees by reducing the number of transactions and storage updates while maintaining security. (Please check our MVP )\nCurrent version supports L2 → L1 only as a proof of concept, but it can be extended to L2 → L2.\nIn proposed protocol, Requesters will still receive their funds first, maintaining the excellent user experience of existing protocols, while Providers operate with less risk. This creates a fair and secure system for all participants, simplifying operations and greatly improving the Provider experience.\nFig. 1 L2→L1 canonical cross-chain swap flow.1478×974 150 KB\nHere’s how our proposed protocol works, step-by-step:\n1. Requester makes a cross-chain swap request on L2\nRequester initiates a cross-chain swap on the origin chain by specifying details like the desired L1 token information and locking the tokens to escrow contract.\n2. Provider fulfills the request on L1\nAnyone can observe the requests on L2 and fulfill the request. A network of Providers observes the requests registered in the escrow contract:\n(a) Provider calls the cross-chain messenger contract to provide the requested amount of tokens\n(b) (in the same transaction) the Requester will receive the funds from cross-chain messenger (transferred on behalf of Provider)\n(c) (in the same transaction) L1→L2 settlement request message is sent to the escrow contract, instructing it to release the locked tokens to the Provider.\n3. Cross-chain Messaging to L2\nUpon receiving L1→L2 settlement request message, L2 sequencer relays the message to the escrow contract. This ensures the process remains trustless and fully reliant on the security of the L1→L2 canonical messaging.\n4. Funds Released to the Provider on L2\nUpon receiving the message, the escrow contract verifies the transaction. It releases the locked tokens to the Provider, completing the settlement transaction (the Provider now has their funds on L2)\nTo summarize, in a canonical cross-chain swap:\nRequester executes a single transaction to escrow the tokens for swapping, which can be made gasless using the “permit” mechanism.\nProvider performs a single transaction on L1 to transfer the tokens to the Requester and send the settlement message. This transaction includes the L1→L2 canonical messaging, seamlessly relayed by the L2 sequencer without requiring any additional action from the Provider.\nTo ensure the integrity of the protocol, critical operations like “editing” and “canceling” requests have to originate from L1 (cross-chain messenger contract), even if the request itself was initiated on L2 (escrow contract). This ensures that the trust and security of the system remain intact and that funds are safeguarded for both Requesters and Providers.\nEditing a request\nThe “Edit” function allows the Requester to modify the token amount that the Provider should provide to Requester based on dynamic market conditions like token and gas prices.\nUnlike other services that rely on off-chain oracles or time-based Dutch auction models, this feature handles edits directly through the L1 (cross-chain messenger contract). Since these changes immediately effect the Provider’s economic incentives, the edits take effect instantly when a Provider fulfills a request.\nFig. 2 Editing request flow. Useful to change the incentive based on the market condition.1478×672 94.5 KB\nHere’s how “edit” works:\nRequester sends an edit transaction to the cross-chain messenger to update the requested amount.\n\nFor example: Requester initially wanted to exchange 10 USDC on Optimism for 8 USDT on Ethereum. When Ethereum gas prices drop, the Requester can edit their request to receive 9 USDT on Ethereum instead.\n\n\nFor example: Requester initially wanted to exchange 10 USDC on Optimism for 8 USDT on Ethereum. When Ethereum gas prices drop, the Requester can edit their request to receive 9 USDT on Ethereum instead.\n“Edit” is only applied if no Provider has fulfilled the request yet.\n\nEnsures that the Provider always has the most up to date information about the request and the request cannot be edited or cancelled after the liquidity has been provided.\n\n\nEnsures that the Provider always has the most up to date information about the request and the request cannot be edited or cancelled after the liquidity has been provided.\nBy enforcing edits and cancellations on L1, the proposed protocol strikes a balance between flexibility and system integrity, ensuring that both Requesters and Providers benefit from L1 security throughout the entire transaction lifecycle.\nCanceling a Request\nThe “Cancel” function allows the Requester to cancel their request, reclaiming their tokens from the escrow contract.\nThis process has to originate from the cross-chain messenger contract and only takes effect if the request has not been fulfilled. This ensures that scenarios where the Provider has already fulfilled the request are avoided, preventing the Requester from maliciously canceling the request and stealing the Provider’s funds without releasing the locked tokens on the escrow contract.\nFig. 3 Cancelling request flow.1486×974 143 KB\nHere’s how “cancel” works:\nRequester executes the “Cancel” function on the cross-chain messenger contract (L1).\n\nFor example, Requester decides to not swap because there are better opportunity on L2.\n\n\nFor example, Requester decides to not swap because there are better opportunity on L2.\nIf the request is not fulfilled, the cross-chain messenger sends a cross-chain message to the canonical L1→L2 messenger, instructing it to release the escrowed tokens back to the Requester.\nAfter the request is sequenced on L2, the escrowed tokens are returned to the Requester.\nSome of these trade-offs are intentional, prioritizing decentralization, security, and trustless transactions over optimizing for minimal gas costs.\nAlthough they may introduce certain inconveniences, these decisions align with the protocol’s objective of building a cross-chain swap system that relies solely on the security trust assumptions of L1 and L2.\nOur protocol can integrate ERC 7683, offering support for intent-based systems while enhancing decentralization (the “fillDeadline” parameter has to set at the maximum value to avoid timelock vulnerability). This ensures Providers and users with the flexibility of interoperable standards while ensuring a more secure and efficient transaction process.\nWhile the introduction of standards like ERC and RIP has made these systems possible, they also come with additional security assumptions. It’s encouraging to see these efforts advancing cross-chain functionality, but there remains a need for a protocol that rely solely on the inherent security of L1 and L2 networks.\nThe canonical cross-chain swap protocol enhances decentralization and efficiency in cross-chain liquidity by leveraging native L1→L2 messaging, reducing reliance on centralized intermediaries. This approach mitigates risks, improves capital efficiency, and addresses L2 fragmentation.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": []
    },
    {
        "title": "Falcon as an Ethereum Transaction Signature: The Good, the Bad, and the Gnarly",
        "link": "https://ethresear.ch/t/falcon-as-an-ethereum-transaction-signature-the-good-the-bad-and-the-gnarly/21512",
        "article": "This is Part 2 of a blog series exploring the feasibility of implementing a post-quantum signature scheme for Ethereum. In Part 1, we introduced the fundamental challenges and considerations involved in transitioning Ethereum to a quantum-resistant future. In this installment, we’ll dive deeper into Falcon, a promising post-quantum signature algorithm, examining its strengths, weaknesses, and the practical hurdles of integrating it into Ethereum’s transaction framework.\nFalcon Signature Scheme - Technical Overview\nFalcon (Fast-Fourier Lattice-based Compact Signatures over NTRU) builds upon the lattice-based signature framework of Gentry, Peikert, and Vaikuntanathan (GPV). It applies this framework to NTRU lattices and employs a “fast Fourier sampling” trapdoor sampler. The scheme relies on the Short Integer Solution (SIS) problem over NTRU lattices, which is considered computationally hard to solve in the general case, even with quantum computers, as no efficient solving algorithm is currently known.\nCore Components\nFalcon is based on the hash-and-sign paradigm and is an evolution of the traditional RSA signature scheme. However, instead of relying on number-theoretic problems, it leverages the hardness of lattice-based problems. Falcon’s security is based on the hardness of finding short vectors in NTRU lattices, leveraging Gaussian sampling techniques for generating trapdoor bases with reduced norms. This ensures efficient key generation and signing.\n\nKey Generation:\n\nGiven an NTRU polynomial ring ( \\mathbb{Z}[X] / (X^n + 1)), a private key consists of two short polynomials ( f, g ) satisfying the NTRU equation.\nThe public key is derived as ( h = g / f ) in the ring ( \\mathbb{Z}_q[X] / (X^n + 1) ).\n\n\nGiven an NTRU polynomial ring ( \\mathbb{Z}[X] / (X^n + 1)), a private key consists of two short polynomials ( f, g ) satisfying the NTRU equation.\nThe public key is derived as ( h = g / f ) in the ring ( \\mathbb{Z}_q[X] / (X^n + 1) ).\n\nSigning Process:\n\nA message is hashed into a challenge vector in the lattice domain.\nA short solution is sampled using fast Fourier sampling, ensuring a compact signature size while maintaining security against lattice reduction attacks.\nThe signature consists of the short lattice vector satisfying the challenge.\n\n\nA message is hashed into a challenge vector in the lattice domain.\nA short solution is sampled using fast Fourier sampling, ensuring a compact signature size while maintaining security against lattice reduction attacks.\nThe signature consists of the short lattice vector satisfying the challenge.\n\nVerification:\n\nThe verifier checks whether the signature satisfies the public key relation in the lattice ring.\nVerification involves computing norms and ensuring the validity of the lattice basis under modular arithmetic.\n\n\nThe verifier checks whether the signature satisfies the public key relation in the lattice ring.\nVerification involves computing norms and ensuring the validity of the lattice basis under modular arithmetic.\nKey Generation:\nGiven an NTRU polynomial ring ( \\mathbb{Z}[X] / (X^n + 1)), a private key consists of two short polynomials ( f, g ) satisfying the NTRU equation.\nThe public key is derived as ( h = g / f ) in the ring ( \\mathbb{Z}_q[X] / (X^n + 1) ).\nSigning Process:\nA message is hashed into a challenge vector in the lattice domain.\nA short solution is sampled using fast Fourier sampling, ensuring a compact signature size while maintaining security against lattice reduction attacks.\nThe signature consists of the short lattice vector satisfying the challenge.\nVerification:\nThe verifier checks whether the signature satisfies the public key relation in the lattice ring.\nVerification involves computing norms and ensuring the validity of the lattice basis under modular arithmetic.\nFalcon is designed to offer a robust post-quantum signature solution, combining lattice-based cryptography with efficient sampling techniques. While its security benefits are clear, like any cryptographic system, it presents certain trade-offs in terms of complexity and implementation challenges. Now, let’s break down the highlights, potential pitfalls, and some of the more challenging aspects of Falcon.\nThe Good\nAside from the well-known benefits highlighted by NIST, such as Compact Signatures, Fast Operations (efficient key generation and verification via FFT techniques), and Security Proofs (relying on lattice reductions and worst-case hardness assumptions). Falcon also provides Ethereum-specific advantages. Notably, it has a well-defined worst-case running time, making it particularly useful for the Ethereum Virtual Machine (EVM), where predictable performance and execution times are essential for scalability and reliability.\nThe Bad\nFalcon’s reliance on floating-point arithmetic and specialized number-theoretic transforms (NTT/FFT) can lead to implementation complexity and sensitivity to side-channel vulnerabilities during signing. However, this is NOT a significant concern for Ethereum, as signing occurs off-chain, where performance is less critical. The main focus is on optimizing the verification process, which happens on-chain, ensuring efficient and secure execution.\nThe Gnarly\nThere has been ongoing research into efficiently aggregating Falcon signatures, such as the work presented in this paper. Assuming the aggregation will be efficient enough, using Falcon in the consensus layer to replace the BLS signature (instead of the alternative proposal based on Hash-Based Multi-Signatures) would help maintain a more homogeneous stack across the Ethereum network.\nConclusion\nFalcon is a strong candidate for post-quantum cryptography applications, including blockchain systems like Ethereum, where signature size and verification efficiency are critical. In Part 3 of the series, we will begin implementing the hybrid approach introduced in Part 1, initially focusing on Account Abstraction and a Solidity contract for Falcon verification, bridging the gap between post-quantum security and Ethereum’s current infrastructure.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "Native Rollup for 3SF",
        "link": "https://ethresear.ch/t/native-rollup-for-3sf/21632",
        "article": "Author: @adust09, @banr1 from Titania Research, @keccak255 from Titania Research\nSpecial Thanks: @grandchildrice\n1. Introduction\nIn this proposal, we introduce a new architecture that aims to further improve user experience(UX) by combining Native Rollup with 3-Slot Finality (3SF). Specifically, we demonstrate the possibility of merging the advantage of Native Rollup where rollup EVM execution can be directly verified on L1 with 3SF, which finalizes blocks in stages. We also provide an estimate of how much time can be allocated for proof processing to the developers of the zkEL (zk execution layer). This estimate serves as useful information for teams developing a zkEL.\n1.1. Native Rollup\nNative Rollup is a mechanism that achieves high security by leveraging and verifying the L1 EVM directly. Concretely, it utilizes a newly proposed EXECUTE precompile contract, enabling L1 validators to directly verify rollup EVM transactions.\nスクリーンショット 2025-01-29 18.38.432160×1350 159 KB\nA key feature of this approach is that it achieves exactly the same level of security and upgrade compatibility as Ethereum L1, without the need for external security councils or complex fraud proof games.\nSince it is no longer strictly necessary to verify zkRollup on-chain, there is the advantage of flexible off-chain verification while controlling gas costs. Moreover, real-time settlement can be achieved, significantly simplifying synchronous composability.\n1.2. 3-Slot Finality\n3-Slot Finality (3SF) is a protocol design aimed at finalizing proposer-submitted blocks within three slots. In previously proposed Single Slot Finality (SSF), it was necessary to conduct about three voting rounds within one slot. In contrast, 3SF unifies these voting rounds into a single round per slot. This reduces the number of signature aggregations and P2P network propagations needed for each vote.\n3SF assumes that network delay remains within a known constant Δ and that at least two-thirds of the validators behave honestly. The process within each slot of 3SF is as follows:\nBlock proposal (Δ)\nhead-vote + FFG-vote (2Δ)\nFreezing (2Δ)\n\nfast-confirmation (Δ)\n\n\nview-merging (Δ)\n\n\nfast-confirmation (Δ)\nview-merging (Δ)\nIn step 1, the proposer proposes a block.\nIn step 2, both the head-vote (to select the chain head) and the FFG-votes (for source and target) are executed in the same round. The current Ethereum L1 aggregation scheme is taken as a basis. First, votes are broadcast, then an aggregator collects them and broadcasts again, making the total time 2Δ.\nIn step 3, based on the results of steps 1 and 2, if the proposed block receives more than two-thirds of the head-votes, fast-confirmation is achieved, and the block is considered nearly irreversible. Furthermore, when the block that achieved fast-confirmation is shared among all validators as the chain head at the start of the next slot, the view is merged.\nAfter that, the block proposed in slot 1 is justified in slot 2 and finalized in slot 3. In other words, 3SF is an approach that lengthens the finality time while shortening the confirmation time. At the same time, by combining the head vote and the FFG vote, the slot duration becomes shorter compared to SSF. Consequently, this balance is considered sufficient for most users. The slot structure of 3SF is very similar to that of the current Ethereum L1.\n2.1. Native Rollup for existing Ethereum\nzk provers in Native Rollup are anticipated to take longer to process compared to proposers or attesters. If we want the proof to be completed within one slot, we will likely have to wait for further advancements in ZKP and cryptographic technology. Hence, storing the stateRoot of the previous block rather than the current block has already been proposed in EIP-7862. This allows the proposer to delay EVM execution. Native Rollup is premised on this approach.\nBelow is an illustration of slot transitions when Native Rollup is applied to the existing Ethereum.\nスクリーンショット 2025-01-29 22.59.592160×1350 241 KB\nHere, the number after each role name means assigned to that slot. For example, attesters2 is the attesters assigned to slot 2. Also, the shaded area means that the following tasks are performed in the corresponding slot.\nGreen shaded: proposer in EL execution\nRed shaded: proposer in zkEL for proof generation\nYellow shaded: attesters in EL, execution by zkEL, verification of proof\nThe proposer has been idle for 4 to 12 seconds, but EIP-7862 enables delayed execution for proposer in EL. This is shown in green in the figure.\nIn Native Rollup, the proposer must run not only the EL and CL but also the zkEL locally. Thanks to EIP-7862, proof generation can be delayed as well. The role of the zkEL on the proposer is to generate proofs for L2 state transitions, shown in red. Unlike the EL, this can be postponed until just before the proposer step in the next slot.\nNote that the stateRoot verified by attesters2 is from slot 1. Due to EIP-7862, this offset occurs, but it provides the benefit of delayed execution. Each stateRoot has an L1 and an L2 version. The L1 side confirms correctness by actually executing the computation, while the L2 side confirms correctness by verifying the zero-knowledge proof.\n2.2. Native Rollup for 3SF\nAdapting 3SF to this architecture would result in the following.\nスクリーンショット 2025-01-29 22.57.282160×1350 238 KB\nFor convenience, attesters1 and attesters2 are shown separately, but the specifications are not firmly decided, so they could be the same entities.\nWhereas the existing ethereum slot processing comprises propose, vote, and aggregate, 3SF adds a freeze step. This is common processing for all validators, providing fast-confirmation and view merge.\nAs before, the following apply:\nGreen shaded: execution by EL in proposer\nRed shaded: proof generation by zkEL in proposer\nYellow shaded: verification of execution and proof by EL, zkEL in attesters\nEven under 3SF, the situation is largely the same as with Native Rollup for the existing Ethereum. By delaying execution and verification respectively for the proposer and attester, there is some flexibility in execution time, and this does not conflict with the 3SF steps. We believe Native Rollup can be applied under 3SF as well.\nIn the zkEL, the proof generation task begins when the EL requests proof generation at the start of the slot. There is then leeway up to just before the vote & aggregation phase of the next slot. In other words, if the proof can be completed within Δ + 2Δ + 2Δ + Δ = 6Δ, this scheme can be realized. The attesters need the proof for their verification, so as long as it is generated by their verification phase, it will be in time.\nHowever, given the current performance of zkVM, proof generation in the zkEL might takes several minutes to tens of minutes, which is not realistic. Relying on high-spec servers might be the only option for fitting it within 6Δ. If we rely on high-spec servers, there is a possibility that specialized parties will become centralized.\n3. Discussion & Improvement\nThe following discussion and improvements can be made.\n3.1 zkEL Proof Market\nOne potential solution to the computational cost of proof generation in the zkEL is to delegate it to an external market. If such a market can be established, even solo-stakers could easily generate proofs by outsourcing them. This is somewhat analogous to MEV-Boost. However, the following concerns arise:\nCentralization Risk\n\nReduced redundancy and censorship resistance\nLoss of diversity in zkVM proof methods\n\n\nReduced redundancy and censorship resistance\nLoss of diversity in zkVM proof methods\nDistortions in incentive design\n3.2 Additional research on 3SF\n3SF still faces a tradeoff between security and usability; additional research on how to support a large number of validators may be beneficial from a security perspective when reducing finality time as 3SF does.\nAre there more efficient ways to aggregate and propagate messages? What bottlenecks exist in the current aggregation scheme in the first place?\nIs security really sufficient when discussing reducing the number of messages, such as Orbit SSF?\nHow do we set issuance rewards and how much do we allocate to validators?\nCan we estimate the length of Δ and provide more specific specifications that the prover should meet?\nIs the direction of this proposal useful in the first place?\n4. Summry\nThis proposal presents a new architecture that integrates Native Rollup with 3SF to enhance user experience by leveraging the security and efficiency of both systems. By enabling direct verification of rollup EVM transactions on L1 and finalizing blocks in three stages, the combined approach offers real-time settlement and flexible off-chain verification while maintaining high security standards. Overall, this integration aims to optimize scalability and security for Ethereum-based applications.\n",
        "category": [],
        "discourse": [
            "layer-2",
            "single-slot-finality"
        ]
    },
    {
        "title": "So you wanna Post-Quantum Ethereum transaction signature",
        "link": "https://ethresear.ch/t/so-you-wanna-post-quantum-ethereum-transaction-signature/21291",
        "article": "Thanks to Vitalik Buterin, Justin Drake, Renaud Dubois, Marius Van Der Wijden and Zhenfei Zhang for fruitfull discussions.\nIntroduction\n2024 will probably be remembered as one of the years marking the acceleration of the quantum computer menace. Google, under its CEO Sundar Pichai, finally unveiled its quantum chip, Willow, via a loud tweet!\nScott Aaronson, one of the most famous quantum experts in the world, has changed his message to people asking whether they should be worried about quantum computers. He shifted from saying\n… Maybe, eventually, someone will need to start thinking about migrating from RSA, Diffie-Hellman, and elliptic curve cryptography to lattice-based crypto or other systems that could plausibly withstand quantum attacks,…\nto\nYes, unequivocally, worry about this now. Have a plan.’\nVitalik has already written about how to hard-fork to save most users’ funds in a quantum emergency. Also, few days ago, he highlighted in a podcast the four main Ethereum components potentially vulnerable to quantum attacks. They are:\nEthereum transaction signatures (notably using ECDSA)\nBLS signatures in consensus\nData Availability Sampling (leveraging KZG commitments)\nVerkle trees (if shipped with Bandersnatch)\nAn attentive reader might have noticed that these four points have something in common—yes, it’s my beloved elliptic curves. Unfortunately, the discrete logarithm problem for elliptic curves (ECDLP) is broken by Shor’s Algorithm, a famous quantum algorithm.\nIn this short note, we are going to analyze a possible post-quantum replacement for the first point, namely a potential post-quantum Ethereum transaction signature.\nWhich PQ signature?\nNow, a legitimate question is: which post-quantum (PQ) signatures should we use? Fortunately, we don’t need to overthink this too much if we had to choose right now. Zhenfei Zhang, a former Ethereum Foundation cryptographer, has already written about the NIST Post-Quantum Cryptography Standardization Process. If we analyze the three possible signature choices (two of which leverage lattice-based cryptography), it’s clear (at least for now) that Falcon appears to be the most promising candidate. The computation for the verifier should be roughly the same as other lattice-based signature schemes (like Dilithium), i.e., bounded by an FFT. However, Falcon does have a smaller signature size.\nShip it!!!\nNow that we’ve ‘settled’ on the signature to use, the next question is: how are we going to ship it?  There is a big dichotomy now: one implies a hard fork, and the other doesn’t. Let’s dig a bit deeper.\nThe Account Abstraction way\nThe first approach we will discuss, arguably the most elegant and promising, involves Account Abstraction (AA). It has been advocated by Justin Drake and Vitalik on various occasions.\nFor people not familiar with it, AA is a proposed improvement to make the Ethereum ecosystem more flexible and user-friendly by changing how transactions and accounts are managed. It shifts certain functionalities traditionally reserved for externally owned accounts (EOAs) into smart contracts, effectively “abstracting” the differences between EOAs and smart contract accounts.\nEthereum developers have introduced various proposals for implementing AA, including ERC-4337. This is a practical solution that achieves AA without requiring a consensus-layer upgrade. It uses a mechanism called User Operation objects and introduces a separate Bundler layer to handle transactions.\nAdding Falcon as the Ethereum transaction signature in this scenario means coding a Falcon verifier contract that is responsible for verifying the validity of User Operation objects before they are executed by the Entry Point contract.\nNow, this may sound like all sunshine and rainbows, but there is at least one substantial underlying issue. Coding Falcon in Solidity might not be the best experience (and it’s probably quite gas-costly). On top of that, there are even nastier problems, such as the fact that Falcon deals with 13-bit numbers, while Solidity only supports U256. The latter is the kind of issue that could be addressed by adding SIMD and EVMMAX to the EVM.\nPros: It is an elegant and flexible solution.\nCons: It is costly in terms of gas consumption.\nThe hard fork way\nThe method we discuss here is probably the simplest technically. It is inspired by previous work done by Marius Van Der Wijden and essentially involves introducing a new transaction type signed with Falcon signatures instead of BLS signatures. The biggest problem here is that, by doing so, we are tightly bound (through a new EIP) to a favored master signature scheme.\nSo, to recap this approach\nPros:  Easy to code and fast.\nCons: Not future-proof.\nHybrid\nA really tempting approach would be to take the best of the two methods above and combine them into a single one. In a nutshell, we could leverage AA in a similar way that RIP-7212 does, but of course, we would need a new RIP for Falcon. This might provide the time to experiment with the feature in rollups and determine if Falcon is truly the way to go. However, it is important to note that this approach does not solve the original problem of introducing a new signature scheme at the L1 level.\nPros: Easy to code and fast.\nCons: Temporary (does not solve the L1 use case).\nConclusion\nThe rise of quantum computing demands urgent action to secure Ethereum, particularly its transaction signatures vulnerable to Shor’s Algorithm. Falcon, a lattice-based signature scheme, emerges as a strong candidate due to its efficiency and compact size. Deployment strategies, including Account Abstraction, hard forks, or a hybrid approach, each offer distinct benefits and trade-offs. A careful evaluation is essential to ensure Ethereum remains robust against quantum threats while maintaining scalability and usability.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "Same-Slot L1→L2 Message Passing",
        "link": "https://ethresear.ch/t/same-slot-l1-l2-message-passing/21186",
        "article": "image1792×1024 183 KB\nCo-authored by Lin Oshitani, Conor McMenamin, Anshu Jalan, and Ahmad Bitar, all Nethermind. Thanks to Brecht Davos, Jeff Walsh, and Daniel Wang from Taiko for their feedback. Feedback is not necessarily an endorsement.\nRollups can import the L1 state root into L2 to facilitate message passing between the two layers, such as user deposits and cross-chain contract calls. However, because the EVM cannot access the state root of the current block, rollups can only pull in state roots from past blocks. This restriction makes it impossible for rollups to process L1→L2 messages within the same slot using the state root alone.\nTo overcome this limitation, this post introduces a protocol that enables the L2 proposer to selectively inject L1 messages emitted in the same slot directly into L2, bypassing the need to wait for a state root import. By combining this protocol with the L2→L1 withdrawal mechanism discussed in our previous post, users can execute composable L1<>L2 bundles, such as depositing ETH from L1, swapping it for USDC on L2, and withdrawing back to L1—all within a single slot.\nWe use the terminology from the Taiko protocol, on which this research is based.\nMessages (a.k.a, Signals): Units of data exchanged between chains to facilitate interoperability, such as transferring tokens or executing cross-chain contract calls. Messages are emitted on the source chain and consumed on the destination chain.\nSame-Slot Messages: Messages emitted in the same slot as the L2 block proposal.\nMessage Service Contract: A contract deployed on both L1 and L2 to facilitate message passing between the two layers. Users emit messages in the L1 message service contract and consume them in the L2 message service contract (and vice versa).\nAnchor block: The anchor block is the historical L1 block referenced by an L2 block to import the L1 state root into the L2 environment, effectively “anchoring” the L2 execution to a specific L1 state root. Note that the anchor block must be one slot or more in the past as the EVM does not have access to the current block header.\nBelow is a high-level diagram of the protocol:\nimage1001×1122 100 KB\nThe flow is explained below, with bolded text highlighting the components newly introduced by this proposal for enabling same-slot message passing. The non-bolded text follows what is implemented in the current Taiko protocol.\n(A) Users invoke the L1 message service contract to initiate the L1→L2 message transfer.\n\nThe messages are hashed and stored in the L1 message service contract.\n\n\nThe messages are hashed and stored in the L1 message service contract.\nThe L2 proposer:\n\n(B) Selects which same-slot messages to import into L2.\n(C) Selects the anchor block ID, which is the L1 block ID of the anchor block. This determines which historical L1 block’s state root will be imported into L2.\n(D) Submits the selected same-slot message hashes to the batch inbox contract, the L1 anchor block ID, and the L2 batch to the batch inbox contract.\n\n\n(B) Selects which same-slot messages to import into L2.\n(C) Selects the anchor block ID, which is the L1 block ID of the anchor block. This determines which historical L1 block’s state root will be imported into L2.\n(D) Submits the selected same-slot message hashes to the batch inbox contract, the L1 anchor block ID, and the L2 batch to the batch inbox contract.\nThe batch inbox contract:\n\n(E) Verify with the L1 message service that the message hashes have been recorded. If the verification fails, the L2 batch proposal is reverted. The messages\n(F) Fetch the L1 anchor state root (block header incorporating the state root to be accurate) of the given L1 anchor block ID via the BLOCKHASH opcode. The anchor block must be at least one slot in the past, as the EVM cannot access the current block header.\n(G) Emit an event containing the message hashes, the L2 batch, and the anchor state root, allowing the L2 execution to access and process them.\n\n\n(E) Verify with the L1 message service that the message hashes have been recorded. If the verification fails, the L2 batch proposal is reverted. The messages\n(F) Fetch the L1 anchor state root (block header incorporating the state root to be accurate) of the given L1 anchor block ID via the BLOCKHASH opcode. The anchor block must be at least one slot in the past, as the EVM cannot access the current block header.\n(G) Emit an event containing the message hashes, the L2 batch, and the anchor state root, allowing the L2 execution to access and process them.\nThe L2 execution will:\n\n(H) Import the message hashes and the L1 anchor state root into the L2 message contract.\n(I) The batch is executed. L2 transactions in the batch can:\n\n(J-1) Consume the imported same-slot messages by calling the L2 message contract. Note that no Merkle proof is needed in this case, as the message hashes can be stored in a more easily retrievable way in the L2 message contract.\n(J-2) Consume messages emitted in blocks at or before the anchor block ID. Users (or relays acting on their behalf) achieve this by submitting the original message and Merkle proof to the L2 message contract. This proof verifies that the message was emitted on L1 using the anchor state root.\n\n\n\n\n(H) Import the message hashes and the L1 anchor state root into the L2 message contract.\n(I) The batch is executed. L2 transactions in the batch can:\n\n(J-1) Consume the imported same-slot messages by calling the L2 message contract. Note that no Merkle proof is needed in this case, as the message hashes can be stored in a more easily retrievable way in the L2 message contract.\n(J-2) Consume messages emitted in blocks at or before the anchor block ID. Users (or relays acting on their behalf) achieve this by submitting the original message and Merkle proof to the L2 message contract. This proof verifies that the message was emitted on L1 using the anchor state root.\n\n\n(J-1) Consume the imported same-slot messages by calling the L2 message contract. Note that no Merkle proof is needed in this case, as the message hashes can be stored in a more easily retrievable way in the L2 message contract.\n(J-2) Consume messages emitted in blocks at or before the anchor block ID. Users (or relays acting on their behalf) achieve this by submitting the original message and Merkle proof to the L2 message contract. This proof verifies that the message was emitted on L1 using the anchor state root.\nNote that the message service contracts do not provide native replay detection, and it delegates this responsibility to the applications that consume the messages\nNext, we will explore the key features of this protocol that shape its design decisions.\nSelective Message Imports\nAn important feature of this protocol is that L2 proposers can choose which same-slot L1→L2 messages to import into their L2 blocks. This ensures:\nState Determinism: Proposers maintain full control over the post-execution L2 state root of their L2 batch by avoiding unexpected state changes due to unanticipated inbound messages from L1. This is especially important for enabling same-slot message passing even when the L2 proposer is not the L1 proposer/builder.\nCost Management: Proposers can choose to import only same-slot messages that compensate for the additional L1 gas cost required to process them. Additionally, block proposals will have no gas cost overhead when no same-slot messages are imported.\nConditioning\nFurthermore, note that the batch proposal transaction reverts if the specified same-slot messages were not emitted in L1 (see (C) in the protocol description). This lets the L2 proposer trustlessly condition their L2 batch on the dependent same-slot messages.\nSuppose an L1 user wants to deposit ETH from L1, swap it into USDC on L2, and withdraw back to L1—all within the same slot. This can be achieved by combining this proposal with a same-slot L2→L1 withdrawal mechanism we introduced in Fast (and Slow) L2→L1 Withdrawals. Specifically, the L2 proposer can submit an L1 bundle containing the following three transactions:\nAn L1 transaction by the user that emits the message for depositing ETH into the L2.\nAn L2 batch proposal transaction by the L2 proposer that imports the above deposit message and includes:\n\nThe DEX trade by the user that swaps the ETH into USDC.\nThe withdrawal transaction by the user that sends the USDC back to L1.\n\n\nThe DEX trade by the user that swaps the ETH into USDC.\nThe withdrawal transaction by the user that sends the USDC back to L1.\nAn L1 solution transaction by a solver that conducts the withdrawal.\nAre Shared Sequencers/Builders Needed Here?\nIt’s important to note that the L2 proposer does not have to be the L1 builder in the above L1→L2→L1 scenario. That is, a shared sequencer/builder is not needed. What matters is that the bundle is executed atomically—either all transactions in the bundle succeed or the bundle is not included at all. This atomicity can be achieved through builder RPCs like eth_sendBundle or via EIP-7702 (once implemented), ensuring that the L1→L2→L1 bundle will execute atomically within the same slot. In other words, the L2 proposer does not need to know exactly the L1 state in which the L2 batch is executed. Instead, the L2 proposer just needs to know “enough” L1 state—specifically, the imported same-slot messages—to execute the L2 batch.\nThis means that there isn’t necessarily a need for the rollup to be “based” in the sense of having a shared sequencer between L1 and L2, which is one of the value propositions of based rollups. However, relying on same-slot L1 messages for L2 execution creates a tight coupling between L1 and L2. As a result, the L2 would need to “reorg together” with the L1 during reorganization events. In practice, only based rollups would accept such reorgs.\nIncentivization\nThe current protocol does not introduce mechanisms to incentivize the L2 proposers to include and consume same-slot messages. These incentives can be implemented on the L2 execution side. We can introduce features like L2 transactions conditioned on inclusion in specific L2 blocks or those with a “decaying” priority fee. Note that such a feature will also be important to solve the fair exchange problem for preconfirmations, as both preconfirmations and same-slot message passing aim to incentivize proposers to include transactions early.\nTowards Arbitrary Reads: Taiko Gwyneth\nOne limitation of this protocol is that the L2 can only read the L1 state of the messaging service contract within the same slot. Can we get reads for arbitrary L1 state, not just the message service contract state, to enable more seamless composability between the L1 and L2?\nGwyneth, a new exciting protocol being developed by the Taiko team, aims to enable such arbitrary same-slot L1 state reads from the L2. An interesting approach under consideration involves introducing a new “introspection precompile” (EIP-7814) that exposes the current transaction trie and opcode counter to the EVM. With this information available, the batch inbox contract can fetch the transaction trie and opcode counter and pass them to the L2 execution. This would enable the L2 to simulate the entire L1 execution up to the batch proposal and compute the middle-of-the-slot L1 state root at the exact opcode counter of the batch proposal.\nRead more about Gwyneth’s design here.\nA rough estimate of L1 gas per each same-slot message import is as follows:\n32 bytes * 16 gas/bytes = 512 gas for the message hash in call data.\n2600 gas for a function call from inbox to L1 message service. We can batch-call the message service for all messages to share this cost.\n1 SLOAD = 2100 gas for reading the message hash in the message service\nHence 512 + 2100 = 2612 gas, plus the 2600 gas cost for calling message service shared among the messages. Furthermore, if we enable L2 proposers to free EVM slots of messages they consumed for same-slot inclusion, they can receive a gas refund, compensating for the additional gas cost.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "rollup",
            "based-sequencing"
        ]
    },
    {
        "title": "EVM in Motoko for Trustless Execution Environments",
        "link": "https://ethresear.ch/t/evm-in-motoko-for-trustless-execution-environments/19981",
        "article": "Hello ethResearch  it has been a while since I posted. Thanks for your patience as I wade back into the eth universe.\nAn organization that I’m running called https://icdevs.org is funding an evm built in Motoko that is targeted to run on the Internet Computer(and that we think will slide well into the AO universe as well). We should have started this 3 years ago, but there is no time like the present. To date this work has been funded through #GG19 and #GG20. The eventuality of this project is trustless execution and consensus agents and the ability to monitor and relay messages between EVMs(and other chains) in a trustless manner.\nThe bounty has reached its first milestone and we’re looking for experienced EVM implementors to tell us what we’ve missed and how to make it better. I realize this forum seems to have moved on to bigger and harder scaling challenges, but I’m hoping you all can point me in the right direction to find the right audience. It is a bit too technical for r/ethereum but may be too basic for this forum and not quite an EIP.\nWhy we are looking to build out an EVM execution layer for the Intenet computer(from our thread at Open - ICDevs.org Bounty #63 - EVM OpCodes - Motoko - 1.9 ckETH - Bounties - Internet Computer Developer Forum )\nThe obvious - we can’t build an EVM in motoko without the op-codes. Now building an evm in motoko isn’t particularly a priority at the moment, but long term the Ethereum Foundation has made it a priority to have EVMs in as many languages as possible as a security feature. Would it make sense to have IC canisters as evm nodes for other chains? Probably depends on network config and a few other things, but I could certainly see it being of value long term. Having the op-codes defined separates the execution concerns from any future project that might want to wire up the rest of the EVM machinery. From building from the ground up you get an EVM that takes the IC’s compute pattern and restrictions into account in ways that existing EVMs written in other languages would need significant rewrites to support.\nGeneral education - These opcodes are an awesome way to learn about stacks, memories, and crypto primitives. Education is the primary goal of ICDevs.org  and we feel like Motoko versions of these libraries would make a really interesting set of examples for people learning about how EVMs work, why they work, and what concepts mirror over into the IC(and which ones don’t).\nLibraries and Integrations - these libraries build on top of a number of other Bounties that we’ve funded that could use some burn-in and integration testing to improve them and make sure they are working properly. GitHub - f0i/merkle-patricia-trie.mo: A Merkle Patricia Trie implementation in Motoko  GitHub - relaxed04/rlp-motoko: RLP implementation on motoko. In addition, some of the op codes implement core functionality that we’ll need to do cross-chain like ecrecover which would be important for a motoko canister trying to verify a signature from the evm universe.\nMicro EVMs - In one universe bitfinity EVMs proliferate and we end up with a garden of highly specialized evms on the IC that interact and interoperate in unique ways. These libraries would allow you to pull in the memory, storage, etc from those EVMs and run transaction simulations to check for opportunities or to automate actions against them using things like the event logs. The always-on nature of IC canisters makes them ideal for writing bots/agents that seek opportunities and execute on them by signing tecdsa messages and relaying them.\nOur bounty hunter has completed the first milestone, arithmetic functions.\nproject file:\nGitHub - icdevsorg/evm.mo: EVM Based Libraries for Motoko\nEVM Based Libraries for Motoko. Contribute to icdevsorg/evm.mo development by creating an account on GitHub.\nmain code file:\ntests:\n",
        "category": [
            "EVM"
        ],
        "discourse": []
    },
    {
        "title": "How to hard-fork to save most users' funds in a quantum emergency",
        "link": "https://ethresear.ch/t/how-to-hard-fork-to-save-most-users-funds-in-a-quantum-emergency/18901",
        "article": "Suppose that it is announced tomorrow that quantum computers are available, and bad actors already have access to them and are able to use them to steal users’ funds. Preventing such a scenario is the goal of quantum-resistant cryptography (eg. Winternitz signatures, STARKs), and once account abstraction is in place, any user can switch to using a quantum-resistant signature scheme on their own schedule. But what if we don’t have that much time, and a sudden quantum transition happens long before that?\nI argue that actually, we are already well-positioned to make a pretty simple recovery fork to deal with such a situation. The blockchain would have to hard fork and users would have to download new wallet software, but few users would lose their funds.\nThe main challenge with quantum computers is as follows. An Ethereum address is defined as keccak(priv_to_pub(k))[12:], where k is the private key, and priv_to_pub is an elliptic curve multiplication to convert the privkey into a pubkey. With quantum computers, elliptic curve multiplications become invertible (because it’s a discrete-log problem), but hashes are still safe. If a user has not made any transactions with their account, then only the address is publicly visible and they are already safe. But if a user has made even one transaction, then the signature of that transaction reveals the public key, which in a post-quantum world allows revealing the private key. And so most users would be vulnerable.\nBut we can do much better. The key realization is that in practice, most users’ private keys are themselves the result of a bunch of hash calculations. Many keys are generated using BIP-32, which generates each address through a series of hashes starting from a master seed phrase. Many non-BIP-32 methods of key generation work similarly: eg. if a user has a brainwallet, it’s generally a series of hashes (or medium-hard KDF) applied to some passphrase.\nThis implies the natural structure of an EIP to hard-fork the chain to recover from a quantum emergency:\nRevert all blocks after the first block where it’s clear that large-scale theft is happening\nTraditional EOA-based transactions are disabled\nA new transaction type is added to allow transactions from smart contract wallets (eg. part of RIP-7560), if this is not available already\nA new transaction type or opcode is added by which you can provide a STARK proof which proves knowledge of (i) a private preimage x, (ii) a hash function ID 1 <= i < k from a list of k approved hash functions, and (iii) a public address A, such that keccak(priv_to_pub(hashes[i](x)))[12:] = A. The STARK also accepts as a public input the hash of a new piece of validation code for that account. If the proof passes, your account’s code is switched over to the new validation code, and you will be able to use it as a smart contract wallet from that point forward.\nFor gas efficiency reasons (after all, STARKs are big), we can allow the STARK to be a batch proof, proving N STARKs of the above type (it has to be a STARK-of-STARKs rather than a direct proof of multiple claims, because each user’s x needs to be kept private from the aggregator).\nThe infrastructure to implement a hard fork like this could in principle start to be built tomorrow, making the Ethereum ecosystem maximally ready in case a quantum emergency does actually come to pass.\n",
        "category": [
            "Execution Layer Research"
        ],
        "discourse": []
    },
    {
        "title": "Releasing Constantine v0.2.0 (Jan 2025), a modular cryptography stack for Ethereum",
        "link": "https://ethresear.ch/t/releasing-constantine-v0-2-0-jan-2025-a-modular-cryptography-stack-for-ethereum/19990",
        "article": "I am very proud to release the very first version of Constantine, a high-performance modular cryptography stack for blockchains and proof systems.\nIt is currently as of July 2024 the fastest implementation of Ethereum-specific cryptographic primitives:\nBLS signatures\nBN254 precompiles (EIP-196 and EIP-197, repriced in EIP-1108)\nBLS12-381 precompiles (EIP-2537)\nKZG Polynomial commitments (EIP-4844)\nConstantine has bindings in C, Go, Nim and Rust.\nHistory\nConstantine is written in Nim, the language was chosen by Status for Nimbus for its expressiveness, its type system strength, the ease to wrap C and C++ and syntactic closeness to Python so that ethereum/research and PyEVM could be ported with ease.\nIn February 2018, after woes with C++ in Nimbus, the first library I built was a fixed precision big integer library for uint256.\nThen we (at Status) realized that we would also need elliptic curves for secp256k1 and BN254 (also known as BN256 or alt_bn128).\nHow hard could it be to implement elliptic curves, with cryptographic hardening, once you know how to write big integers?\nTurned out it was too hard, after a week or so another approach was taken for time-to-market and correctness reasons:\nUse libsecp256k1 from Bitcoin\nPort 1-1 bncurves from Zcash for BN254\nUse Apache Milagro for BLS12-381\nIt was then restarted as a personal side-project in February 2020 after learning a lot from implementing hashing-to-curve and Ethereum BLS signatures and identifying significant performance gap. Note that this predates BLST which was initially released in June 2020.\nSince then Constantine has seen regular contributions (sometimes with couple months gap) up to where it is today.\nEthereum BLS signatures (Consensus Layer)\nBenchmarks are done on an AMD Ryzen 7840U, a low-power ultra-mobile 8-core CPU from 2023.\nNim-blscurve is the backend of Nimbus-eth2. As Nim compiles to machine code through C (or C++), calling C has zero-overhead from Nim.\nRepro.\nInstall the latest Nim version, Nim v2.0.8.\n2 benchmarks will be done with 2 different memory management solutions (different implementations of refcounting)\nBLST is as-of v0.3.12 (May 2024) with runtime CPU features detection\nGCC generates poor code everwhere assembly is not used, hence we force Clang as a compiler.\nFurthermore, it is in theory possible to achieve a 2x performance improvement for signing if there is a need for it.\nKZG Polynomial commitment for EIP-4844 (Consensus Layer)\nI will reuse my benchmarks from Dec, 2023: Productionize KZG EIP-4844 by mratsim · Pull Request #304 · mratsim/constantine · GitHub\nAnd Constantine offers paralellization to improve those numbers 4~6x on my 8-core machine.\nEVM precompiles (Execution Layer)\nNote:\nConstantine also offers a fast MODEXP precompile that reaches 80% to 110% of GMP, without assembly.\nSHA256 is faster than OpenSSL and BLST for data size less than 4MB and within 3% otherwise.\nConstantine achieves over 200Mgas/s for a wide range of cryptographic precompiles on a laptop CPU with restricted power consumption (7840U, 15W to 30W)\nnote, I suggest a repricing for EIP-2537 to help SNARKS applications.\nSecurity\nConstantine, as it names indicates, as a strong focus on security and especially constant-time cryptography is used by default in the core of the library.\nIt HAS NOT been audited yet, but it has undergone extensive fuzzing by Guido Vranken, thanks to the sponsoring of the Ethereum Foundation in Summer 2023. It has also been added to OSS-Fuzz ([bls-signatures] Remove Chia, add Constantine by guidovranken · Pull Request #10710 · google/oss-fuzz · GitHub), the Google 24/7 open-source fuzzing initiative.\nThe Future\nConstantine will follow and support future Ethereum cryptographic needs. In particular I thank the Ethereum Foundation Fellowship Program and Status for sponsoring work on implementing Verkle Tries in Constantine the past year.\nConstantine also supports accelerating Zero-Knowledge proof systems, for example it is possible to use it through PSE (Privacy Scaling Explorations, a branch of the EF) Halo2: ZAL: ZK Accel Layer by mratsim · Pull Request #308 · mratsim/constantine · GitHub.\nConstantine has the fastest MSM on x86, all libraries benchmarked as of July 2024 (Arkworks, Barretenberg, Bellman, Gnark, Halo2) and by a factor 2x over popular Rust libraries Arkworks and Halo2. And I do plan to build proof systems on top.\nHidden in Constantine is a compiler for GPU code generation and there are plans for accelerating ARM.\nNow I don’t know what a snarkified EVM will look like, but I certainly hope to contribute to make it a reality.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": [
            "library"
        ]
    },
    {
        "title": "Gas Fee Schedule update proposal",
        "link": "https://ethresear.ch/t/gas-fee-schedule-update-proposal/21603",
        "article": "Based on our research, we have devised a proposal to radically update the existing gas cost schedule. The exact proposal is available here: https://github.com/imapp-pl/gas-cost-estimator/blob/bf31b21917e399aa51bc20cec651f4d420f9191b/docs/gas-schedule-proposal.md#radical-gas-schedule-proposal.\nWe are working on EIP which includes those changes, but here is the place to gather some thoughts and feedback on the proposal.\n",
        "category": [
            "EVM"
        ],
        "discourse": []
    },
    {
        "title": "Consolidation incentives in Orbit/Vorbit SSF",
        "link": "https://ethresear.ch/t/consolidation-incentives-in-orbit-vorbit-ssf/21593",
        "article": "1.1 Background\nA key proposition of Orbit SSF is that validators rotate based on size, such that those with larger balances are active more frequently, while still giving all validators roughly equal yield. Active validators can be slashed, so large validators will therefore assume greater risk than small validators. Setting aside mass slashing events, a staking pool might prefer to run smaller validators so that a faulty setup can be caught early and affect only a small fraction of its total stake. Yet it is desirable that stakers consolidate when possible—consolidation level will directly influence the economic security that the protocol can offer, ceteris paribus. Therefore, Orbit SSF should provide individual consolidation incentives. These can be combined with collective consolidation incentives that benefit everyone equally upon consolidation.\nExactly how yield should vary with validator size and activity rate has not yet been exhaustively investigated. The Orbit SSF post offers a good starting point, but it would be valuable with a thorough review. It might also be difficult to know beforehand how to distribute incentives such that they do not favor certain validator sizes, thus leading validators to congregate at specific sizes. Therefore, a mechanism for automating and adapting the distribution could be desirable. Furthermore, Vorbit SSF proposes a less linear relationship between validator size and activity rate, which strengthens the case to account for both when designing incentives.\nAnother question is how the magnitude of the consolidation incentives should vary with consolidation level and staking yield, and how consolidation should be quantified. This pertains both to the shape and scale of aggregate incentives. One issue here is that the level of the MEV is unknown to the protocol, while proposal rights still ought to be distributed according to stake as they are today. Another thing to study further is the consolidation level at which incentives should go to zero.\n1.2 Overview\nThis post analyzes how consolidation incentives can be designed to match protocol requirements, offering a systematic framework. This framework consists of two shape variables f_1 and f_2 in the range 0-1 and one scale variable f_y. The consolidation force f_1 varies with consolidation level, calculated from the validator count V at some specific stake deposit size D. It is presented in Section 2. Section 3 then explores the force distribution f_2 that provides individual consolidation incentives to each validator based on their activity rate a, potentially relying also on their size s. These variables are scaled to enact an adjustment to the yield by multiplication with a consolidation force scale f_y, described in Section 4. Focusing on endogenous variables, the attenuating collective incentive can be parameterized as\nand the attenuating individual incentive as\nIf f_1 and f_y are kept identical for both incentives (a realistic design goal), the full equation of the attenuating consolidation incentive y_c becomes\nwhere c is the relative strength of the collective incentive. When including the previously outlined exogenous variables, the consolidation incentive y_c(v) for validator v with activity rate a_v and size s_v becomes\nwhere y' reflects some measure capturing the staking yield, potentially including estimates of the MEV (Section 4 presents several variants). Section 5 examines attenuating, boosting and issuance-neutral modes for the incentives, with attenuating and issuance-neutral modes highlighted as the most interesting. The attenuating mode is used as the primary example in this post, by which y_c updates the target issuance yield for each validator via y'_i=y_i-y_c. Section 6 offers a concluding specification based on the analysis.\nAppendix A presents approximations of the validator count under a Zipfian distribution of staker balances and Appendix B offers a detailed comparison between this post and the incentives design in the Orbit SSF post. Appendix C provides equations for generating validator sets with gradually varying Zipfianess, which are useful when simulating the impact of consolidation.\n2. Consolidation force f_1\nThe consolidation force f_1 is the same for all validators and varies with consolidation level, specifically the validator count V and stake deposit size D, i.e., f_1(V\\!,D). It approaches 1 under poor consolidation and 0 under good consolidation when adopted as an attenuating incentive, which is reversed if applied as a boosting incentive (i.e., 1-f_1). Section 2.1 first provides a simple approximation of when consolidation is in line with expectations, in terms of a Zipfian distribution of stakers. Section 2.2 then explores appropriate shapes for f_1 and Section 2.3 outlines benefits and drawbacks of a dynamic f_1.\n2.1 Approximated validators under a Zipfian quantity of stakers\nIt seems reasonable to relate the consolidation force to some tangible measure of the consolidation level of the dataset. An early assumption is that Ethereum can hope to attract stakers with capital distributed according to Zipf’s law. Once the dataset has reached a corresponding consolidation level, incentives for further consolidation should then arguably be very small. The Vorbit SSF post presents two equations that can be used to compute the number of validators V_Z under a Zipfian distribution of staker balances. These equations are however rather complex and not particularly suitable as part of a protocol specification.\nAppendix A of this current post therefore derives three simplified equations with Figures A1-A2 detailing their accuracy. The log adjusted approximation\nwill be used in this post due to its simplicity and relative accuracy. Given a specific stake deposit size D, the number of validators under a Zipfian distribution of staker balances can thus easily be computed.\n2.2 Consolidation force shapes\nPotential shapes of the f_1 curve will now be defined. The theoretical maximum quantity of validators is V_{\\text{max}}=\\lfloor D/s_{\\text{min}}\\rfloor, where s_{\\text{min}} is the minimum validator size of 32. The theoretical minimum quantity of validators is V_{\\text{min}}=\\lceil D/s_{\\text{max}}\\rceil, where s_{\\text{max}} is the maximum validator size of 2048. It can be noted that the settings for s_{\\text{max}} and s_{\\text{min}} thus influence the output.\nOne example is to decrease the consolidation force linearly with a decreasing validator count (increasing consolidation). An equation for forming such a linear f_1, given a starting point V_{\\text{start}}, is:\nFour examples relying on this equation are shown in Figure 1. The simplest is to extend f_1 across the full range by setting V_{\\text{start}}=V_{\\text{min}} (black line) or to start f_1 at V_Z by setting V_{\\text{start}}=V_Z (red line). Another option is to let f_1 reach 0 at the halfway point between V_Z and V_{\\text{min}} through V_{\\text{start}} = (V_{\\text{min}}+V_Z)/2 (black dashed line). A fourth option is to enforce some specific f_1 at V_Z, here denoted f_z, through the equation\nIn the figure, f_z was set to 0.05 (red dashed line). The benefit of this option  is more precision regarding what remains of the incentives at a Zipfian consolidation level, whereas the first and to a lesser extent the third option will see f_z drift slightly across D (compare with the linear approximation of f_1 in Appendix A). The reason for letting the incentive reach 0 earlier than at V_{\\text{min}}, here by relying on V_Z, is to promote fairness for small validators at sufficient consolidation levels. A potential reason to avoid setting f_1=0 as high as V_Z is that it precludes an equilibrium at V_Z if many stakers will only consolidate when they earn a higher yield from it, which is probably a reasonable assumption (more on this below).\nAn incentive that is linear with respect to the proportion of the stake that is active, D_a/D, has been explored previously. Appendix B offers a comparison with that strategy. The linearity is particularly appealing for collective consolidation incentives, where the derivative of the curve represents the incentive. Stakers gain from a reduction in f_1, but its magnitude does not by itself affect the incentive to consolidate.\nFigure 13113×1948 271 KB\nFigure 1. Four examples of a linear consolidation force f_1, reaching zero at different validator counts.\nA more versatile alternative begins with the first step as previously:\nA second step is then applied to produce a sigmoid:\nThe parameter t controls where the sigmoid’s main transition occurs, and the power (here 2) adjusts its steepness. Figure 2 illustrates a sigmoid (power 2) that pushes the transition to the left, close to V_Z, by settng t=5. The start point was set to V_{\\text{start}} = (V_{\\text{min}}+V_Z)/2 (as in the dashed black curve of the previous Figure 1).\nFigure 23113×1948 188 KB\nFigure 2. A consolidation force with a sigmoidal shape, which has an adjustable steepness and transition point.\nA steep sigmoidal consolidation force is more relevant to consider for individual incentives, where the f_1 magnitude regulates the yield differential between stakers and thus represents the consolidation incentive. For collective incentives, the slope of the f_1 magnitude instead represents the consolidation incentive. A compromise that can be used both for collective and individual incentives would be to mix the linear and sigmoidal shape.\nA mixed shape can be generated through the equation\nThe first part of the equation creates the sigmoidal shape, which is weighed by w against the linear shape in the second part. Figure 3 shows an example with an equal weighing w=0.5, once again with t=5 and the same V_{\\text{start}} at the halfway point between V_{\\text{min}} and V_{\\text{start}} as previously. Note the rather fixed slope between 400k and 1M validators, associated with the linear shape.\nFigure 33113×1948 195 KB\nFigure 3. A mixed consolidation force, weighing together the linear and sigmoidal f_1 curve.\nJust as with the other options, a question to consider is whether f_1 should go to zero already around V_Z, or if it is reasonable to leave some small incentive in place throughout the full range. The latter option seems perhaps more attractive since, presumably, some individual incentive must remain in place under equilibrium. In other words, setting the individual incentive to zero at V_Z precludes this point from being reached if it is not otherwise a profitable option for staking service providers (SSPs). But the existence of any remaining collective consolidation incentives at V_Z can then also factor in.\n2.3 Fixed or dynamic schedule\nConsolidation incentives can have a fixed or dynamic schedule. Under a fixed schedule, a specific validator composition at some quantity of stake will give a specific f_1 as illustrated in Figures 1-3. With a dynamic schedule, there is also a time component, and f_1 will adjust slowly towards its stipulated target. The dynamic schedule can still have the same type of f_1 curve as a target, as previously exemplified for the issuance reward curve, with a typical gradual shift captured here.\nThe most natural choice is to have a fixed schedule with no time component. The reason is the same as when it comes to the reward curve: the focus is the effect in the long run. The curve can capture the sought balance between fairness and the incentive to consolidate without involving time as a parameter. However, if the protocol wishes to puruse more extreme measures, such as for example forcing a Zipfian distribution by otherwise letting the f_1 of the individual incentive (f_1f_2f_y) approach infinity, then a dynamic schedule is required. But such a strategy can lead to very high yield differentials between small solo stakers and delegating stakers under equilibrium. It is then instead preferable to strike a balance between the need for consolidation and the need for fairness via f_1 curves similar to those outlined in this post.\nShould there be particular concerns around stakers temporarily altering the validator composition, for example in response to changes in MEV or as a means for discouraging other stakers, it is of course possible to let the change to the long-run force be applied gradually when a shift occurs. Besides additional complexity, an additional risk is that if the f_1 does not adjust quickly enough with changing circumstances, there is a risk of stakers “overshooting” the natural equilibrium consolidation level.\n3. Force distribution f_2 for individual incentives\nThe force distribution f_2 distributes the consolidation force across the validator set, forming individual incentives. In the baseline “attenuating” mode, a validator with a specific f_2 receives an individual yield attenuation of f_1f_2f_y. In the boosting mode it receives a boost of f_1(1-f_2)f_y under the same f_2 curve, but this section will be centered on the attenuating mode (a further review of mode is presented in Section 5 wih a comparison in Table 1). Validators of the maximum size will generally have f_2=0 and thus no attenuation. Validators of the minimum size will generally have f_2=1 and thus receive the maximum attenuation f_1f_y. In essence, given a yield differential f_1f_y between the biggest/most active and smallest/least active validators, the role of f_2 is to determine the effect on all validators in between.\nThere are two main avenues for how to compute f_2 for each individual validator:\nAID (Section 3.1): incentives-differential based on an applied notion of fairness such as related to activity rate or validator size. This solution is the easiest to implement.\nCID (Section 3.2): cumulative incentives-differential as an overlay on top of AID, that distributes the designed incentives based on a sorted list of validator sizes. This is an attempt to adjust rewards if certain validator sizes become too favorable, but it is more complex.\nA third option “BID” (Section 3.3) attempts to strike a balance by computing both an AID and a CID, letting f_2 be a weighing of both.\n3.1 AID\nWith AID, the protocol adjusts the yield based on risks assumed or space taken up by validators. A natural assumption is that risks vary with activity rate a (the proportion of the time that a validator is active as an attester), which in turn varies with validator size s. If the activity rates are not the same for validators when they attest to the available chain and the finality gadget, but still vary between validators for both attestation duties, a unified measure would need to be defined. This section will review four options for the f_2 curve:\nSec. 3.1.1; f_2(a): attenuate the yield based on an estimate of assumed risks derived from a.\nSec. 3.1.2; f_2(s): simulate a validator fee based on the size s of the validator.\nSec. 3.1.3; f_2(a) or f_2(s) or f_2(a, s): rely on a log-scaled measure with equidistant reduction across either a or s or both.\nSec. 3.1.4; f_2(a, s): use a weighed average of the first two options.\nSection 3.1.5 finally compares the options and plots them.\nDefine the activity rate of validator v as a_v. An intuitive solution favored previously is to let f_2 capture inactivity percentage by deducting the activity rate\nThe equation can be re-scaled to always have the force extend across the full range. Define the minimum possible activity rate as a_{\\text{min}} and the maximum as a_{\\text{max}}. The f_2 for a validator with activity rate a_v can then be computed as\nThe type of linear scaling of f_2 presented in the above two equations results in the following feature: since stakers pay specifically for inactivity, an agent with an endowment to be staked—realizing some specific expected active stake—is affected equally regardless of which exact distribution of validator sizes it selects. For example, an agent staking 4096 ETH using one 2048-ETH validator and sixty-four 32-ETH validators will get almost the same expected active stake as when opting to run four 1024-ETH validators, and thus almost the same f_2 on average across its stake. Any other combination yielding the same amount of active stake will yield the same average f_2.\nWith a focus on activity as risk, this seems like a natural solution. There are many nuances to the question of how activity rates and validator compositions actually translate to risk. Further dialogue with staking service providers could provide important perspectives and more elaborate measures.\nFrom the protocol’s perspective, should the two example distributions in Sec. 3.1.1 actually be treated equally? The second option with 4 1024-ETH validators might seem better, given that the first option instead results in 65 validators. It is preferable to be able to fully finalize the 4096-ETH endowment by providing space for only 4 validators in total in one slot, as opposed to requiring space for 65 validators. What would an equation that instead is neutral w.r.t. validator count look like? Define the size of a validator as s_v. The sought equation is then\nwith validators of size 32 receiving the full attenuation (f_2=1). This equation can also be re-scaled to always have the force extend across the full range:\nThe inversely proportional f_2 can be thought of as simulating a classical “validator fee”, and the mechanism is neutral with regard to the number of validators that some specific endowment is distributed across. The fee must be taken via an inverse relationship because of how the consolidation force is constructed to be deducted from the earnings. If the same f_2 was applied to every validator, a bigger validator would have to pay a higher nominal fee, given that its total rewards are higher. The inverse scaling rectifies that by weighing across stake. One way to understand the equation is to consider the outcome for 2048 ETH staked using any validator size. The total fee for 64 32-ETH validators then becomes 64 times higher than for 1 2048-ETH validator, the fee for 32 64-ETH validators becomes 32 times higher, etc (ignoring the re-scaling normalization). Another way to understand it is that f_1f_y comes to stipulate the validator fee for one 32-ETH validator. Running a 64-ETH validator costs the same in total as a 32-ETH validator, and thus f_1f_y must be halved when distributed across twice the stake.\nIn Orbit SSF with a 2048-ETH threshold, a_v and s_v will be linearly related, so the equation could then easily also be framed in terms of a_v:\nHowever, this will not work if thresholding at for example 1024, or in Vorbit SSF with a more elaborate connection between validator size and activity rate, as illustrated in Figure 22 of that post.\nWhile the inversely proportional scaling across s_v could lead to smaller validator sets, it might seem less “fair” than the linear construction across a_v. An agent in control of less stake might overlook its lower yield if the distribution specifically is designed to compensate validators for being more active, thus assuming greater risk. On the other hand, fairness could however also be constructed to reflect the strain that a staker puts on the protocol.\nNote further that a distribution that is neutral w.r.t. expected active stake (Sec. 3.1.1)  will give very similar outcomes for validator sizes ranging between 32-128 ETH (see also Figure 4 in Section 3.1.5). A distribution that is neutral w.r.t. number of validators (Sec. 3.1.2) will give very similar outcomes for validator sizes ranging between 512-2048 ETH. It might instead seem desirable for stakers with a compromise distribution where f_2 is affected equally at any point from the same log-scaled change to either a_v or s_v. Such a force distribution, scaled to span the full range, is\nor\nor a weighed combination of the two. This construction avoids the more extreme outcomes of the previous two ideas.\nA fourth option is to weigh together the linear (3.1.1) and inversely proportional (3.1.2) constructions. A benefit is that both have a clear interpretation, which then extends to the weighed measure. The equation becomes\nassuming a_{\\text{max}} \\neq a_{\\text{min}}, with f_2=0 otherwise. The average shown in yellow in Figure 4 in the next subsection sets w=0.5. Note that this option is particularly useful when the relationship between a and s is less linear (e.g., Vorbit SSF).\nFigure 4 shows the four options previously presented using the validator sizes and activity rates of Orbit SSF thresholded at 2048, which is also s_{\\text{max}}, such that a and s always are linearly related. Green and red $f_2$s are neutral w.r.t. estimated risk or occupied validator spots, and this gives fairly “lopsided” distributions. The log-scaled (blue) and average (yellow) distributions are less lopsided (at least from a log-scale perspective) and will see a more or less equal impact on f_2 when a validator doubles in size.\nFigure 43136×1637 337 KB\nFigure 4. Four options for the force distribution f_2, plotted under a baseline Orbit SSF thresholding. In green, a distribution that is linear in a and neutral w.r.t. the expected active stake of some given endowment. In red, a distribution that is inversely proportional to s and neutral w.r.t. the number of validators used for some endowment. In yellow, the average of the green and red force distributions, and in blue a log-scaled distribution.\n3.2 CID\nComputing the f_2 via AID is simple and intuitive, but two things can be noted:\nIf the mechanism does not model risks correctly, validators might congregate at sizes that produce the best risk-adjusted rewards, and the distribution may excessively diverge from some natural Zipfian state.\nThe protocol’s priorities may change with consolidation level, focusing on how much space a staker takes up under low consolidation and focusing on fairness w.r.t. risk when the consolidation level already is good.\nPoints (1) and (2) can be addressed by applying a cumulative incentives-differential (CID) as an overlay on top of a specific AID policy. First, a neutral validator composition is defined and the applicable AID policy at this composition specified. Then, validators are sorted according to size and given the f_2 associated with their specific position in the sorted list.\nA reasonable approach could be to define a Zipfian distribution of validators as neutral, such that the total ETH per log-spaced bin is fixed. Note here, that such a composition is a fair bit less consolidated than that generated from a Zipfian distribution of stakers, where each staker with more than 2048 ETH divides its stake into several big validators. Compare for example with Figure 2 in a previous post analyzing a Zipfian distribution of stakers.\nDefine a Zipfian distribution of validators as neutral and apply the CID across a log-scaled AID. Outcomes for various validator compositions with this specification are shown in Figures 5-10. As evident from Figures 5-6, when the validator distribution is Zipfian, a validator shifting its size will see its f_2 change just as under a pure log-scaled AID (blue line in Figure 4). If the consolidation level is less than Zipfian, the f_2 is closer to the inversely proportional distribution (red line in Figure 4). If the consolidation level is better than Zipfian, the f_2 is instead closer to the linear distribution (green line in Figure 4). The latter scenario is illustrated in Figures 7-8. Thus the protocol’s priorities shift with consolidation level, as discussed in point (2) above. To align even closer with that point in the case where the baseline Orbit weighting is not pursued, there would also need to be a gradual shift in focus from s to a as consolidation improves. One could consider (1-f_1)a+f_1s or something slightly more refined.\nFigure 53497×2148 327 KB\nFigure 5. Force distribution f_2 under a Zipfian validator set plotted against the number of validators per bin.\nFigure 63496×2148 285 KB\nFigure 6. Force distribution f_2 under a Zipfian validator set plotted against total ETH per bin.\nFigure 73497×2148 302 KB\nFigure 7. Force distribution f_2 under a validator set that skews larger,  plotted against the number of validators per bin.\nFigure 83496×2148 286 KB\nFigure 8. Force distribution f_2 under a validator set that skews larger plotted against total ETH per bin.\nIf validators congregate at some specific size, it could imply that this size offers the best risk/reward. The force distribution will then automatically adapt, with the goal of stipulating a “fairer” yield for validators. An example is provided in Figures 9-10, with two peaks in validator sizes and a corresponding reaction in the force distribution. For illustrative purposes, the example is rather pronounced. In reality, there would likely be more subtle peaks and valleys, and thus more subtle changes to the f_2.\nFigure 93497×2148 318 KB\nFigure 9. Force distribution f_2 under a validator set with local peaks,  plotted against the number of validators per bin.\nFigure 103496×2148 299 KB\nFigure 10. Force distribution f_2 under a validator set with local peaks plotted against total ETH per bin.\nIt might be beneficial to use a discretized measure of validators’ active balances at the boundaries of 32 ETH and 2048 ETH, to avoid micromanagement of validator balances. For example, all validators between 32-33 ETH and between 2040-2048 ETH might receive the same f_2, computed as the mean of the lowest and highest f_2 among validators within the range. Discretization at other ranges can be considered for computational reasons, but would presumably only lead to more micromanagement of balances.\nSince CID introduces additional complexity, its benefits must be significant if it is to be adopted. It might then seem reasonable to first ship a pure AID if the overall strategy is to be pursued. The CID overlay can always be introduced if necessary at a later time.\n3.3 BID – balancing between AID and CID\nNaturally, a mixed measure of AID and CID, here referred to as BID, can be considered. This is an attempt to avoid problematic aspects of both variants when used in isolation. Both an AID f_2 and a CID f_2 are thus computed for the validator, and the final f_2 is a weighing of the two. It would then seem natural to use the AID underlying the CID as the measure for the AID.\n4. Consolidation force scale f_y\nThe consolidation force scale f_y determines how much the yield should change given a consolidation force between 0 and 1. It can also be used for scaling total yearly issuance, then denoted f_I. This is the final parameter of the three used for consolidation incentives. Recall from previously that in the attenuating mode, the full equation for collective incentives is\nand for individual incentives it is\nThe appropriate f_y will depend on the currently offered yield, how much MEV that is available relative to the yield, perceived risk of being active as a staker, the composition of the staking set, etc. Factors such as the existence of MEV burn at the time of adoption can therefore influence the appropriate scale (see further discussions in Sections 4.2 and 4.4 below).\nToo weak consolidation will not sufficiently encourage consolidation. Too strong individual incentives will be interpreted (rightfully so) as Ethereum treating smaller validators unfairly. Strong collective (and individual) incentives can lead to frictions among consensus participants if the action of one party can have great influence on the yield of another party. For example, if there are no individual incentives, and some SSPs deconsolidate to lower their risk while significantly reducing yield for everyone, this might lead to particularly strong frictions.\nFour designs of f_y are outlined in the following subsections. The right approach will depend on the shape of the reward curve, and a change in issuance policy could thus influence which approach that is the best.\n4.1 Fraction of the issuance yield\nOne approach is to scale by a fraction of the issuance yield\nor correspondingly by a fraction of the issuance\nSetting k_1=0.1 gives a 0.1% reduction in yield if the issuance yield is 1% and the force is 1.\nThe idea behind using a fraction of the issuance yield is that the equilibrium staking yield implies something about the risks of staking. If the equilibrium yield is high, a larger incentive might be required to encourage consolidation than if the yield is low. Stakers are thus charged some fraction of their income for reducing their risk (or occupying consensus spots). One downside is that issuance yield is not the only reward currently befalling stakers: MEV accrues to all stakers equally and this income will not be reflected in k_1y_i. Another issue is that the equilibrium yield also compensates for other more general “costs” of staking (e.g., hardware) and those staking costs are much less affected by the activity rate of a validator.\n4.2 Fixed ETH/year\nAnother idea is to derive the scale as a fixed ETH/year through\nor correspondingly\nSetting k_2=60\\,000 would give a 0.1% reduction in yield at 60M ETH staked if the force is 1.\nIf rewards are dominated by MEV (including priority fees) and the MEV remains roughly constant, then this way of scaling the incentives will work rather well. The staking yield still varies with deposit size because the fixed MEV must be distributed to all stake. The consolidation incentive will thus adapt with the staking yield without attaching too much weight to the less relevant issuance yield.\n4.3 Fixed yield, regardless of issuance and MEV\nA third option is to specify a fixed yield component, i.e., simply\nor correspondingly\nSetting k_3= 0.001 would give a 0.1% reduction in yield if the force is 1.\nWith this design, the nominal incentive will remain the same, regardless of the level of the staking yield (if f_1 and f_2 stay the same). If risks associated with being active as a staker remain constant, such that an equilibrium shift in yield does not imply a change to such risks, then this approach can be reasonable. It could also be suitable if the stake supply curve is rather flat and remains fixed. In this case, any change in MEV would be offset by a shift in deposit size, such that the equilibrium yield also remains fixed. A probabilistic argument can also be made for the design under a shifting supply curve, with a reward curve rather similar to the present one. An equilibrium at a high deposit size is then likely associated with an increase in MEV (because the supply curve would otherwise need to fall improbably low). The suggestion is that the most probable staking yield across deposit size is more fixed than what the reward curve alone would indicate.\n4.4 Mixed f_y – fraction of y_i and fixed ETH/year\nThe force scale can be a mix of several of the previous suggestions, because they can complement each other. For example, the specification could be\nor correspondingly\nWith the previously discussed settings and D = 60\\text{M} ETH, if an issuance yield of 1% is offered at this deposit size, the outcome would be\nthus 0.2%. The suggested mixed f_y can be regarded as an attempt to relate the consolidation incentive to both issuance and MEV yield, and thus to the overall staking yield, which seems like a desirable objective. This can be expressed a little differently, as\nwhere y'_v is k_2/D, but reframed as an estimate of the MEV yield. In this formulation, k_1 regulates the strength of both y_i and y'_v—that together represent the staking yield. The MEV cannot currently be directly computed (“seen”) by the protocol, and would thus need to be updated outside of its domain. What could be considered is to have some agreed-upon way to compute y'_v, or for that matter to leave it fixed at the current level and commit to only change it if MEV burn is instituted. As an example, say that the force scale is set to k_1 = 20\\% of the staking yield. The equation then becomes 0.2(y_i + y'_v). As a guideline, 310k ETH of MEV was distributed via MEV boost during 2023. It fell to around 220k ETH in 2024. Relying only on MEV boost, a rough long-run estimate of y'_v at 34M ETH staked could thus be\nThe estimate always pertains to yearly MEV and not MEV yield specifically. The deposit size D is known by the protocol, just as issuance yield, and need not be estimated. The issuance yield under ideal consensus performance at 34M ETH staked is around 2.85%, and the maximum incentive would thus be\ni.e., 0.7%. This is a rather strong incentive, albeit only activated under a completely deconsolidated validator set (f_1=1). Even if there is a desire to ultimately pursue such a strong incentive, it might still be reasonable to start out softer, at k_1\\leq 0.1.\n5. Attenuating, boosting, or issuance-neutral incentives\nThere are three possible modes for individual incentives: attenuating, boosting or issuance neutral (relative to the reward curve); and two modes for collective incentives: attenuating or boosting. Collective incentives can never be issuance neutral since the point of the mechanism is to collectively (for everyone) adjust rewards with consolidation level. This section reviews benefits and downsides of the possible modes and also highlights equivalences between them. The conclusion is that attenuating incentives are the most preferable, and that issuance-neutral incentives are the second best option (applicable to individual incentives). For clarity, Table 1 specifies the baseline equation for the different modes, assuming that f_1 and f_2 are computed as in Sections 2-3, with the issuance-neutral equation described in Section 5.2.1.\nTable 1. Baseline equations for the different modes for collective and individual incentives, when the f_1 and f_2 are computed as described in Sections 2-3, and with issuance-neutral equation described in Section 5.2.1.\nThe mode of the collective incentive is of less importance than the mode of the individual incentive, since the attenuating and boosting modes in the collective incentive can be made equivalent. Let a higher reward curve A be attenuated and a lower reward curve B be boosted, and set the difference between them as the force scale f_y (ignoring complications regarding its computation). Further let y_c = f_1(V)f_y for A and y_c = (1-f_1(V))f_y for B under any V. Then, y_i-y_c for A will equal y_i+y_c for B:\nThe equality is trivial to achieve, and the chosen mode will then have no influence on the actual incentives for consensus participants.\nHowever, there is still some relevance to the mode. With an attenuating mode, the reward curve comes to stipulate the maximum possible issuance—an important feature to have plainly encoded. With a boosting mode, the maximum possible issuance can still be deduced, but it requires adding f_y to the reward curve. The reward curve then instead stipulates the minimum possible issuance under ideal performance when the staking set is deconsolidated—a somewhat less important feature. While the minimum possible yield is relevant both to stakers and as a general design criterion, the actual outcome is influenced also by luck in special duties assignments (e.g., block proposals), the actions of other consensus participants, etc. In either case, the minimum possible yield under ideal performance can still be calculated for attenuating incentives by deducting f_y.\nIf the collective incentive is re-parameterized without altering the reward curve, it will affect the maximum issuance under boosting incentives and the minimum issuance under attenuating incentives with ideal attestation. In this case, for boosting incentives, the two possible outcomes are an increased maximum issuance (very sensitive) and a decreased maximum issuance (less sensitive). For attenuating incentives, the two possible outcomes are an increased minimum issuance under ideal performance (less sensitive) and a decreased minimum issuance under ideal performance (sensitive). Politically, having to increase the maximum issuance (or alter the reward curve) to achieve stronger collective incentives would likely be the most problematic. However, a reduction to the minimum would also be politically sensitive. This indicates that boosting incentives make future changes more socially costly.\nAs an aside, note that any premeditated change to collective incentives is problematic. Developers should encode the utility-maximizing incentive from the beginning. If it is known or suspected that f_1 or f_y will under some outcomes be adjusted, for example, a decreased boost under good consolidation, then the collective incentive to consolidate does not actually exist in the first place (c.f. the ratchet effect in production strategy).\nAnother thing to consider is re-parameterization due to changes in y'_v. If pursuing the parameterization in Section 4.4: f_y=k_1(y_i+y'_v), the maximum or minimum can change if y'_v changes. Once again, the sensitivity to increases and decreases must be considered, in particular the downside of having an increase in peak issuance any time y'_v increases.\nBefore comparing modes in individual incentives, the issuance-neutral mode will be presented. In the attenuating mode, f_2 is in the range 0-1, with small/inactive validators close to 1 and big/active validators close to 0. The consolidation incentive y_c derived from f_1f_2f_y is then subtracted from the targeted issuance yield for each validator. The “issuance neutral” approach computes a global linear displacement f'_2 = f_2-d such that aggregate issuance is left unaffected, while preserving the yield differential between small (inactive) and large (active) validators. This ultimately boosts the yield for large/active validators through a negative f_2 and attenuates it (but relatively less) for small validators, with a (smaller) positive f_2. Specifically, the mean of all f'_2\\text{s} when weighted by size becomes 0. Let s_i be the size of validator i and f_2(i) its force distribution. The issuance-neutral displacement d can then be computed from the equation\nGiven that d applies equally to all validators, it can be understood as reflecting a collective incentive. In this context, it is however better described as an “anti-collective incentive”. Whatever aggregate shift in yield the attenuating or boosting individual incentive would produce, the variable d restores—acting equally for everyone—leaving the aggregate unaffected by consolidation level. Observe that this implies that boosting and attenuating individual incentives also contain some collective incentives—a boosting individual incentive will increase the aggregate yield and an attenuating individual incentive will decrease it. The issuance-neutral approach removes any trace of collective incentives from the individual incentives.\nThe role of c in the joint equation for consolidation incentives y_c = f_1f_y(c+f_2) can be compared with the role of the displacement d imposing f_2-d. The difference is that c is added whereas d is subtracted, and thus c \\equiv -d. The collective part of the attenuating or boosting individual incentive is thus made explicit. It follows that if Ethereum uses both collective and individual incentives, they ought to be analyzed jointly. With this in mind, note that the boosting individual incentive in Table 1 from the beginning of the section ends up f_1f_y higher than then attenuating counterpart. They are thus separated by a collective component, and can be made equal through a shift f_1f_y.\nNote that c is fixed beforehand and d is computed from—and varies with—f_2. The strength of c can be attuned to the protocol’s needs, but an adjusted strength can also be used for d. This adjustment can be stipulated as d' = dk_d. With 0<k_d<1, d' will see the colletive part of the individual incentive positioned in between the issuance neutral and attenuating mode. Finally, it can be mentioned that to the individual validator, the aggregate is naturally of less direct importance, and the direct change to its f_2 is what matters—at least before any associated shifts in the equilibrium take place.\nTwo benefits of issuance-neutral individual incentives can be identified, which are more relevant when combined with lower or no collective incentives. Firstly, protocol issuance will end up closer to the issuance level stipulated by the reward curve (regardless of consolidation level), which arguably improves design clarity. Secondly, one suggested approach to Ethereum’s issuance policy is to ensure that diligent 32-ETH solo stakers always receive positive regular rewards. At the same time, it is desirable to set the reward curve close to 0 at high deposit ratios. Issuance-neutral individual incentives allow Ethereum to compress the yield of small validators under low consolidation closer to the yield offered under good consolidation, such that the reward curve can be stipulated closer to 0 while still ensuring positive regular rewards. The f_1 is highest under low consolidation. At this point, there are more small validators than big, and the small validators rewards will only need to be reduced slightly to preserve neutrality (the few large validators’ rewards will at the same time be raised more substantially).\nTo illustrate the compressed yield differential from issuance-neutral individual incentives for smaller 32-ETH validators, validator sets with smoothly varying Zipfianess were generated. A detailed description of the method is presented in Appendix C. Each validator set was generated by feeding the equally spaced uniform distribution u in the range [0, 1] into the equation\nThe composition of each validator set is in this equation governed by the exponent \\alpha, where \\alpha=-1 leads to a Zipfian distribution of validators. Appendix C.1 illustrates the effect of \\alpha. The integral of the equation relates validator size V to deposit size D, and the closed-form solution can therefore be used to approximate the number of validators given a specific D and \\alpha as:\nAppendix C.2 presents the derivation. Figure 11 illustrates the effect of an issuance-neutral policy on 32-ETH validators at 80M ETH staked. It is created for \\alpha between -6 and 2, at each point calculating the approximate V from the previous equation, generating the validator set at the specific \\alpha, and computing the issuance-neutral displacement d. The linear f_1 that reaches 0 at (V_{\\text{min}}+V_Z)/2 was used, and the log scaled f_2. The new issuance-neutral force f' for a validator with displaced force f'_2 then becomes f'=f_1f'_2.\nFigure 113058×1948 245 KB\nFigure 11. Simulation of an issuance-neutral force aggregation for 32-ETH validators. The attenuating force can have f_2=1 for small validators at any consolidation level, and the maximum f is thus also 1 (thin black line). The issuance-neutral f'_2 (dashed blue line) only approaches 1 when almost no validators are of the minimum size, and after multiplication with f_1 (dotted blue line), the aggregated f' (black line) peaks at around 0.1.\nAs illustrated by the figure, f'_2 for 32-ETH validators will only approach 1 under consolidated validator sets. Larger validators then hold a clear majority of all ETH, and a small gain for them must be offset by a large reduction for small validators to preserve the yield differential under an issuance-neutral policy. But f_1 should at that point be set close to zero since the validator set already is consolidated, and a large yield differential would be perceived as unfair. The issuance-neutral aggregated force f' for 32-ETH validators therefore never exceeds 0.12. This allows the reward curve to be pushed much closer to zero while ensuring positive regular rewards than when using an attenuating policy (where f will approach 1 under low consolidation).\nIndividual incentives should have a high yield differential when the validator set is deconsolidated and this is instituted by letting f_1 approach 1. A key problem under boosting individual incentives is therefore the rise in aggregate yield (further discussed in Section 5.2.1): deconsolidation is collectively rewarded. Note that this is opposite to how boosting collective incentives work (as evident from reviewing the equations in Table 1), where consolidation is rewarded instead (f_1 must then approach 1 if the validator set is consolidated). However, just as with the issuance-neutral incentive, the interaction of f_1 and f_2 must be accounted for. Define the average f_2 across the validator set, weighed by stake, as \\bar{f_2} (this is in fact the same measure as previously computed for d). The average force per staked ETH \\bar{f} by which issuance will be collectively boosted (after scaling by f_y) is shown in Figure 12, using the same validator distributions and shapes of f_1 and f_2 as previously for Figure 11. Since d offsets a 32-ETH validator with an f_2 of 0 (Figure 11) and the boosting incentive relies on 1-f_2 (Figure 12), Figures 11-12 appears identical, eventhough they illustrate two different concepts. The fall in 1-\\bar{f_2} as consolidation level decreases moderates the actual collective outcome of the boosting incentive, with issuance even falling beyond 1M validators. This highlights that even with a boosting individual incentive, issuance may not rise substantially.\nFigure 123058×1948 243 KB\nFigure 12. Analyzing collective effects of individual boosting incentives. The force per staked ETH peaks at around 0.12, which—scaled by f_I—is the maximum increase in issuance possible for the linear f_1 and log-scaled f_2 with the distribution of Appendix C.\nFigure 13 instead shows the average force per staked ETH \\bar{f} by which issuance will be collectively attenuated. Both f_1 and \\bar{f_2} rises as consolidation falls, which is reflected in \\bar{f}. The implication is that the “collective” portion of the individual attenuating incentive becomes much stronger than its boosting counterpart.\nFigure 133058×1948 265 KB\nFigure 13. Analyzing collective effects of individual boosting incentives. The force per staked ETH approaches 1 as , leading the issuance to be reduced by at most f_I.  peaks at around 0.12, which—scaled by f_y—is the maximum increase in issuance possible for the linear f_1 and log-scaled f_2 with the distribution of Appendix C.\nStakers can potentially actively deconsolidate to directly profit under boosting individual incentives. A staker holding many big validators who divides one of them into several smaller validators will see all remaining validators become more profitable. If the local derivative of the f_1 curve is large, and no attenuating collective incentives are in place, the gain among the remaining validators can come to offset the loss from the deconsolidated validator. For these reasons, if there are no collective incentives, certain implementations of boosting individual incentives would be very problematic. When there are collective incentives, their magnitude must be accounted for—recall from Section 5.2.1 that boosting individual incentives gives f_1f_y higher yield than attenuating individual incentives for all stakers (this is also evident by the fact that the black lines in Figures 12-13 sums to be equal to the dotted blue line). The general recommendation is to not pursue boosting individual incentives unless care is taken to never incentivize deconsolidation.\nHow about issuance-neutral individual incentives? It can be noted that since consolidation level will not affect issuance, the loss for a staker who deconsolidates a big validator will be precisely offset by an aggregate gain for all other validators. This alleviates concerns about a collective gain under deconsolidation, but a deconsolidating staker could still benefit. As f_1 increases, the yield will increase for big validators and fall for small validators, and a staker with many big validators may theoretically gain from deconsolidation—if the local derivative of f_1 is large. These concerns are however not comparable to those present for the boosting mode, and the issuance-neutral mode should not be ruled out given that it also brings benefits. Edge cases must however still be examined, but by adopting c>0 or changing d to d' with 0<k_d<1 (Sec. 5.2.1), such issues can be overcome.\nNote that the equilibrium effect after stakers adjust their positions due to the change in yield will be smaller than the direct effect. An increase in f_1 from the initial deconsolidation will incentivize other stakers to consolidate, pushing f_1 back down. The equilibrium outcome can be a theoretical concern even in the attenuating mode. In this mode, a staker with many big validators can deconsolidate to push down the yield for small validators in the hope that some stop staking, leading to an increase in staking yield for the remaining big validators. While rather esoteric, the equilibrium outcome will at least be much more similar for the different modes than the direct outcome—the yield differential between big and small validators is kept fixed regardless of mode under the same f_1. However, the equilibrium outcome is more speculative, can be affected by the collective component in the individual incentive, and takes effect more slowly.\nThe issues concerning collective incentives discussed in Section 5.1 apply also to individual incentives. This is due in part to the traces of collective incentives also present in individual incentives outlined in Section 5.2.1. Boosting individual incentives will also increase the maximum possible issuance. In particular, it is unfortunate that attenuating individual incentives will specifically push down the issuance of 32-ETH validators. The strong point of issuance-neutral incentives is that they do not increase the maximum possible issuance, while at the same time not worsening the outcome for 32-ETH validators too much in the worst-case scenario. This “compression” of the yield differential for 32-ETH validators was presented in Section 5.2 and illustrated in Figure 11.\nFinally consider discouragement attacks with the additional twist of only targeting validators of a specific size. If small validators are targeted, some of them will stop staking and consolidation will improve. An attacker can thus gain from the collective consolidation incentive. An attenuating individual incentive will however lead to a direct increase in yield specifically for small validators upon consolidation, mitigating some concerns. Note further that an attenuating individual incentive makes it relatively more profitable for small validators to stage this attack (in terms of the direct effect), and a boosting incentive makes it relatively more profitable for large validators. If large validators are targeted, the collective consolidation incentive brings down the yield, so this attack is therefore only a concern under boosting individual incentives.\n6. Concluding specification\nA systematic framework for consolidation incentives in Orbit/Vorbit SSF has been presented. Benefits and drawbacks of various settings for the three consolidation forces f_1, f_2 and f_y were provided, and their optimal mode reviewed. Based on that analysis, the most beneficial settings will now be suggested, resulting in an overarching specification for how to incentivize consolidation.\n§1. The post is centered around the attenuating mode with the main equation\nThe shape of the consolidation force is determined by f_1, the scale by f_y, the distribution between validators by f_2 (thus the f_2 is unique to each validator size) and the relative strength of collective incentives by c. The resulting consolidation incentive y_c differs with validator size and is used to update the target issuance yield for each validator:\n§2. While it is not strictly necessary for f_1 and f_y to be the same for individual and any collective incentives, it is a realistic design goal to pursue.\n§3. Let the shape of f_1 be based on the mixed equation, parameterized as outlined in Section 2.2 and captured in Figure 3. This is a versatile equation that can be adjusted to fit requirements as the architecture nears adoption.\n§4. Use a fixed schedule for f_1 with no dynamic time component, for simplicity (Section 2.3).\n§5. Let f_2 be a pure AID as a starting point. If relying on a Vorbit design, or generally any design with a less linear relationship between validator size and activity rate, use the mixed\npresented as the yellow curve in Figure 4. Setting w=0.5 seems like a natural default choice (an even balance between focusing on active stake and occupied validator spots), but w>0.5 is also viable and arguably more fair. Even if the direction is to focus on active stake out of fairness concerns, w=0.8 should be sufficient to ensure that stakers opt for 2048-ETH validators over 1024-ETH validators when Orbit is thresholded at 1024, if they have the capital to do so. If the mixed measure is unnecessary, for example in Orbit SSF thresholded at 2048, it is also viable to use the log-scaled\npresented as the blue curve in Figure 4. Two log-scaled f_2\\text{s} computed both for s and a could also be averaged/weighed together under Vorbit SSF.\n§6. The primary option is to let f_y be defined according to the mixed equation f_y=k_1(y_i + y'_v) with a long-run estimate in y'_v that is to be adjusted only after a substantial shift in MEV (e.g., after implementing MEV burn). It is preferable to start at a modest scale, i.e., k_1\\leq0.1. Determining f_y based on a fixed ETH/year (Section 4.2) seems like the second-best option.\n§7. The primary option is to use the attenuating mode both for individual and any collective incentive. However, the issuance-neutral approach also brings certain benefits due to its ability to compress the yield variability for small validators. It should be considered and analyzed further once a general design has been settled on. This analysis should rule out and parry edge cases of profitable deconsolidation outlined in Section 5.3.1.\n§8. Consider using only the individual incentive, with no pure collective incentive, i.e., c=0. As noted in Section 5.2.1, the individual incentive still has a collective effect (directionally correct for attenuating incentives). If tensions in the staking set are not a concern, a relative modest collective incentive can be considered.\nAppendix A – Approximated V under a Zipfian quantity of stakers\nAs outlined in Section 2.1, it seems intuitive to scale the strength of the consolidation incentives according to the Zipfianess of the staking set. For example, when the validator set is consolidated in line with a Zipfian staking set (when V \\approx V_Z), consolidation incentives should be very small, to uphold fairness. It is therefore desirable to have an estimate of the number of validators under a Zipfian distribution. The number of validators V, assuming a pure Zipfian staking distribution with stakers consolidating all available stake, can be calculated in accordance with the equations of Appendix A in the post on Vorbit SSF. Given a specific D, the Zipfian staking set size is first computed as\nwhere W denotes the Lambert W function and \\gamma is the Euler–Mascheroni constant, approximately 0.577. If the minimum validator balance is 32 ETH, the Zipfian validator set size under full consolidation becomes\nIt would be desirable to have some simpler approximation of V_Z instead of these rather complex equations, given that Zipfianess in itself merely is a rough guideline and that the equation would be part of the consensus specification.\nFor large x, the Lambert W function behaves approximately as W(x) \\approx \\ln x - \\ln \\ln x. An approximation of N_z is therefore\nwhere the numerator increases linearly with D and the denominator logarithmically with D. The linear term comes to dominate with large D, where the denominator changes relatively slowly. In the second of the first two equations, deriving V_Z from N_Z, the linear term once again dominates. A linear approximation should therefore come fairly close, but a log adjusted or loglog adjusted approximation would be more accurate. Figure A1 illustrates agreement with the Zipfian ground truth for the following four approximations:\nConstants were selected to favor integers and simplicity. For example, setting the constant in the log adjusted equation to 19.9 instead of 20 brings the results slightly closer in line with the ground truth. However, a constant of 20 still keeps the difference roughly equal across the range at around 1000, and is perfectly sufficient for our purposes. The fourth option is included since all consensus clients already have implemented a function for taking the square root of D when computing the current reward curve. If the minimum validator balance changes, the general equations\nand\ncan instead be used.\nFigure A13171×2121 297 KB\nFigure A1. Various approximations of a pure Zipfian distribution.\nFigure A2 shows the difference to the computed V_Z for the approximations in a more precise relative comparison. As evident, the loglog adjusted approximation comes very close, but the other approximations are also perfectly sufficient for the purpose of this specification.\nFigure A23213×2121 250 KB\nFigure A2. The difference between the outlined approximations and the ground-truth computation. The log adjusted equation produces a fairly even distance from the ground truth of 1000 validators and the loglog adjusted equation is nearly perfect.\nB.1 Collective incentives as a function of D_a\nA comparison can be made with the incentives design previously proposed in the Orbit SSF post. That post outlines another strategy for collective consolidation incentives, where issuance depends directly on active stake I(D_a) as opposed to total stake I(D).\nAn equivalence must be based on the equation for the reward curve, such that the stipulated issuance at I(D) can be reflected back to the issuance at I(D_a) via the attenuation. Use the consolidation force scale defined in Section 4.1: f_I=k_1I with k_1=1. This means that the total attenuation I_c of issuance becomes I_c=f_If_1=If_1. The goal is to determine the f_1 that lets I - I_c become I(D_a) under the current reward curve cF\\sqrt{D}, which is cF\\sqrt{D_a} when using D_a. The equation is first expanded:\nThe consolidation force f_1 must then simply reflect back to the point D_a on the issuance curve. Therefore, if D_a/D stake is active, f_1 for the current reward curve must be\nThe equivalence is illustrated continuing from previously:\nAs a numerical example, say that half the stake is active. This means that the f_1-function must map f_1(1/2)=1-\\sqrt{1/2}. Starting from cF\\sqrt{D}=cF\\sqrt{2D_a} (since half the stake is active), the operation becomes\nThe reader is encouraged to review the Orbit SSF analysis that also reviews similarities between I(D) and I(D_a), reaching results that can be linked to those presented here with the aid of the equations  Appendix B.2. One detail worthy of further consideration is that f_1 is computed based on the number of validators V and not active stake D_a. When using V, the issuance curve will not be influenced by the Orbit specification. For example, should the threshold T in vanilla Orbit be adjusted, D_a would be adjusted, and thus issuance/consolidation incentives. There are both potential benefits and drawbacks of this.\nWhen T=2048, the relationship between V and D_a is simple, given that the average validator size can be written both as\nand\nThis means that the number of validators is\nB.2 Collective incentives as a function of D\nThe appendix of the Orbit SSF post further suggests the alternative I(D)\\frac{D_a}{D}, which is equivalent to using f_I=k_1I (from Section 4.1) with k_1=1 and f_1 = 1-D_a/D:\nThe possible link between D_a and V was further presented in Appendix B.1. A more general equivalence with references to the formulation in the Orbit SSF post is to set f_1 = 1-\\delta(D_a/D).\nB.3 Individual incentives\nConcerning individual incentives, the Orbit SSF design can again be understood as f_y=k_1y_i with k_1=1. There is also a suggestion of limiting the scale to 25% of the issuance yield, i.e., similar to setting k_1=0.25. The equation corresponding to f_2 is linear in a, as in the first equation in Section 3.1.1, with the normalized version shown as the green line in Figure 4. The Orbit SSF post however suggests boosting yield as opposed to attenuating it, which explains the difference to the first equation in Section 3.1.1. Boosted individual incentives are highlighted as problematic in Section 5.3.1, due to the collective gain from deconsolidation. In isolation, they should thus be avoided, but it can be noted that the Orbit SSF post employs a strong collective incentives that compensates. The effect is similar to the compensatory effect discussed in Section 5.2.1.\nThere is finally also the notion of letting a term corresponding to f_1 reach 0 at some sufficient level of consolidation, here at D_a/D=0.8. This can be directly compared with letting the f_1-curve reach 0 once a Zipfian distribution of stakers has been achieved in terms of V.\nC.1 Simulated validator distributions across \\alpha\nThe equations for simulating a smooth variation in Zipfianess from Section 5.2.3 can be useful beyond this specific study. This appendix therefore provides some further insights and details for the previously presented equations. Begin with the generation of validators\nand consider the Zipfian scenario when \\alpha=-1. In this case, the exponent becomes 1/-1=-1, and the equation\nThe first uniformly spaced sample of 0 will produce a validator of size 1/(1/32)=32 and the last sample of 1 will produce a validator of size 1/(1/2048)=2048, as required. The inverse of the equal spacing in u serves to generate a harmonic series. The equation thus recreates the baseline approach in Section 2.1 of the Vorbit SSF post (this time applied to validators instead of stakers), but in a form that can be generalized to non-Zipfian distributions varying smoothly with \\alpha. When \\alpha=-1, around half the validators will have a balance below 64, as expected of the harmonic series. When \\alpha<-1, validators are spaced tighter at low balances, leading to a less consolidated validator set, and when \\alpha>-1, validators are spaced tighter at high balances, leading to a more consolidated set. Figure C1 shows the outcomes for \\alpha=-1.9 (green), \\alpha=-1 (orange), and \\alpha=-0.1 (blue) in terms of total ETH located within each histogram bar. Note that \\alpha=0 must always be avoided in simulation since 1/\\alpha would go to infinity.\nFigure C13200×1948 262 KB\nFigure C1. Histograms weighted by stake for three validator distributions at 80M ETH staked. A higher \\alpha gives a distribution that is more consolidated, and \\alpha=-1 gives a Zipfian distribution of validators.\nC.2 Approximating V at any given D\nThe integral of the equation from Appendix C.1 can be used to capture the relationship between the number of validators (think of it as samples across the x-axis in Figure C1) and the total amount of ETH (the resulting area under the curve):\nThe closed-form solution is\nThe integral can be understood as stipulating the average validator size, given that it covers the range 0-1 in the uniform distribution u. It follows that the deposit size can be generated as D=VI_u and thus I_u=D/V. Substituting in gives\nand the approximation can finally be solved for V as\n",
        "category": [
            "Proof-of-Stake",
            "Economics"
        ],
        "discourse": [
            "single-slot-finality",
            "consensus-incentives",
            "validator-consolidation"
        ]
    },
    {
        "title": "Ethereum Macroeconomics via Dynamics",
        "link": "https://ethresear.ch/t/ethereum-macroeconomics-via-dynamics/21539",
        "article": "This is a cross-post from the 20Squares blog, written by my colleague @eric-downes as part of an Ethereum Foundation grant:\nWe are deeply grateful to the Ethereum Foundation for their support of this work.  I, the author, feel indebted to the profound patience and forbearance of @randomishwalk and my colleagues at 20Squares. This work has benefited from conversations with @dpl0a and Philipp Zahn, as well as @randomishwalk, Andrew Sudbury, @adietrichs, @MacBudkowski, and Ignat Insarov.\nThis is the first of two posts on Ethereum macroeconomics.\nEthereum has grown into a major economic force; between its native asset Ether (ETH), the smart contract ecosystem this supports, and the Layer-2 blockchains, a conservative valuation might be half a trillion dollars.  At the core of Ethereum’s “brand”, distinguishing it from other smart contract platforms, is the consistent effort put into decentralized governance.  Via its consensus mechanism no central authority can censor a transaction, freeze the native asset of a user, etc.  This brand commitment depends in turn on a sufficient diversity of validators staking ETH to participate in consensus.\nThe share of Ether staked by “centralized” staking services, such as exchanges and Liquid Staking Providers (LSPs) is considerable, and continues to grow.  This has provoked concerns, among Ethereum researchers that the future of Ethereum might involve a confluence of three interrelated challenges\nNearly all Ether becomes staked.\nInflation becomes excessive.\nGovernance becomes centralized.\nupon which we focus.  The view of inflation emphasized in this work in particular feels quite different to us, than the views expressed for instance, in this very helpful review podcast.\nLookahead\nIn this blog post we address the first of these concerns “runaway (near 100%) staking” s\\to1 and how it relates to the second, using a “stock and flow” macroeconomics model built with guidance from dynamical system theory.  In contrast with other research, we find inflation playing a positive role in moderating runaway staking, but eventually inflation must subside, along with the moderation it provides.\nIn the second post, we look more closely at governance centralization and discuss a means for evaluating macroeconomic interventions inspired by bifurcation theory.  Briefly, we are not optimistic that reducing issuance will prevent governance centralization, either.\nIn both posts, we provide a few code examples using ethode, a thin units-aware wrapper we built around scipy.integrate.solve_ivp, to streamline model evaluation.  Readers desiring to follow our derivations, dive into technical mathematical points not covered here, run their own simulations, or learn some dynamical systems are recommended to look at our ethode guide, which contains References section below.  The guide is certainly a work in progress, but should have enough to get you going.\nFor The Impatient!\nIssuance does not all get dumped into native unstaked Ether.  Some portion of it is reinvested by staking businesses at ratio r; indeed this process is coded into Liquid Staking Token (LST) smart contracts.   It is important to distinguish between transient behavior, such as speculation in staking, and medium/long-term behavior, such as the reinvestment of staking rewards by staking businesses.  When staking is dominated by reinvestment instead of speculation, inflation persistently decreases.\nWe use our macroeconomics model to identify a friend “Mr.LI;ELF”: “Medium r. Low Inflation; Even Lower Fees”.  Without Mr.LI;ELF convergence to a desirable future without runaway staking is unlikely.  Strong deflation, in which the magnitude of deflation exceeds the reinvestment of transaction fees, probably corresponds to unstable dynamics.  Under zero or weak deflation, the tendency toward runaway staking can be moderated only by high churn and/or slashing.\nIn contrast with Mr.LI;ELF staked ETH fraction approaches to a value above moderate reinvestment ratio r.  How far above depends on the “ELF” part.  Thus, runaway staking can be avoided only while inflation is held\n\nlow enough, that concerns over inflation do not dominate the reinvestment of profits by staking businesses at equilibrium, \\left(\\frac{dr}{d\\alpha}\\right)^\\star<0 (no news here) but simultaneously\n\n\nhigh enough to not be numerically dominated by the reinvestment of priority fees and MEV, as a fraction of unstaked Ether; \\alpha^\\star\\gtrsim r^\\star f^\\star.\n\nlow enough, that concerns over inflation do not dominate the reinvestment of profits by staking businesses at equilibrium, \\left(\\frac{dr}{d\\alpha}\\right)^\\star<0 (no news here) but simultaneously\nhigh enough to not be numerically dominated by the reinvestment of priority fees and MEV, as a fraction of unstaked Ether; \\alpha^\\star\\gtrsim r^\\star f^\\star.\nInflation will eventually decay though, driving the equilibrium point itself s^\\star\\to1, though it may take considerable time.  This “L2 future” has been recognized by many others: most Ether is staked, with the majority used for settlement of L2 rollups.  We’ll discuss it more next time.\nGiven all the above, we advise caution.  Intervening to reduce the issuance yield curve seems quite capable of exacerbating the very problems we seek to avoid.\nModeling an Open Zeppelin[1]\nFirst, a warning!  In an act of hubris, and with apologies, but not without reasons[2], we have chosen S to refer to Staked ETH, while others have at times used S for “circulating (S)upply”, which we call instead A, so s=S/A.  Please proceed!\nEthereum as a balloon with compartments.1920×1189 97.6 KB\nConsider a “balloon” with variable internal compartments.  The average size of each is measured by stocks\n(S)taked Ether (participating in consensus) is a compartment, as is - (U)nstaked unburnt Ether,   – containing the (V)alidator reward queue. - (\\cancel{O}) is all irrecoverable (burned, lost, etc.) Ether, and - (A)ccessible/Circul(A)ting Ether supply, A=S+U\\approx120.4\\times10^6 in Dec 2024. - \\mathcal{Q}_\\pm the Ether in the staking (+) and unstaking (-) queues\nThe net change in time of a stock is written using a dot, such as \\frac{dA}{dt}:=\\dot{A}[3], the net change in accessible Ether supply.  Stocks grow or shrink based on flows which add to or subtract from their derivatives.  Here all flows are positive real numbers with units [ETH/yr].\nBy averaging over “long” timescales (at least quarterly)[4] we approximate the staking and unstaking queues as equilibrated, and average over many cycles of the erratic base fee oscillations.\nSo, our conceptual model:\n\\displaystyle \\begin{array}{rcl} \\dot{A} &=& I - B - J\\\\ \\dot{V} &=& I + P - R - K\\\\ \\dot{U} - \\dot{V} &=& K + Q_- - Q_+ - F\\\\ \\dot{S} &=& R + Q_+ - Q_- - J\\\\ \\end{array} \nFlows (B,J,Q_-,\\ldots) have a “domain” (U,S,S,\\ldots), where the flow is coming from, and a “codomain” (\\cancel{O},\\cancel{O},U,\\ldots), where the flow is going to.[5:1] Flows obey constraints, often expressed as (in)equalities relating a flow to its domain.  In case you’ve forgot or are skimming, (co)domains are summarized in this glossary.\nIn response to the concerns about s\\to1, the recent Deneb upgrade implemented EIP 7514, an upper limit on R+Q_+ chosen so as to not limit any present flows. We also ignore the pre-existing symmetric limits on (un)staking Q_\\pm.  The purpose of our models is to show, in the absence of such limits, where the dynamics push the system.  If you wish to study an extreme of dynamics post-Deneb, you could make R+Q_+ constant; we will revisit EIP 7514 in our next post in this series, on staking composition.  So the constraints (Q_-<S,Q_+<U) could be tightened significantly, but more accurate upper limits would play little role in our analysis.\nA few flows deserve specific comment: I and (R,Q_+,K).\nBounding Issuance\nAll of these stocks and flows, (I,S,\\ldots), are moving time-averages over spot values (I^\\bullet,S^\\bullet,\\ldots) defined at a given block.  For issuance, we’ll assume that\n\nIssuance is sublinear 1\\ll I\\approx yS\\ll S[7] to avoid discouragement attacks, and that\n\n\nThe large-stake scaling of yield (like, on a log-log plot) is not substantially altered by time averaging \\frac{\\partial{d\\log{y}}}{\\partial{d\\log{S}}} \\approx\\frac{\\partial{d\\log{y}^\\bullet}}{\\partial{d\\log{S^\\bullet}}}.[8]\n\nIssuance is sublinear 1\\ll I\\approx yS\\ll S[7] to avoid discouragement attacks, and that\nThe large-stake scaling of yield (like, on a log-log plot) is not substantially altered by time averaging \\frac{\\partial{d\\log{y}}}{\\partial{d\\log{S}}} \\approx\\frac{\\partial{d\\log{y}^\\bullet}}{\\partial{d\\log{S^\\bullet}}}.[8]\nThe first is common and almost certainly an overestimate with I\\leq{yS} more precise.  We reason as follows.  We can express spot issuance as a known function of the yield curve I^\\bullet = y^\\bullet S^\\bullet.  From this we obtain an inequality for the quarterly-averaged issuance I\\leq yS using time-covariance.\n\\displaystyle \\begin{array}{rcl} I &=& \\frac{1}{\\tau}\\int_{t-\\tau}^ty^\\bullet S^\\bullet dt'\\\\  &\\approx& yS + \\frac{1}{\\tau}\\int_{t-\\tau}^t(y^\\bullet-y)(S^\\bullet-S)dt'\\\\  &=& yS - |COV(y^\\bullet,S^\\bullet)|\\\\ I &\\leq& yS \\end{array} \nOther approximations are also possible, see guide. This should work for any positive definite yield curve with finite slope, erring in a conservative direction without explicit dependence on the present curve y^\\bullet = y_0(1)/\\sqrt(S^\\bullet) with y_0(1)\\approx166.3/yr.  We deem this a good direction in which to err in light of our results concerning (the lack of) runaway inflation.\nBounding Reinvestment\nReinvestment of staking rewards by validators R is achieved by staking a new validator from existing rewards, or post-Electra increasing the stake on an existing validator.  While clearly a stochastic process, we approximate the net effect as smooth on timescales of at least \\tau.  R represents a feedback loop S\\overset{+}{\\rightsquigarrow}S quite evidently related to the potential positive-feedback between staking and inflation that people have found concerning.\nTo express this concept succinctly in one flow variable, we require that the averaging timescale \\tau be adjusted upward until most validators claim and reinvest the bulk of their staking rewards within it.  That is \\dot{V}=0, so R+K=I+P so R\\leq I+P.  The quantity r=R/(I+P), the ratio of staking rewards reinvestment over issuance and priority fees is one of the distinguishing features of our model, and also why we have split the staking queue flow R+Q_+. These are our motivations:\nModeling r is absolutely necessary to model LSTs.[9] 2. We want to separate the transient externally-driven dynamics Q_+ from the long-term endogenous feedback R,[10] 3. r could be measured and monitored with onchain data, and 4. Low r might be hazardous.\nIf the \\tau required to achieve r=R/(I+P)<1 in practice becomes too large, one might refine the approximations used to model issuance, or use data to better model \\dot{V}.  In neither case do we expect this to make a huge qualitative difference for the issues considered here, but please, prove us wrong!\nIntensive Flows give Dynamical Systems\nFlows obey inequalities, usually as a fraction of the source, except for r,b. We convert these inequalities; for each uppercase extensive flow (J,F,B,\\ldots) we define a lowercase *intensive variable[11] (\\jmath,f,b,\\ldots) with [units]: the fractions [1] and fractional rates [1/yr].  In forming these, the ideal is to apply the tightest available bounds that still capture the asymptotic behavior[7:1] in the limit of interest S\\to A.  We do not assume the intensive parameters are constant, but suppress their dependence for readability.  Unless otherwise stated, the intensives are functions of the dynamical variables and time, so the burn: b(A,S,t)=B/F.[12]\nThe use of intensive variable parameters and the approximation \\dot{V}\\approx0 allows us to reshape our conceptual model into one that is defined in its own dynamical and intensive variables, a dynamical system.  We’ll build this up one step at a time.  As you follow along you may find it useful to look at models in python.  You can use these extremely rough estimates of constant parameters along with A_{now}\\approx120e6,\\ s_{now}\\approx.3 as a start for the models that follow.\n(S,U); not just a UNIX Command!\nWith the above, you should be able to construct the following (S,U) system:\n\\displaystyle \\begin{array}{rrrlcrl} \\dot{S} &=& \\ \\ (ry-\\jmath-q_-) & S & + & \\ \\ \\left(q_++r(1-b)f\\right) & U\\\\ \\dot{U} &=& \\left((1-r)y+q_-\\right) & S & - & \\left(rf+(1-r)bf+q_+\\right) & U\\\\ \\end{array} \nThese coefficients of staked S and unstaked ,U ETH are miserably complicated-looking, but as written they are all (but one) positive, and so we can reason about this model’s evolution. Specifically, so long as ry(S)>\\jmath+q_- staked ETH S just continues growing and growing.  In contrast it is harder for U to get as big, limited by its own loss term -\\left(rf+(1-r)bf+q_+\\right) U.  At some point in the (far) future S becomes big enough that ry(S)<\\jmath+q_- and the system becomes capable of oscillation, depending on parameters and a zoo of partial derivatives.[13]\nThese dynamic variables are kind of boring, but critically and unlike (A,S), there are no extra conditions (such as S<A) that we haven’t told the math about.  The equations are not stiff; they can be simulated without too much pain, though we always recommend to backup a simulation result with some analysis: numerical regimes can miss important dynamics when perturbation series are insufficient.  Most importantly, the variables of interest in the ongoing debate are functions of S,U, so we lose nothing by calculating them post-simulation; we’ll demonstrate how.  Even as we change dynamical variables for intuition building, we recommend using models like (S,U) as a base for simulation whenever possible.\nInflation\nInflation is used to refer to many things, but here we mean specifically the quarterly fractional change in accessible Ether. Consider \\alpha:=\\dot{A}/A\\approx(I-B-J)/A in light of the above table.   In general and under the existing yield curve we have (where \\beta=bf=B/U):[14]\n\\displaystyle \\alpha\\ \\approx\\ y(sA)s-\\beta(1-s)-\\jmath s \\ =\\ y_0(1)\\sqrt{s/A}-\\beta(1-s)-\\jmath s \nYou can explore this by adding alpha(), sfrac() as @output methods\nPersistent inflation cannot maintain\nA key feature of \\dot{A} under the current yield curve y_0(S) is sublinear issuance I\\leq yS\\lesssim S, chosen to avoid discouragement attacks. Because of this, positive inflation cannot maintain indefinitely.  We will demonstrate with the existing yield curve, but the argument is general.  Unusually for this blog post, we show most of the steps so the argument is hopefully understood.  Elsewhere we use I\\approx yS, but here we use I\\leq yS as greater rigor is appropriate.\n\\displaystyle \\begin{array}{rcl} dA = \\alpha Adt  &\\leq&  \\left(ys-\\beta(1-s)-\\jmath s\\right)Adt\\\\ dA  &\\leq&  ysAdt = y_0(1)\\sqrt{sA}dt \\leq y_0(1)\\sqrt{A}dt\\\\ \\int_{A(0)}^{A(t)}A^{-1/2}dA  &\\leq&  \\int_0^{\\ t} y_0(1)dt\\\\ \\left.\\frac{1}{2}\\sqrt{A}\\right|^{\\sqrt{A(t)}}_{\\sqrt{A(0)}}  &\\leq&  y_0(1)t\\\\ A(t)  &\\leq&  \\left(\\sqrt{A(0)}+2y_0(1)t\\right)^2\\\\ \\therefore A(t) &\\lesssim& t^2 \\ll  e^{kt} ~\\forall ~\\mathrm{const.}~k>0 \\end{array} \nThe last line is Vinogradov asymptotic notation, used in the rest of this post[7:2] For two positive functions, g dominates f, written f(t)\\ll g(t) just when \\lim_{t\\to\\infty}[f(t)/g(t)]=0. When the limit is a non-zero constant we say f\\sim g. We use \\gg/\\ll/\\sim for numbers as well, by which we mean the order of magnitude is much larger / much smaller / similar.\nThe point.  Since supply A(t) is eventually less than a powerlaw of t, it is subexponential.  Thus, no positive rate of Ether supply expansion can maintain indefinitely.[15]\nThis does not mean we would find every intermediate inflation rate pleasant.  Following surges in Q_+ and/or drops in supply, inflation can accelerate quite alarmingly.  A good example will be the Ethereum staking-mania following the 2132 Atlantia-v-Eurasia market crash, in which 99% of present-day Ether will have been burned.\n\nWe aren’t excited to hodl through multiple decades of 10% inflation, and we expect you aren’t either!  Silliness aside, we encourage you to find more realistic scenarios in which such sustained inflation occurs.\nWe mean to separate concerns, not dismiss inflation as a problem. Unpleasantly high inflation in the medium term, even if that “medium term” lasts decades, is a dynamics problem, not an equilibrium problem, and so dynamical solutions (like EIP 7514) seem better suited.  Unfortunately we will see that given the above, s^\\star\\to1 is an equilibrium problem.\nStaking Fraction\nWe have equations for \\dot{A},\\dot{S}, what about \\dot{s}=d(S/A)/dt?  Using the quotient rule \\dot{s}=\\frac{\\dot{S}}{A}-s\\frac{\\dot{A}}{A}, and after an algebraic massage, we obtain for staking fraction\n\\displaystyle \\begin{array}{rcl} \\dot{s} &=& y(sA)\\ (r-s) + \\\\  && \\left[q_++f(1-s)\\left(bs +(1-b)r\\right)\\right]\\cdot(1-s) + \\\\  && \\left[\\jmath(1-s+r)+q_-\\right]\\ (0-s). \\end{array} \nThe coefficients of (r-s),\\ (1-s),\\ (0-s) are variable but positive. Recalling how s increases just when \\dot{s}>0, these terms draw s toward respective points r,1,0.  We emphasize that the action of yield y is x\\to r, which may not be the same as x\\to1.\nAs expressed by the quotient rule, an increase in staking fraction can be driven by more people staking, and/or it can be driven by a reduction of the inflation rate.  The latter can be achieved in principle by a reduction of issuance relative to the base fee “burn rate”.  Because of this quotient rule tradeoff, issuance plays a beneficial “infrastructure” role in moderating staking fraction, drawing it toward r, which can be less than one.\n(A,\\alpha,s) Dynamical System\nThere are many reasons, especially in the context of the world economy, to care about total circulating supply A.  Recent discussions however have focused most on inflation \\alpha=\\dot{A}/A, the growth in supply over time.  It also turns out that modeling inflation \\alpha directly simplifies our analysis of the (A,s) system considerably.\nLet subscripts denote partial derivatives x_y:=\\frac{\\partial{x}}{\\partial{y}}.  Using the correct partial derivative relations for variables (A,\\alpha,s,t)[3:1] we have\n\\displaystyle \\begin{array}{rcl} \\dot{A} &=& \\alpha A\\\\ \\dot{s} &=& \\alpha(r-s) + (rf+q_+)(1-s) - (q_-+(1-r)\\jmath)s\\\\ \\dot{\\alpha} &=& \\xi\\dot{s} - \\gamma\\alpha s +\\chi \\end{array} \nWhere the new greek letters are fractional rates, defined below, and y':=\\frac{dy}{dS}.\n\n\\mu:=\\beta_\\alpha(1-s)+\\jmath_\\alpha s is the implicit   sensitivity of the inflation loss-term to increases in inflation.   We judge 0\\leq\\mu; if anything inflation increases burn and   slashing fractional rates.[16]\n\n\n\\xi:=(y+y'A+\\beta-\\beta_s(1-s)-\\jmath-\\jmath_s)/(1+\\mu) is the net   correlation between changes in s and changes in \\alpha   normalized by 1+\\mu. \\xi can be of either sign.  Under the   current yield curve y_0+\\frac{dy}{dS}A=y_0(sA)(1-1/(2s)), which   changes its sign at 50% ETH staked.\n\n\n\\gamma:=\\jmath_{\\log{A}}s+\\beta_{\\log{A}}(1-s)+s|y'|A is a   positive coefficient expressing how quickly \\alpha\\to\\alpha^\\star,   and the partials are constant when initial supply is known.[17] We   have extracted the sign from the final term because sublinear   issuance implies y'<0; under the current yield curve the term   sA|y'|=\\frac{1}{2}y_0(sA).\n\n\n\\chi:=-\\jmath_ts-\\beta_t(1-s) represents externalities affecting the   inflation loss term encoded as explicit time-dependencies.  We   neglect externalities \\chi\\approx0 because we have nothing   intelligent to say about them, but you might not want to.\n\n\\mu:=\\beta_\\alpha(1-s)+\\jmath_\\alpha s is the implicit   sensitivity of the inflation loss-term to increases in inflation.   We judge 0\\leq\\mu; if anything inflation increases burn and   slashing fractional rates.[16]\n\\xi:=(y+y'A+\\beta-\\beta_s(1-s)-\\jmath-\\jmath_s)/(1+\\mu) is the net   correlation between changes in s and changes in \\alpha   normalized by 1+\\mu. \\xi can be of either sign.  Under the   current yield curve y_0+\\frac{dy}{dS}A=y_0(sA)(1-1/(2s)), which   changes its sign at 50% ETH staked.\n\\gamma:=\\jmath_{\\log{A}}s+\\beta_{\\log{A}}(1-s)+s|y'|A is a   positive coefficient expressing how quickly \\alpha\\to\\alpha^\\star,   and the partials are constant when initial supply is known.[17] We   have extracted the sign from the final term because sublinear   issuance implies y'<0; under the current yield curve the term   sA|y'|=\\frac{1}{2}y_0(sA).\n\\chi:=-\\jmath_ts-\\beta_t(1-s) represents externalities affecting the   inflation loss term encoded as explicit time-dependencies.  We   neglect externalities \\chi\\approx0 because we have nothing   intelligent to say about them, but you might not want to.\nWe don’t recommend modeling this numerically; these equations are stiff.  We use it for mathematical analysis only.\nObserve that every term in (\\xi,\\gamma) are small fractions, fractional rates, or derivatives thereof.  Indeed, when \\alpha\\approx0 and sensitivities are weak, |\\xi|\\sim\\gamma\\sim y obtains.  Generally if |\\xi|,\\gamma\\ll1 then the derivatives obey |\\dot{\\alpha}|\\ll|\\dot{s}|: a separation of timescales.  For durations when this obtains, one or more periods at intermediate times, inflation can be usefully approximated as a parameter instead of its own dynamic variable, with staking fraction equilibrating to s^\\star more quickly than \\alpha^\\star equilibrates.\nDoes this hold presently?  For a sanity-check, a quick look at YCharts since Sept 2022 (the Merge) shows that s,\\dot{s} do indeed seem to vary over a much greater range than (\\log{A},\\alpha) under Proof-of-Stake.\nThe staking fraction from YCharts1462×769 181 KB\nThe inflation rate from YCharts1322×453 80.8 KB\nFor the remainder of this post, we will assume this obtains. Anecdotally, even when it does not, the revealed interplay between inflation and staking fraction shows up, and is a very important concept for Ethereum macroeconomics.\nRecall our approximate equation for the fraction of staked ETH s, in which all coefficients are positive but inflation \\alpha:\n\\displaystyle \\dot{s} = \\alpha\\ (r-s) + (rf+q_+)\\ (1-s) + (q_-+(1-r)\\jmath)\\ (0-s) \nSo assuming |\\dot{\\alpha}|\\ll|\\dot{s}|, let us examine the fixed point s^\\star during a period in which we may treat inflation as constant \\alpha=\\alpha_{const}.\n\\displaystyle s^\\star =    \\frac{r^\\star(\\alpha_{const} + f^\\star) + q_+^\\star }{    m^\\star := (\\alpha_{const} + r^\\star f^\\star + q_+^\\star + q_-^\\star + (1-r)\\jmath^\\star) } = 1 - \\frac{(1-r)\\jmath+q_-}{m^\\star} - \\frac{\\alpha_0(1-r^\\star)}{m^\\star} \nWe will see that without \\alpha>0 an interior market equilibrium is very unlikely.  First, as a thought experiment, consider \\alpha=0.\nConcerning Churn\nA fixed point s^\\star(\\alpha=0)<1 would require either high churn or a persistent unstaking/capitulation of existing validators q_-+\\jmath>0.  Validators try to minimize slashing as intended, but what would a large unstaking flow q_-s\\gtrsim rf at s^\\star mean?\nHigh unstaking requires churn, a persistent supply of new validators to take their place q_+^\\star(1-s)\\sim q_-s\\gtrsim rf, or it is only a transient and q_-^\\star\\ll rf; recall that reinvestment by existing validators is not counted in q_+.  Persistently high unstaking q_-\\sim rf could only describe a market equilibrium if one group of stakers was actively capitulating and withdrawing their stake, while another group with a higher r were aggressively reinvesting in their business, and their reinvestment of fees and MEV offset the unstaking, adjusted for inflation.\nHigh churn, not counting reinvestment, cannot maintain forever: eventually there will be no new Capitulators left, and s^\\star must once again grow as required by the Reinvestors’ higher r, so s^\\star was not a fixed point at all.  So q_-^\\star\\ll (rf)^\\star.\nSimilarly, at some point everyone who wants to stake should have staked.  If we judge the \\tau-averaged flows due to the issuance of new humans and the burn rate of legacy humans to be small, additional validators count overwhelmingly toward r^\\star and q_+^\\star\\ll(rf)^\\star.  Thus, the fixed point s^\\star simplifies to\n\\displaystyle s^\\star \\approx r^\\star \\frac{\\alpha_{const} + f^\\star}{     \\alpha_{const} + r^\\star f^\\star + (1-r^\\star)\\jmath^\\star} \nWe will explore the stability of this fixed point below, and based on the range of \\alpha categorize three basic behaviors.\nStability in One Dimension\nFixed-points are market equilibria just when they are stable.[18] Local stability is easy to asses for one dimensional maps.  In general a fixed point is locally stable when small changes (perturbations) shrink over time.  For a continuous map \\dot{x}(x) like ours, this concerns the derivative of the RHS at the fixed point.  If it is negative, then small perturbations shrink and the fixed point is a stable sink, and x “flows”[19] toward it.  If the derivative is zero, the fixed point is a degenerate center, unrealistic outside of physics.  If positive, the fixed point is an unstable source and repels x.\n1D Stability Conditions1526×1159 141 KB\nSpecifically for staking fraction, we want the sign of \\left.\\frac{\\partial\\dot{s}}{\\partial s}\\right|^\\star to determine whether s^\\star is (un)stable.  We will be ignoring the partial derivatives (“sensitivities”) by assuming they are small in comparison with their corresponding intensives.[20] The full no-churn stability condition, including variations in \\alpha is (with subscripts denoting partials)\n\\displaystyle (\\alpha^\\star/f^\\star)\\ +\\ r\\ +\\ (\\jmath^\\star/f^\\star)\\ \\ >\\ \\ \\log{r}_{\\log s}^\\star\\left[ (\\alpha^\\star/f^\\star)\\ +\\ (1/r^\\star-1/s^\\star) (\\alpha/f)_r^\\star\\right] \nIf we assume that sensitivities are dominated by their respective intensives, this reduces to a simple \\alpha^\\star+r^\\star f^\\star \\gtrsim -\\jmath^\\star.\nWeak Deflation.  If -(rf+(1-r)\\jmath)^\\star<\\alpha_{const}\\leq0 then an interior market equilibrium s^\\star requires high slashing, as we argued before based on Churn.  All other things being equal, it is also less stable.  Assuming low slashing continues, as validators must pay the cost themselves so are incentivized to minimize it, then s^\\star>1 and runaway staking is inevitable if the fixed point is stable: the numerator is larger than the denominator.  If unstable, s is pushed toward zero, and in practice becomes unpredictable by our model: externalities intervene to do… something.\nStrong deflation.  Consider now \\alpha<-(rf+(1-r)\\jmath)^\\star.  This could happen for instance if the issuance curve were reduced particularly bluntly, or changes in fundamentals drove either MEV or the base fee (and thus f) to a persistently higher amount, such that ys\\ll bf(1-s)+\\jmath s.  These conditions cannot maintain of course.  You don’t need differential equations to see that \\alpha<0 shrinks A, which eventually raises y(sA).  But as a temporary intervention to tame runaway staking fraction how would this work? Our fixed point is negative and the simplistic “ignore the constants” stability criteria is no longer met.  So again, 100% staking becomes inevitable (the source s^\\star<0 pushes s instead of pulling it), or (more likely) the behavior simply becomes unpredictable as externalities intervene.\nThere’s a lot of potential complexity here, but none of it is desirable!  If you don’t like inflation, wait until you try deflation! In all seriousness, while we agree largely with the aversion to inflation in the context of national economies, it is really important to recognize that those economies have a level of demand diversity and robustness that Ethereum at present can only dream of.  One day hopefully we will have the luxury of bemoaning inflation’s effects on hodlers, and reducing it, without fear of the infrastructural dynamics of these market equilibria running amok.\nMedium r; Low Inflation; Even Lower Fees\nWell, that was deflating!  Let’s cheer ourselves up by considering the behaviors under \\alpha_{const}>0.  A positive role for inflation can be seen in the contours of the market equilibrium staking fraction s^\\star corresponding to \\dot{s}=0, shown here with \\alpha^\\star=\\alpha_{const} and no slashing.\n\nTo find the equilibrium values (\\alpha^\\star/f^\\star,\\,r^\\star) necessary to achieve a desired staking fraction s^\\star, simply pick a colored contour in the figure: these are the values of constant s^\\star.  For every point on this curve, the equilibrium inflation:fee ratio \\alpha^\\star/f^\\star is the x-coordinate, and the equilibrium reinvestment ratio r^\\star is the y-value.\nA breakdown of limiting behaviors is illustrative under positive inflation.  For any value of non-negative inflation, r^\\star is a lower bound for the equilibrium staking fraction we should expect.  If inflation dominates fees, \\alpha_{const}\\gg f^\\star then s^\\star is larger by a very small amount than r^\\star, while if fees dominate inflation 0<\\alpha_{const}\\ll f^\\star then s^\\star becomes insensitive to non-zero reinvestment ratio and s^\\star\\to1.  In the intermediate range r^\\star f^\\star\\sim\\alpha^\\star, r^\\star < s^\\star still but the gap is bigger.\nFor a numerical comparison, eyeballing charts (so extremely rough approximations here, possibly off by an order of magnitude, maybe more) rf \\approx .004\\sim.005\\approx\\alpha so to within 15-20% error above, s^\\star\\approx r^\\star over the range of r\\in(.5,.75) inferred from the Lido yield rate.  Clearly we are not yet at equilibrium, or r is much lower than our extremely rough estimates.\nHow the transient values (\\alpha_{now}/f,r) relate to the true equilibrium values (\\alpha^\\star/f^\\star,r^\\star) depends on some considerations:\n\nIf indeed churn dies down and slashing stays relatively rare, then   r increases to reflect the growing share of businesses that   reinvest the most; r^\\star\\approx r_{max}, where r_{max} is   assessed over all staking pools with at least 10% of S.\n\n\nWe are holding \\alpha_{const}=\\alpha^\\star, so   \\alpha_{now}\\approx\\alpha^\\star but a more sophisticated approximation   is likely possible keeping within the two-timescale context… maybe   you’ll find one!\n\nIf indeed churn dies down and slashing stays relatively rare, then   r increases to reflect the growing share of businesses that   reinvest the most; r^\\star\\approx r_{max}, where r_{max} is   assessed over all staking pools with at least 10% of S.\nWe are holding \\alpha_{const}=\\alpha^\\star, so   \\alpha_{now}\\approx\\alpha^\\star but a more sophisticated approximation   is likely possible keeping within the two-timescale context… maybe   you’ll find one!\nRunaway r from Inflation pressure\nCould the sensitivity r_\\alpha be sufficient such that even at intermediate timescales we see s^\\star\\to1?  This is certainly possible; per the arguments of Ethereum researchers, high inflation could still lead to runaway staking if r is sensitive enough.\nIn our model the net effect of inflation on staking fraction  equilibrium is reflected by taking the derivative 0<\\left.\\frac{ds^\\star}{d\\alpha}\\right|^\\star assuming r,f are implicit functions of \\alpha.  That is, the necessary condition for inflation to push the market equilibrium s^\\star itself into runaway staking is (see below for explanation):\n1 \\ \\ < \\ \\ \\left(\\frac{\\partial\\log\\ r}{\\partial\\log\\ \\alpha}\\right)^\\star \\cdot \\frac{1 + \\alpha^\\star f^\\star}{1 - r^\\star} \\ \\ + \\ \\ \\left(\\frac{\\partial\\log\\ f}{\\partial\\log\\ \\alpha}\\right)^\\star\nIn practice it all depends on what ETH users consider sufficiently “high” inflation to respond; the above conditions just reflect a quantification of this preference.  We hope that this work can be built upon to determine what threshold of inflation or business conditions satisfy the above condition.  No mean feat, but it would help focus inflationary pressure arguments into empirically measurable assertions that can be tracked for Ethereum health.\nWhence “Mr”; Dangers of Low r\nBefore you start advocating for “EIP 7514 On Steroids” on crypto twitter (that is, throttling the staking queue to prevent r from rising, such as due to inflation-pressure), consider the danger of low staking, that is low r in the absence of significant churn.  In the simplified staking fraction equation, the term \\alpha(r-s) is positive only when s<r and \\alpha>0… what if r\\to\\epsilon\\ll{s_{now}} for some reason?  Then the only positive term (rf+q_+)(1-s) gets very small in the absence of new staking. But, given sufficient inflation and fees and very marginal sensitivities, the stability requirement \\alpha+rf+\\jmath>0 could still obtain, so the new fixed point, presuming inflation maintains in LI;ELF territory, s^\\star\\sim\\epsilon remains stable.\nThat’s quite a lot of ifs, but consider exponential growth of supply under present issuance: y_1\\sqrt{sA}… A\\sim e^{\\alpha t} can maintain so long as s\\sim\\epsilon + s_{now}e^{-\\alpha t}.  For roughly \\alpha^{-1}\\log\\epsilon^{-1} Ethereum could in principle experience simultaneously lowered security, and consistent inflation. For small \\epsilon and \\alpha\\sim.1 that’s more than a decade in principle.  Whether this has any bearing on reality, we don’t know, but its worth being aware of.  In the guide we will be adding upper limits on the time-duration of inflation as time-permits.\nWe saw above a few things:\nReinvestment r is a lower-bound for the staking fraction fixed point s^\\star    unless Ether is deflating, when likely s\\to1 or the dynamics are unstable. 1. Low (but positive) inflation moderates staking fraction closer to this lower bound    at intermediate timescales 1. Positive inflation cannot maintain indefinitely, so eventually s^\\star\\to1.\nConceptually, how can inflation moderate staking fraction, though? Shouldn’t more staking lead to more issuance, which leads to more inflation, etc.?  Briefly the reasons are:\nShort Q_+ vs. Long Term R Investment * The Quotient Rule \\dot{S}=\\dot{S}/A-s\\dot{A}/A\nShort Term vs. Long Term.\nNovel investment in staking Q_+ is driven largely by speculation, and new users encountering Ethereum.  Q_+ acts to increase staking fraction, as seen above, and indeed the glut in Q_+ since the Merge may have been the source for much of the alarm that prompted this study. Novel speculative investment must eventually dry up, and be replaced by long-term investment R, because\neveryone with money who wants to stake eventually will, so will be counted in R not Q_+ 2. any business that wants to stay in business cannot consistently reinvest more than its revenue R\\leq I+P (issuance plus priority fees).\nOf the long term signal R, only the issuance portion of reinvestment, that is the part that contributes to inflation, can moderate s, which brings us to the quotient rule.\nThe Quotient Rule\n\\dot{s}\\ \\ =\\ \\ \\frac{\\dot{S}}{A}\\ -\\ \\frac{S\\dot{A}}{A^2}\\ \\ =\\ \\ \\frac{\\dot{S}}{A}\\ -\\ s\\alpha \nWhat increases s is any increase in staked Ether S, but also any net decrease in A=S+U. Inflation \\alpha increases both U and S, because some of that increase is used to meet costs and take profit (1-r)\\alpha, which increases U relative to S.  In contrast, the reinvestment of transaction fees can only ever increase S at the expense of U.  Thus transaction fees always act to increase staked fraction, while the effect of inflation depends on the relative values of reinvestment ratio and staking fraction.\nSo, while we could certainly model reinvestment differently, and there are lags we are blithely integrating over, we think that these market forces will still act as described above in a different model.  It is possible that even during sustained inflation, these effects will be unable to prevent the upward creep in s, because r is too large, or the sensitivity of r to inflation at equilibrium is too great, a condition which we mathematized above.  In fact we expect that every argument about inflation effects driving increased staking, overpaying for security, etc. could (perhaps should) be rephrased in terms of reinvestment of staking rewards.  All these critically depend on the preferences of ETH users for, and thus their behavior in reaction to, inflation rate etc., which thusfar are not measured, as far as we know.  We encourage the community to rectify this!\nCan reflexivity prevent s\\to1?\nRegarding the possible effects of reflexivity.  We have neglected even discussing oscillations in (S,U), even though the model is plainly capable of such behavior under different parameters or when coupled to price.  Why such negligence?  If cycles do arise, we expect market participants, anticipating such cycles, would act to profit off of these cycles in a way that should reduce them.  Buy late in the inflation cycle, sell late in the deflation cycle, etc.  This would show up in our model via the partial derivatives including externalities.\nNotably though, we only expect this to happen because it does not require the coordination of market participants: each individual blindly pursuing their own utility should help en masse control these oscillations, or they were never very great to begin with.\nCould there be a similar effect with staking fraction or inflation? In short we have no idea, but put it as a challenge to the reader. Can you devise a cryptoeconomic protocol or trading strategy that forestalls the seemingly inevitable s^\\star\\to1?  If not, can you prove this is impossible?\nShould We Change y?\nSo, finally… should the Ethereum community reduce issuance?\nGlib answer\nNo.\nShort answer\nLet’s adopt a dynamical solution to a dynamical problem.  If you are very inflation averse or you want to slow down the transition to high staking, please study and simulate downward adjusting the constants in EIP 7514, adopted during the Deneb upgrade.  This already directly limits R+Q_+, but was very nicely designed to not interfere with existing staking flows.  It would be particularly interesting to see if R can be safely decoupled from limits on Q_+, to avoid the potential danger of low-r scenarios.  EIP 7514 does not solve, and does not claim to solve, the long term problems, but we have been forced to conclude that reducing issuance doesn’t solve them either!\nLong answer\nAsk the users, especially the validators, especially the LSPs.  Model user preferences so that the demand curve becomes semi-empirical instead of theorized.  Near-100% staking seems to be baked-in eventually, as persistent inflation cannot sustain under an issuance yield curve designed to avoid discouragement.\nSo the question becomes essentially “How bad will it get in the meantime?”  We recommend of course that you use these tools to run simulations.  But that is only half the answer… this is really a question about user preferences.  Austrian School devotees may be so inflation-averse that they are already staking all their previously-liquid ETH at \\alpha\\approx0.5%/yr.  In contrast users who were content to grow up with fiat currencies during periods of \\approx3% inflation or even worse might not care, or would just stake in Compound or buy stETH.\nFor users seeking to passively preserve wealth, staking in Compound (or Aave, or whatever) is ideal for generating raw Ether demand, of course.  Staking in LSTs, such as stETH, though concerning from a governance angle, may present less of an issue than some have feared. LSPs must share some yield with users in order to have users, but if they want to stay in business they must maintain r_{LST}<1, and both profit-taking and covering fiat-denominated costs go into unstaked Ether U.\nSo how high can r_{LST} go before LSPs find the loss of gross profits unacceptable?  Great question.  We will see next time that as many have recognized, the entity/group i that can maintain the highest r_i wins the race eventually.  It’s worth noting however that if you apply price uncertainty via a risk-discounting rate of, well anything really, you will see that the “maximize r_{mine}” strategy is far from the most profitable, risk-adjusted.  Of course, a speculative investor seeking to profit from our analysis would find our study woefully short of details.  Put another way, since you have insisted on reading the “long answer”, we will end with the classic and cowardly refrain of academics and academic-adjacents everywhere “it requires more research!”\nWe hope to develop the ethode guide so it can serve a pedagogical role.  For now we have assumed some basic familiarity with nonlinear dynamics, asymptotic methods, etc. at the level of the first few of Prof. Steven Strogatz youtube lectures.\n\nNonlinear Dynamics\n\n\nPerturbation Theory\n\nNonlinear Dynamics\nPerturbation Theory\nHighly Recommended books in order of increasing difficulty and sophistication if you decide you want to understand this stuff:\n\nKun (2020) A Programmer’s Introduction to Mathematics\n\n\nStrogatz (2024) Nonlinear Dynamics and Chaos\n\n\nHirsch, Smale and Devaney (2003) Differential Equations …\n\n\nBender and Orszag (1997) Advanced Mathematical Methods for Scientists and Engineers\n\n\nArnol’d (Ed.) the Dynamical Systems Series     - esp. V (1994) Bifurcation and Catastrophe\n\nKun (2020) A Programmer’s Introduction to Mathematics\nStrogatz (2024) Nonlinear Dynamics and Chaos\nHirsch, Smale and Devaney (2003) Differential Equations …\nBender and Orszag (1997) Advanced Mathematical Methods for Scientists and Engineers\nArnol’d (Ed.) the Dynamical Systems Series     - esp. V (1994) Bifurcation and Catastrophe\nUnfortunately the fonts used in markdown on the blog are not the greatest at rendering nicely for some of the chosen syntax, especially on certain monitors/browsers.  Due to feedback from people with bleeding eyes, we can at least offer this table.  We also included, or tried to include, the common variables not present in other tables.\nOpen Zeppelin is an early icon of smart contract best practices, and continues to provide templates and auditing services in high demand.  They have absolutely no connection to this post, our models, etc. and hopefully they will not sue us for using their name in a bad dynamical systems joke. ↩︎\nFor derivations involving differential equations, D (used for staking Deposit) and its corresponding intensive d are cursed variables. s was already in use in some places for staking fraction, and we are resolute on keeping the intensive and its corresponding extensive the same letter.  C is a more natural choice for circulating supply, but then the three variables of most interest are something like (C,s,ς) which is masochistic in its sibilance, even for squares.  We prefer “accessible” to circulating because the former implies you could access it, at some cost, while the latter sometimes implies a velocity of money.  A velocity which S and much of U may lack depending on dynamics: backed up unstaking queue, leveraged or looped CDPs, etc.  But even if our terminology were actually superior, we’re not going to change economic jargon any time soon. ↩︎ ↩︎\nSometimes “dot x” =dx/dt is used for the partial derivative of x with time t, which we denote x_t.  The full relation is dx = x_t + x_A dA + x_s ds + x_α dα in which each partial is taken holding all the other variables constant, and x_t is used in practice to smuggle in any variability from non-dynamical variables. In principle x_A and x_α are distinct; a quantity can depend on supply (how big ETH market cap is compared to BTC, say) and inflation independently. ↩︎ ↩︎\nWe use moving quarterly averages, though any timescale τ sufficiently long that the erratic and fast dynamics of the base fee are integrated out, and the lags from (un)staking queues are not appreciable.  As we are averaging quarterly, we set the staking, unstaking, and reward queues to zero, including their respective flows (R+Q+,Q-,I+P) in their codomain stocks (S,U,V); even if ethereum produces empty blocks, so long as the reward queue is not empty U > 0. See also our section on I<=yS. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎\nWe use domain/codomain in imprecise analogy with category theory mainly because we want to reserve “source” for an attractor, as per dynamical systems.  The analogy, while inexact is not inappropriate.  It is routine to implicitly use associativity to account for fibers of flows through multiple steps; “electricity from wind/nuclear/gas” even though the electrons are indistinguishable. Flows such as tx fees U--F-->V,Ø involve a categorical product VxØ in that the smaller fractional flows U--B-->Ø must factor through it. Similarly the staking queue V+(U-V)---R+Q+-->S involves a coproduct in the domain.  Whether there is content here beyond “flows correspond to injective maps between measurable sets” is unclear.  None of this matters in the least for Ethereum dynamics, of course.  If you’re reading it consider this an easter egg / attempt to detect a living and alert audience. ↩︎ ↩︎ ↩︎\nIntensives expressed as fractions of flows such as R/(I+P), instead of fractional rates of sources (like J/S or Q_-/S) occur when the source dynamical variable, here V, is assumed to equilibrate dV/dt=0.  Then the outgoing flows R+K must equal the incoming flows I+P, so we choose R=r(I+P). If onchain data indicates, say, \\approx70% reinvestment of staking rewards into S takes a lot longer than three months, we would revisit this assumption, though we do not expect our qualitative results to change re inflation and staking fraction. ↩︎ ↩︎ ↩︎\nFor computer scientists f << g is equivalent to F = O(g) if you’re more familiar with big-O notation.  Specifically I << S means that the limit of I/S as t gets very large is 0.  Contrast to I <= yS, which could just be a matter of coefficients.  Asymptotic Notation is well-explained on wikipedia; see the bottom for Vinogradov.  The art of using it to your advantage in calculations is demonstrated by Prof. Carl Bender. ↩︎ ↩︎ ↩︎ ↩︎\nOur (dlog y)/(dlog S) = 1 - p in the discouragement paper ↩︎\nA non-zero r=R/(I+P) is built into the smart contract of every Liquid Staking Provider (LSP).  Here, token-holders provide Ether and receive a redeemable token (LST) that shares some staking rewards with them. This fraction of rewards r_LST is a lower bound on R/(I+P) at the fixed point. ↩︎\nSplitting the staking queue into R + Q_+ allows us to somewhat separate short-term transient behavior from long-term dynamics.  Speculative investment in staking by venture capitalists and novice stakers is expected to die down eventually; either they give up or they run staking like a business where making a profit matters.  Every business that wants to stay in business reinvests some portion of its profits, so r,R > 0 is what matters in the long run, once most everyone who wants to stake is staking. ↩︎\nThe use here is related to but slightly different than the simplified use “independent of systemm size” common in thermodynamics. Specifically we use that Ether is something preserved in the flow to bound the measure of the flow by the size of its domain above and zero below, with subleading terms possible.  So B ~ U - log(1+U) - U**(1/2) is possible but B ~ U + U**2 and B ~ U - U**2 are out. We care most about the limit S -> A (U -> 0); see[7:3] also[21] other[6:2] footnotes[22] for[14:1] examples[16:1] and context. ↩︎\nWe can often use the dependence on t to smuggle in any forces, like market panics, etc. that we neglected to include as dynamical variables.  If we cannot add something essential this way, we must add a dynamical variable. ↩︎\nReaders wishing for more detail are encouraged to use the two dimensional local stability criterion (see Prof. Steven Strogatz) to solve for the condition of eigenvalues with an imaginary part.  But simulate it too! ↩︎\nRegarding B = bf(1-s)A the burn.  While slashing could believably go to zero on quarterly timescales, no burn B=0 implies blocks are empty.  Obviously s=1, B=0 isn’t really a functioning state for Ethereum.  A better asymptotic limit would be s = 1-ε making A ~ (1/ε)**2 (that’s squared… so very big as ε is very small). Detailed treatment of the burn, staking queues using expansions in ε would be necessary here, and are among our desiderata.  We anticipate the need to model churn, slashing, and burn in light of stochasticity/quantization.  One can use difference equations or Ito Calculus, but a useful generic behavior of such systems is obtained in a “weak coupling” limit. Perturbations due to quantization move the dynamics away from the fixed point, apparently randomly. Yet! Somehow, the average rate of precession about s=1-ε is often given by the imaginary component of the largest eigenvalue of the simpler model. ↩︎ ↩︎\nA warning!  Simulations are necessarily imperfect, and especially if you use method='RK4' (Runga-Kutta) or other methods based on local approximations only, it is not hard to find simulations in which it sure as hell seems like the system has settled down into a small but positive inflation fixed point.  The following is a useful sanity check for the existing issuance yield curve, obtained using Jensen’s inequality. An inflation rate α cannot persist longer than Δt, according to exp(α Δt/2) - 1 < 2y1 Δt sqrt(s_ave / A_now) where s_ave is the average of staking fraction over Δt, A_now is the supply at the beginning of the inflation period, and y1 is 166.3/yr. ↩︎\nIf anything the fractional rates of slashing and burn are positive with small changes in inflation, due to either a single ETH potentially being of less real value, or stimulation of economic activity attracting more validators and higher average burn. ↩︎ ↩︎\nHere gamma γ expresses the sensitivity of inflation to supply initial conditions; the partial x_A always holds (α,s) constant, but dA = α dt and the partials in gamma are j_A and (bf)_A. ↩︎\nGlobal stability involves either trajectories infinitely returning to a region of the fixed point (think comets) or a contraction map showing the system shrinking to a limit set.  We won’t rule out global stability, but recommend you look first for locally stable fixed points.  Assessing the stability of equilibrium zero inflation in the (A,s,α) model is interesting but probably academic.  One of the eigenvalues at any fixed point with α=0 is zero, so higher-order terms matter (the fixed point is degenerate), and linear-stability analysis is insufficient: we need to care about global stability not just local.  A reader imbued with mathematical athleticism and free time is encouraged to think of a Lyapunov function л(α=0) = 0 <= л(α), and obtain a contraction mapping dл(α)/dt <= 0. ↩︎\nConfusingly the movement of a dynamic variable toward/away-from a fixed point is often also called a “flow”. Terms in equations like R,J,... could then be called “fluxes”.  But you’re not confused, right? ↩︎\nSmallness of sensitivities wrt intensives is not guaranteed. Certainly a large magnitude, say (bf)_s>1 cannot maintain for too long; 0<bf<1 afterall.  Locally, a large spike in derivative (bf)_s > bf is still possible. ↩︎\nWhy not simply choose B = bA, essentially as was done in this 2021 post by Elowsson?  (We use S for his D and A for his S.[2:1]) Obviously if there is no unstaked Ether no one can afford tx fees. Here s is a dynamical variable, so b = B(A-S) = bf(1-s)A is more appropriate for our model.  The function B might do all kinds of complicated nonsense, but it can never go negative and it can never exceed U. ↩︎\nVariable parameters that are positive fractions cannot contribute fixed-points themselves, but they can strongly influence where a fixed point is. Example: as s -> 1, if the leading terms were bf~(1-s) and j~(1-s)**2 this gives increasingly larger equilibrium A as s->1. ↩︎\n",
        "category": [
            "Economics"
        ],
        "discourse": []
    },
    {
        "title": "Gossipsub Network Diameter Estimate",
        "link": "https://ethresear.ch/t/gossipsub-network-diameter-estimate/21561",
        "article": "The ProbeLab team (probelab.io) has been working extensively on the behaviour of Gossipsub in the Ethereum network (see recent posts). We now present a new stochastic, theoretical model to estimate how quickly messages travel across a Gossipsub network (its diameter). By focusing on a broadcast-only approach with a fixed mesh degree and random connections, our analysis found that the Gossipsub diameter of the Ethereum network is seven.\nIn this post, we develop a theoretical model for estimating the diameter of a Gossipsub network. By “diameter,” we refer to the maximum number of hops (or discrete time steps) required for a message to reach all nodes in the system. This metric provides direct insight into how quickly broadcasts propagate across the network. Additionally, understanding the diameter helps reason about redundant (duplicate) messages and offers guidance on how to optimize the protocol to minimize unnecessary overhead.\nAnalyzing network diameter and duplicate messages ratio yields practical benefits:\nPerformance assessment: Quantifying propagation speed allows us to gauge whether nodes receive critical data in a timely fashion.\nProtocol optimization: Observing the duplicate messages distribution can uncover opportunities to reduce bandwidth consumption and network congestion.\nResilience insights: Knowing the worst-case propagation paths can guide improvements that bolster reliability under various stress conditions.\nTo focus our analysis, we make several assumptions:\nBroadcast-only model: We consider just the pub-sub (broadcast) aspect of Gossipsub, ignoring its “gossip” component.\nConstant mesh degree: The mesh degree D is fixed for every node in the network.\nRandom graph topology: Each node connects to exactly D peers, chosen at random. All links are bidirectional, ensuring the graph is connected.\nStatic mesh: The mesh is assumed to remain static during the propagation process.\nUniform link latency: All peers are equidistant from each other in terms of network latency.\nThese assumptions enable a stochastic model that remains tractable while still capturing key properties of the Gossipsub network. In the following sections, we develop this model step by step, and illustrate its implications for message propagation.\nLet’s consider a simple illustration of how a message propagates through a network where each node has four connections (D=4). When a node receives the message for the first time, we say it becomes “infected.” In the following round, that newly infected node sends the message to its own neighbors, who in turn become infected. Step by step, we see how the broadcast ripples through the network until all nodes have received the message.\nRound 0\nAt the very start, we have exactly one “infected” node holding the message.\nRound 1\nIn this round, the original node sends the message to its D=4 neighbors. Those four nodes become “infected” and are now aware of the broadcast. They will, in turn, propagate the message in the next round.\nInfection process: round 11920×1382 213 KB\nRound 2\nEach of the four newly infected nodes repeats the broadcast, sending it to their own three remaining neighbors. (1) Note that two peers may try to infect the same node, which hasn’t been infected yet. (2) Also note that two nodes that have been infected in the previous round may be connected to each other, and try to infect each other, even though they are both already infected.\nIt is important to take into account these 2 kinds of duplicates when computing the number of nodes infected in the next round.\nInfection process: round 21920×1623 204 KB\nRound 3\nBy the third propagation step, most or all of the reachable nodes (in our simple example) will have been infected. (1) Note that the node that was infected twice in the previous round is only able to infect two additional nodes, and not three. This is so because all nodes have exactly D=4 connections, and the number of outgoing messages is exactly D minus the number of incoming messages. (2) Similarly, a node being infected 4 times, will not try to infect any other node at the next round, since it doesn’t have extra links.\nAs the number of infected nodes grows, so does the number of duplicates.\nInfection process: round 31920×1068 99.2 KB\nRound 4\nBy the fourth and last propagation step, all nodes have been infected. We observe that at this point the vast majority of infections are actually duplicates. (1) Note that nodes infected in the last round can be connected to each other, which means that there would be a fifth round in which these nodes are infecting each other again. However this round doesn’t count toward the network diameter, since all nodes have been infected at this point.\nInfection process: round 41920×1120 123 KB\nOur goal is to determine, at each round t, how many nodes become newly infected given the count of newly infected nodes in round t-1. Tracking this progression across rounds reveals the point at which all N nodes in the network have received the message, thereby allowing us to estimate the network’s diameter.\nDuplicates\nAs stated above, we distinguish between two main sources of duplicate infections: same-layer connections and next-layer collisions.\n\nSame-layer connections\nThese occur between pairs of nodes that both become infected in the same round and also share an edge. When these nodes attempt to infect each other in the following round, no new infections result because they are already infected.\n\n\nNext-layer collisions\nThese arise when a node receives multiple initial infections during the same round. Since each node has a fixed mesh degree D, each “extra” infection effectively reduces the number of fresh nodes it can infect in the next round. Put differently, a node that is already infected by multiple peers cannot use those same edges to infect additional new peers in subsequent rounds, thereby diminishing its overall infecting capability.\n\nSame-layer connections\nThese occur between pairs of nodes that both become infected in the same round and also share an edge. When these nodes attempt to infect each other in the following round, no new infections result because they are already infected.\nNext-layer collisions\nThese arise when a node receives multiple initial infections during the same round. Since each node has a fixed mesh degree D, each “extra” infection effectively reduces the number of fresh nodes it can infect in the next round. Put differently, a node that is already infected by multiple peers cannot use those same edges to infect additional new peers in subsequent rounds, thereby diminishing its overall infecting capability.\nInitial case\nAt t=0, we start with a single infected node, which it the one publishing the message, hence\nAt t=1, the single infected node from round 0 infects its D neighbors. There are no next-layer collisions since all nodes to which the initial node is connected are distinct. Therefore,\nInductive Step\nStarting from t=2, both same-layer connections and next-layer collisions are possible.\nAt round t, the number of infection attempts x_t is given by:\nThis formula accounts for the following:\nEach node that was infected exactly once in the previous round (I(t-1)) has D-1 remaining links available for new infection attempts.\nWe subtract the next-layer collisions from the previous round (C(t-1)) because those connections were already consumed during the previous round’s multiple infections and cannot be reused.\nThis adjustment ensures that x_t reflects only the valid, unused links available for new infections in the current round.\nLet’s define the number of nodes that haven’t been infected yet at the start of round t.\nWe can now calculate how many infection attempts reach the next layer (including next-layer collisions) by determining the probability that each connection of round t is not a same-layer connection.\nHere’s the reasoning:\nNodes infected at round t-1 can potentially connect to any of the N-R(t-2)-1 other nodes in the network (excluding themselves).\nAmong these, M_t=N-R(t-1) nodes are still uninfected and thus eligible for new infections.\nThe remaining I(t-1)-1 nodes represent infections that occurred during round t-1. These nodes could form same-layer connections, which would not create new infections.\nThe number of same-layer connections is as follows\nTo compute the number of new infections, we must account for and discard duplicates.\nThe probability that a single infection attempt successfully targets a specific node among the M_t eligible nodes is:\nThe probability of not infecting this specific node in a single attempt is:\nIf y_t  infection attempts are made, the probability that a specific node is never infected is:\nThus, the probability of infecting this specific node at least once in y_t attempts is:\nSince there are M nodes eligible for infection in the current round, the expected number of fresh infections is:\nThe number of next-layer collisions can then be computed as the difference between the total infection attempts and the number of newly infected nodes:\nAnd the total number of duplicates at round t, D(t) is the sum of the next-layer collisions and same-layer connections.\nWith these formulas, we now have all the components needed to run the model.\nIn the Ethereum network D=8 (source), and N \\approx 9,000 (source). For these parameters, we obtain the following results:\nGossipsub message propagation over time in the Ethereum network1000×600 37.8 KB\nPython simulation code\nNetwork Diameter\nAfter seven rounds, an average of 8,999.68 nodes (out of 9,000) become infected, indicating that in some instances, an additional round might be required to reach the last remaining node. Based on these results, it is reasonable to consider the network diameter to be seven under these conditions.\nInfections\nAs expected, the number of new infections rises steadily until round five, where it reaches a turning point. Afterward, it begins to decline as the pool of uninfected nodes shrinks and becomes harder to reach.\nDuplicates\nA particularly noteworthy observation is the surge in duplicate messages during rounds five and six. This spike occurs because many nodes are newly infected in the previous round, yet there are fewer uninfected targets available, resulting in more redundant transmissions.\nEach node is expected to send and receive an average of D-2 duplicates (source). However, when we apply this model to the Ethereum network, we see an average of about 4.83 duplicates per node (roughly D-3). This discrepancy occurs because our stochastic model, which works in discrete steps, doesn’t fully capture the timing-dependent nature of duplicates. Nevertheless, we expect the model to accurately represent how duplicates are distributed, especially since a larger number of duplicates tend to appear in the later stages of message propagation.\nWe developed a stochastic model to approximate the diameter of GossipSub networks based on their mesh degree and overall size. Applying this model to the Ethereum network reveals a GossipSub diameter of seven. During this analysis, we also observed that the majority of duplicate messages are sent in rounds five and six, coinciding with the period when most nodes have just become infected yet there are fewer remaining targets.\n",
        "category": [
            "Networking"
        ],
        "discourse": []
    },
    {
        "title": "Unbundling at the Relay Level for frontrunning protocol hacks",
        "link": "https://ethresear.ch/t/unbundling-at-the-relay-level-for-frontrunning-protocol-hacks/21471",
        "article": "Abstract\nWe propose the implementation of two new Ethereum JSON-RPC methods, eth_clandestineSubmitTransaction and eth_clandestineSubmitBlock for MEV Relays. These methods allow direct interaction with MEV Boost Relays, providing advanced mechanisms for transaction and block submission that are ‘clandestine’ in nature.\nThe proposal aims to mitigate risks associated with malicious activities (i.e. hacks of projects) by front running the hacker’s transaction and ensuring that the block that includes the ‘clandestine transaction’ wins by lying to the validator about the value of its proposed block. This ensures exclusivity by the relay that the successful mitigation is completed. The funds that are recovered must be returned to the project, with an optional proportion being retained for a pooled insurance treasury less payments to participating security companies for their services.\nThis proposal requires multiple security firms to participate by fuzzing transactions and reaching a quorum such that there is some agreement as to a malicious transaction. This method need not also be used for frontrunning a transaction hack, as it can also be used by security collectives such as SEAL911 for submitting whitehack transactions.\nMotivation\nAdvances in fuzzing by various security companies have proven their ability to find just in time malicious transactions and attempt to ‘front run’ (i.e. outbid the gas and fees paid to validators) them to protect the vulnerable project. However, this leads to a tit for tat increase in gas costs and fees paid in an attempt to ‘front run the front run’. By leveraging the trusted nature of the MEV Relay we can ‘lie’ to the validator ensuring that our protected transaction (called ‘clandestine’) is chosen unambiguously.\n~95% of DeFi hacks would be eliminated.\nRequire multiple security companies to participate.\nAdditional benefits by ensuring intrablock state consistency (by virtue of fuzzing transactions).\nHacks can still occur, a hacker can spin up a validator and build a block locally to execute the attack.\nA portion of the ‘recovered’ funds can be retained for funding a cooperative insurance pool for projects, to compensate the security firms for their work and development.\nAdds explicit support for security cooperation with security groups like SEAL911 by providing a way to also submit transactions that are related to preventive measures such as a transaction that updates some protocols state (e.g. disabling depositing to the project because of a newly disclosed vulnerability).\nLST Operators/Protocols no longer have contentious issues of ‘receiving stolen goods’, they can also be compensated from the ‘recovered funds’. Example: Validators would get 2% of the total recovered funds.\nAll methods must be signed with an address that is whitelisted ahead of time. Authorization: Requires the Ethereum-Signature header for permission.\nDescription: Submits a confidential transaction directly to an MEV Boost Relay for prioritized block inclusion, with optional parameters for enhanced control.\nParameters:\nExample Request:\nExample Response:\nDescription: Submits an entire block for exclusive processing by an MEV Boost Relay.\nParameters:\nExample Request:\nExample Response:\nThis is meant for preventive measures not for front running.\n[!NOTE]\nThis method is optional and really not necessary, I only include it for soliciting feedback\nPurpose: Broadcast a transaction at the top of a target block, bypassing certain conventional mempool checks to enable ultra-low-latency inclusion.\nKey Features:\nPriority Inclusion: Ensures the transaction appears at the block’s top.\nBypass Checks: Ignores nonce and gas price validations; the user is responsible for correctness.\nOptional Multiplier: Adjusts the relay’s bid or payment mechanism.\nConcluding remarks\nI would like to thank Justin Drake for entertaining the idea when first discussed, and to Vasily (p2p.org) for the most helpful suggestion of the quorum agreement. I would also like to thank fuzz.land, dedaub, and blocksec for their helpful input.\nMEATBAL may not be the best name, maybe the world computer needs a CPU: a Clandestine Protection Unit.\n",
        "category": [
            "Security"
        ],
        "discourse": [
            "security",
            "mev"
        ]
    },
    {
        "title": "Introducing OneBalance",
        "link": "https://ethresear.ch/t/introducing-onebalance/19557",
        "article": "By Stephane Gosselin and Ankit Chiplunkar on behalf of Frontier Research\nFor most recent information about OneBalance, please visit frontier.tech.\nSee Research Day 2024 slides and recording.\nWeb3 and the crypto ecosystem more broadly has historically had a chain centric worldview. We believe this is an outdated framework originating from a perceived scarcity in blockspace due to the bundling of credible commitment machines with global consensus.\nWe believe an ecosystem wide transition towards an account centric worldview which bundles accounts with credible commitment machines is both necessary and inevitable in order to consolidated a fragmented user experience.\nWe propose a new account model called “Credible Accounts” and introduce a framework called “OneBalance” for creating and managing these accounts. With this proposal, we hope to provide a missing component of the web3 stack that will help onboard the first billion humans onto crypto.\nOneBalance is a framework for creating and managing accounts on credible commitment machines. We call these Credible Accounts.\nEach account can thought of as an rollup which allows users to conveniently manage their state and reliably request state transitions on any chain.\nThis is achieved through the introduction of two key concepts:\nAccounts on credible commitment machines, and\nResource locks\nA credible commitment machine is responsible for securing the account, issuing resource locks over the state it holds, and validating the fulfillment of such locks.\nBy introducing Credible Accounts, we hope to accelerate the transition of the ecosystem from EOAs, the JSON-RPC API, and the transaction supply chain, towards an architecture built around message passing of resource locks and fulfillment proofs.\nA Credible Account on OneBalance can:\nCombine token balances from every chain\nAbstract gas on any chain\nSwap and send tokens to and from any chain\nIssue complex permissions over any subset of user state\nIncentivize and enforce atomic asynchronous composability across multiple chains\nAuthenticate user using modern methods such as passkey, session keys, FIDO\nFast confirmations through separation of fulfillment from settlement\nMany use cases are unlocked with these new capabilities. Users can spend any token to pay for state transitions. They can aggregate liquidity that lives both on-chain and off-chain. They even have the building blocks to build a decentralized Fireblocks, or a non-custodial Binance.\nOneBalance Account Framework2666×1500 481 KB\nThe web3 market structure equilibrium, as defined by Ethereum, is the use of public-private key-pair, aka, Externally Owned Account (EOA) to manage all aspects of a user’s state.\nSince EOAs sit outside the blockchain, the chain has no view over what message have been signed. The chain relies on the use of nonces to sequence user state transaction requests, often called transactions.\nHere, we simplify down a smart contract blockchain such as Ethereum to a Credible Commitment Machine bundled with Global Consensus.\nExternally Owned Accounts3345×1195 157 KB\nCredible Commitment Machines\nThomas Schelling’s 1960 The Strategy of Conflict famously introduced the game theory concept of “focal point” better known as his name sake “Schelling point” .\nA lesser know contribution of this work was the introduction of “credible commitments” which he describes as “a promise or threat believable to others by creating a situation where the costs of reneging on the commitment are higher than the benefits”.\nAccording to Schelling, a credible commitment must provide:\nIrrevocability: Making the commitment in such a way that it cannot be easily reversed. This might involve physical actions or formal agreements that lock the party into their commitment.\nIncreased Costs: Ensuring that backing out of the commitment imposes significant costs, either financially, reputationally, or strategically.\nObservable Commitment: The commitment must be visible and understandable to other parties, ensuring they believe in the credibility of the commitment.\nThe concept was further refined by Nick Szabo in 1997 with the introduction of the concept of smart contracts as software enforced contracts, which was further explored by Mark S. Miller and Marc Stiegler in 2003.\nThese investigations lead to the Ethereum Whitepaper published by Vitalik Buterin in 2014 proposing Ethereum as the first smart contract blockchain capable of creating and enforcing arbitrary credible commitments.\nMany important contributions to the topic followed with a noteworthy contribution by Mohammad Akbarpour and Shengwu Li in 2019 providing a formal definition to Credible Mechanisms and introducing the auction trilemma.\nLoosely defined, we refer to a Credible Commitment Machine as a secure computer able to programmatically issue and enforce commitments such that they are believable or “credible” to a third party observer.\nEOAs and Request Equivocation\nEOAs are not credible commitment machines. Since the chain has no view over what the user has signed, it must consider any transaction signed with the same nonce as valid. This means that at any time, a user can equivocate their state transition request by signing and submit a new transaction which overwrites their nonce.\nThis makes EOAs, and the transaction supply chain more broadly, incapable of providing credible commitments as they violate the principle of irrevocability. ****Downstream parties in the transaction supply chain such as solvers are unable to rely on the commitments made by EOAs as they may be equivocated at any time.\nTo date, every system looking to protect against request equivocation have relied on locking funds in a deposit smart contract. This is the approach used by all types of bridges (L2, intent, POA, IBC, ZK). All implementations of secure cross chain interactions rely on the transfer of assets to a smart contract as a request which cannot be equivocated.\nThe class of networks based on HTLCs such as the Interledger Protocol all use timelocked requests as a form of credible commitment for security passing economic messages between ledgers.\nSmart Contract Accounts (SCAs) solve the request equivocation problem by bringing the signer on chain. This allows the account to leverage the chain’s credible commitment machine and global consensus to timestamp and sequence each state transition requests issued by the user thereby preventing equivocation.\nSmart Contract Accounts3345×1195 150 KB\nVitalik has long been a proponent of smart contract wallets as these offer great UX features broadly referred to as account abstraction which includes gas abstraction, social recovery, permissions policies, and modern authentication methods.\nDespite addressing the equivocation problem, smart contract accounts deployed to chains with global consensus like Ethereum are prohibitively expensive and slow. This is because virtual machines such as the EVM require the sequential execution of global locks over all user accounts for every state transition request. This is equivalent to forming a single global queue of all users waiting to do something on chain.\nThe Account Dilemma\nWe observe a tradeoff between EOAs and SCAs. On one side we have EOAs which are fast and cheap, but cannot make credible commitments due to request equivocation. On the other side we have SCAs which can make credible commitments, but are slow and expensive due to global consensus. We call this tradeoff the Account Dilemma.\nThe Account Dilemma2476×1615 141 KB\nWe propose a new account model called Credible Accounts which aims to solve the Account Dilemma. Credible Accounts live in a secure computer of the users choice that can make credible commitments about what messages it will and won’t sign.\nBy unbundling global consensus from the Smart Contract Account model, we are able to keep the speed and cost advantage of EOAs while retaining the UX improvements and non-equivocation guarantees of SCAs.\nCredible Accounts3345×1195 149 KB\nOneBalance: a framework for Credible Accounts\nEach OneBalance account can be thought of as its own rollup. The account wraps individual user state from all chains and replicates it in a virtual environment. This virtual environment issues state transition requests as “resource locks” and fulfills those state transitions through cross-chain execution proofs. This virtual environment is secured by a credible commitment machine.\nSince a OneBalance account provides the same guarantees as a SCA, it comes with all the same UX benefits of account abstraction, such as gas abstraction, social recovery, permission policies, and modern authentication methods.\nA OneBalance account can create an arbitrary number of sub accounts across any number of chains and manage any state present on those chains. It is backwards compatible with all chains, smart contracts, and assets including Ethereum, Solana, Bitcoin, ERC20s, NFTs, DAOs, multisigs, defi protocols, and deposits or points programs. This multi-ecosystem compatibility is not possible with other current account model.\nThe OneBalance framework for Credible Accounts is implemented in a modular way using standards developed by the CAKE Working Group to allow users / apps / wallets to pick an choose any component of the CAKE Framework needed for their use case.\nOneBalance Framework2666×1500 481 KB\nResource Locks\nA resource lock is a credible commitment made by a user to escrow some state conditional on particular conditions fulfilled, or an expiry time.\nAn example could be a cross-chain request to purchase an NFT on Solana using USDC deposited in the OneBalance account from Ethereum.\nResource locks are necessary to prevent solvers from being griefed by a user through double spending or equivocating their request during execution.\nSince the user makes a commitment not to overwrite their request within a time window, it removes the uncertainty solvers typically incur between a transaction being signed and the finalized chain state.\nA lock is analogous to depositing funds in a smart contract, or issuing an ERC20 approval, but without spending gas or waiting for on-chain finality since it is done within the account itself.\nThe lock expiry needs to be of long enough duration to provide solvers the chance to execute the requested state transition on the destination chain and submit a proof of fulfillment to the fulfillment engine.\nCrucially, this introduces a separation between fulfillment time and settlement time. Since the account provides local assurance of the lock, solvers can bring a requested state transition on a destination chain without waiting for finality on the origin chain.\nThis allows users to buy SOL with ETH at the speed of Solana without being constrained by the speed of Ethereum. This fulfillment speed can be extended to execution of any state transition such as sniping an NFT, sending money to your grandmother, or anything else users do on blockchains.\nResource locks can implement constraints which sit anywhere along the spectrum of permissions.\nPermission Spectrum3474×1151 201 KB\nPermissions could be stateful or stateless. For example:\nScoped session keys: a stateless permission for an app like a Telegram bot to take arbitrary actions on subsets of a user’s token balances\nCircuit breaker: a stateful permission to sell all open positions if there is no account activity or market volatility above a predefined threshold\nLimit order: a stateful permission to post an order if a pair reaches a certain price on a DEX\nMFA: a stateless permission to post a transaction if two valid authentication methods are provided\nCredible Commitment Machine\nA credible commitment machine is a secure computer on which the account lives and is trusted to provide assurances over the valid issuance of resource locks and the validation of their fulfillment.\nWe present here four possible architectures of credible commitment machines which provide for secure issuance and enforcement of locks: Trusted Execution Environments (TEEs), Multi-Party Computation / chain signatures (MPC), Smart Contract Accounts (SCAs), and in protocol virtual machine changes.\nThese mechanisms are being developed and refined as we speak, it is likely that the ideal architecture today will look vastly different than the one five years from now.\nThis is why a OneBalance account allows users to migrate between CCMs over time as they look for better properties.\nCredible Commitment Machine2430×1645 232 KB\nOneBalance v1:\nadd support for transactions and swaps of any token on any chain\nadd support for session keys for trusted applications\nadd support for user rage quit through exit hatch\nOneBalance v2:\nadd support for stateless policies\nadd support for arbitrary transactions\nadd support for authentication modules\nOneBalance v3:\nadd support for stateful policies\nOneBalance v4:\nadd liveness guarantees through account replication\nAcknowledgements\nThank you to the collective consciousness of the crypto ecosystem for fostering a fertile ground for innovation.\nIn no particular order, thank you to the following collaborators for the many stimulating discussions which lead to the creation of this proposal:\nMurat Akdeniz, Ahmed Al-Balaghi, Viktor Bunin, Jonah Burian, Vitalik Buterin, Philippe Castonguay, Vaibhav Chellani, Valery Cherepanov, Jasper De Gooijer, Nicolas Della Penna, Justin Drake, Brendan Farmer, Ben Fisch, Mattia Gagliardi, Johnny Gannon, Matt Garnett, Ivo Georgiev, Christopher Goes, Pedro Gomes, Mason Hall, Sam Hart, Connor Howe, Sreeram Kannan, Hart Lambur, Zaki Manian, Robert Miller, Alex Obadia, Puja Ohlhaver, Anatolii Padenko, Nick Pai, Illia Polosukhin, Karthik Senthil, Tomasz Stanczak, Henri Stern, Alex Stokes, Caleb Tebbe, Dror Tirosh, Dean Tribble, Drew Van der Werff, Alex Watts, Yoav Weiss, Nathan Worsley, Evgeny Yurtae, Philipp Zentner, Noah Zinsmeister, apriori, jxom, awkweb.\nWhy are you doing this?\nOur mission is to help the web3 ecosystem to transition to an account centric worldview in order to bring web3 to the first 1 billion people.\nWe believe non-coercive credible commitment machines are essential for human coordination in the digital age.\nWe believe the chain centric worldview is an outdated framework originating from the historical scarcity in blockspace. (sorry Joel)\nMuch like the shift from the Geocentric worldview to the Heliocentric worldview unlocked a wealth of discoveries, we believe the shift from a chain centric worldview to an account centric worldview will unlock the full potential of web3.\nWhat does this mean from a user perspective?\nUsers don’t need to care about which chains they are interacting with + can get close to instant fulfillment.\nLets take a complex, yet common example:\nUser wants to buy an NFT traded on Solana with a price of 10 SOL, but only has USDC on Ethereum.\nThis state transition request requires the following sequential operations to take place:\nGenerate: Create a new Ed25519 keypair in a solana wallet\nSwap: USDC for ETH to pay for gas on Ethereum\nBridge: Send USDC to bridge contract on Ethereum and get USDC minted on Solana\nSwap: USDC for SOL to pay for gas on Solana\nSwap: USDC for SOL to purchase NFT\nExecute: Execute calldata on marketplace to purchase the NFT\nToday, users are required to manually perform each of these actions and wait for the previous one to be settled or finalized before performing the following one. Some of these operations are even technically impossible in a non-custodial way using EOAs without chain level gas abstraction.\nThe critical path of execution here requires waiting for the settlement of two Ethereum transactions and two Solana transactions + waiting for the finality of Ethereum. With Ethereum’s current block finality time, we are looking at a minimum of 15min to complete execution.\nLets look at the equivalent using a OneBalance account:\nUser wants to buy an NFT traded on Solana with a price of 10 SOL, but only has USDC on OneBalance.\nThis state transition request requires the following sequential operations to take place:\n\nCreate a resource lock on OneBalance as follows:\nresource_lock: {\n\tlock: 1500 USDC,\n\tfulfill: DeGods #12345,\n\texpiry: Solana block 245547084\n}\n\n\nCreate a resource lock on OneBalance as follows:\nThere are no additional steps for the user to take.\nBehind the scenes, a solver purchases the NFT and credits it to the OneBalance proxy account of the user on Solana, and claims the resources in the lock.\nSince a OneBalance account separates fulfillment from settlement, the user gets execution at the speed of transaction execution on their destination network, in this case Solana. The user can perform any operation on any chain using any token they hold in their OneBalance account.\nShow me some sequence diagrams.\nOk.\nLets walkthrough a few examples on how interoperability is achieved under different conditions:\nThe user has EOA account and wants to do a cross-chain contract call while not having gas on target chain\n\nThe user signs two transactions using their EOA, the first transfers gas amount from the origin chain to the solver escrow address and the second calls the contract on the target chain.\nAs soon as the solver simulates these two transactions, they have a guarantee that the user will pay them the correct gas amount (user’s commitment is enforced).\nThe solver instantly funds user’s EOA account on the target chain, and executes the contract call transaction without waiting for settlement or finality of first transaction.\nThe solver can include the first transaction (gas payment) within the expiry window of commitment.\n\n\nThe user signs two transactions using their EOA, the first transfers gas amount from the origin chain to the solver escrow address and the second calls the contract on the target chain.\nAs soon as the solver simulates these two transactions, they have a guarantee that the user will pay them the correct gas amount (user’s commitment is enforced).\nThe solver instantly funds user’s EOA account on the target chain, and executes the contract call transaction without waiting for settlement or finality of first transaction.\nThe solver can include the first transaction (gas payment) within the expiry window of commitment.\nimage2788×1434 258 KB\nThe user has Smart Contract account and wants to do a cross-chain swap while not having gas on target chain.\n\nThe user signs a UserOp authenticating the transfer of required tokens to the solvers escrow address.\nAs soon as the solver simulates the transaction, they have a guarantee that the user will pay them the required tokens (user’s commitment is enforced).\nThe solver procures the required tokens on the target chain and deposits them into the users account on the target chain without waiting for settlement or finality.\nThe solver can include the UserOp (token deposit in escrow) eventually.\n\n\nThe user signs a UserOp authenticating the transfer of required tokens to the solvers escrow address.\nAs soon as the solver simulates the transaction, they have a guarantee that the user will pay them the required tokens (user’s commitment is enforced).\nThe solver procures the required tokens on the target chain and deposits them into the users account on the target chain without waiting for settlement or finality.\nThe solver can include the UserOp (token deposit in escrow) eventually.\nimage2817×1419 319 KB\nAs we can see in the above examples a user commitment via nonce lock is same as delegating eventual state update to solvers. This reduces latency introduced by finality especially in a cross-chain setting.\nHow does this relate to account abstraction and 4337?\nAccount abstraction and 4337 are often associated with Smart Contract Accounts and refer to both the set of UX improvements offered by an SCA (account abstraction) and a specific standard for implementation of these UX improvements (EIP-4337).\nThe OneBalance account model provides the same UX improvements as SCAs and is backwards compatible with using 4337 for settlement of locks on chain as user ops.\nHowever, since OneBalance is a general framework for the creation of credible accounts, it is not limited to chains where account abstraction and 4337 are actively being used.\nOneBalance is supportive of chains and proxy accounts which opt for using dos resistant user ops like 4337.\nHow do you guarantee atomic asynchronous execution across chains?\nWe can’t. But we can incentivize it.\nAccounts can enforce two types of constraints:\nConstraints over what states of the world it requires in order to issue a lock\nConstraints over what states of the world it requires in order to fulfill a lock\nOneBalance can incentivize atomicity, but it cannot guarantee it.\nFor example: Take a lock that requests sequencing of state transitions across chain A and chain B, with a desired atomicity such that B is executed if and only if A is executed. If the lock is public and the only condition on inclusion of B is the existence of a valid signature from the requesting account, then B could be included regardless of A if it exposes sufficient MEV to compensate its inclusion.\nHowever! Since executing B without A invalidates the fulfillment conditions of the lock, then the solver cannot extract any value from the lock. This means that from the user perspective, atomicity is maintained. Some people refer to this as “economic atomicity”. For locks with complex multi chain atomicity requirements, Solvers take on the risk of non-execution.\nThis “lock leaking” due to MEV problem can be resolved at the routing layer by routing the state transition request through a secure OFA that prevents information leakage.\nNovel mechanisms are being developed at the settlement layer to help solvers manage their settlement risk using things like pre-confirmations or proof aggregation. Importantly, these help manage solver execution risk and therefore help minimize non-execution risk for users of a OneBalance account, but the OneBalance account already has economic atomicity guarantees.\nIs OneBalance just HTLCs for Ethereum?\nI prefer to call it turbo HTLCs \nThe design of intent bridges work similarly to HTLCs in that they lock user funds on the originating chain until a proof is provided of the completion of a state transition on the destination chain.\nInstead of being constrained by the speed of the originating chains, OneBalance accounts are constrained by the speed of the credible commitment engine in generating these locks. This means that on a TEE architecture, locks can be issued at clock speed of a single server, hence turbo HTLCs!\nHow does this thing scale to one billion concurrent users?\nOneBalance accounts create “local” locks, whereas regular accounts can only create “global” locks. Global locks require locking the state of all accounts in the execution environment during sequencing, whereas local locks only require locking the state of accounts which are party to the lock.\nUnlike global locks, local locks provide the opportunity for lock sequencing to be parallelized on distinct machines.\nWhere does the account live?\nThe account lives in a secure computer of the users choice that can make credible commitments about what messages it will and won’t sign.\nThe four architectures for credible commitment enforcement presented above provide such environments, but each have their tradeoffs. OneBalance accounts are not opinionated to the type of credible commitment enforcement mechanism used.\nCrucially, as the sophistication of these architectures evolve over time, the tradeoff space will change and so will user preferences. As such, OneBalance accounts must remain flexible to supporting different architectures.\nIs OneBalance a standard?\nNo.\nOneBalance is a framework for building accounts on top of credible commitment machines. OneBalance will use the standards developed by the CAKE Working Group plug and play with all components of the CAKE stack.\nAre you providing pre-confirmations?\nNo. OneBalance provides resource locks.\nLets explore the difference between the two.\nPre-confirmations is a mechanism being actively developed by several teams in the ecosystem to offer better inclusion guarantees for entities sending transactions to the blockchain.\nResource locks is a mechanism for offering guarantees to solvers that a user cannot double spend or equivocate on their state transition request.\nBoth mechanisms are aimed at reducing the execution risk of solvers, pre-confirmations are guarantees provided by proposers, resource locks are guarantees provided by users.\nBoth pre-confirmations and resource locks can be enforced by the same credible commitment mechanism.\nFor example: If Ethereum validators were to deposit their staked ETH in a OneBalance account, they could create resource locks which specify slashing rules if certain transaction inclusion commitments are invalidated. This could be implemented with a restaking contract like eigenlayer.\nimage3587×1114 138 KB\nHow does a lock turn into a state transition on chain?\nOneBalance accounts are responsible for issuing and enforcing resource locks over state transition requests.\nRouting of the request in order to provide fulfillment is a downstream module which the upstream app / user must define. For certain kinds of requests, it may be beneficial to route directly to a chain’s transaction pool, other requests may benefit from using a solver network or orderflow auction, and others from specifying an exclusive solver.\nOneBalance is unopinionated about the routing mechanism.\nIs OneBalance a competitor to (insert my bags here)?\nProbably not.\nOneBalance is providing a framework for orchestration of stateful accounts. Our goal is to displace the industry wide reliance on imperative state transition requests issued by user managed EOAs. OneBalance is not opinionated on the architecture of the stateful accounts or the credible commitment mechanisms used to secure them.\nOneBalance uses a modular architecture that allows for the following components to be integrated:\nOrderflow sources: wallets / apps / tg bots / waas\nFulfillment engines: orderflow auctions / solvers / solver networks / market makers / intent networks / bridges\nSettlement engines: any L1 or L2 (yes, even BTC)\nThe design for OneBalance emerged from our work with members of the CAKE working group.\nLooking at the CAKE framework, OneBalance sits between the permissioning layer and the solver layer and is compatible with all the other necessary components of the cake.\nimage1763×2267 143 KB\nAre OneBalance accounts custodial?\nNo.\nAll OneBalance accounts are issued withdrawal receipts which allow them to permissionlessly exit their assets back to the settlement chain where the assets originate.\nThis means at any point, a user is able to “rage quit” and recover their assets on the source chain.\nThis mechanism is implemented differently depending on the accounting engine used, but essentially boils down to the same outcome: users can withdraw their funds by submitting withdrawal receipts to their proxy accounts on origination chains.\nI firmly believe freedom of exit is an essential characteristic of non-oppressive human coordination system design. I have never and will never design a system without freedom of exit.\nAre OneBalance accounts really compatible with any chain?\nKinda.\nAll chains are supported, but the capabilities differ based on the VM used and the fulfillment speed differs based on the consensus. More details on this in the future.\nAre OneBalance accounts vulnerable to vampire attacks?\nYes. This is a good thing.\nIf someone can build a system which delivers higher combined utility (functionality + economic incentive) to users than OneBalance accounts, users should be able to exit. This is necessary to avoid anti-competitive monopolies that censor innovation.\nTo whoever wants to try, I say bring it on.\nIs this a Keystore Rollup?\nNo.\nAs they are defined today, a keystore rollup solves the problem of having a central source of truth for account permissions, across all chains. It doesn’t offer cross-chain guarantees to solvers. A OneBalance account does not need a keystore rollup since on chain keys don’t change.\nIf a Keystore Rollup was to attempt to issue locks, they could only be communicated at L1 finality speed.\nWhat is the trust model?\nInstead of requiring mutual trust between users and solvers, the OneBalance model requires each party to trust the credible commitment mechanism used to issue and enforce resource locks.\nDoes OneBalance remove the need for bridges?\nNo.\nBridges are necessary mechanisms to provide fulfillment of locks and settlement of inventory outside of the critical path of user request execution.\nWhat is the relationship between Frontier Research, OneBalance, and the CAKE Working Group?\nFrontier Research is and remains an independent research and advisory group formed to bridge the gap between fundamental research and commercial products.\nFrontier founded the CAKE Working Group along with other collaborators such as Anoma, Across, and Ethereum Foundation members to foster conversation around common interfaces and language to accelerate the development of chain abstraction technology.\nFrontier is spinning out OneBalance as an independent project to accelerate the transition of an account centric worldview for web3.\nAll three groups of humans will continue to follow their individual missions moving forward.\nWhat are the latency and consistency guarantees of this account model?\nwip\nHow do you prevent sequencer DOS?\nwip\nHow does a OneBalance account recover from a fault in an underlying chain?\nwip\nHow quickly can lock resolution take place?\nwip\nIs this a new kind of blockchain?\nwip\nWhat are the benefits and drawbacks of using a OneBalance account vs a Smart Contract Account?\nwip\nCan you build complex applications like a dex on top of resource locks?\nwip\n",
        "category": [
            "Applications"
        ],
        "discourse": [
            "mev",
            "rollup",
            "zk-roll-up"
        ]
    },
    {
        "title": "L1 improvement based on Minus Theory",
        "link": "https://ethresear.ch/t/l1-improvement-based-on-minus-theory/21494",
        "article": "TLDR\nThe Minus Theory guides us to build blockchains in a new way, enabling very high TPS and asynchronous calculation of the global state. This could be used to improve existing ETH L1 or any other blockchains.\nSolution\nThe 'Minus’ed chain is said blockchain node should focus on achieving consensus on the messages (transactions) input to the chain, while moving verification, execution tasks, and the global state out of the blockchain node and into an indexer.\nThis design unlocks the ability for blockchain nodes to handle at least 10k TPS and potentially up to 200k TPS. The indexer can be continuously optimized with upcoming technologies such as parallel VMs or asynchronous calculation of the global state during traffic surges.\nMinus Theory\nMinus Theory was created in January 2024. It was inspired by the popularity of inscriptions in 2023, but we developed the theory from a consensus perspective to separate verification from the consensus process. Its security is backed by state machine theory.\nIt doesn’t require ZK at all. Monad also shares this vision: Determined ordering implies state determinism. Similarly, based rollups design adopt a comparable approach by achieving consensus on the order of transactions on L1 while handling execution on L2. We believe that the L2 in based rollups design is equivalent to an indexer. So, why not focus on improving performance directly on L1?\nSeveral links of Minus:\nTitle Thin Blockchain Protocol based on Minus Theory  Team member names 0xKJ and Yanbo  Short summary of your improvement idea In early 2024 Dr KJ proposed the Blockchain Minus Theory showing a new direction to build the high performance...\n\nReading time: 1 mins 🕑\nLikes: 2 ❤\n\nInscription Instrumentality Project\nInscription gained a sudden surge in popularity in late 2023, which caught many blockchain developers off guard. This article title borrows the Human Instrumentality Project concept from the Japanese anime Neon Genesis...\nZentra\nAfter Minus Theory, we spent one year building an indexer named Zentra, which proves that the theory works in practice, not just on paper. Rather than using the EVM, Zentra brings Python to the blockchain. Zentra does not aim to be a Layer 1, so we are contributing this idea to the Ethereum community and the broader blockchain world, hoping it will help improve the blockchain infrastructure.\nIf the Ethereum community adopts this approach, the EVM can be moved from the node to the indexer. This shift would allow other indexers, like Zentra, to utilize L1 in a more cost-effective way while enabling the exploration of more complex applications developed in Python.\nMaintain  Decentralization\nMinus Theory proposes moving the Virtual Machine (VM) from the blockchain node to an external indexer. This shift does not compromise the decentralization of the network because it leaves the consensus process intact and independent.\nIn traditional blockchain architectures, nodes are responsible for both achieving consensus and executing transactions, which often limits scalability. By relocating the execution workload to the indexer, Minus Theory ensures that nodes remain focused on consensus alone.\nSince consensus is the core mechanism that ensures decentralization, this design preserves the distributed and trustless nature of the blockchain. Meanwhile, indexers can be decentralized themselves, further reinforcing the decentralized ethos.\nHigher TPS\nThe primary bottleneck for blockchain scalability lies in the execution of transactions within the EVM. Minus Theory addresses this limitation by introducing asynchronous execution.\nBy decoupling transaction execution from the consensus process, the blockchain can confirm and “freeze” a significantly larger volume of transactions without requiring additional computational resources. This design enables the network to handle higher throughput, as consensus is achieved on transaction inputs, while execution is processed separately and asynchronously.\nCheap gas fee\nOne of the key benefits of the Minus Theory approach is the significant reduction in gas fees. The cost of consensus in a blockchain is generally fixed: in Proof of Work (PoW), it is determined by electricity costs, while in Proof of Stake (PoS), it is based on the interest earned from staking.\nBy enabling the blockchain to handle a much higher number of transactions per second (TPS), the fixed cost of consensus can be distributed across a larger pool of transactions. This reduces the per-transaction cost significantly, leading to lower gas fees for users.\nSharding on transactions by wallet address\nBlockchain sharding often encounters challenges when attempting to partition the global state, as maintaining consistency and synchronization across shards can be complex. However, sharding transactions by wallet address provides a simpler and more efficient approach.\nIn this model, transactions are distributed across different nodes based on wallet addresses. This reduces the burden on individual nodes, as their primary responsibility shifts to maintaining a history of transactions rather than managing the entire global state. The node are light to focus on its DA job. By focusing on transaction history, blockchain nodes become more robust and stable.\nSimplify the blockchain node design\nThe design of the blockchain node in Minus Theory focuses on achieving consensus for the blockchain input. By narrowing the scope of the node’s responsibilities to this critical function, the overall complexity of the node is significantly reduced.\nThis simplified design makes the blockchain node more robust, as it only needs to verify and agree on the validity of incoming transactions or blocks. Without the need to handle computationally expensive tasks such as execution or state management, nodes become more efficient and less resource-intensive.\nLess risk when optimizing the execution engine\nIn traditional blockchain architectures, a significant amount of work and logic related to transaction execution optimization is handled directly within the blockchain node. However, in the Minus Theory framework, this execution logic is offloaded to an external indexer, which decouples the execution process from the core blockchain node.\nBy moving the execution engine out of the node, the risk associated with optimizing and updating the execution code is substantially reduced. Making changes to the indexer’s code carries less risk than modifying the blockchain node itself, as the node remains focused on consensus and data availability, which are critical for network security and stability.\nMore complex applications\nOne of the key limitations in traditional blockchain designs is the gas limit, which prevents smart contracts from consuming excessive computational resources. This gas limit is set to ensure that blockchain nodes can execute all smart contracts within a block before the next one is generated. However, this constraint limits the complexity and capabilities of decentralized applications (dApps) that can be built on the blockchain.\nWith the introduction of the indexer in Minus Theory, this limitation can be relaxed. By offloading execution to the indexer, we can gradually reduce the gas limit as the performance of the execution engine improves over time. The indexer is not bound by the same block time constraints as the blockchain node, enabling it to process more complex computations and handle more sophisticated applications.\nThis shift allows for the development of more resource-intensive decentralized applications, such as advanced decentralized finance (DeFi) protocols, complex gaming systems, and enterprise-level solutions, all without compromising the scalability or security of the blockchain. As the performance of the indexer improves, the range of possible applications grows, pushing the boundaries of what can be achieved in the blockchain space.\nHard fork is required\nWith the transition of the global state and VM responsibilities to the indexer, blockchain nodes no longer directly track the current state of wallet balances, such as whether a wallet has enough ETH to pay for gas fees. This change introduces new challenges in how transactions and fees are validated across the network.\nThis shift may require further design considerations, particularly regarding whether it is necessary to maintain an ETH balance at the node level. There are numerous engineering questions that need to be addressed in this new framework, once the community agrees to adopt the Minus Theory approach.\nGiven these complexities, a hard fork may be required to implement the necessary changes and ensure compatibility with the new structure. This would enable a smooth transition to the new paradigm while maintaining network stability and ensuring that the system functions as expected.\nGlobal State Explosion problem still exists\nThe issue of Global State Explosion has been a persistent challenge for Ethereum and other blockchain networks. This occurs when the global state grows too large, making it more difficult for people to afford the hardware necessary to run a node. Minus Theory does not attempt to solve this problem directly.\nHowever, in Zentra, we implement a mechanism to mitigate the abuse of global state. By using tokens, we provide a way to limit excessive interactions with the global state.\nSummary\nWe propose a method to enhance L1 using Minus Theory and discuss its advantages and disadvantages.\nAfter many years of focusing on L2 scaling, the era of L1 is making a comeback.\n",
        "category": [
            "Execution Layer Research",
            "SoP 2024 RFC"
        ],
        "discourse": []
    },
    {
        "title": "Block-level fee markets: Four easy pieces",
        "link": "https://ethresear.ch/t/block-level-fee-markets-four-easy-pieces/21448",
        "article": "Many thanks to Ansgar Dietrichs, Matt “Lightclients”, Toni Wahrstätter, Thomas Thiery, Davide Crapis, Dankrad Feist, Julian Ma, Vitalik Buterin, and others for discussions and comments on these ideas. Errors remain my own.\nIn this post, we consider alternative proposals for metering and pricing blockchain resources. Many of these proposals were discussed previously (sometimes extensively), and our aim here is to provide a unified approach to consider their opportunities and trade-offs. This is the first step in a larger exploration which will require us to make precise the arguments presented below, but we hope to convince the reader that the framing is useful, and gesture at some of its features and instantiations.\nA fee market finds users and the protocol coming to some allocation of resources via pricing mechanisms. These resources are provided by nodes, which either consume them during the act of producing a block or during the act of receiving, executing and/or verifying the contents of the block. We can differentiate between two approaches to obtaining a fee market:\nDirect pricing: A user brings their transaction to the protocol, which prices the resources used by the transaction and charges the user some protocol-determined price for their use.\nMediated pricing: A party (say, a broker) buys a set of resources from the protocol and re-sells these resources to users who demand them. The brokers are responsible for ensuring their own budget-balance, and broker competition achieves efficiency under certain conditions.\nFrom a first pass, the mediated view feels strictly worse: If we have access to direct pricing, why bother with a third-party mediating the interaction between users and the protocol? To see this, we consider two different ways to meter resources offered by the protocol to its users:\nTransaction-level metering: The resources used by a transaction are metered strictly within the context of the transaction, with no other information than what the transaction itself consumes.\nBlock-level metering: The resources used by a transaction are metered within the context of the whole block, using information regarding what other transactions have consumed.\nNote that we could be more or less granular than the two metering levels proposed above, but for the purposes of this post we focus on these two alone.\nMetering and pricing operate at different levels. First, we must ensure that we have the correct measure of the resources used by a transaction or a block, as this tells us how much load we can put on the nodes of our network, by comparing the measured resources with our available budget of resources. The process of metering outputs a measure denominated in gas, bytes, blobs or any other such quantity.\nOnce the resources are metered, they should be priced, i.e., a user should be charged for the use of these resources, either as a way to remunerate the node(s) supplying these resources, when their provision is costly, or to indicate a higher willingness to obtain these scarce resources when multiple users are competing for them, or for both reasons (roughly, this is what the priority fee and the base fee indicate, respectively).\nIn this post, we present four case studies and discuss three claims:\nClaim 1: Block-level metering obtains in general better allocations of resources.\nWe mean this in the sense that by using the whole information of resources consumed throughout the block, block-level metering loosens the budget constraint faced by users and block producers, allowing for more bang for our resource bucks. This sounds great, so how can we get block-level metering essentially for free?\nClaim 2: Block-level metering is very easy to pair with mediated pricing of resources.\nBy introducing a broker responsible for buying “in bulk” the resources from the protocol, and letting the broker figure out the resource allocation which unlocks the biggest bang for their buck, the protocol need not concern itself with directly pricing resources. This gives us a generic method for unlocking scalability improvements via block-level metering.\nClaim 3: Some desirable allocations unlocked with block-level metering cannot be satisfyingly obtained with direct pricing of resources.\nThe third claim is an open question. Block-level metering increases our feasible space of resource allocations, i.e., potentially allows for the creation of more blocks than transaction-level metering would otherwise allow for. Direct pricing means that the protocol has the ability to perform fair pricing, i.e., considering the context of the whole block’s allocation of resources, rebates and distributes the costs appropriately between all users served with the allocation. Can we always find efficiently a fair pricing rule? In some cases, which we will discuss below, this appears simple enough. Are there cases where it is not so?\nAfter setting up some formalism, we’ll go on to discuss 4 case studies related to the claims we have made here.\nMachine cost models\nThe Resonance paper (Bahrani, Durvasula, 2024) gives us a good foundation to review some ideas that have been floating around for a while, and organise them under one coherent framework, so we borrow here some of the notation. In the paper, an abstract cost function is defined, which determines the metering used by the node. In this post, we unpack a bit this abstraction, to allow ourselves to compare different cost functions applied to the same machine.\nWe assume that there is a machine with some resource model, e.g., it offers computation, bandwidth and storage. A set of transactions T requests resources from this machine. When some subset X \\subset T of transactions (assumed ordered for the purposes of this post) is run on the machine, the machine incurs a cost c(X), which we can assume to belong to some multi-dimensional space of resources. The high-level argument of the present post is that by exposing positive externalities to its model of costs c, the machine is able to leverage these externalities to offer better pricing to its users.\nFor instance, as we’ll cover in the next section, take two identical machines A and B equipped with distinct cost models c_A and c_B. Suppose c_A encodes that machine A meters accessing a piece of state in some transaction without considering whether this piece of state was accessed in a previous transaction within the same block. Suppose c_B does encode this in its metering, charging a lower amount of gas when a piece of state is accessed for a second time, even within the context of a different transaction. Then though machine A, being identical to machine B, could cache a piece of state during the execution of a whole block, machine A does not meter it as such, and thus thinks that its costs are significantly worse than those of machine B, which meters its ability to cache properly. When the costs are significantly worse, machine A must then inform its users that the operations they do should lead to higher resource consumption, and must price them as such.\nWe may introduce something akin to a topology of cost models. Topologies are introduced to consider subsets of a set, with coarser topologies discriminating less precisely between “neighbourhoods” of points contained in the set. With a coarser topology, a machine cost model could gloss over differences in behaviour when two similar yet distinct operations are performed, e.g., it could consider that loading the same piece of state twice over two distinct transactions has the same properties and cost as loading two distinct pieces of state over two distinct transactions. A cost model with higher discriminatory power can price sequences of operations more accurately, being able to distinguish between two scenarios incurring different costs for the machine.\nIn our post, we assume that whenever we have a finer cost model, we can only improve our metering and lock in savings. We call some cost model c_1 coarser than some other model c_2 whenever for all possible allocations X, c_1(X) \\geq c_2(X), i.e., whenever c_1(X) weakly Pareto-dominates c_2(X), i.e., whenever c_1 meters an equal or higher resource use than c_2.\nClaim 1: Block-level metering obtains in general better allocations of resources.\nNote that our resource constraints typically come in two shapes:\nBurst/flow constraints: The current batch of resources being provided cannot exceed some limits, with limits given per resource. E.g., today, we do not expect to be able to propagate in time a block of size 100 MB.\nLong-term/stock constraints: Some resources such as history or state size accumulate over time, and so we aim to target some growth rate of these resources which is consistent with the capacity of nodes on the network.\nIn either case, but perhaps more so in the case of burst constraints, these limits bind the chosen cost model of our machines, e.g., tell us that we cannot use more than g amount of gas. If the current gas metering schedule encodes a coarser cost model than it needs to, i.e., if machines on the network are actually able to implement a finer cost model, then we immediately obtain access to a larger space of possible allocations.\nIndeed, suppose transaction-level metering c^t is coarser block-level metering c^b, then for any allocation X, if X is feasible under c^t, we have L \\geq c^t(X) for some vector of resource limits L. This implies that X is also feasible under c^b as L \\geq c^t(X) \\geq c^b(X).\nTo prove Claim 1, we should prove that block-level metering is finer than transaction-level metering. Of course, this depends on how we set up c^b, but we could restrict our attention to metering schedules satisfying some properties, such as:\nFor |X| = 1, i.e., when the block is made up of a single transaction, then c^t(X) = c^b(X).\nFor all X, we always find \\sum_{x \\in X} c^t([x]) \\geq c^b(X), where [x] is a block with a single transaction x.\nThe latter is quite tautological, since we have c^t(X) = \\sum_{x \\in X} c^t([x]), by assumption that transaction-level metering uses only information within the context of a single transaction to meter the cost. So we just engineered a definition that gives us a proof of Claim 1.\nWhat we are really trying to express is that in the worst case, block-level metering does not worsen things, but at best, something is improved. In other words, block-level metering may capture some positive externalities, but never captures any negative externalities. Negative externalities would happen if something happening in some transaction raised the marginal cost of something happening in a future transaction. In practice there could be some very weak negative externalities. Adding a new entry into the state could raise the cost of accessing another piece of state in a future transaction, as the size of the state has grown in the meantime, but this is such an insignificant increase that it could be ignored. Besides, the gas cost of accessing the state today is fixed to some value, and itself does not increase with the overall size of the state.\nNote, we also haven’t defined what “better” means in Claim 1, but even a simple model such as assuming that each user has some value for their transaction being processed, would tell us that having many more allocations X to choose from under c^b means that we can find a more efficient allocation than under c^t, i.e., one that realises greater value.\nPiece 1: Block-level state warming\nBlock-level metering is sensible whenever the resource model of a machine benefits from interactions between transactions executed within the same block. For instance, if the resource model of a node caches any piece of state accessed in the current block for the duration of the block’s execution, then it makes sense to reflect this in the pricing of the state itself.\nEthereum already has such capabilities. Before a piece of state is accessed, it is “cold”. Accessing a cold piece of state costs a user 2100 gas for this access, in this case to read the value of the piece of state. If the state is accessed again within the same transaction, its cost is then 100 gas only, as the state is now “warm”.\nHowever, once the transaction is done executing, this resets, and if the same piece of state is accessed in the next transaction, it is first accessed “cold”. This does not map well to the actual resource model of a node, which typically does keep a piece of state in the cache for the duration of the block’s execution, if not longer. By accurately pricing the fact that the piece of state is indeed “warm” when it is accessed again in the next transaction (if the machine can perform caching efficiently), gas savings are immediately obtained!\nWe’ll introduce some notation for this part and the remainder. A transaction is given by t, from some set T of user-produced transactions. A node executes X \\subseteq T, an ordered subset of transactions from the available set. The node then incurs (machine-)cost c(X), which we can think of as some physical, real-world cost in the most optimised resource model available to the node, e.g., caching warm states over the course of a whole block instead of resetting the cache after every transaction.\nSuppose we have two transactions t_1 and t_2 who access the same state. Generally, whenever we have two transactions whose execution in the same context yields a smaller cost than their separate execution in distinct contexts, we have the opportunity for savings:\nHere, we simply mean that warming the state in t_1 before accessing it again in t_2 while cached, incurs a lower cost to the node than warming the state in t_1, and warming it again to execute t_2.\nIn this case, direct pricing is fairly straightforward for the block-level metering of the transactions’ execution. We can simply split the costs between the two users, yielding:\nwhere \\pi_1 and \\pi_2 are user payments. In this case, since the total cost is 2100 + 100, each user would pay 1100 gas (2200 divided by 2).\nFinally, the idea of “block-level access lists” has also been discussed in the past (e.g., here and here). In this model, the block producer is responsible with providing in, e.g., the block header, an access list declaring all the pieces of state that the block’s execution will touch. If during the block’s execution a piece of state is touched that was not declared, the block is considered invalid. This metadata helps the machine batch the loading of the relevant pieces of state ex ante, and the block producer could be charged for all the “cold” accesses performed by the machine during this initial load.\nPiece 2: EIP-7623 and gas sheltering risks\nWe now look at a case where transaction-level pricing creates a myopic incentive to exploit the semantics of a transaction in order to benefit from savings occurring within the context of that transaction.\nEIP-7623: Increase calldata cost, by Toni Wahrstätter and Vitalik Buterin, was introduced to provide a different pricing rule for calldata, one of the resources consumed by Ethereum transactions. The pricing rule is set such that in the worst-case under EIP-7623, a transaction includes significantly less calldata than in the worst-case today. This is done by raising the gas cost of calldata past a certain point. However, recognising that calldata-heavy transactions do not harm adding further execution (the storage of calldata and execution being two somewhat orthogonal resources to provide), the pricing rule ignores marginal execution performed by the transaction once the calldata is “maxed out”. Precisely, the pricing rule is given by:\nFrom this piece of code, the reader may be able to infer that once the second term in the max function overtakes the first term, the pricing rule charges the transaction based solely on the number of tokens in the calldata, i.e., roughly in the length of the calldata. In contrast, should the first term remain the highest of the two, then the transaction is not charged solely on the number of calldata tokens, but also based on its evm_gas_used. In other words, once enough calldata is charged to the transaction, the transaction “maxes out” and execution performed within the frame of the transaction is not charged.\nThis allows for “gas sheltering”, as noted by wjmelements in the Ethereum Magicians thread for the EIP. Namely, suppose that Alice requires a lot of calldata, while Bob wishes to pay for some gas. In some cases, Alice may find it profitable to “shelter” Bob’s transaction, by “bundling” the two transactions into a single meta-transaction.\nFormally, suppose some cost model c^t is equipped with EIP-7623. Then we would find that, if Alice’s transaction is t_A and Bob’s transaction is t_B, and their concatenation into a single meta-transaction is t_A || t_B,\nIn other words, the machine rewards Alice and Bob for combining their transactions into a single one, charging the overall execution less than the two executed separately in the same block.\nIs gas sheltering an issue? If the cost model is sound, i.e., if it does make sense to allow a transaction maxing out calldata to execute at no marginal cost, then this is not great UX to allow users to benefit from such savings, as pointed out by wjmelements. On the other hand, for this specific case, Toni (Nerolation) argues that maxing out should be a rare enough case to not worry too much about gas sheltering becoming endemic.\nGas sheltering can be seen as a specific instantiation of mediated allocation, where some user must emit the meta-transaction combining intents or transactions from other distinct users. If this is where we end up, we may as well fully lean into this, and remove some of the issues inherent to meta-transactions. First, a meta-transaction is just a transaction, to the protocol, which cannot distinguish between a single user doing something and a bundle doing things on behalf of many more users. This means that the bundled user is at the mercy of the bundler, if they wish to benefit from cost savings. On the contrary, if the cost model could be achieved while keeping the bundled user’s transaction an actual transaction, the user could immediately benefit from inclusion lists to prevent their censorship.\nHere too, block-level metering is valuable. We could make it such that if any transaction within the block maxes out on calldata, then some amount of execution gets unlocked for free, i.e., is not charged to the broker at the end when all is tallied. Under this cost model c^b:\nThis yields a more granular cost model than one without EIP-7623, while removing the UX headaches of gas sheltering. Note that regardless of these possible UX issues, EIP-7623 still accomplishes its goals of limiting maximum gas used over execution and calldata dimensions. The UX issues are also not so important in a context where bundlers exist, who bundle ERC-4337 UserOps and seek these types of cost savings through amortising. See also the conclusion for more discussion on the relation between account abstraction and block-level fee markets.\nAn aside: Gas smuggling\nNote also EIP-7778: Prevent block gas smuggling, by Ben Adams, which tackles a related but separate issue. Gas smuggling occurs when a transaction is offered gas refunds for certain operations, e.g., setting the value of an account to zero or to an original value. Some of these operations lead to immediate cost reductions in terms of node resources, while others provide relief in the future, e.g., setting a storage slot to zero reduces the state size, making future accesses cheaper. If a transaction sets many slots to zero, there is a cost incurred now for writing this to the state, and benefits received in the future, which allows the sender to use more gas now from the refund, leading to loosened burst resource constraints.\nThe effects of EIP-7778 on the gas market depend on additional details of its implementation, namely on whether the EIP-1559 update rule uses the “burst gas” as metered by EIP-7778 (i.e., not counting refunds for benefits received in the future) or the “full gas” as metered today. If using the burst gas, EIP-1559 would “fill up quicker”. As an example, suppose half of our demand comes from transactions using only burst resources, i.e., transactions for which the “burst gas” metering is equal to the “full gas” metering. Suppose the other half of our demand are transactions obtaining half of their cost in refund, i.e., their “burst gas” measure is twice as large as their “full gas” measure.\nPiece 3: Block-level base fee\nThis one is not an instance of moving stuff around in the machine’s resource model, as in the previous example, but rather an instance of loosening constraints placed on the provision of resources by the virtual machine’s resource model. As a historical tidbit, the block-level version of EIP-1559 with block-level base fee was the first committed to specs. If memory serves me well, it was eventually decided to ship instead the transaction-level EIP-1559 instead of the block-level one, as the block-level version felt like an additive feature over transaction-level EIP-1559, and thus was deemed to merit its own separate EIP, which never materialised.\nSo what is the difference? Today, for its valid inclusion in a block, a transaction must declare a maxFeePerGas value superior to the baseFeePerGas quoted by the block. In other words, if a transaction is not willing to pay at least the base fee to enter the block, it simply cannot be included. This eliminated the use case of gas-less transactions, where fresh accounts could transact without any asset in their balance. This was recently noted by Hayden Adams (Uniswap) in a Bell Curve podcast recorded with Mike Ippolito and Hasu. [1]\nIt’s however possible to perform block-level metering of gas, requiring base fee to be paid in aggregate at the end of the block for all the gas consumed during the whole block, and “fix” this use case. By the end of a block, if the block uses g(X) amount of gas, and the prevailing base fee is p, then we charge the block producer g(X) \\cdot p. Up to them to figure out a way to have at least g(X) \\cdot p in their balance by the time the block’s execution terminates.\nCompare this with transaction-level base fee, where the constraint is that should a user transaction consume g(t) gas, then the user must be charged out of their balance at least g(t) \\cdot p (not accounting for the priority fees paid to the block producer). Loosening the constraint to a block-based ones recovers agency for the broker to include transactions as they wish, as long as they can cover their resource cost to the protocol. The protocol’s cost is c(X), which it prices at g(X) \\cdot p.\n“Agency” is defined in a talk by Maryam Bahrani, given at this year’s Devcon. While EIP-1559 has put us further from the “agentic” end of the spectrum, block-based base fee instead finds us in between the agentic end and EIP-1559, not quite fully unconstrained but less constrained than in the vanilla, transaction-level version of EIP-1559.\nDoesn’t EIP-1559 lose then some of its salience as an oracle of market price to the users, who under the block-level base fee rule could enter while not actually paying the base fee out of their own pocket? In a sense yes. When there is enough room in a block for all transactions paying the base fee as well as those which the broker wishes to include at their own cost, there is no issue. When congestion is high however, a transactor declaring their willingness to pay at least the base fee could be “pushed out” by a more valuable transaction to the broker.\nOverall, block-level base fee does not help us increase the gas limit or gas productivity, but may be a simple improvement to allow greater control by the block producer while retaining a fairly good oracle for congestion. Well, “simple” may not be entirely true. As I learned from Mr. Lightclients earlier in December, there is more than meets the eye here, given the decoupling of the CL and EL (see footnote [2]).\nAn aside: Block-level base fee in “AMM-1559”\nA type of mediated pricing was discussed in Vitalik’s “Make EIP-1559 more like an AMM curve” post. As Vitalik notes:\nNote that because of the nonlinearity of the burn, EIP 1559 would need to be adjusted. There are a few options:\nThe proposer pays the burn and the full fees from the transactions (including the basefee component) go to the proposer. Note that this still requires an algorithm to determine how high the basefee is considered to be in transactions that specify their gasprice in the form basefee + tip. [Barnabé’s note: this is mediated pricing with block-level base fee!]\nThe transaction origin pays a basefee equal to the maximum it could pay (that is, the basefee at excess_gas_issued + TARGET * SLACK_COEFFICIENT), and then at the end of block execution, everyone gets refunded so that the end result is that everyone pays the implied average basefee (the “implied average basefee” is (eth_qty(excess_gas_issued + gas_in_block) - eth_qty(excess_gas_issued)) / gas_in_block; the refund is the difference between the originally paid basefee and this amount) [Barnabé’s note: this is direct pricing with transaction-level base fee!]\nAs my reply argues, the protocol performing direct pricing induces a trickier block building problem for the block producer:\nWith the average implied basefee payment (or likely any basefee determination rule that depends on the number of transactions included in the block) there is a non-trivial optimisation problem for the miner to solve due to the interaction between the transaction tip = min(premium, fee cap - basefee) and the transaction fee cap. For instance, a high premium but low fee cap transaction may become less valuable to include in a larger block (with higher implied basefee), vs a low premium but high fee cap transaction. This was already noted to figure out how the transaction pool should be reordered and accessed under EIP-1559. There are likely good heuristics around this issue, in fact likely the same ones that transaction pools will use.\nPiece 4: Block-level data availability\nOur last case study relates to an idea discussed in several places by the community, including by Dankrad Feist in “Commit to pre-state instead of post-state on the executable beacon chain”. To the trained reader of protocol literature, the “free DA problem” seems to have plagued many of the most appealing proposals to fix various things. In particular, block co-creation, where multiple parties act concurrently on the transcript of a block, has been identified as a valuable thing. However, there is an inherent tension between allowing the input of transactions into the ledger by many (hopefully uncoordinated) parties, and outputting a ledger that is valid from the merging of all inputs.\nFor instance, two parties may each add the same transaction, or two parties may add conflicting transactions, e.g., two distinct transactions from the same user with the same nonce. Should these transactions be written on-chain, a rational party could hope to publish an invalid transaction that, while not executed, obtains data availability as consensus is formed on the transaction’s contents. An invalid transaction may not be able to pay for the footprint of its data, as its balance could have been drained by a previous (valid) transaction from the same user. This is more generally an issue when the chain performs asynchronous execution, i.e., commits transaction data to the transcript before transactions are executed by the nodes later on.\nThere are partial solutions, e.g., never writing the invalidated transactions on-chain. In FOCIL, the transactions contained in the inclusion lists (ILs) are never written on-chain until they are delivered in a block. To ensure validity of the block and the satisfaction of the list by the block, transactions contained in the list but not included in the block are run to determine whether they could have been validly included. Thus, no invalid transaction is ever written on-chain. Problem solved.\nBut the FOCIL model is explicitly fork-choice driven, which has its own trade-offs, and one may prefer a model where all “mini-blocks” produced by the uncoordinated parties are written on-chain, before being executed by nodes who receive them. This would be important to have, for instance if it was decided to reward in-protocol FOCIL committee members based on their produced ILs. In this case, invalid transactions in the mini-blocks consume data availability, and should pay for it. [3]\nMore robust solutions exist, such as Monad’s carriage cost. In this model, the users set aside some balance to pay for inclusion of their transaction, before it is executed and determined to be valid later on, a model of asynchronous execution. Though this solution lies closer to direct pricing, this represents a fairly drastic departure from the current account model of Ethereum, and so here too mediated pricing could help.\nWe compare here two cost models:\nc^t, where an invalid transaction simply is not feasibly allocated, i.e., if X contains some invalidity, then c^t(X)=+\\infty and can never be delivered under any resource limit.\nc^b, where an invalid transaction requires some payment charged to the broker once all costs are tallied. This payment should roughly compare to the extra load incurred by the chain from the carriage of blocks containing more data coming from invalid transactions.\nThen, for X containing invalid transactions, we have c^b(X) < c^t(X), i.e., c^b is finer than c^t. By obtaining a cost model which allows for the inclusion of invalid transactions in the transcript (simply because it doesn’t outright rule them out), we also allow ourselves to consider block construction mechanisms which possibly rely on the inclusion of invalid transactions.\nThis doesn’t tell us what the UX of mediated pricing looks like, when a broker is charged for the cost of invalid transactions being part of the block, especially when there isn’t a single broker determining the allocation of transactions and resources. We offer some thoughts here, but there is clearly more work to do on these questions.\nIn the simplest case, there is a single actor (the proposer) outputting a block over which they have full control in terms of its construction. If all nodes execute the transactions as they receive the block, they can simply reject the block as invalid if any transaction included by the proposer is invalid.\nIf execution is asynchronous, we may validly accept a block containing invalid transactions into the transcript of the chain, and we then must charge someone for having made the transaction data available. This someone ought then to be the proposer.\nObviously, this model is not so great if our wish is to keep the proposer unsophisticated, and responsible simply for including transactions into the transcript. If we want low-powered proposers on the network, but the ability to include many transactions into the chain (to be executed or verified by different nodes perhaps), we’d expect proposers to delegate the role of building their blocks to someone else. In other terms, we’d probably see the emergence of brokers who attempt to minimise the amount paid by proposers from the inclusion of invalid transactions. In the best case, the broker can simply make a block which includes no invalid transaction. If the broker cannot guarantee to not include invalid transactions, they pay for it themselves, while offering to the proposer the promised payment.\nExtending this model to a multi-proposer world could require different primitives, and it’s not clear to me yet which works best in this setting. If more than one proposer includes the (eventually) invalid transaction, the carriage cost could be paid out for as many proposers include this transaction [4]. In a mediated pricing framework for multiple proposers, it appears that the only solution is for each single proposer to be charged for the cost of their inclusion of an eventually invalid transaction, but this makes for a somewhat janky mechanism, where proposers are supposed to be uncorrelated yet incur much higher costs when this non-correlation induces them to invalidate each other’s transactions.\nWat do\nHopefully, the four pieces above would have convinced you of two things:\nBlock-level metering gives us access to better allocations of resources.\nMediated pricing meshes nicely with block-level metering, if we’re not confident enough with figuring out the appropriate cost-sharing rules for direct pricing.\nEquipped with this, we could look towards harder things to obtain, such as fee markets suitable for parallel execution (see e.g., this thread from 2017). Ensuring proper allocation of transactions to threads is a hard problem, but we could outsource it to brokers who figure this out for the protocol. The protocol then only needs to meter costs according to a simple enough model accounting for parallelism, e.g., metering the worst-case for a block maxing out a single thread, vs metering less for a block packing more execution over two distinct threads, i.e., making better use of the resources exposed by the protocol.\nThere are counter-arguments to even trying for block-level metering. Some may argue that with the rise of account abstraction, we may soon see a block become virtually a single large transaction, and thus transaction-level metering and block-level metering become one and the same, making most of the points above moot as long as the cost model e.g., accounts for state caching or charges for the data of invalid transactions. And indeed, some of the questions we are discussing here are relevant to the setting of account abstraction, as one of our previous posts on “Embedded fee markets“ argued.\nThere are also counter-arguments to mediated pricing. Yes, we already have a sophisticated supply chain behind the production of a block, but is it wise to overload it further with yet another responsibility? Here are some ideas: As long as inclusion lists keep ensuring good inclusion properties for users, there are not so many ways for a broker to abuse their position of resource allocators. One may argue that builders are already performing this function, by trading off certain transactions against others, e.g., MEV-carrying transactions against heavier blobs. Harnessing their savviness for competition towards the aim of a reduction in aggregate costs charged to the users, even as these costs may not find themselves as fairly redistributed as some idealised direct pricing, could still make us happy enough given the possible scaling benefits. We’d want to see enough competition to ensure that the surplus unlocked by smarter resource allocation by brokers isn’t monopolised by them, but we’d also expect the most efficient resource allocation to become table stakes soon enough given the history of searcher competition on these metrics.\n[1] Note that there exists a way to circumvent this issue by using a third-party coordinator to originate transactions from the desired unfunded account, by funding the coordinator privately beforehand. Read more in the “Gas Ticketing - A Backstage Pass to Ethereum Blocks” proposal by Toni.\n[2] The block proposer (a CL account) sets a feeRecipient variable to their EL address, assuming the block.coinbase role. If block-level base fee is allowed, the block.coinbase may be led to pay for the difference between baseFeePerGas * block.gasUsed and the sum of priority fees received from transactions included in the block, if they wish to sponsor transactions. In this case, without a more sophisticated mechanism, the block proposer could set an arbitrary feeRecipient to make some EL account (which the block proposer may not control themselves) pay for this difference. We’d require something like a signature from the declared feeRecipient address “agreeing” to be deducted from.\n[3] Still, checking that the non-included transactions were indeed invalid is costly, and this cost is still not quite adequately charged.\n[4] A payment or collateral lock-up increasing in the number of concurrent proposers has a similar flavour to the “Censorship Insurance Markets for BRAID” proposed by Jonah Burian. This could be considered a case of mediated pricing, where the censorship risk is shifted from the user paying for it directly to the protocol, to a “censorship insurer”, loosening the individual user constraint in favour of an aggregate constraint borne by the insurer.\n",
        "category": [
            "Economics"
        ],
        "discourse": [
            "fee-market"
        ]
    },
    {
        "title": "ArcPay: a Trustless Validium",
        "link": "https://ethresear.ch/t/arcpay-a-trustless-validium/16334",
        "article": "Thanks to @blockdev for the feedback and discussion.\nArcPay is a payment validium with a fully trustless escape hatch.\nWe achieve this by giving users ownership proofs, which can be used to recover funds in a shutdown procedure in case the operator stops updating the state and the state can’t be recovered. Users can forcibly acquire ownership proofs from the operator on-chain.\nBy having a slashable centralised operator, we also get strong instant finality guarantees, web2-style privacy guarantees, and cheap SNARK provers by using optimistic verification.\nConcretely, this gives ArcPay extremely low fees regardless of L1 gas prices, practically unbounded throughput, and rollup-style security guarantees.\nThe problem with Validia\nLike existing validia, users’ funds are held on-chain by a smart contract, but the state is stored off-chain in a centralised server, and the smart contract only knows the Merkle root of the state. To update the state, the operator makes a SNARK proof for the state transition function which checks signatures, etc, and this SNARK is checked by the smart contract.\nThis is an extremely scalable architecture that can handle an arbitrary number of transactions per second with very low costs. Gas costs for on-chain SNARK verification are amortised across all transactions, and the only marginal cost per transaction is for proof generation. Moreover, that marginal cost is small and constant with modern recursive SNARKs such as Nova + Spartan.\nThe downside of existing validia is that user’s funds will be locked forever if the operator refuses to update the on-chain state. Rollups solve this by making all the data necessary to rebuild the state available on-chain, but this imposes a gas cost on every transaction. Existing validia partially solve this by having a DAC (data availability committee) that replicates the state and can recover the state if m-of-n committee members act honestly. However, this is still a trust assumption on a set of external parties, and the costs of the DAC scale with its redundancy/security, i.e., n/m.\nBy contrast, ArcPay decentralises the DAC by giving each user the necessary data to recover their funds. Specifically, users hold Merkle proofs that show they owned a particular set of coins at a given point in time. They can use these to claim their coins back, even if the state is not entirely recovered.\nState Structure\nIn ArcPay, every coin is numbered, which is necessary to determine ownership during shutdown. The state of the validium is just a Merkle tree, where each leaf specifies the owner of a set of coins and has a unique id to prevent replay attacks.\n\nState2332×1856 93.1 KB\n\nNote, the Merkle tree doesn’t have to be ordered, either by coin or ID, but the coin ranges must be disjoint, which is ensured by the state transition proofs.\nThere are only 3 types of transaction: mint, send, and withdraw: mint moves coins from L1 to the validium; send moves coins within the validium; and withdraw sends coins back from the validium to L1. Send and withdraw require a signature, and that the signer actually owns the coins they’re trying to send. Mint requires that the user actually sent coins on L1. These checks are done by the state transition proof.\nNormal Operation\nArcPay has two distinct modes: normal operation, and shutdown. In normal operation, we assume that the operator doesn’t want to lose their stake. In shutdown, we only assume L1 correctness and liveness, and that the user has gathered appropriate ownership proofs during normal operation.\nCensorship Resistance\nDuring normal operation, censorship resistance is guaranteed by on-chain forcing.\nAny user can call the force_include function on the smart contract, which stores transactions in an accumulator until they are included in the state. The operator must address all forced transactions to update the state. If the state is not updated frequently, the operator is slashed and the validium is shut down.\nIdeally, users never have to use force_include, since posting a transaction on-chain costs gas, making the cost model essentially equivalent to a ZK rollup. Thankfully, the operator is incentivised to process transactions off-chain since they don’t earn any fees for processing forced transactions.\nIn short, the operator generally will not censor, and even if they do, the users can circumvent it, assuming that the operator isn’t slashed and L1 operates correctly.\nOwnership Proofs\nUsers need to get ownership proofs from the operator to make sure they can recover their funds in case of a shutdown. An ownership proof is just a Merkle proof for a leaf in the state at a given point in time, i.e., user X owns coins [a,b].\nIf the operator is honest, a user can query an API and get their ownership proofs. But if that fails, the user can use the smart contract’s force_ownership_proof function, where the user asks “who owned coin X at block b?” This request is stored on-chain, along with the time it was made.\nIf the operator doesn’t respond promptly, they are slashed and the validium is shut down. To respond, the operator must provide the relevant ownership proof. The smart contract checks the Merkle proof against the relevant historical state root stored on-chain. Note, we can avoid the gas cost of checking the Merkle proof on-chain by optimistically accepting the operator’s response and allowing users to slash the operator if the response is incorrect.\nLike with force_include, the operator is incentivised to respond to requests off-chain. For force_ownership_proof, this is because responding on-chain costs gas, and the operator can avoid those costs by addressing users’ needs off-chain.\nNote, letting anyone query who owns any coin is a privacy concern, but this can be mitigated with ZKPs.\nShutdown\nDuring shutdown, users have a month to prove ownership of their coins. Every coin is numbered and newer proofs invalidate older proofs for the same coins. If a user owned a given coin when the validium shut down, they know no one else can have a newer ownership proof for that coin.\nDuring the one-month claiming period, users post their ownership proofs on-chain. Once the claiming period is done, we run a deterministic resolution algorithm that figures out the most recent owners of each coin and outputs a Merkle tree that can be used to withdraw the funds.\n\nClaiming1496×1224 7.98 KB\n\nThe horizontal lines represent valid claims, and the green sections are the claims in the final Merkle tree. \nThere are many ways to approach the claiming/resolution algorithm. One simple solution is to verify every ownership proof on-chain as it’s posted and add it to an accumulator, then once the claiming period is over, someone can post a ZKP that takes the accumulator, checks every claim against one another, and outputs the Merkle root.\nOptimisations\nIt’s possible for each claim to only cost a few hundred gas by batching claims together into a ZKP so the Merkle proofs don’t have to be directly verified or even posted on-chain. This makes the cost of a claim approximately as expensive as a transaction in a ZKP rollup. However, that algorithm is beyond the scope of this document.\nEven if the claiming process is optimised to a few gas per claim, the limited throughput of L1 puts an upper bound on the number of claims that can be made. Moreover, the gas price spike caused by the shutdown of a very large ArcPay-style validium may make trustless withdrawal impractical for users with smaller deposits. Instead of shutting down entirely, we can put the validium into receivership, where users have the option of proving ownership on L1 as usual, or off-chain to a new operator. This requires ideas around promises described below, and is also beyond the scope of this document.\nMany rollups have centralised sequencers, which they use to offer a kind of instant finality. For example, Arbitrum offers “soft finality”, where the user can instantly learn the result of their transaction if they trust the sequencer.\nIn ArcPay, instant finality works similarly, except that it’s trustlessly enforceable.\nThe ArcPay operator responds to off-chain transactions with promises like Bob will have tokens [87, 97] in block 123. Later, users can use ownership proofs to show that the promise was broken. If a promise was broken, the operator must reimburse the user with ~100x the coins that were promised, or the operator will be slashed.\nExample Transaction\nSuppose Alice is buying an orange from Bob. She sends her transaction directly to the operator, who signs it, promising that the tokens will go to Bob in block 123. Alice gives the promise to Bob, who takes it as a strong guarantee of payment and gives the orange to Alice.\n\nAlice and Bob Interaction1804×1108 55.2 KB\n\nBob holds on to the promise until block 123, when he asks the operator for the ownership proof for coin 87 at block 123. If the operator is honest, they respond with a proof showing that they honoured their promise.\n\nBob and Sequencer Interaction1804×788 44.1 KB\n\nIf the operator doesn’t respond, he uses force_ownership_proof as described above. If the operator didn’t honour their promise Bob will prove it on L1 and demand compensation. If there is no compensation, the operator is slashed and the validium is shut down.\nForcing Delay\nTo make sure promises can be honoured, force_include must have a delay. If there were no delay, Alice could get the operator to promise that Bob will have tokens [87,97] in block 123, but then use force_include to send coins [87,97] to someone else, meaning the operator can’t honour their promise.\nIn ArcPay we do this with two accumulators, where one is locked and the other is unlocked at any given time. force_include adds transactions to the unlocked accumulator. When the state is updated, the locked accumulator is passed as input to the proof, which shows that all the transactions have been included. Then locked accumulator is zeroed out, and unlocked, and the unlocked accumulator is locked.\n\nTx Holding Pen2464×2060 109 KB\n\nThe operator can safely make promises about the state of the next block because they can know all the forced transactions that will be included in it by looking at the currently locked accumulator.\nHot Potatoes\nUsers may want to send their coins within the same block that they recieve them. However, for that to happen, the operator has to promise that two different parties will own the same coins in the same block, and this will leave the operator liable for breaking a promise.\nThe operator isn’t forced to make the second promise, but if we want to implement same block transactions, we can let the operator dismiss an allegation of a broken promise by proving that the user owned the coins in between blocks. To do this, the smart contract stores a Merkle root of intermediate states for each block which lets the operator prove that the promise was fulfilled as some point between the blocks.\nUnlike rollups or L1 chains, validia have privacy by default (assuming they use true zkSNARKs and a hiding hash function for the Merkle tree). ArcPay has trusted privacy in the style of web2 platforms - you and the company running the server know your financial details. Full privacy in the Zcash/Aztec style would require another layer of ZKPs, and introduces regulatory complexity, especially for a centralised entity.\nHowever, the force_ownership_proof function allows anyone to forcibly request the owner of any coin. To fix this, we only let people use force_ownership_proof for coins they own. Specifically, if the coin doesn’t belong to the requester, the operator responds with a ZKP proving that it doesn’t belong to the requester. This way, people can still acquire the ownership proofs they need to safeguard the coins in case of a shutdown, and they can still prove whether the operator broke a promise. To prove a broken promise, the user just presents the promise that Bob will have coins [1,10] in block 100, and the ZKP showing that Bob doesn't own coins [1,10] in block 100 to the chain and demands compensation.\nCurrently, there is a tradeoff between prover time and verifier time for all production-ready SNARKs. Generally, validia and rollups use Plonk-like systems or Groth16 for their fast verifiers and small proof size. On the other hand, STARK or Spartan proofs are cheaper to generate but are less practical to verify on-chain.\nBecause ArcPay has a centralised operator with a large stake, we can optimistically verify our state transition proofs. The proofs are only checked on-chain if someone finds that the proof is invalid. Then they get the proof checked on-chain, slash the operator, and earn a reward. The state is reverted to the last valid state and the validium is shut down. There must be a period where anyone can take the reward from the original slasher by showing that there was an earlier invalid proof.\nNote, this means there must be a withdrawal delay, and it weakens the security model from cryptographic to game theoretic. In the long run, there will probably be proof composition solutions that enable fast provers and fast verifiers, and optimistic verification will become unnecessary.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "data-availability",
            "layer-2",
            "zk-roll-up"
        ]
    },
    {
        "title": "Poqeth: Efficient, post-quantum signature verification on Ethereum",
        "link": "https://ethresear.ch/t/poqeth-efficient-post-quantum-signature-verification-on-ethereum/21554",
        "article": "TL,DR: We anticipate a future with account abstraction and quantum computers. In this, perhaps not-so-distant future, post-quantum signatures will be verified by on-chain contracts. In this new work, we implement and evaluate the gas cost of verifying four post-quantum signatures on-chain:  WOTS+, XMSS, SPHINS+, and MAYO. We invite the community to review, extend, and collaborate with us on poqeth, an open-source library that implements these verification algorithms in Solidity. We expect massive gas-cost improvements in the future. PRs, issues, comments and questions are welcome!\npoqeth Eprint: https://eprint.iacr.org/2025/091.pdf\npoqeth Github repo: GitHub - ruslan-ilesik/poqeth: poqeth: Efficient, post-quantum signature verification on Ethereum\nimage926×180 13.6 KB\nHow to choose an appropriate post-quantum signature scheme for Ethereum transactions?\nChoosing the right digital signature for Ethereum is non-trivial as the blockchain context has completely different limitations than, say, TLS. Large public keys are a no-go in the context of Ethereum. For more discussion, see the intro of our paper or Antonio’s latest ethresear.ch post.\nOne thing for sure: “The proof of the pudding is in the eating”; that is, we need to implement these signature schemes’ verification algorithms to get a real sense how fast they can be in the EVM.\nThis is exactly what we did in poqeth.\nTwo evaluated verification modes\nWe considered two verification modes: 1) on-chain verification, when the full signature is verified by the contract 2) Naysayer verification mode.\nIn the Naysayer verification mode, the contract only needs to check the signature if the signature is faulty, and even in that case, the contract only needs to be convinced about the incorrectness of the signature, which can be done much faster than verifying the entire signature.\nimage1419×547 101 KB\npoqeth: an extendible contract library for post-quantum signature verification on Ethereum\nWe decided to implement and evaluate three hash-based signature schemes (WOTS+, XMSS, and SPHINCS+) and a multivariate-quadratic signature scheme, MAYO. For each scheme, we propose optimal parameter choices that minimize the on-chain verification cost at NIST security level 1.\nFuture directions and open research problems:\nPrecompile contracts in the EVM for PQ signature verification: shall we enshrine some signature verification algorithms in the EVM? There are already two EIPs advocating for this in the case of Falcon. See EIP-7592 and EIP-7619. Or the protocol should just be more modular and only support, say SIMD operations, that would significantly speed up multivariate-quadratic signatures, such as MAYO.\nSuccinct proofs of signature verification. maybe in the future, we should just use very on-chain STARK proofs that attest to the validity of a bunch of PQ signatures. This approach can effectively amortize the verification cost of numerous PQ signatures. A similar approach is taken in a recent paper by Drake, Khovratovich, Kudinov and Wagner. See also this great work.\nBenchmarking more post-quantum signature algorithms: it’d be cool to extend the poqeth library with the implementation and evaluation of more PQ signatures. Please hit us up, if you are interested in working together on this topic.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": [
            "post-quantum"
        ]
    },
    {
        "title": "Fast (and Slow) L2→L1 Withdrawals",
        "link": "https://ethresear.ch/t/fast-and-slow-l2-l1-withdrawals/21161",
        "article": "Co-authored by @The-CTra1n , @linoscope , @AnshuJalan and @smartprogrammer , all Nethermind. Thanks to @Brecht and Daniel Wang from Taiko for their feedback on this solution. Feedback is not necessarily an endorsement.\nWe introduce a new fast path for L2 withdrawals to L1 within the same L1 slot, enabled by solvers. To mitigate risks for solvers and maximize participation, solvers can observe preconfirmed L2 batches containing fast withdrawal requests and condition solutions for these requests on the batch containing the requests being successfully proposed to L1. This ensures no risk for solvers beyond the need to wait for the L2 state root containing the request to be proven on L1 to receive payment for facilitating the withdrawal.\nThis post introduces a protocol which we describe as Fast & Slow (FnS) withdrawals to enable rollup users to atomically withdraw tokens from L2 to L1. When we say atomically withdraw, we mean that the first time the L2 withdrawal transaction appears on L1, tokens can be made available for the L1 user in the same slot without introducing reorg risk to the L2 bridge. This is achieved through solvers who facilitate these atomic withdrawals.\nAny L1 user can act as a solver. Prospective solvers observe L2 withdrawal requests on the L2 and solve the request on behalf of the L2 users on L1. This repurposes the entire L1 network as a network of solvers for fast L2 withdrawals. Leveraging that rollups post their transaction batches to the L1, FnS allows solvers to condition their withdrawal solutions on the sequence in which the L2 withdrawal exists. This allows the L1 solver to guarantee that the tokens being withdrawn by the L2 user will be made available to the solver when the state root containing the withdrawal request is proved on L1.\nWhen the state root containing a request is proven on L1, the L2 tokens requested for withdrawal become available on L1. This is the “slow withdrawal”, which always happens. The solver of a request gains unique access to the slow withdrawal tokens corresponding to the request. As such, L1 solvers must wait some time before the L2 tokens become available on L1. As mentioned though, opting in to a fast FnS withdrawal is fully optional for solvers with no token lock-up for solvers except when they must wait for a slow withdrawal after providing a valid solution. Therefore, it is up to the L2 user to provide a fee covering:\nthe cost of waiting to receive the L2 tokens on L1\nthe L1 fees for submitting the solution and collecting the slow withdrawal tokens from the bridge.\nHow does it work?\nIn the following, we capitalize the Request, the Solution of the Request, the Users submitting the Request, and the Solvers providing the Solution, to ensure the objects are clearly differentiated from the verbs.\nThe diagram below illustrates how the fast path withdrawal facilitated by Solvers works:\n1188×883 102 KB\n(A) L2 Users wanting fast withdrawals send a Request to the L2 FnS contract, including the tokens the User wishes to withdraw and the L1 address to which the tokens should be sent.\n(B) L1 Solvers (being a Solver is permissionless with no registration requirement) observing the FnS Request in an L2 batch through preconfirmations (or L1 mempool) can attempt to back-run the L2 batch with a Solution to satisfy the withdrawal Request. Solutions send the specified tokens to the specified destination address.\n(C) Solutions can be conditioned on the L2 batch containing the Request, ensuring no reorg risk for Solvers who execute the L2 block: if the L2 block is recorded, the Solution doesn’t execute.\n(D) At any time before a proven L2 state root containing the Request is provided to L1, a fast withdrawal via a Solution can take place.\n(E) After a proven state root containing the Request is executed on L1, the slow withdrawal path is triggered. At that point, the L2 tokens in the FnS contract are available for withdrawal from the L1 FnS contract.\n\n(E-1) If a valid Solution was provided for a valid Request (validity checked in state root proof), the Solver who submitted the Solution (only one Solution per Request gets accepted on L1) can withdraw the tokens from the L1 FnS contract.\n(E-2) Otherwise, the User can withdraw their tokens from the L1 FnS contract.\n\n\n(E-1) If a valid Solution was provided for a valid Request (validity checked in state root proof), the Solver who submitted the Solution (only one Solution per Request gets accepted on L1) can withdraw the tokens from the L1 FnS contract.\n(E-2) Otherwise, the User can withdraw their tokens from the L1 FnS contract.\nThis section specifies the algorithm for FnS withdrawals.\nGlossary of Smart Contracts and Transactions Used\nWe introduce the following terms, concepts, data structures, and smart contracts.\nL1 rollup contract. The smart contract on L1 where the rollup/L2 state transitions are recorded and proven.\nL2 transaction batch. The data structure through which L2 transactions are posted to the L1 smart contract representing the L2. Each L2 transaction batch can be accompanied by a signature verifying the signer proposed the batch.\nL2 proposer. With respect to a given L2 batch of transactions, the entity with permission to post the batch to the L1 rollup contract. This proposer role can either be:\n\npermissioned. In the case of centralized sequencer, or preconfirmer, the L2 proposer is the respective centralized sequencer or preconfirmer.\npermissionless. Anyone can sign for and submit the batch.\n\n\npermissioned. In the case of centralized sequencer, or preconfirmer, the L2 proposer is the respective centralized sequencer or preconfirmer.\npermissionless. Anyone can sign for and submit the batch.\nL2 Withdrawal Request: An L2 transaction from an L2 user specifying\n\nRequest ID. A unique identifier for each Request which is a collision resistant commitment to the other variables in this Request, e.g. a hash of the encodings of these variables.\nL2 input tokens. Sent to by the user (token address and amount)\nOutput Condition 1. L1 output tokens (token address and amount)\nOutput Condition 2. L1 output address. Address where the L1 output tokens must be received.\nNonce. Nonce used to generate a unique Request ID.\n\n\nRequest ID. A unique identifier for each Request which is a collision resistant commitment to the other variables in this Request, e.g. a hash of the encodings of these variables.\nL2 input tokens. Sent to by the user (token address and amount)\nOutput Condition 1. L1 output tokens (token address and amount)\nOutput Condition 2. L1 output address. Address where the L1 output tokens must be received.\nNonce. Nonce used to generate a unique Request ID.\nSolver: Entities that “solve” user Requests by satisfying the L1 output conditions of Requests.\nSolution: An L1 transaction from a Solver which satisfies the L1 output conditions of a Request. Although Solvers can solve Requests by any means producing the output conditions, only one Solution is allowed per Request ID. Solutions have the following inputs:\n\n( L2 input tokens, Output Condition 1, Output Condition 2, Nonce)\nRequest ID. Must correspond to the commitment function output results from the previous variables in the Solution. Note, the L1 smart contract does not need to read L2 state to match Solution “Request IDs” with actual Request IDs. This is the job of the Solver.\nChained L2 batch commitment. A commitment of (previous chained L2 transaction batch commitment, current L2 transaction batch commitment) pair. Needed to prevent double spends of Request input tokens.\nL1 Solver output address. Address where the L2 input tokens of Request ID are to be sent if Request ID was correctly solved.\nCalldata (where the actual solving is done, can be any instruction e.g. send the tokens to the L1 Solver contract, which then sends tokens according to the output conditions).\n\n\n( L2 input tokens, Output Condition 1, Output Condition 2, Nonce)\nRequest ID. Must correspond to the commitment function output results from the previous variables in the Solution. Note, the L1 smart contract does not need to read L2 state to match Solution “Request IDs” with actual Request IDs. This is the job of the Solver.\nChained L2 batch commitment. A commitment of (previous chained L2 transaction batch commitment, current L2 transaction batch commitment) pair. Needed to prevent double spends of Request input tokens.\nL1 Solver output address. Address where the L2 input tokens of Request ID are to be sent if Request ID was correctly solved.\nCalldata (where the actual solving is done, can be any instruction e.g. send the tokens to the L1 Solver contract, which then sends tokens according to the output conditions).\nL1 Solver Contract: An L1 smart contract which takes as input Solutions. For each solution, it should record the Request ID to prevent double solving (a bad thing). For each Solution, the L1 Solver Contract must verify that the L2 output conditions specified in the Solution were satisfied.\n(L1-L2) Bridge Contract: A contract that lives on L1. L2 tokens sent to specific burn contracts on L2 can be withdrawn on L1 from the Bridge contract when a proven L2 state root is provided to the bridge which contains those token burns on L2. For this document, we assume this is the only way to withdraw tokens from L2 to L1.\nL2 Solver Contract: An L2 smart contract where Requests are sent. Tokens sent to the L2 Solver Contract are burned and signalled for withdrawal on L1 from the Bridge Contract.\nL1 Solver Withdrawal Transaction: Contains:\n\nL2 State Root ID: ID of a proven L2 state root posted to L1.\nRequest ID.\nMerkle Proof: Proof of existence of Request ID in the state root corresponding to L2 State Root ID.\n\n\nL2 State Root ID: ID of a proven L2 state root posted to L1.\nRequest ID.\nMerkle Proof: Proof of existence of Request ID in the state root corresponding to L2 State Root ID.\nL1 User Withdrawal Transaction: Contains:\n\nL2 State Root ID: ID of a proven L2 state root posted to L1.\nA full L2 Withdrawal Request transaction: (L2 input tokens, L1 output token address, L1 output address, Nonce, Request ID)\nMerkle Proof: Proof of existence of Request ID in the state root corresponding to L2 State Root ID.\n\n\nL2 State Root ID: ID of a proven L2 state root posted to L1.\nA full L2 Withdrawal Request transaction: (L2 input tokens, L1 output token address, L1 output address, Nonce, Request ID)\nMerkle Proof: Proof of existence of Request ID in the state root corresponding to L2 State Root ID.\nProtocol Description\nWe will now step through the protocol.\nL2 User submits a valid Request to L2 Solver Contract.\nL2 proposer includes the Request in an L2 transaction batch, signs the batch.\nSolver reacts to Request in a valid signed batch:\n\nObserve a valid Request in a signed batch.\nConstruct a Solution to the Request (how to satisfy Request output conditions), according to the format specified in the Glossary.\nEither:\n\nSolver creates a bundle of L1 transactions, with the first transaction being the L2 batch containing the Request, and the second being the Solution to the Request\nSolver submits the Solution to a third party, e.g. the L2 proposer, another Solver solving another Request(s), to be bundled together with the L2 batch, with this bundle posted to L1.\n\n\n\n\nObserve a valid Request in a signed batch.\nConstruct a Solution to the Request (how to satisfy Request output conditions), according to the format specified in the Glossary.\nEither:\n\nSolver creates a bundle of L1 transactions, with the first transaction being the L2 batch containing the Request, and the second being the Solution to the Request\nSolver submits the Solution to a third party, e.g. the L2 proposer, another Solver solving another Request(s), to be bundled together with the L2 batch, with this bundle posted to L1.\n\n\nSolver creates a bundle of L1 transactions, with the first transaction being the L2 batch containing the Request, and the second being the Solution to the Request\nSolver submits the Solution to a third party, e.g. the L2 proposer, another Solver solving another Request(s), to be bundled together with the L2 batch, with this bundle posted to L1.\nL2 transaction batch is received on L1 to the L1 rollup contract.\n\nIf the L2 proposer is permissioned, verify the accompanying signature was from the L2 proposer (NOT the same as verifying msg.sender).\n\n\nIf the L2 proposer is permissioned, verify the accompanying signature was from the L2 proposer (NOT the same as verifying msg.sender).\nSolution received on L1 to L1 Solver contract. For given Solution of the form,\n(L2 input tokens,\nL1 output token address,\nL1 output address,\nNonce,\nRequest ID,\nChained L2 batch commitment,\nL2 solver output address,\nCalldata ),\ndo the following, or revert:\n\nVerify no other Solution has been submitted corresponding to Request ID.\nVerify Request ID matches the commitment of (L2 input tokens, L1 output token address, L1 output address, Nonce).\nVerify there exists L2 batches corresponding to chained L2 batch commitment.\nExecute Calldata.\nVerify Request’s Output Conditions were satisfied.\n\n\nVerify no other Solution has been submitted corresponding to Request ID.\nVerify Request ID matches the commitment of (L2 input tokens, L1 output token address, L1 output address, Nonce).\nVerify there exists L2 batches corresponding to chained L2 batch commitment.\nExecute Calldata.\nVerify Request’s Output Conditions were satisfied.\nState Root containing Request is proven and submitted with proof to the L1 rollup contract.\nSolver of a Request submits an L1 Solver Withdrawal Transaction to the Bridge Contract. Complete all of the following, or revert:\n\nVerify Withdrawal’s Merkle Proof is valid.\nVerify Solver address matches the L1 Solver Output Address corresponding to the Solution which corresponds to Request ID of the L1 Solver Withdrawal Transaction.\nSend L2 Input Tokens corresponding of the Request corresponding to Request ID to the Solver.\n\n\nVerify Withdrawal’s Merkle Proof is valid.\nVerify Solver address matches the L1 Solver Output Address corresponding to the Solution which corresponds to Request ID of the L1 Solver Withdrawal Transaction.\nSend L2 Input Tokens corresponding of the Request corresponding to Request ID to the Solver.\nUser who submitted a Request sends an L1 User Withdrawal Transaction to the Bridge Contract. Complete all of the following, or revert:\n\nVerify Withdrawal’s Merkle Proof is valid.\nVerify no Solution exists corresponding to Request ID.\nSend L2 Input Tokens of the Request corresponding to the User*.*\n\n\nVerify Withdrawal’s Merkle Proof is valid.\nVerify no Solution exists corresponding to Request ID.\nSend L2 Input Tokens of the Request corresponding to the User*.*\nAcross\nThe proposed solution can be viewed as a variant of an intent-based solver solution, similar to protocols like Across. However, a key distinction lies in enabling Solvers to condition their solutions on batches from the source chain, thereby eliminating the reorg risk of the source chain. This is possible because the source chain is a rollup, while the destination is the L1 it settles to—i.e., the source chain batch is submitted directly to the destination chain.\nAggregated Settlement\nAggregated settlement enables synchronous composability between L2s by conditioning one rollup’s settlement on the state of others. This allows a rollup to import messages from another, execute L2 blocks based on them, and later revert execution if the messages are found invalid by verifying the other rollup’s state root at the shared settlement time. This protocol effectively enables synchronous composability between L2s by utilizing the fact that the L2s share a settlement layer. However, aggregated solutions do not address L2→L1 composability, as the L1 cannot reorg based on the settled state of an L2.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "based-sequencing"
        ]
    },
    {
        "title": "Key Management for Autonomous AI Agents with Crypto Wallets",
        "link": "https://ethresear.ch/t/key-management-for-autonomous-ai-agents-with-crypto-wallets/21431",
        "article": "Autonomous AI Agents equipped with crypto wallets are attracting growing attention due to their capability to interact directly with blockchains and smart contracts. These agents can perform a variety of tasks, including sending and receiving tokens, calling smart contracts, and even writing and deploying smart contracts on-chain. Unlike traditional systems, these autonomous AI agents are proactive, capable of making independent decisions without direct human intervention. An example is an autonomous crypto trading agent which leverages sophisticated deep learning algorithms to execute trades by interacting with on-chain DEXes. In this scenario, a user might provide the agent with an initial fund and delegate trading decisions entirely to the agent, aiming for long-term profitability. This hands-off approach, powered by the agent’s ability to analyze market trends and execute trades autonomously, exemplifies the transformative potential of combining AI and crypto in decentralized finance (DeFi) and beyond.\nTo enable these promising capabilities, an AI Agent needs to possess a private key to initiate blockchain transactions. If the agent runs in a local device, such as a smartphone or a laptop, managing the private key becomes relatively straightforward. However, AI Agents often require substantial computational resources — for example, to run advanced large language models (LLMs) — making this simple design impractical for many use cases. To help address this challenge, below we informally define the problem:\nProblem definition: A user seeks to deploy an autonomous AI Agent that proactively acts on their behalf. The user provides the Agent with a private key which enables direct or indirect access to valuable on-chain crypto assets. Due to the significant computational demands of the Agent — such as running advanced deep learning models or performing resource-intensive tasks — it may need to operate in a potentially adversarial environment, such as a remote server. The challenge is to design a system such that, even in the event of a server compromise, the crypto assets accessible through the private key remain secure.\nBelow we sketch a few possible approaches to tackle to the above problem:\n\nTEE based: The first approach involves the user securely storing the Agent’s private key within a Trusted Execution Environment (TEE) and executing the entire AI Agent code inside the TEE. Provided the TEE remains uncompromised, adversaries would be unable to either alter the Agent’s code or extract the private key. However, while TEEs are designed to be secure, they could still be susceptible to sophisticated attacks targeting specific vulnerabilities in their implementation. Additionally, the use of TEEs may introduce performance overhead, as running code within the protected environment can be slower compared to execution outside of it.\n\n\niO based: Indistinguishable Obfuscation is a powerful cryptographic tool. As Vitalik discussed in this article, one direct application of iO is to hide the private key in the AI Agent code. The primary advantage of iO lies in its ability to ensure that, even if the key is included in the obfuscated code, adversaries should be unable to extract it, even when the code is executed on a remote server. However, iO is still in a nascent stage, both in terms of theoretical development and practical implementation. Current constructions of iO are highly resource-intensive, requiring significant computational overhead and large memory footprints, making them impractical for many real-world applications.\n\n\nMPC based: A more practical approach is to leverage cryptographic tools such as multi-party computation (MPC) and threshold signature scheme (TSS). In this setup, multiple instances of the AI Agent code are run in parallel across several worker nodes. In this setup, we run multiple instances of the AI Agent code in parallel with multiple worker nodes. The user splits the private key into multiple shares, and securely sends each share to a different worker node, ensuring that no single node possesses the entire key. To interact with the blockchain, the worker nodes execute a consensus algorithm to propose and agree on specific actions. Once consensus is achieved for a particular transaction, the nodes collaboratively execute an MPC-based threshold signature protocol to jointly sign the transaction. Crucially, this process allows the signature to be generated without reconstructing the private key in its entirety. This ensures that even if an adversary compromises some worker nodes, the private key remains protected, provided a majority of the nodes remain secure. Although this approach requires the additional overhead of running multiple instances of the AI Agent, it significantly enhances security while allowing the Agent to operate safely in untrusted environments.\n\nTEE based: The first approach involves the user securely storing the Agent’s private key within a Trusted Execution Environment (TEE) and executing the entire AI Agent code inside the TEE. Provided the TEE remains uncompromised, adversaries would be unable to either alter the Agent’s code or extract the private key. However, while TEEs are designed to be secure, they could still be susceptible to sophisticated attacks targeting specific vulnerabilities in their implementation. Additionally, the use of TEEs may introduce performance overhead, as running code within the protected environment can be slower compared to execution outside of it.\niO based: Indistinguishable Obfuscation is a powerful cryptographic tool. As Vitalik discussed in this article, one direct application of iO is to hide the private key in the AI Agent code. The primary advantage of iO lies in its ability to ensure that, even if the key is included in the obfuscated code, adversaries should be unable to extract it, even when the code is executed on a remote server. However, iO is still in a nascent stage, both in terms of theoretical development and practical implementation. Current constructions of iO are highly resource-intensive, requiring significant computational overhead and large memory footprints, making them impractical for many real-world applications.\nMPC based: A more practical approach is to leverage cryptographic tools such as multi-party computation (MPC) and threshold signature scheme (TSS). In this setup, multiple instances of the AI Agent code are run in parallel across several worker nodes. In this setup, we run multiple instances of the AI Agent code in parallel with multiple worker nodes. The user splits the private key into multiple shares, and securely sends each share to a different worker node, ensuring that no single node possesses the entire key. To interact with the blockchain, the worker nodes execute a consensus algorithm to propose and agree on specific actions. Once consensus is achieved for a particular transaction, the nodes collaboratively execute an MPC-based threshold signature protocol to jointly sign the transaction. Crucially, this process allows the signature to be generated without reconstructing the private key in its entirety. This ensures that even if an adversary compromises some worker nodes, the private key remains protected, provided a majority of the nodes remain secure. Although this approach requires the additional overhead of running multiple instances of the AI Agent, it significantly enhances security while allowing the Agent to operate safely in untrusted environments.\nScreenshot 2025-01-12 at 11.16.29 PM1246×1126 31.5 KB\nSNARK based: In this approach, we run a SNARK prover along with the Agent in the powerful server. Meanwhile, we run the corresponding SNARK verifier in a local personal device (smartphone, laptop, etc.). The local personal device also possesses the private key. The user first generates a cryptographic commitment to the AI Agent code and publish it on the blockchain. Then, each time the server generates a transaction requiring the signature of the private key, the local device uses the SNARK verifier to ensure that the transaction is generated by the committed Agent code. If the SNARK verification succeeds, the local device signs the transaction with the private key and submits the signed transaction to the blockchain. Unlike the MPC-based approach, this method eliminates the need to run multiple copies of the AI Agent code. However, despite recent advancements in zkML, generating SNARK proofs for cutting-edge deep learning models remains highly challenging due to the computational complexity involved. Nonetheless, if the Agent code is relatively simple or if the SNARK proof is required only for specific parts of the Agent’s logic, this approach becomes a practical and efficient solution.\nScreenshot 2025-01-12 at 11.03.19 PM820×1262 22.6 KB\nThe above outlines several potential solutions we are exploring to address the AI Agent key management challenge. We welcome any feedback or suggestions to refine and improve these approaches!\n",
        "category": [
            "Cryptography",
            "Multiparty Computation"
        ],
        "discourse": []
    },
    {
        "title": "You can *kinda* abuse ECRECOVER to do ECMUL in secp256k1 today",
        "link": "https://ethresear.ch/t/you-can-kinda-abuse-ecrecover-to-do-ecmul-in-secp256k1-today/2384",
        "article": "Here’s the code for elliptic curve signature recovery, copied from pybitcointools, and cleaned of details that are irrelevant to our exposition\nSuppose that we feed in msghash=0, and s=r*k for some k. Then, we get:\nGz = 0\nXY = (r,y) * r * k\nQr = (r,y) * r * k\nQ = (r, y) * r * k * inv(r) = (r, y) * k\nHence, the elliptic curve signature feeds out k times the point (r, y), where r and k are values you feed in, and you specify the parity of y via v.\nUnfortunately, the secp256k1 precompile outputs the truncated hash of Q, and not Q itself. But you can get around that by requiring the function caller to submit Q as a witness. The extra cost of 64 bytes of data is ~4000 gas, so altogether you can get an ECMUL with <10k gas, under a quarter of what you need if you use the ECMUL precompile that uses the alt_bn128 curve.\nThis could be used as a ghetto hack to optimize ring signatures in the EVM today, as well as other applications.\n",
        "category": [
            "Applications"
        ],
        "discourse": []
    },
    {
        "title": "Vocdoni Protocol: Enabling Decentralized Voting for the Masses with ZK Technology",
        "link": "https://ethresear.ch/t/vocdoni-protocol-enabling-decentralized-voting-for-the-masses-with-zk-technology/21036",
        "article": "At Vocdoni, we’ve spent the last six years advancing decentralized voting solutions, focusing on bridging web2 applications with web3 technologies. We’ve successfully executed high-stakes voting for organizations such as football clubs, city councils, associations, political parties, professional bodies, movements under prosecution and more.\nUntil today, we have been relying on a customized proof-of-authority Layer 1 network.\nBut we believe it’s now time to transition to a zero-knowledge (zk) based infrastructure to achieve full decentralization and address the main challenges of digital voting systems\nOn the road to this new protocol implementation, we would like to receive feedback from the Ethereum community .\nBy taking ideas from our expertise, MACI, and others. We introduce a new universal voting protocol that tackles critical issues like receipt-freeness, voter privacy, scrutiny transparency, universal auditing, and eliminating the need for a trusted coordinator.\nDesigned for scalability and accessibility, the system enables high-frequency, low-cost voting suitable for mass adoption.  We laverage on zkSNARKs and threshold homomorphic encryption (ElGamal), to ensure end-to-end verifiability and anonymity for the end user.\nA decentralized zkSNARK-based state machine, operating as specialized layer 2 on the Ethereum blockchain, provides censorship-resistance, integrity, trutless operation and a transparent scrutiny of results. A distributed key generation (DKG) among sequencers, coordinated via smart contracts, allows for secure and decentralized encryption key creation without reliance on a central authority.\nMost components have been implemented using accessible technologies and have undergone proof-of-concept testing, confirming that the protocol is practical and ready for immediate deployment. We plan to launch the testnet in Q1–Q2 2025.\nOur implementation uses Circom and SnarkJS on the voter side, enabling voting from any device, including smartphones and web browsers. For the sequencers, we use Gnark with curves BLS12-377 and BW6-761 for native recursion in vote aggregation. This setup produces a final BN254 proof that can be verified on Ethereum.\nFocusing on decentralization, we designed the sequencer to operate on accessible machines—CPU-based systems with 64 GiB of memory—so that participation doesn’t require specialized hardware.\nOrganizers set up and manage voting processes, defining parameters such as voting options, duration, and voter registries (census).\nVoters interact with the system through user-friendly interfaces, enabling them to cast their votes securely and privately. Voters generate zkSNARKs to prove that their encrypted votes comply with voting rules without revealing their choices.\nSequencers are specialized nodes responsible for collecting votes, verifying their validity, and updating the shared state. They participate in the Distributed Key Generation (DKG) protocol to collaboratively generate encryption keys without any single party controlling the private key.\nimage1383×1076 37.2 KB\nPrivacy is maintained using homomorphic encryption. Votes are encrypted with the ElGamal cryptosystem over elliptic curves, allowing the aggregation of encrypted votes without decrypting individual ballots, thus keeping voter choices confidential.\nIntegrity is ensured through the collaborative efforts of a decentralized network of sequencers, who maintain a shared state represented by a Merkle tree that summarizes the current status of the voting process, including accumulated votes and nullifiers to prevent double voting. Each time the state is updated with new votes, a sequencer generates a zkSNARK proof attesting to the validity of the state transition.\nReceipt-freeness is achieved by preventing voters from being able to prove to third parties how they voted, mitigating risks of coercion and vote-buying. This is accomplished through ballot re-encryption and vote overwriting mechanisms. When a voter submits an encrypted ballot, sequencers re-encrypt it before storing it in the state, making it computationally infeasible to link the original and re-encrypted ciphertexts. Voters are also allowed to overwrite their votes; if a voter casts a new vote, the sequencer subtracts the previous encrypted vote from the tally and adds the new one. Regular re-encryption of random ballots by sequencers conceals when overwrites occur, enhancing receipt-freeness by making it indistinguishable whether a ballot was overwritten or simply re-randomized.\nThe system utilizes the ElGamal threshold encryption scheme over the elliptic curve bn254, which provides additive homomorphic properties essential for securely aggregating votes.\nEncryption\nA voter’s choice is represented as a message m \\in \\mathbb{Z}_q. To encrypt the vote:\nThe voter encodes the message as a point on the elliptic curve: M = m G.\nThe voter selects a random scalar k \\in \\mathbb{Z}_q^*.\nThe ciphertext is computed as: C = (C_1, C_2) = (k G, M + k H).\nHomomorphic Addition\nThe ElGamal cryptosystem over elliptic curves supports additive homomorphism for messages represented as points. Given two ciphertexts (C_1^{(1)}, C_2^{(1)}) and (C_1^{(2)}, C_2^{(2)}), their component-wise addition yields:\nC_1^{(\\text{sum})} = C_1^{(1)} + C_1^{(2)}\nC_2^{(\\text{sum})} = C_2^{(1)} + C_2^{(2)}\nThe aggregated ciphertext decrypts to the sum of the messages:  M^{(\\text{sum})} = M_1 + M_2\nThreshold Decryption\nAfter the voting period ends, the sequencers collaboratively decrypt the aggregated ciphertext. Each sequencer P_j computes a partial decryption share:\nCompute:  D_j = s_j C_1.\nThe partial decryptions are combined using Lagrange interpolation coefficients \\lambda_j:\nD = \\sum_{j \\in T} \\lambda_j D_j = s C_1\nwhere T is a set of at least t sequencers.\nThe plaintext message is recovered by computing:\nM = C_2 - D = M + k H - s k G = M\nThe final result m is obtained by solving M = m G, which yields m.\nTo eliminate the need for a trusted authority, the encryption key used for ballot encryption is generated collaboratively by the sequencers through a Distributed Key Generation protocol, which proceeds as follows:\nInitialization: Let G be the generator of an elliptic curve group of prime order q. The threshold t and the number of sequencers n are predefined, with t \\leq n.\nSecret Sharing: Each sequencer P_i randomly selects a secret polynomial f_i(x) of degree t - 1, where f_i(0) = a_{i,0}, and the coefficients a_{i,j} are chosen uniformly at random from \\mathbb{Z}_q.\nCommitments: Each sequencer publishes commitments to their polynomial coefficients: C_{i,j} = a_{i,j} G \\quad \\text{for} \\quad j = 0, \\ldots, t - 1.\nShare Distribution: Sequencer P_i computes shares for every other sequencer P_j: s_{i,j} = f_i(j), \\quad \\text{for} \\quad j = 1, \\ldots, n\nThese shares are securely transmitted to the respective sequencers using a simplified version of ECIES.\nVerification: Each sequencer P_j verifies the received shares s_{i,j} by checking: s_{i,j} G \\stackrel{?}{=} \\sum_{k=0}^{t - 1} C_{i,k} \\cdot j^k\nPrivate Key Share Computation: Each sequencer computes their private key share: s_j = \\sum_{i=1}^n s_{i,j} \\mod q\nPublic Key Computation: The collective public key is computed as: H = \\sum_{i=1}^n C_{i,0} = s G, where s = \\sum_{i=1}^n a_{i,0} \\mod q is the aggregate private key known only in a distributed form among the sequencers.\nThe vote consists of several components and mechanisms listed below.\nProcess Identifier: A unique identifier \\text{ProcessId} for the voting process. This ensures that votes are correctly associated with the specific voting event and prevents cross-process interference.\nCensus Proof: A Merkle proof demonstrating the voter’s inclusion in the authorized voter registry (census). This proof allows the sequencers to verify the voter’s eligibility without revealing the entire voter list, preserving privacy and efficiency.\nIdentity Commitment: The voter computes a commitment using a cryptographic hash function: C = Hash(\\text{Address} \\parallel \\text{ProcessId} \\parallel s), where H is a cryptographic hash function, \\text{Address} is the voter’s unique identifier (such as a public key or an address), and s is a secret known only to the voter.\nBy incorporating the secret s into the commitment C, we effectively detach the nullifier from any direct association with the voter’s identity in the publicly accessible data. This means that even if future quantum computing advancements were to compromise the ElGamal encryption and reveal the contents of encrypted ballots, there would be no practical method to link a decrypted ballot back to a specific voter’s address using the nullifier.\nNullifier: To prevent double voting and handle vote overwriting, the voter computes a nullifier: N = Hash(C \\parallel s). The nullifier acts as a one-time token that uniquely represents the voter’s participation without revealing their identity.\nBy avoiding the use of the private key or deterministic signatures when computing the nullifier, we ensure compatibility with hardware wallets and non-deterministic signature schemes.\nBallot: The ballot represents the voter’s choices as an array of individual selections that must adhere to the ballot protocol rules defined by the organizer. This flexible approach allows the protocol to support various voting configurations, including range voting, ranking, quadratic voting and more.\nSuppose an organizer wants to implement a quadratic voting system with the following parameters:\nMaximum Selections: Up to 5 options can be selected.\nValue Range: Each selection must be a non-negative integer.\nTotal Cost Constraint: The sum of the squares of the selections must not exceed a budget of 100 credits.\nUnique Values Constraint: Duplicate values are allowed.\nThe protocol would enforce that for a ballot \\mathbf{m} = (m_1, m_2, \\ldots, m_5):\nEach m_i \\geq 0.\n\\sum_{i=1}^5 m_i^2 \\leq 100.\nEncryped Ballot: The voter encrypts their ballot using the homomorphic ElGamal encryption scheme, proceeding as follows:\nThe voter’s choices are encoded into a message vector \\mathbf{m} = (m_1, m_2, \\ldots, m_n), where each m_i corresponds to a selection in the ballot array.\nEach element m_i is encoded as a point on the elliptic curve: M_i = m_i G, where G is the generator of the curve.\nThe voter selects a random scalar k \\in \\mathbb{Z}_q^*, where q is the order of the curve.\nThe ciphertext is computed as: C = (C_1, C_2) = \\left( k G,\\; \\sum_{i=1}^n M_i + k H \\right)\nwhere H is the public key obtained from the Distributed Key Generation protocol.\nZero-Knowledge Proof (zkSNARK): The voter generates a zkSNARK that proves, without revealing the ballot content, that:\nCorrect Encryption: The encrypted ballot is correctly formed according to the encryption scheme.\nProtocol Compliance: The voter’s selections adhere to the ballot protocol rules defined by the organizer.\nCorrect Nullifier and Commitment: The nullifier and commitment are correctly computed using the voter’s secret s and address.\nKnowledge of Secrets: The voter knows the secret s and the random scalar k used in encryption.\nSignature: The voter signs necessary components of the vote using their private key associated with their address. This authenticates the vote and binds it to the voter’s identity in a verifiable manner. Many signature schemes may be supported (ECDSA, EdDSA, RSA, etc.).\nThe State Merkle Tree is a cryptographic data structure that allows efficient and secure verification of the data it contains. The tree is structured to store various types of information at predefined indices or addresses:\nProcess Parameters: Stored at static indices, containing essential information such as the \\text{ProcessId}, the root of the census Merkle tree (\\text{censusRoot}), ballot protocol configurations, and the public encryption key H generated through the DKG protocol.\nResults Accumulators: Two accumulators are maintained to handle vote additions and subtractions:\n\nAddition Accumulator (C_{\\text{add}}): Stores the homomorphically aggregated encrypted votes that have been added to the tally.\nSubtraction Accumulator (C_{\\text{sub}}): Stores the homomorphically aggregated encrypted votes that have been subtracted due to vote overwrites.\n\n\nAddition Accumulator (C_{\\text{add}}): Stores the homomorphically aggregated encrypted votes that have been added to the tally.\nSubtraction Accumulator (C_{\\text{sub}}): Stores the homomorphically aggregated encrypted votes that have been subtracted due to vote overwrites.\nNullifiers: Stored to prevent double voting. Each nullifier N is associated with a voter’s commitment and is unique to that voter for the specific voting process.\nCommitments: Stored to keep track of voter participation and to facilitate vote overwriting.\nSequencers are responsible for processing new votes and updating the shared state. Each state transition involves the following steps:\n\nBatch Collection of Votes: The sequencer collects a batch of up to N new votes from voters. Batching votes enhances efficiency and scalability, allowing the sequencer to process multiple votes simultaneously. The value of N is determined by system parameters that balance computational constraints and network throughput.\n\n\nVerification of Votes: For each vote in the batch, the sequencer performs:\n\nzkSNARK Proof Verification: Ensures that the zero-knowledge proof submitted with each vote is valid and that the vote complies with the protocol rules, including correct encryption, adherence to ballot protocol constraints, and proper computation of the nullifier and commitment.\nEligibility Check: Verifies the voter’s eligibility by checking the provided census Merkle proof against the stored \\text{censusRoot} in the state.\nDouble Voting Prevention: Checks whether the nullifier N already exists in the state. If it does not, the vote is processed as a new vote. If it does, the vote is considered a vote overwrite.\n\n\nzkSNARK Proof Verification: Ensures that the zero-knowledge proof submitted with each vote is valid and that the vote complies with the protocol rules, including correct encryption, adherence to ballot protocol constraints, and proper computation of the nullifier and commitment.\nEligibility Check: Verifies the voter’s eligibility by checking the provided census Merkle proof against the stored \\text{censusRoot} in the state.\nDouble Voting Prevention: Checks whether the nullifier N already exists in the state. If it does not, the vote is processed as a new vote. If it does, the vote is considered a vote overwrite.\n\nHandling Vote Overwrites: If a voter submits a new vote with the same nullifier N, the sequencer:\n\nSubtracts the previous encrypted vote from the subtraction accumulator C_{\\text{sub}}.\nAdds the new encrypted vote to the addition accumulator C_{\\text{add}}.\nReplaces the new encrypted ballot within the State.\n\n\nSubtracts the previous encrypted vote from the subtraction accumulator C_{\\text{sub}}.\nAdds the new encrypted vote to the addition accumulator C_{\\text{add}}.\nReplaces the new encrypted ballot within the State.\n\nRandom Re-encryptions: To enhance receipt-freeness and prevent linkage between votes and voters, the sequencer performs random re-encryptions of existing encrypted ballots:\n\nSelects a random subset of encrypted ballots in the state.\nRe-encrypts each selected ballot by adding an encryption of zero, using a new random scalar.\nUpdates the encrypted ballots in the state with the re-encrypted versions.\n\n\nSelects a random subset of encrypted ballots in the state.\nRe-encrypts each selected ballot by adding an encryption of zero, using a new random scalar.\nUpdates the encrypted ballots in the state with the re-encrypted versions.\n\nHomomorphic Aggregation of Votes: The sequencer updates the accumulators using the homomorphic properties of the ElGamal encryption:\n\n\nGeneration of State Transition zkSNARK: The sequencer generates a zkSNARK proof that attests to the validity of the state transition from the previous root \\text{Root1} to the new root \\text{Root2}. The zkSNARK proof verifies all previous constraints and operations.\n\n\nOn-Chain Submission: The sequencer submits:, the updated state root \\text{Root2}. The proof attesting to the validity of the state transition. A hash commitment to the data blob containing the votes and state updates, ensuring data availability.\n\nBatch Collection of Votes: The sequencer collects a batch of up to N new votes from voters. Batching votes enhances efficiency and scalability, allowing the sequencer to process multiple votes simultaneously. The value of N is determined by system parameters that balance computational constraints and network throughput.\nVerification of Votes: For each vote in the batch, the sequencer performs:\nzkSNARK Proof Verification: Ensures that the zero-knowledge proof submitted with each vote is valid and that the vote complies with the protocol rules, including correct encryption, adherence to ballot protocol constraints, and proper computation of the nullifier and commitment.\nEligibility Check: Verifies the voter’s eligibility by checking the provided census Merkle proof against the stored \\text{censusRoot} in the state.\nDouble Voting Prevention: Checks whether the nullifier N already exists in the state. If it does not, the vote is processed as a new vote. If it does, the vote is considered a vote overwrite.\nHandling Vote Overwrites: If a voter submits a new vote with the same nullifier N, the sequencer:\nSubtracts the previous encrypted vote from the subtraction accumulator C_{\\text{sub}}.\nAdds the new encrypted vote to the addition accumulator C_{\\text{add}}.\nReplaces the new encrypted ballot within the State.\nRandom Re-encryptions: To enhance receipt-freeness and prevent linkage between votes and voters, the sequencer performs random re-encryptions of existing encrypted ballots:\nSelects a random subset of encrypted ballots in the state.\nRe-encrypts each selected ballot by adding an encryption of zero, using a new random scalar.\nUpdates the encrypted ballots in the state with the re-encrypted versions.\nHomomorphic Aggregation of Votes: The sequencer updates the accumulators using the homomorphic properties of the ElGamal encryption:\nGeneration of State Transition zkSNARK: The sequencer generates a zkSNARK proof that attests to the validity of the state transition from the previous root \\text{Root1} to the new root \\text{Root2}. The zkSNARK proof verifies all previous constraints and operations.\nOn-Chain Submission: The sequencer submits:, the updated state root \\text{Root2}. The proof attesting to the validity of the state transition. A hash commitment to the data blob containing the votes and state updates, ensuring data availability.\nVocdoni introduces a new token (VOC) to align incentives among participants and ensure the sustainability of the decentralized voting ecosystem. The token serves multiple essential functions: it incentivizes sequencers, facilitates payments for voting processes, and enables decentralized governance.\nSequencers must stake tokens as collateral to participate in the network, promoting honest behavior and network security. They earn rewards in VOC tokens based on their contributions to processing valid votes and maintaining the network’s integrity.\nOrganizers use tokens to pay for creating and managing voting processes. The costs depend on factors such as the maximum number of votes (\\text{maxVotes}), the voting duration (\\text{processDuration}), and the desired security level, which relates to the number of participating sequencers.\nThe total cost for a voting process is calculated using the formula:\n\\text{totalCost} = \\text{baseCost} + \\text{capacityCost} + \\text{durationCost} + \\text{securityCost}\nComponents of the Cost:\n\nBase Cost: \\text{baseCost} = \\text{fixedCost} + \\text{maxVotes} \\cdot p, where \\text{fixedCost} is a fixed fee, and p is a small linear factor.\n\n\nCapacity Cost: \\text{capacityCost} = k_1 \\left( \\frac{\\text{totalVotingProcesses}}{\\text{totalSequencers} - \\text{usedSequencers} + \\epsilon} \\cdot \\text{maxVotes} \\right)^a, where k_1 is a scaling factor, a controls non-linearity, and \\epsilon is a small number to prevent division by zero.\n\n\nDuration Cost: \\text{durationCost} = k_2 \\cdot \\text{processDuration}^b, with k_2 as a scaling factor and b controlling the scaling based on duration.\n\n\nSecurity Cost: \\text{securityCost} = k_3 \\cdot e^{c \\left( \\frac{\\text{numSequencers}}{\\text{totalSequencers}} \\right)^d}, where k_3 is a scaling factor, and c, d control the exponential scaling related to the number of sequencers used.\n\nBase Cost: \\text{baseCost} = \\text{fixedCost} + \\text{maxVotes} \\cdot p, where \\text{fixedCost} is a fixed fee, and p is a small linear factor.\nCapacity Cost: \\text{capacityCost} = k_1 \\left( \\frac{\\text{totalVotingProcesses}}{\\text{totalSequencers} - \\text{usedSequencers} + \\epsilon} \\cdot \\text{maxVotes} \\right)^a, where k_1 is a scaling factor, a controls non-linearity, and \\epsilon is a small number to prevent division by zero.\nDuration Cost: \\text{durationCost} = k_2 \\cdot \\text{processDuration}^b, with k_2 as a scaling factor and b controlling the scaling based on duration.\nSecurity Cost: \\text{securityCost} = k_3 \\cdot e^{c \\left( \\frac{\\text{numSequencers}}{\\text{totalSequencers}} \\right)^d}, where k_3 is a scaling factor, and c, d control the exponential scaling related to the number of sequencers used.\nSequencers earn rewards based on the number of votes processed and the number of vote rewrites (including re-encryptions for receipt-freeness). The total reward for a sequencer i is calculated as:\n\\text{sequencerReward}_i = R \\left( \\frac{\\text{votes}_i}{\\text{maxVotes}} \\right) + W \\left( \\frac{\\text{voteRewrites}_i}{\\text{totalRewrites}} \\right)\nThis is subject to the constraints:\n\\frac{\\text{voteRewrites}_i}{\\text{votes}_i} \\leq T and R > W\nEnsuring that sequencers prioritize processing new votes over rewrites. Here, R and W are parts of the reward pool allocated for processing votes and vote rewrites, respectively, and T is a predefined constant limiting the number of rewrites per vote.\nSequencers who fail to meet their obligations may have their collateral slashed, calculated as:\n\\text{SlashedAmount}_i = s \\cdot \\text{StakedCollateral}_i\nwhere s is the slashing coefficient (0 \\leq s \\leq 1).\nFull version of the whitepaper: https://whitepaper.vocdoni.io\nList of repositories where the MVP version of the protocol is being implemented:\nCircom circuits for voter GitHub - vocdoni/z-ircuits: Vocdoni Z snark circuits.\nSequencer crypto primitives GitHub - vocdoni/gnark-crypto-primitives: A set of custom circuits writted in Gnark that are required to support anonymous voting on Vocdoni.\nCircom to Gnark parser GitHub - vocdoni/circom2gnark: Circom to Gnark Groth16 parser and recursion example\nElGamal and DKG sandbox: GitHub - vocdoni/vocdoni-z-sandbox\nContact: pau [at] vocdoni [dot] io\nDiscord: https://chat.vocdoni.io\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": [
            "governance",
            "zk-roll-up"
        ]
    },
    {
        "title": "Block-Level Warming",
        "link": "https://ethresear.ch/t/block-level-warming/21452",
        "article": "The proposal is to introduce block-level address and storage key warming, allowing accessed addresses and storage keys to maintain their “warm” status throughout an entire block’s execution. Accessed slots can be effectively cached at the block level, allowing for this optimization.\nFurthermore, a future transition to multi-block warming could unlock additional potential.\nMotivation\nCurrently, the EVM’s storage slot warming mechanism operates at the transaction level, requiring each transaction to “warm up” slots independently, even when accessing the same storage locations within the same block. This design doesn’t take advantage of the fact that modern node implementations can effectively cache storage access patterns at the block level. By extending the slot warming duration to the block level, we can:\nReduce redundant warming costs for frequently accessed slots\nBetter align gas costs with actual computational overhead\nImprove overall network throughput without compromising security\nImpact Analysis\nFor the following, I analyze 22,272 blocks between 12 and 15 January 2025. The opcodes relevant for this analysis are those that distinguish between “warm” and “cold” access: These are SSTORE, SLOAD, BALANCE, EXTCODESIZE, EXTCODECOPY, EXTCODEHASH, CALL, CALLCODE, DELEGATECALL and STATICCALL.\nThe code used for conducting this analysis can be found here.\nUnlocking Potential\nBlock-level warming presents an immediate opportunity to achieve 5-6% efficiency gains in gas consumption.\nAn initial analysis shows significant improvements in efficiency when comparing block-level warming to the current situation. In typical blocks consuming 15m gas, block-level warming could save approximately 0.5-1m gas per block - gas which would otherwise be spent on warming addresses and storage keys that were already accessed earlier in the block.\nEven more promising results emerge when we extend the warming window. When accessed addresses and storage keys remain warm for 5 blocks, we can observe savings of up to 1.5m gas (~10%), as illustrated in the green line of the following chart:\nsave_over_time (4)900×500 66.8 KB\nThe length of the warming window significantly impacts potential efficiency gains. This analysis shows that:\nA 5-block warming window yields approximately 10% gas savings\nExtending to 15 blocks increases savings to 15%\nThis relationship between window size and efficiency gains is demonstrated here:\nslot_warming_potential (1)700×500 23.3 KB\nTo understand the maximum potential of this approach, we compared these results against a theoretical scenario where all addresses and storage keys remain permanently warm (eliminating all cold access costs):\nslot_warming_potential_2 (2)700×500 22.2 KB\nThis comparison highlights that over 35% of current gas consumption is attributed to the costs of warming addresses and storage keys. The efficiency curve shows a steep initial improvement before quickly plateauing, suggesting that even relatively short warming windows can capture a significant portion of the potential benefits.\nAmong the most popular addresses and storage slots we find candidates such as WETH, USDC and Uniswap. However, there are also less obvious contracts (e.g., 0x399121f5b1bab4432ff8dd2ac23e5f6641e6f309) that primarily serve to burn gas (via warm SSTOREs). This tactic is used to circumvent on-chain priority gas limits, thereby increasing the total priority fees paid to the transaction’s fee recipient. For an example, see this transaction.\nMechanics\nWhen a storage slot is accessed within a block:\nThe first access to a slot in a block incurs the cold access cost as of EIP-2929.\nAll subsequent accesses to the same slot within the same block incur only the warm access cost as of EIP-2929.\nThe warm/cold status resets at block boundaries\nThe following uses the ethereum/execution-specs. Find a first draft implementation here.\nThe proposal modifies the block execution process to maintain block-level sets of accessed addresses and storage slots.\nThis code illustrates the implementation of block-level slot warming at the execution layer. The block_level_accessed_addresses and block_level_accessed_storage_keys sets are maintained throughout the block’s execution and passed to each transaction’s environment.\nThis adds the block-level accessed addresses and storage keys to the preaccessed addresses and storage keys.\nAs a result, from the perspective of a transaction, block-level accessed addresses and storage keys are treated the same as precompiles or the coinbase address.\nThe message call processing tracks accessed addresses and storage keys during execution, which are then propagated back up to the transaction level and ultimately to the block level.\nRationale\nThe proposal builds on several key observations:\n\nCaching Efficiency: Today’s Ethereum clients already implement sophisticated caching mechanisms at the block level. Extending address and storage key warming to match this caching behavior better aligns gas costs with actual computational costs.\n\n\nBackward Compatibility: The worst-case scenario for any transaction remains unchanged - it will never pay more than the current cold access cost for its first access to a slot.\n\n\nFirst Access Warming System\nThe proposed mechanism operates on a “first warms for all” principle: when a transaction first accesses and warms multiple addresses or storage slots in a block, it bears the entire warming cost. Subsequent transactions can then access these warmed slots without additional costs.\nThis approach aligns well with current dynamics, as early block positions are typically occupied by professional builders who specifically target top-of-block execution. Since the cost difference is relatively minor, this straightforward approach is preferred over more complex alternatives aimed at better fairness.\nAn alternative approach would distribute warming costs across all transactions within a block that access the same slots. Under this system:\n\nEach transaction would initially pay the full cold access cost\nAfter block execution completes, these costs would be evenly distributed among all transactions that accessed the slots\nThe excess payments would then be refunded to transaction originators\n\nWhile this alternative creates a more equitable cost distribution, it introduces significant additional complexity. The tradeoff between fairness and simplicity remains a key consideration. In particular, when combined with multi-block warming, refund redistribution might be even more challenging.\n\nEach transaction would initially pay the full cold access cost\nAfter block execution completes, these costs would be evenly distributed among all transactions that accessed the slots\nThe excess payments would then be refunded to transaction originators\n\nAlternative Block Warming Windows: Instead of applying warming at the block level, more advanced multi-block warming approaches can be considered. Potential options include:\n\nWarming addresses and storage keys over the duration of an epoch\nUsing a ring buffer spanning x blocks\n\nSince the Execution Layer currently operates without regard to epoch boundaries, it may be preferable to maintain this design and not consider epochs. Therefore, the second option, involving a ring buffer of size x, might be more suitable.\n\nWarming addresses and storage keys over the duration of an epoch\nUsing a ring buffer spanning x blocks\nCaching Efficiency: Today’s Ethereum clients already implement sophisticated caching mechanisms at the block level. Extending address and storage key warming to match this caching behavior better aligns gas costs with actual computational costs.\nBackward Compatibility: The worst-case scenario for any transaction remains unchanged - it will never pay more than the current cold access cost for its first access to a slot.\nFirst Access Warming System\nThe proposed mechanism operates on a “first warms for all” principle: when a transaction first accesses and warms multiple addresses or storage slots in a block, it bears the entire warming cost. Subsequent transactions can then access these warmed slots without additional costs.\nThis approach aligns well with current dynamics, as early block positions are typically occupied by professional builders who specifically target top-of-block execution. Since the cost difference is relatively minor, this straightforward approach is preferred over more complex alternatives aimed at better fairness.\nAn alternative approach would distribute warming costs across all transactions within a block that access the same slots. Under this system:\nEach transaction would initially pay the full cold access cost\nAfter block execution completes, these costs would be evenly distributed among all transactions that accessed the slots\nThe excess payments would then be refunded to transaction originators\nWhile this alternative creates a more equitable cost distribution, it introduces significant additional complexity. The tradeoff between fairness and simplicity remains a key consideration. In particular, when combined with multi-block warming, refund redistribution might be even more challenging.\nAlternative Block Warming Windows: Instead of applying warming at the block level, more advanced multi-block warming approaches can be considered. Potential options include:\nWarming addresses and storage keys over the duration of an epoch\nUsing a ring buffer spanning x blocks\nSince the Execution Layer currently operates without regard to epoch boundaries, it may be preferable to maintain this design and not consider epochs. Therefore, the second option, involving a ring buffer of size x, might be more suitable.\nReference Implementation\nFind a first draft implementation of block-level warming here:\nGitHub - nerolation/execution-specs at block-level-warming\nblock-level-warming\nSpecification for the Execution Layer. Tracking network upgrades.\n",
        "category": [
            "Execution Layer Research"
        ],
        "discourse": []
    },
    {
        "title": "Embedded Rollups, Part 2: Shared Bridging",
        "link": "https://ethresear.ch/t/embedded-rollups-part-2-shared-bridging/21461",
        "article": "image1792×1024 143 KB\nA rare image of a rollup embedding another rollup\nCo-authored by Lin Oshitani & Conor McMenamin, both Nethermind. Thanks to Patrick McCorry, Chris Buckland, Swapnil Raj, Ahmad Bitar, and Denisa Diaconescu for their feedback. Feedback is not necessarily an endorsement. This is the first part of a two-part series on embedded rollups.\nVitalik Buterin once said:\nImagine a world where all L2s are validity proof rollups, that commit to Ethereum every slot. Even in this world, moving assets from one L2 to another L2 “natively” would require withdrawing and depositing, which requires paying a substantial amount of L1 gas. One way to solve this is to create a shared minimal rollup, whose only function would be to maintain the balances of how many tokens of which type are owned by which L2, and allow those balances to be updated en masse by a series of cross-L2 send operations initiated by any of the L2s. This would allow cross-L2 transfers to happen without needing to pay L1 gas per transfer, and without needing liquidity-provider-based techniques like ERC-7683.\nIn this article, we describe how embedded rollups achieve the concept of a minimal shared rollup without the need to commit proofs to Ethereum in every slot.\nDon’t worry, we got you Vitalik.\nIn the first part of this series, we introduced the concept of embedded rollups (ERs)—rollups that are embedded within and shared among other rollups. When embedding an ER, a rollup stores and updates the ER’s state alongside its own, which gives the rollup a local read-only view of the ER’s state during execution. If multiple rollups embed the same ER, they can share this read-only state without relying on the L1 state or execution.\nIn this article, we describe how embedded rollups can be used to create a shared bridge between L2s embedding the same rollup. Such an embedded rollup enables fast and cheap interoperability between L2s, removing the need to execute transactions on L1 to send tokens from one L2’s state to another. This simple functionality has the potential to address one of the main concerns in Ethereum’s long term roadmap; fragmentation of users and their liquidity among L2s (check reference).\nIssues with Existing Interoperability Solutions\nToday, to move assets between L2s, you have some options, generally described by the following:\nSlow Path via L1 Settlement: Go through L1 by withdrawing from the source L2 and depositing to the target L2. This requires paying expensive L1 gas fees for each transfer, which is expensive.\nFast Path via Solver Networks: Use third-party solvers who maintain liquidity pools across L2s. This approach becomes increasingly inefficient as the number of L2s grows since solvers must split their capital across all L2s, resulting in thin liquidity and poor pricing on less popular routes.\nAll-or-Nothing Dependencies: This class of solution is exemplified by AggLayer and Superchain approaches, where rollups are connected via a shared bridge and the execution of one rollup conditions on the execution of all other rollups in the bridge (with some caveats).To send tokens between rollups, rollup nodes are effectively required to run full nodes for all other rollups (or trust third-parties to verify the execution of other rollups).\nWe can address all of these issues by creating a shared bridge through an embedded rollup. The embedded rollup maintains a single unified ledger of how many tokens of each type are owned by each L2. Each rollup in the shared bridge ecosystem is only required to maintain a local view of the embedded rollup state, without needing to verify anything about the execution of the other rollups embedding the embedded rollup.\nThis section describes the core protocol that must be followed by a rollup embedding an embedded rollup, as introduced in the previous article in this series.\nPrimer: Conventional Rollups\nTo consider how rollups can embed a rollup, let’s first consider how rollups currently progress their state. For some conventional rollup A with no embedded rollup, A progresses its state according to the following protocol:\nUpdate A's local view of L1.\nRead rollup A txs from the rollup A inboxes* on L1. Rollup A may additionally read preconfirmed txs in the rollup A inboxes on L1 provided by the sequencer(s) of Rollup A**.\nUpdate rollup A by executing the rollup A txs.\nNotes:\n*Most rollups have two inboxes, a delayed inbox and a sequencer inbox:\n\na delayed inbox, which can be added to permissionlessly. After a delayed inbox window, transactions in the delayed inbox must be applied to state of the rollup.\n\n\na sequencer inbox, which can only be updated by the sequencer. Sequencer inbox transactions are immediately applied to the state of the rollup in the order in which they appear. The sequencer inbox can pop transactions from the delayed inbox at any time before the delayed inbox window.\n\na delayed inbox, which can be added to permissionlessly. After a delayed inbox window, transactions in the delayed inbox must be applied to state of the rollup.\na sequencer inbox, which can only be updated by the sequencer. Sequencer inbox transactions are immediately applied to the state of the rollup in the order in which they appear. The sequencer inbox can pop transactions from the delayed inbox at any time before the delayed inbox window.\n** The sequencers of rollup A, the ER, and the L1 can be distinct, shared or partially shared. Distinct, shared or partially shared sequencers all change the strength of preconfirmations from one sequencer to another.\nRecap: Embedded Rollup Protocol Description\nSuppose rollup A wants to embed a rollup ER. In this case, rollup A progresses its state by the following:\nSynchronize its local L1 node.\nRetrieve rollup A and rollup ER transactions from rollup A and rollup ER inboxes on L1, along with their accompanying calldata or blobs.\n\nRollup A may additionally read preconfirmed txs from the rollup A sequencer and/or rollup ER sequencer.\n\n\nRollup A may additionally read preconfirmed txs from the rollup A sequencer and/or rollup ER sequencer.\nUpdate their local view of ER by executing the ER transactions.\nUpdate rollup A's state by executing the A transactions.\n\nThese transactions may read from A's local view of ER.\n\n\nThese transactions may read from A's local view of ER.\nEmbedded rollups can be used to provide fast (almost instant) and cheap (no L1 transactions necessary) interoperability between L2s, without fragmenting liquidity across L2s. The shared bridge implemented as an embedded rollup is described as follows:\nDepositing from L1 to an L2\nWhen users want to get their tokens into an L2 in the presence of an embedded shared bridge rollups, they have two options:\nDeposit through the native bridge (as normal):\na. Users send their tokens to the L2 inbox contract on L1 .\nb. The L2 reads from the L1 inbox\nc. The L2 mints corresponding tokens to the user on L2.\nDeposit through the shared bridge:\na. Users send their tokens to the shared bridge rollup inbox contract on L1.\nb. The shared bridge rollup reads the inbox on L1, then mints corresponding tokens on the shared bridge rollup, creditting these to the L2 inbox on the shared bridge rollup.\nc. The L2 reads their inbox on the shared bridge rollup, then mints corresponding tokens on L2, creditting these to the user on L2.\nimage745×458 86.5 KB\nSlow Path via Settlement\nMoving shared bridge tokens from A to B through settlement has the following flow. For simplicity, we represent the L2s as A and B. Importantly, given A and B both embed the same shared bridge, SB,  the flow from A to B is identical to the flow from B to A.\nA submits an A → SB  transfer request to their local SB contract. The tokens in this transfer request are effectively burned in A.\nA's state root (containing the A → SB transfer request) settles to L1.\nA's state root is imported into SB by being pushed to the SB inbox contract on L1, which is then read by SB.\nA's state root is pushed to the SB → A bridge contract, unlocking the tokens corresponding to the A → SB transfer request.\nA transfer message is sent by (or on behalf of) the user to A's contract on the SB rollup, where A's tokens are stored. This messages proves that the user indeed sent a valid transfer request to the SB contract on A. Note that this transfer message can be bundled or executed as part of the message from Step 4.\nSB updates its ledger, sending tokens from A's inbox to B's inbox.\nB:\na. Reads its inbox on SB, and mints corresponding tokens on B.\nb. These tokens are credited to the user on B.\nUnlike the traditional slow path via settlement, this requires no separate withdrawal or deposit transactions on L1.\nNote that even if B does not embed SB into its state transition function, it is still possible for B to read SB state roots on L1, and incorporate SB → B transfers without any L1 transactions. For this functionality, B needs to wait for the shared bridge to settle a state root containing the SB → B transfer on L1.\nOn one hand, this requires no changes to B's state transition function, while on the other hand, transfer from SB are now limited by the proving and settling time of SB state updates.(H/T Paddy Mc)\nimage815×434 95.6 KB\nFast Path via Solvers (with Conditioning)\nFor faster transfers, Solvers can maintain their liquidity in the shared bridge, SB, and send tokens to target rollups on behalf of users, in exchange for receiving those tokens plus a fee via the slow path. This is a key improvement over legacy solver-based protocols that required fragmenting liquidity throughout individual L2s. Using this SB liquidity, solvers can enable fast cross-L2 token transfers.\nFast Transfer Requests\nTo describe the fast path protocol, first an aside on fast (& slow) transfer requests, originally introduced here. Fast A → B transfer requests effectively send tokens via the slow path to B. However, the conditions for withdrawing those tokens are different to a conventional withdrawal via settlement. Fast requests give exclusive rights to the tokens eventually unlocked on SB to the first Solver who submits a valid solution on SB on behalf of the requester. A valid solution sends a specific amount of tokens to another address on SB on behalf of the requester, as specified by the request. To withdraw tokens on SB corresponding to a fast request on A, someone must prove that the request exists in a settled state root of A , and that either:\na Solution was submitted, allowing the solver to withdraw, or\na Solution was not submitted, allowing the user to withdraw.\nNext, let’s go through this protocol step by step in more detail.\nProtocol Description\nAs before, we assume we are moving tokens from a rollup A to a rollup B, with both embedding the same SB.\nUser submits a fast (& slow) A → B transfer request on A, sending the tokens to be burnt at the SB bridge contract on A.\nAfter observing a fast A → B transfer request in an A batch, Solver executes a conditional solution as follows:\na. A batch containing a fast transfer request is pushed to A's inbox contract on L1.\nb. A hash of A's batch containing the fast request is pushed to SB inbox contract on L1.\nc. This hash is read from SB ’s inbox on L1 and imported to SB.\nd. The solution is created by the Solver, conditioning its execution on the inclusion of an A batch corresponding to a specific commitment e.g a hash of the A batch containing the valid fast transfer request.\ne. The solution transfers tokens corresponding to the fast transfer request to $Bs inbox.\nf. The solution pushes a record of the valid transfer on A's inbox on SB.\nB then:\na. Reads its inbox on SB, and mints corresponding tokens on B.\nb. These tokens are credited to the user on B.\nA's state root (containing the fast transfer request) settles to L1.\nA's state root is imported into SB:\na. By being pushed to SB inbox contract on L1,\nb. Read by SB from the inbox contract on L1.\nA's state root is pushed to A's inbox on SB.\nThe Solver withdraws tokens from A's inbox corresponding to the fast transfer request, proving that the request is present in the proven state root.\nIf no Solver submits a solution on SB, the user can withdraw the funds from the SB → A contract when the A state root containing the request settles on L1.\nimage843×507 80.3 KB\nAs mentioned in the protocol, solutions can condition execution on the existence of certain SB or L1 transactions. For risk-free solutions (Solver gains tokens or nothing happens), Solvers can condition SB solutions on the inclusion of A batches on L1, as SB reads from L1. In this way, if the Solver knows an included A batch executes and results in a fast transfer request executing on A, the Solver also knows that when the A state root is proven on L1, the tokens corresponding to the request will be made available on the SB contract. If the A batch is forked by the rollup sequencer and is not pushed to the L1 inbox, the condition for the SB transfer will not be satisfied, and the SB transfer will not execute.\nSupercharging the Fast Path with Preconfirmations\nThis protocol requires the batch containing a fast transfer request to appear on L1 before a Solver can execute the Solution on the bridge rollup. With no coordination between the L1 and rollup sequencers, Solutions cannot be generated until fast transfers are observed in a rollup batch on L1. This theoretically takes at least 12s for users to receive funds on rollup B, the time between observing an L1 block containing the batch, and generating a shared rollup batch containing a Solution. No bueno.\nWith preconfirmations, a rollup batch containing fast requests can be preconfirmed on L2 immediately after the requests are submitted to the rollup sequencer. More than this, this rollup batch can be preconfirmed on L1 with some coordination between the rollup and L1 sequencers. Even better, if the rollup is based, these rollup and L1 sequencers will be the same entity. In such a system, the Solver can generate their shared bridge solution conditioned on the preconfirmed rollup batch on L1. Finally, this Solution can be included in a shared bridge rollup batch and preconfirmed on L1 arbitrarily fast (immediately after the rollup batch). In summary, this means the user can receive tokens on the target rollup in the time it takes to generate and preconfirm 2 transactions on L1.\nAs amazing as this sounds, this supercharged solution depends on several technological advancements that have not been fully deployed to this point; Based rollups and based preconfirmations. These are active areas of research and development, with preconfirmations hopefully emerging on mainnet in the near future. Although sub-second dependent preconfirmations across many based rollups is infeasible with low-resource based proposers, high-resource preconfirmers (through gateways or rainbow staking) may emerge to meet the demand of solutions such as the embedded shared bridge presented here.\nSolving without conditioning is also possible. Although we describe how solutions can be provided with conditioning, Solvers can also submit unconditional solutions. These unconditional solutions would depend on trust between Solvers and the sequencers of the rollups involved in the transfer request; that is, at least the rollup where the transfer originated, and the shared bridge rollup. This trust is simplified in some sense when a single sequencer is sequencing both the rollup where the transfer originated and the the bridge rollup, although this is not a requirement.\nEmbedded Shared Bridges in Practice\nShared bridging through an embedded rollup can be seen either as an add-on or a replacement to native bridges. This section lays out how this might happen for existing rollups, as well as some additional considerations for embedding rollups.\nNo Embedding to Embedding\nIn this section, we consider what it means for cross-rollup composability with and without embedding the rollup into a rollups state transition function:\nNo embedding; The state transition function of a rollup does not maintain a local view of the embedded rollup state. Instead, the rollup only reads embedded rollup state updates via proven state roots in the the embedded rollup contract on L1. These state roots can specify deposits to rollup bridge contracts on the embedded rollup.\nEmbedding; The state transition function of rollups maintain a local view of the embedded rollup state. To advance the state of a rollup embedding an embedded rollup, the local view of the embedded rollup must be consistent with previous state updates, potentially with the requirement to advance the head of the local view of the embedded rollup for liveness.\nNative Bridge to Shared Bridge\nNow we consider how tokens will be distributed between the shared bridge and the native bridges.\nNo composability: all rollups keep their tokens in their own native bridges. Normal UX for rollup users. Moving tokens from rollup A to B must be done via an L1 transaction.\nWith some demand for composability, some tokens are locked in native bridges, some in the shared bridge contract. Within the shared bridge rollup, tokens are either unlocked, or locked in one of the rollup contracts that exist on the shared bridge rollup. To go from rollup A to rollup B, a user would need to acquire shared bridge tokens on rollup A (distinct from native bridge tokens), then transfer those tokens to rollup B via either the Fast or Slow path on the shared bridge. Then, when those shared bridge tokens get sent to rollup B via the shared bridge contract, the user may either keep the tokens in their shared bridge form, or sell the tokens for native tokens on rollup B.\nMaximum composability: no tokens are stored in native bridges, and instead all tokens are stored in the shared bridge. As before, in the shared bridge rollup, those tokens can either be unlocked, or locked in one of the rollup bridge contracts that exist in the shared bridge rollup.\nAdditional Considerations\nFor bridge security, tokens originating from a rollup’s native bridge will likely need to have a different representation than the tokens originating from the shared bridge. Otherwise, native bridge users may be forced to take on shared bridge user risk to withdraw their tokens if shared bridge token holders are able to withdraw through the native bridge, and vice versa.\nAlthough “fragmentation bad” is the main qualm with existing solver networks, we omit a qualitative analysis of how bad this is vs. embedding a shared bridge. We encourage anyone interested to simulate such an analysis. Regardless of the costs of fragmentation in a competitive solver market, the shared bridge approach democratizes access to solving as Solvers have no settlement risk, making it accessible to non-professionals.\nIn the Supercharging section, we mention that solutions in one batch of rollup transactions can condition on any batch preceding it on L1. Technically, this could be improved by enabling rollup transactions from different rollups to be intertwined in the same batch. With this (minor) advancement, chains of dependent cross-rollup transactions would be enabled.\nEmbedded rollups have similarities to several different protocols, which we outline in this section. As we approach real-time proving, embedded rollups will blend closer and closer together with many of the solutions we outline. For now however, there are key differences between embedded rollups and existing solutions.\nSuppose rollup A wants to transfer funds to rollup B. In this section, we compare how existing interop solutions handle such transfer and compare with ER.\nERC-7755\nRollup A settles to L1, rollup B imports the settled A state root and reads the message. This method minimizes dependency between the executions of rollup A and rollup B, as rollup B only acts after rollup A's state has been proven and settled. However, this process is slow because rollup B must wait for the proving and settlement of rollup A's state on L1, which can take a significant amount of time, equivalent to the slow-path using the embedded shared bridge.\nAggregated Settlement\nAggregated settlement enables synchronous composability between L2s by conditioning one rollup’s settlement on the settlement of others. This allows a rollup to import messages from another, execute L2 blocks based on them, and later revert execution if the messages are found invalid by verifying the other rollup’s state root at the shared settlement time. This protocol effectively enables synchronous composability between L2s by utilizing the fact that the L2s share a settlement layer.\nHowever, this approach requires tight coupling of execution between rollup A and rollup B, as the sequencer must execute both chains. Furthermore, it can cause “cascading” reverts of rollups in case the inbox-outbox mismatch is detected at settlement. In comparison, in our embedded rollup solution, although both rollups A and B embed ER (they are coupled to ER), no coupling between A and B is introduced.\nAggLayer’s fast path and Optimism’s Superchain adopt this approach.\nExisting Solver-based Solutions\nOur shared bridge solution utilizes solvers on the shared bridge rollup to facilitate composability, similar to protocols like Across. However, traditional solver-based methods face issues with liquidity fragmentation because solvers need to maintain separate liquidity pools for each chain pair they support. This is particularly challenging for smaller chains with lower liquidity demands. In contrast, our bridge ER solution concentrates all solver liquidity within the embedded shared bridge. This setup allows solvers to facilitate transactions across any combination of rollups embedding the shared bridge without moving their liquidity outside the shared bridge. Furthermore, solvers don’t need to execute both A and B. To facilitate an A to B transfer, solvers just need to maintain the state of A and the shared bridge to transfer tokens to B, via $Bs inbox on the shared bridge.\nL3s\nL3s using the L2 as a base layer can utilize faster block times of L2 than L1, by facilitating the state root passing of L3s in a much quicker way. However, L3 liveness will now depend on the L2’s liveness. In comparison, L2s interoperating via an embedded rollup inherit liveness from the L1.\nThis solution is adopted by ZKSync’s Elastic Chain and Arbitrum Orbit L3s.\nBooster Rollups\nEmbedded rollups have a close similarity to booster rollups. In booster rollups, each rollup maintains a local view of the entire L1 state, which can be easily accessed by any contract within the rollup. Similarly, embedded rollups function like booster rollups, but instead of “boosting” on top of the L1, they boost on top of another rollup. Implementations of booster rollups could potentially be adapted to achieve embedded rollups.\nIf you are interested in topics like this and want to be at the cutting edge of protocol research on Ethereum, we’re hiring. Apply through the website, or reach out to one of the co-authors (Conor or Lin) through Twitter. Let’s keep Ethereum great together.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "preconfirmations",
            "cross-shard"
        ]
    },
    {
        "title": "Credibly Neutral Preconfirmation Collateral: The Preconfirmation Registry",
        "link": "https://ethresear.ch/t/credibly-neutral-preconfirmation-collateral-the-preconfirmation-registry/19634",
        "article": "by mteam from Spire Labs\ninspired+reviewed by Justin Drake.\nreviewed by Ladislaus, Murat, and others.\n.\n.\ntl;dr\nWe introduce a design for a credibly neutral preconfirmations registry that solves the collateral problem for solo-stakers who want to earn additional yield from preconfs, while simultaneously improving capital efficiency for large operators who would like to opt-in to preconfirmations.\n.\n.\nIntroduction\nThere is a fundamental requirement of any proposer-commitment design that the L1 proposer must post some collateral. This is true for Based Preconfirmations (even with delegation) and other systems. This collateral is necessary because, according to the Ethereum protocol, a L1 proposer always has the option to self-build a block. The block that they build may violate commitments that they have made (or that other delegated entities have made).\nIn the next few paragraphs, we introduce a simple system for proposer to post this collateral in ETH (the most credibly neutral collateral). This system also enables large operators and solo-stakers alike to post collateral in efficient ways.\nSee my presentation introducing this topic for another intro.\nOnchain Construction\nThe onchain construction is very simple:\nWe introduce the PreconfirmationRegistry L1 smart contract that serves as a source of collateral for backing preconfirmations.\nThe contract provides 4 functionalities:\nPenalties\nThe PreconfirmationRegistry facilitates penalty application (i.e. slashing, freezing).\nDelegates can choose what slashing conditions to opt-in to. These slashing conditions can be described using an EVM bytecode function.\nRegistration\nThe PreconfirmationRegistry provides a way for a registrant to post ETH as collateral. A registrant can be any entity, it is represented in the registry as an Ethereum address.\nLet a registrant’s collateral balance = ETH deposited - ETH currently penalized - ETH withdrawn.\nDelegation\nAllows registrants to delegate their collateral to proposers.\nCan be used to delegate to any proposer.\nLet a proposer’s collateral balance be the sum of the registrant collateral balance of all registrants that have delegated to them.\nIf a proposer’s collateral balance is above a parameter, MINIMUM_COLLATERAL, they are a preconfer. If a proposer’s collateral balance falls below MINIMUM_COLLATERAL, they are no longer considered a preconfer.\nWhen a penalty is applied to a preconfer, the penalty is applied to the registrant collateral of any registrants who delegate to the offending preconfer. The penalty is split proportionally amongst the collateral that is delegated to a proposer.\nNote: Delegation can be done offchain and validated onchain in the case of a slashing event. Offchain signatures need to be shared publicly so that others (gateways) can verify that a given proposer is a preconfer.\nLeader Election\nThe PrceonfirmationRegistry also elects a leader for the current L1 slot. It does this by checking the lookahead to determine the next proposer in line who is a preconfer (meets the MINIMUM_COLLATERAL threshold.) Based Rollups can select this elected leader as the sequencer for their chains to achieve composability through shared sequencing.\nWe call this leader the registry sponsored leader. Any rollup interested in composability can use this leader. Rollups that have other requirements for their leader (regulatory, financial) can use their own leader election system, but may ise the registry sponsored leader as a starting point.\nimage1658×948 110 KB\nUnderwriters\nThe most important offchain construction that we introduce is the Underwriter. The Underwriter acts as a registrant and posts at least MINIMUM_COLLATERAL ETH as collateral to the PreconfirmationRegistry contract on behalf of a group of proposers. The Underwriter accepts collateral from proposers in any form (a liquid staking token, equity, reputation).\nUnderwriters could be trustless DeFi applications, offchain market makers, etc.\nProposer Pools\nWhile large staking operators form large trust networks between many proposers and can provide collateral on their own, solo-stakers do not have the same access to resources.\nSolo-stakers may form “Proposer Pools” of a collection of proposers, all restaking their ETH to a single registrant (through an underwriter) to reach a registrant balance of MINIMUM_COLLATERAL or more.\nNote: Proposer pools have a 1/n trust model: a single proposer could renege on preconfirmation promises for a significant profit but the entire pool would be penalized. To disincentivize this situation, proposer pools could be designed so that EL rewards are distributed between all proposers in the pool (similar to smoothing pools). With a MaxEB increase above MINIMUM_COLLATERAL and DVT, the trust model becomes n/3.\nimage1521×1672 280 KB\nConsiderations\ncredible neutrality: The PreconfirmationRegistry only accepts the purest form of collateral: ETH. This avoids any dependence on any LST/LRT and minimizes smart contract risk.\ncentralization: The PreconfirmationRegistry does not introduce any permissioned actors. Underwriter centralization is not dangerous to the protocol.\ncollateral depeg risk: An external collateral depeg will not affect the collateral that backs preconfers. Underwriters are only vulnerable to permanent depegs as they are not forced to sell during a temporary depeg event.\nsmart contract risk: The PreconfirmationRegistry is a very simple contract but should be audited extensively. Underwriters are especially incentivized to audit the PreconfirmationRegistry contract.\nroadmap compatibility: Proposer Pools have even better trust models with a MaxEB increase like EIP-7251 and DVT. With ePBS, Execution Tickets, Execution Auctions / APS-Burn or any other upgrade where proposers do not serve as monopoly sequencers, the PreconfirmationRegistry is no longer useful.\nmass exits: To avoid a situation where a registrant withdraws collateral that is actively backing a preconfirmation, exits must be delayed. This mandates that there are three states that a proposer can exist in: [INCLUDER, EXITING, PRECONFER]. New preconfirmations should not be backed by collateral from EXITING proposers.\nforced ejection: In a penalty event, some preconfers may no longer have enough unpenalized collateral to back their preconfirmations. If these preconfers are in the lookahead and actively providing preconfirmations, they must be immediately exited and treated as INCLUDER proposers. Registrants who wish to avoid forced ejection of their delegated proposers should top up their registrant balance to a level significantly higher than MINIMUM_COLLATERAL.\nPreconf Chaining: Most preconfirmations are only backed by a single proposer (and therefore a single collateral balance), this enables registrants to delegate to many proposers simultaneously. There may be situations where preconfirmations are backed by multiple proposers (think conditional preconfirmations or multi-block preconfirmations). In these situations, the collateral that backs a preconfirmation is the combined unique proposer collateral balance of proposers involved. The odds of preconf chains applying to proposers using the same collateral are higher for registrants with more delegates, and therefore registrants who can afford to post extra collateral.\nBonus: Dynamic Collateral Requirements\nThe MINIMUM_COLLATERAL parameter is a key parameter for preconfirmation security and bootstrapping a robust Preconfer set. Setting MINIMUM_COLLATERAL ex-ante for all preconfirmations may lead to inefficiencies. For example, many retail users don’t need thousands of ETH backing their preconfirmations. Indeed, many types of transactions (ETH/ERC20 transfers) are time-insensitive; preconfirmations for these transactions do not require huge amounts of collateral.\nThe solution to this inefficiency is setting a low MINIMUM_COLLATERAL parameter and allowing users to set their own collateral requirements (along the lines of User-Defined Penalties: Ensuring Honest Preconf Behavior by Jonah from Blockchain Capital).\nNote that it is still necessary to set a MINIMUM_COLLATERAL value that is significant (likely more than 32 ETH) to avoid an adverse selection problem: A user might not be able to get a preconfirmation with a high collateral backing if a low-collateral Preconfer is first in the lookahead, even if a high-collateral Preconfer is next. More research is required to determine this parameter.\nBonus: Slashing Conditions\nSlashing conditions represent generic agreements between proposers and users. Delegates (proposers) can choose what slashing conditions to use. To declare their preferences, delegates can sign off on EVM bytecode that represents a function. When executed, this bytecode returns the penalties that should be applied:\nRollups that want to incentivize proposers to adopt their slashing conditions can set up incentives (tokens, fees, etc.) out of the registry.\nNote: Registrants might also choose slashing conditions for their delegates. For example, an Underwriter might accept collateral from an operator and register with the slashing conditions that the operator desires. This simply a way for operators to manage the slashing conditions that they need to think about.\nReferences + Resources\nAll documents used can be found here.\nEspecially useful:\nJustin’s Based Rollup post\nJustin’s Based Preconfirmations post\nJonah’s User-Defined Penalties… post\nLimechain’s Based Preconfirmations Research repo\nSimilar (unrelated) work:\nPrimev’s Registry\nPuffer’s Registration System\nCairo’s preconf-operator code\nvarious Smoothing Pool designs\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "based-sequencing",
            "preconfirmations"
        ]
    },
    {
        "title": "Introduction to Blockchain Mechanism Math, Terminology, and Hieroglyphics for Deeply Casual People who want to Sound Smart when Discussing White Papers with their Peers",
        "link": "https://ethresear.ch/t/introduction-to-blockchain-mechanism-math-terminology-and-hieroglyphics-for-deeply-casual-people-who-want-to-sound-smart-when-discussing-white-papers-with-their-peers/21265",
        "article": "An Introduction to Blockchain Mechanism Math, Terminology, and Hieroglyphics for Deeply Casual People who want to Sound Smart when Discussing White Papers with their Peers\nBy Alex Watts et al\nAbstract: Mechanism Designers working in DeFi really like writing white papers. They like it a lot. Perhaps too much. They also like writing these papers in an ancient, hieroglyphics-based language that is difficult for a normal, socially-adjusted person to decipher. This is unfortunate because most of the math and game theory is actually quite simple once you understand their alien terminology. In this paper, I’ll give you the tools you’ll need to sound smart when discussing these white papers with your peers, which is a very important skill set because in all likelihood your peers are also trying really hard to sound smart.\nLet’s start with this section - the “abstract.”  This section is really just a summary of the white paper. In this section, the Mechanism Designers will summarize a problem, then they’ll summarize their solution, and then - if they’re good - they’ll also summarize the shortcomings of the paper. We may never know why the Mechanism Designers like to call this section an “abstract” rather than a “summary” - in fact, we’ll probably never know why they do most of the things that they do. But by reading this paper, hopefully you’ll be better able to understand what they’re doing - and, if you’re into this kinda thing, who they’re doing it to.\nIntuitively, this paper doesn’t have any shortcomings. But if I had to pick only one, it’d be that a lot of the information in this paper is deliberately wrong. Very wrong. I’m way off on a lot of these explanations. But I’m not being byzantine - which means “dishonest” or “adversarial”, by the way, and two thousand years ago it probably would’ve been flagged by HR departments as racially offensive against the citizens of the Byzantine empire. So yeah, I’m not being byzantine - I’m just oversimplifying things to make the concepts easier to understand in expectation.\nA set is a group of things.\nBy the way, if your first thought after reading that last sentence was “that definition left out a lot!” then be warned: it’s all downhill from here.\nSets are important because Mechanism Designers like subjecting entire groups of people or things to their nefarious designs.\nSets are represented with an uppercase letter (such as X or Y).\nWhen sets are defined, they’re often surrounded with brackets. For example, a set of three numbers could be written as X = \\{1,2,3\\}.\nMembers of the set are represented with a lowercase letter (such as x or y).\nThere are a handful of set-related funny-looking symbols that these Mechanism Designers like to use in their white papers that you should probably learn:\n\n\\in means “in” and is used to show membership in a set\n– When you see x ∈ X, that means that x is a member of the set X.\n– Example: if X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20\\}, then x ∈ X means x is one of those 10 values.\n\n\n\\cup means “union” and is meant to take two sets and combine all of their members together (removing duplicates).\n– When you see X ∪ Y, that means you’re combining sets X and Y.\n– Example: if X = \\{1, 2, 3, 4\\}, and Y = \\{3, 4, 5\\} then X ∪ Y = \\{1,2,3,4,5\\}.\n\n\n\\cap means “intersection” and is meant to take two sets and make a new set only out of their common members.\n– When you see X ∩ Y, that means you’re taking the intersection of sets X and Y.\n– Example: if X = \\{1, 2, 3, 4\\}, and Y = \\{3, 4, 5\\} then X ∩ Y = \\{3,4\\}.\n\n\n\\subset means “sub set” - all of the elements of the first set exist in the second set.\n\n\n\\supset means “super set” - all of the elements of the second set exist in the first set. Note that if you see this then you’re dealing with a particularly troublesome brand of Mechanism Designer and you should be on your guard for more trickery.\n\n\n: (the colon) typically means “if” and is often seen together with a funny looking upside-down A. Speaking of which…\n\n\n\\forall means “for all” and is meant to iterate through a set, often times in the creation of another one.\n–If you’re a programmer, whenever you see \\forall think “for loop.”\n– Often times \\forall and : are used to iterate through a set to create a new one.\n– Example: Consider this monstrous expression: Y = \\{2x, \\ \\forall x\\in X : x>3 \\}. The english translation is something along the lines of “Y is a set of numbers that was created by taking each of the x's in X that were greater than 3 and then multiplying that x by 2.” For posterity’s sake, if  X = \\{1, 2, 3, 4, 5 \\} then it follows that (aka \\Rightarrow, as we’ll discuss later) Y = \\{8, 10 \\}.\n\n\nWarning: When it comes to defining sets, Mechanism Designers really like to move variables around or remove them entirely. They do this not to confuse you but to confuse each other - presumably to increase their job security. Take, for example, Y = \\{2x, \\ \\forall x\\in X : x>3 \\}. That would be the same as Y = \\{2 \\cdot x\\in X : x>3 \\} or Y = \\{z \\in \\{2x, \\forall x\\in X \\}  : z>6 \\}. Watch out for anyone using linear algebra to define sets (look for the brackets, sets multiplied by sets, or sets of multiple things in parentheses known as “tuples”). They’re one of the least understandable species of Mechanism Designers and should be avoided by all but the most skilled larpers. In fact, if one of them is reading this right now, they’re probably day dreaming about correcting the author - presumably using made-up words like “vector” or “scalar.”\n\n\n\\mathbf{card}(X) or |X| is the “cardinality” of the set X.  This is a fancy way of saying “the number of things in X.” If X = \\{ 1, 17, 31, 5\\} then |X| = \\mathbf{card}(X) = 4. Keep in mind that |X| is the size of X while |x| is the absolute value of x \\in X; be vigilant, and remember that the Mechanism Designers will only win if you let them.\n\n\\in means “in” and is used to show membership in a set\n– When you see x ∈ X, that means that x is a member of the set X.\n– Example: if X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20\\}, then x ∈ X means x is one of those 10 values.\n\\cup means “union” and is meant to take two sets and combine all of their members together (removing duplicates).\n– When you see X ∪ Y, that means you’re combining sets X and Y.\n– Example: if X = \\{1, 2, 3, 4\\}, and Y = \\{3, 4, 5\\} then X ∪ Y = \\{1,2,3,4,5\\}.\n\\cap means “intersection” and is meant to take two sets and make a new set only out of their common members.\n– When you see X ∩ Y, that means you’re taking the intersection of sets X and Y.\n– Example: if X = \\{1, 2, 3, 4\\}, and Y = \\{3, 4, 5\\} then X ∩ Y = \\{3,4\\}.\n\\subset means “sub set” - all of the elements of the first set exist in the second set.\n\\supset means “super set” - all of the elements of the second set exist in the first set. Note that if you see this then you’re dealing with a particularly troublesome brand of Mechanism Designer and you should be on your guard for more trickery.\n: (the colon) typically means “if” and is often seen together with a funny looking upside-down A. Speaking of which…\n\\forall means “for all” and is meant to iterate through a set, often times in the creation of another one.\n–If you’re a programmer, whenever you see \\forall think “for loop.”\n– Often times \\forall and : are used to iterate through a set to create a new one.\n– Example: Consider this monstrous expression: Y = \\{2x, \\ \\forall x\\in X : x>3 \\}. The english translation is something along the lines of “Y is a set of numbers that was created by taking each of the x's in X that were greater than 3 and then multiplying that x by 2.” For posterity’s sake, if  X = \\{1, 2, 3, 4, 5 \\} then it follows that (aka \\Rightarrow, as we’ll discuss later) Y = \\{8, 10 \\}.\nWarning: When it comes to defining sets, Mechanism Designers really like to move variables around or remove them entirely. They do this not to confuse you but to confuse each other - presumably to increase their job security. Take, for example, Y = \\{2x, \\ \\forall x\\in X : x>3 \\}. That would be the same as Y = \\{2 \\cdot x\\in X : x>3 \\} or Y = \\{z \\in \\{2x, \\forall x\\in X \\}  : z>6 \\}. Watch out for anyone using linear algebra to define sets (look for the brackets, sets multiplied by sets, or sets of multiple things in parentheses known as “tuples”). They’re one of the least understandable species of Mechanism Designers and should be avoided by all but the most skilled larpers. In fact, if one of them is reading this right now, they’re probably day dreaming about correcting the author - presumably using made-up words like “vector” or “scalar.”\n\\mathbf{card}(X) or |X| is the “cardinality” of the set X.  This is a fancy way of saying “the number of things in X.” If X = \\{ 1, 17, 31, 5\\} then |X| = \\mathbf{card}(X) = 4. Keep in mind that |X| is the size of X while |x| is the absolute value of x \\in X; be vigilant, and remember that the Mechanism Designers will only win if you let them.\nThere are a handful of prestigious sets that you should know because Mechanism Designers really like letting you know that they know them, too:\n\\mathbb{Z} is the set of all integers.\n\\mathbb{R} is the set of all real numbers.\n\\mathbb{E} isn’t a set of numbers at all - it means “the expectation of” and is used in probability. But it looks similar to these fancy sets, so be careful not to get it mixed up. That’s how they get you.\n\\mathbb{C} is the set of endless agony. If you see it, run.\nThe Mechanism Designers will typically use these fancy sets when defining a new set. For example, they might say X \\subset \\mathbb{Z}, which means X is a subset of \\mathbb{Z}, which means that all of the possible x's in X must be integers. If you’re wondering “Why doesn’t the Mechanism Designer just say that the set is made up only of integers?” the answer is because they hate you.\nTwo things that Mechanism Designers simply can’t get enough of are intuitions and expectations.\n“Intuitively…” or “The intuition is…” means that the Mechanism Designer is about to tell you something that they think is so obvious that they won’t explain it because only a moron would disagree. Unfortunately, for a Mechanism Designer i in the set of all people P, the set of morons M = \\{ p, \\forall p \\in P : p \\not = i \\}, which, in English, means “The set of morons is all of the people in the set of people who aren’t also the Mechanism Designer”. If you want to understand a Mechanism Designer’s intuition, your best chance is their culture’s traditional approach of asking for an explanation after you beat them in duel with flint-lock pistols. Intuitively, you should be sure to ask quickly.\n“The expectation is…” or “… in expectation.” means that the Mechanism Designer is about to do some math, and we can expect that the math will involve probabilities. Probably.\n\nP(y) is the probability of event y occuring. Example: for a coinflip, P(\\text{heads}) = 0.50\n\n\nP(y \\cap z) is the probability of event y and event z both occuring. This is also abbreviated as P(y, z).\n\n\nP(y \\cup z) is the probability of event y or event z occuring.\n\n\nP(y | z) =  the probability of event y occuring if we assume event z has already occured. For example, assume a coinflip, but this time there is a cheater using a weighted coin that can change the odds from 50-50 to 80-20. In that case, P(\\text{heads} | \\text{cheater bet on heads} | \\text{cheater's flip}) = 0.80 and P(\\text{heads} | \\text{cheater bet on tails} | \\text{cheater's flip}) = 0.20.\n\n\n\\mathbb{E}[X] = The expected (or probabalistic, if you want to sound smart while actually being wrong) value of X. For example, if there’s a game with a coinflip and you get $0 for tails and $1 for heads then \\mathbb{E}[\\text{game}] = $0.50. Note that if we want to treat the game’s outcomes as a set, {E}[X] = \\{ {E}[\\text{heads}], \\ {E}[\\text{tails}]\\} = \\{ \\$0, \\$1 \\}, and \\mathbb{E}[X] is just the average of the expected values (aka “the expectation of”) the possible outcomes in X.\n\n\n\\mathbb{E}[X|y] = The value of X in expectation if we assume that y happened. For example, if there’s a game with a coin flip and you get $0 for tails and $1 for heads, but there’s a cheater who is using a weighted coin that can change the odds from 50-50 to 80-20 (against you) then \\mathbb{E}[\\text{game} | \\text{cheater's flip}] = $0.20.\n–Now imagine that the cheater gets to flip some of the time and you get to flip the coin the rest of the time:\n \\mathbb{E}[\\text{game}] = \\left( \\mathbb{E}[\\text{game} | \\text{cheater's flip}] \\times P(\\text{cheater's flip}) \\right) \\ + \\  \\left( \\mathbb{E}[\\text{game} | \\text{your flip}] \\times P(\\text{your flip}) \\right) \n\n\n\\mathbb{P}(x) is just a fancy way of saying P(x). Some Mechanism Designers insist that if P(x) is the probability of x, \\mathbb{P}(x) is the probability of x in expectation, but nobody knows what that means and so we just ignore them and move on with our lives.\n\nP(y) is the probability of event y occuring. Example: for a coinflip, P(\\text{heads}) = 0.50\nP(y \\cap z) is the probability of event y and event z both occuring. This is also abbreviated as P(y, z).\nP(y \\cup z) is the probability of event y or event z occuring.\nP(y | z) =  the probability of event y occuring if we assume event z has already occured. For example, assume a coinflip, but this time there is a cheater using a weighted coin that can change the odds from 50-50 to 80-20. In that case, P(\\text{heads} | \\text{cheater bet on heads} | \\text{cheater's flip}) = 0.80 and P(\\text{heads} | \\text{cheater bet on tails} | \\text{cheater's flip}) = 0.20.\n\\mathbb{E}[X] = The expected (or probabalistic, if you want to sound smart while actually being wrong) value of X. For example, if there’s a game with a coinflip and you get $0 for tails and $1 for heads then \\mathbb{E}[\\text{game}] = $0.50. Note that if we want to treat the game’s outcomes as a set, {E}[X] = \\{ {E}[\\text{heads}], \\ {E}[\\text{tails}]\\} = \\{ \\$0, \\$1 \\}, and \\mathbb{E}[X] is just the average of the expected values (aka “the expectation of”) the possible outcomes in X.\n\\mathbb{E}[X|y] = The value of X in expectation if we assume that y happened. For example, if there’s a game with a coin flip and you get $0 for tails and $1 for heads, but there’s a cheater who is using a weighted coin that can change the odds from 50-50 to 80-20 (against you) then \\mathbb{E}[\\text{game} | \\text{cheater's flip}] = $0.20.\n–Now imagine that the cheater gets to flip some of the time and you get to flip the coin the rest of the time:\n \\mathbb{E}[\\text{game}] = \\left( \\mathbb{E}[\\text{game} | \\text{cheater's flip}] \\times P(\\text{cheater's flip}) \\right) \\ + \\  \\left( \\mathbb{E}[\\text{game} | \\text{your flip}] \\times P(\\text{your flip}) \\right) \n\\mathbb{P}(x) is just a fancy way of saying P(x). Some Mechanism Designers insist that if P(x) is the probability of x, \\mathbb{P}(x) is the probability of x in expectation, but nobody knows what that means and so we just ignore them and move on with our lives.\n∑ (Sum)\nThis sigma may have been an key part of your fraternity or sorority’s identity during college, but it has another, less-important use case: math.\n \\sum^n_{x=1}f(x) = The sum of f(x) for all x values from 1 to n.\nIn other words, it’s a “for loop” that sums the different values of f(x), with x ranging between 1 (the bottom) and n( the top).\nMath Example:\n\\sum^4_{x=1}2x = 2+4+6+8 = 20\nNote that you can replace the range notation of \\sum with a set.  If X = \\{1, 2, 3, 4 \\} then\n \\sum_{x \\in X} 2x \\ = \\ 2 + 4 + 6 + 8 \\ = \\ 20 \\ = \\ 2\\sum_{ X} \nMechanism Designers really like talking about whether x should start at 1 or 0, although nobody knows why. Leading experts have hypothesized that it’s a core part of their mating ritual, but the results are still inconclusive.\n∏ (Product)\n\\prod^n_{x=1}f(x) = the product of f(x) for all x values from 1 to n.\nThe best way to explain it is through a comparison:\n \\sum : \\prod :: \\text{addition}: \\text{multiplication} \nIf you don’t remember the : :: : comparison format from the SATs then you are beyond saving.\nThe \"Big U\" and the Binomial Coefficient\n\\bigcup^x_y  \\text{ or } \\binom{n}{k}\nUnless x and y are both small, normal-looking numbers, you’re about to have a really bad time. The math isn’t hard, it’s just a real pain in the ass to write out. The one on the left is an iterator for the union of sets and the one on the right is the binomial coefficient.\nd/dx  (Derivatives)\nf(x)\\frac{d}{dx} = f'(x) =  \\text{the derivative of} f(x) \nDerivatives measure the rate that one thing is changing (probably x) relative to the rate another thing is changing (probably y, but maybe t if the Mechanism Designer is fully domesticated). If f(x) is a line, then its derivative is the slope of the line. In other words, it’s the rate of change of the line. If there is a line that graphed “time” on the x-axis and a car’s distance from the starting point on the y-axis, then the derivative of that line would be the car’s velocity (the rate that the position is changing relative to the rate that time is changing). If the velocity of a car is on the y-axis, then the derivative of that line would be the car’s rate of acceleration. This is what they teach in calc 1, but that you haven’t needed to use since highschool because it only recently became cool to discuss “novel mechanisms.” It looks like your teacher was right all along.\n∫ (Integral)\n \\int^x_yf(x) = the integral, aka “antiderivative.”\nIf a function f(x) creates a line on a graph, its integral is the area underneath it. Even your highschool teacher would’ve admitted that it’s unlikely that you’ll need to use integrals in your day-to-day job. After all, integrals are pretty useless unless you’re either a math teacher or having to deal with the probabilities of expected probabilities in a Mechanism Designer’s white paper. Speaking of which…\nCumulative Distribution Function\nF_X(x) is a cumulative distribution function, aka CDF. If you have a distribution X (which is a set of all possible values that x could be) then F_X(x) = \\mathbb{E}[P(x > X)], which is the probability that x is greater than a randomly selected value from the set X.\nExample: If X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20 \\}   and x = 8, then F_X(8) = 0.30 because when you draw a random number from X there is only a 30% chance that you’ll get one of the three that are less than 8 (2, 4, and 6).\nProbability Density Function\nf_X(x)  is a probability density function, aka PDF. It is basically saying the probability that a randomly chosen value from X will equal x.\nExample: If X = \\{2, 4, 6, 8, 10, 12, 14, 16, 18, 20 \\}  then f_X(6) = 0.10 because there’s a 10% chance that we’ll draw a 6 from the set. In other words, f_X(x) = \\mathbb{E}[P(x = X)] . Don’t think about that equation too much - if the thought of x = X seems contradictory to you, that’s a good sign that you’re still a healthy, normal person.\nExample: X = \\{2, \\ 2, \\ 6, 8, 10, 12, 14, 16, 18, 20 \\}  (note that we replaced the 4 with another 2) then f_X(2) = 0.20 and f_X(4) = 0.0.\nNote that F_X(x) is very useful in analyzing auctions because if X is the set of all bids, F_X(x) is the probability that our bid x is greater than a randomly selected bid, and F_X(x)^n is the probability that our bid x is greater than n number of bids.\nThe calculus from the previous section comes into play because the probability density function f_X(x) is the derivative of the cumulative distribution function F_X(x), and the cumulative distribution function is the integral of the probability density function:\nf_X(x) = F_X(x)\\frac{d}{dx} \nF_X(x) = \\int^{\\infty}_{- \\infty} f_X(x)\nWe’ll often have to use calculus to get back and forth between how likely someone is to bid something, and how likely a bid is to be higher than another bid.\nPeople typically refer to the set of players (so-called) in an auction as P, and a player in the set of players as i. Nobody knows why i was chosen over p, but it was probably so that Mechanism Designers could go around saying “i player” to each other and laughing at their clever inside joke. This has been going on for decades.\nIf the bids are referred to as b then b_i would be i's bid.  If you want to compare two players, j is typically a stand-in for “the other player,” whereas -i is the stand-in for “all players other than i.”\nSometimes a Mechanism Designer might want to share a new formula with you that is similar to an existing one, but that’s just slightly different. If you see weird symbols over or under letters, it probably means the equation or variable has had something added to it to make it extra special.\nHere are some examples:\nIf i is a player (bidder) in an auction, i' might be his sworn nemesis.\nIf b_i is the bid of player i, b^*_i might be the optimal bid.\n–Warning: You might be wondering, “Aren’t all bids optimal bids? Why would player i bid an amount that isn’t optimal?” but you should never ask this question out loud - it’s considered deeply inappropriate and you’ll end up on a list.\nIf g(x) is a function that works for everyone, g_i(x) is a function that only works for special players like i.\nIf t_i^2 isn’t meant to be t_i squared, it may mean that it’s the second t belong to i in a sequence of t_i's. Maybe the 2 is on the bottom, but then where would we put the i to mark the t as special? This is a perfect example of the kind of difficult question that Mechanism Designers spend most of their time on.\nϵ (Epsilon)\nMechanism designers use the \\epsilon (epsilon) symbol a non-trivial amount, which is ironic because it represents a trivial amount. Trivial, by the way, is just a cooler-sounding way of saying “teeny-tiny.” A Mechanism Designer might say something along the lines of “the optimal bid value was market price less epsilon”: b^*_i = v_i - \\epsilon.\n⋅ (The Dot Thingy)\nWhile this may mean multiplication, if you see it by itself inside of a function then it probably means the Mechanism Designer is being lazy and didn’t want to copy and paste their math. You’ll typically see this only after you’ve already seen the full version. For example, if you’re unlucky enough to see something like y = z + g(x^2+\\mathbb{E}[Z] - \\epsilon ), then later on you might see a = 2z + g(\\cdot), where the \\cdot is a stand-in for x^2+\\mathbb{E}[Z] - \\epsilon.\n⇒ (Therefore)\nIf the Mechanism Designer wants to prove why something is the way that it is, they might use this big arrow thingy. It means “therefore” or “it follows that…”. For example, if a Mechanism Designer wants to show that size of his mechanism proves that he’s really smart, it might look like:\n\\mathbf{card}(mechanism_i) > \\mathbf{card}(mechanism_{j}) \\forall j\\in P : j \\not = i \\Rightarrow F_{IQ}(iq_i) = 1 - \\epsilon \nA good exercise is to assess whether or not you actually understood that equation. If you did, it means you’ve been paying attention to the paper! Unfortunately, it also means you’re less cool in expectation. The actual english translation is “If the number of things in the mechanism made by player i is greater than the number of things in the mechanism made by player j, for all possible player j’s in the world who aren’t player i, then it follows that the IQ of player i has a 100% chance (minus a teeny-tiny percent) of being greater than a randomly selected IQ from the distribution of all the IQs in the world.”\n→ (Approaches)\nThis small arrow thingy (\\rightarrow) means “approaches.” You’ll often see \\rightarrow right next to \\Rightarrow due to how much the Mechanism Designers like tricking people.\nAn example, where M_i is the set of mechanisms made by player i and F_i is the set of player i's friends:\nWhich, in English, means “If the number of mechanisms made by player i approaches infinity, it follows that the number of friends that player i has approaches zero.”\nϕ,θ,γ,δ,σ,ψ,τ or Other Greek Letters\nMechanism Designers love defining variables. Unlike their sworn enemies the mathematicians, Mechanism Designers really like for their variables to be exotic, and so they’ll often use lower-case greek letters. It’s a best practice to always have a Greek alphabet available when reading a mechanism’s white paper so that you can quickly check to see if the designer is referring to a variable - which is pretty normal - or using some sort of ritualistic-sacrifice-based summoning math - which is a red flag. Intuitively.\nex ante and ex post\nWhen auction players have to figure out what their bid for an item is before they know what the value of the item is, the auction is said to be ex ante.  If the value of the item is known at bidding time, the auction is said to be ex post. This is one of the rare cases in which the obscure auction language is actually less wordy than what its describing. Way to go, Mechanism Designers! You did it!\nFirst-Price and Second-Price Auctions\nA first-price auction is one in which the highest bidder pays what they bid. A second-price auction is one in which the highest bidder pays what the second-highest bidder bid. Mechanism Designers really like second-price auctions because they pay the beneficiary more than a first-price would, but they’re also easier to cheat and therefore require a more robust mechanism.\nWarning: Never ask a Mechanism Designer why second-price auctions are better than first-price auctions. It’s like asking your wife if she’s had any interesting dreams recently, or if that girl at work she doesn’t like has caused any more problems. If a Mechanism Designer ever brings up the subject of first-price vs second-price, just look them directly in the eyes and then tell them with a firm voice, “With a properly designed mechanism and sealed bids, a second-price auction at equilibrium leads to more auction revenue in expectation, obviously!” This response is a strictly-dominant strategy.\nUtility Function\nA utility function, often called a payoff function, is how valuable someone thinks something is. It’s typically measured in expectation, but this was developed by the French so the \\mathbb{E} is silent.\nExample: If U(x) is a utility function, then U_i(b_i, b_{-i}) is the payoff to player i considering their bid ( b_i) and their competitors’ bids  (b_{-i}).\nBid Shading\nBid shading occurs when bidders bid less than their true value - in other words, when v_i > b^*_i. This typically happens because the bidder can make more money by bidding below their true value. Or, in the native tongue of Mechanism Designers, “The utility function of bidders is optimized when their private valuations exceed their equilibrium bid.”. We may never know why Mechanism Designers find it so fascinating that people make more money by paying less for something than they think it’s worth.\nIncentive Compatibility\nSomething is considered incentive compatible (IC) if the participants in an auction are willing to bid their true value. Every bidder i assigns a value v_i to the item.  The mechanism is incentive compatible when v_i = b_i. Mechanisms can be strongly incentive compatible or weakly incentive compatible, but nobody really knows what that means. It probably has something to do with how big the mechanism is.\nBayesian-Nash Equilibrium\nA Bayesian-Nash Equilibrium (or, as the kids say these days, a “BNE”) exists when there are multiple rounds and each bidders’ optimal strategy (AKA their “Best Response” or br_i or s^*_i) stays the same for each round. In other words, they won’t benefit from using any “trick plays” or “bamboozles.”\nNote that a mechanism with incentive compatibility is considered significantly better than a mechanism with just a bayesian-nash equilibrium. If two Mechanism Designers meet in the wild and one of them has an IC mechanism and the other has a BNE mechanism, then the wife and children of the BNE designer will instinctively join the tribe of the IC designer and the BNE designer will have to start rebuilding his mechanism and his family from scratch. Mother nature is cruel, but efficient.\nCredible Neutrality\nNobody actually knows what credible neutrality means. This has led to many Mechanism Designers making up their own definitions, presumably using their intuition. Unfortunately, these different definitions of credible neutrality are not credibly neutral, which is why the term remains undefined.\nFair Exchange Problem\nThe fair exchange problem refers to the difficulties of getting the parties of a trade to actually do what they promised they’d do, such as paying a bid or selling an asset at a certain price.  Mechanism Designers love making people do things, so this is one of their favorite problems to solve.\nWhen someone refers to the free look problem, know that you’re talking to an actual human being - a Mechanism Designer will always out themselves by calling it a fair exchange problem. Usage of the term optionality is a red flag but still inconclusive.\nThe conclusion section of a Mechanism Designer’s white paper is the result of feeding the rest of their paper into a LLM model and then asking it to generate a summary. This is why nobody ever actually reads it, and neither should you. I’d also like to thank my co-author, Et Al.\n",
        "category": [
            "Economics"
        ],
        "discourse": []
    },
    {
        "title": "Forking the RANDAO: Manipulating Ethereum's Distributed Randomness Beacon",
        "link": "https://ethresear.ch/t/forking-the-randao-manipulating-ethereums-distributed-randomness-beacon/21414",
        "article": "TL,DR: In our latest paper, we analyse the manipulability of RANDAO, Ethereum’s distributed randomness beacon (DRB) protocol. Before our work, the only known manipulation strategy was selfish mixing. Toni wrote a nice ethresear.ch post about it.\nIn our work, we identify a new class of manipulation strategies that a RANDAO manipulator can employ: forking the blockchain. In the paper, we show that the forking strategy combined with selfish mixing yields the most powerful RANDAO manipulation strategy.\nE-print: https://eprint.iacr.org/2025/037.pdf\nGithub: GitHub - nagyabi/forking_randao_manipulation: Researching RANDAO manipulation in Ethereum mainnet.\nWe refer to the paper or this Twitter thread for the gory technical details.\nimage707×534 28.7 KB\nHere, we only include open research questions and some food for thought.\nShort-term countermeasures\nThe community could apply various short-term countermeasures to counter forking attacks, though none of these mitigations solve entirely the issue of RANDAO’s biasability.\nMaking the epochs longer. Longer epochs decrease the manipulative power of the tail slots.\nDecrease proposer boost. A decreased proposer boost would require higher stakes to pull off our identified RANDAO forking manipulations.\nSingle-slot finality or other shorter finality mechanisms would also make longer forkings impossible.\nLong-term countermeasures\nThe end goal must be an unbiasable distributed randomness protocol that is scalable and efficient at Ethereum’s scale.\nDishonest majority setting: verifiable delay functions are required as a recent paper has shown that delay functions are necessary for dishonest-majority coin-flipping\nHonest majority setting: since Ethereum (and Bitcoin) already operates in the honest majority setting, maybe could be content with other unbiasable DRB constructions, e.g., weighted threshold VRFs.\n(The ideal conservative, long-term solution should also be post-quantum secure)\nOpen research directions\nAnalysing the game-theoretical properties of a RANDAO bribery market. The idea is that validators could auction off their manipulative power to bribers. This market could be implemented trustlessly using smart contracts.\nStudying RANDAO manipulation in a model incorporating MEV, i.e., block rewards and transaction fees are not uniformly distributed.\nWe did not cover all possible forking attacks. Evaluating them would allow us to claim a little higher RANDAO manipulation percentages.\nIs there any other manipulation strategy a RANDAO manipulator validator could use? Perhaps manipulating the sync committees?\nAny feedback, comments, or questions are welcome!\n",
        "category": [
            "Consensus"
        ],
        "discourse": [
            "random-number-generator"
        ]
    },
    {
        "title": "Decentralized Attestation Aggregation with Quorum Certification based Single-Slot-Finality",
        "link": "https://ethresear.ch/t/decentralized-attestation-aggregation-with-quorum-certification-based-single-slot-finality/21429",
        "article": "Written by @AmbitionCX from Panta Rhei and @Keccak255 from Titania Research. This proposal is a preliminary idea evaluation for another SSF (Single Slot Finality) solution.\nIntroduction\nIn the current Ethereum consensus protocol, validators responsible for a specific slot are divided into different (maximal 64) committees. Within each committee, individual attestations are disseminated across a subnet, then collected and aggregated by an average of 16 aggregators.\nEIP-7549, according to Pectra Upgrade, will move committee index outside of attestation structure. The subnet of attestation dissemination will be extended to all committees for one slot, 64 times larger than before. ~1024 aggregators are responsible to aggregate attestations in a slot. This EIP could be considered as a small step towards final SSF algorithm.\nThis post takes a step further by including all validators (of 32 slot) in a single slot scope, and every validator are able to become an aggregators (parallel aggregation). The subnet of validator scale is 32 times larger than EIP-7549. In another word, try to include attestations from all 1 million validators as many as possible.\nFor any SSF algorithm, refer to this post from Vitalik, they have to consider how to process so many attestations in a single time slot. A algorithm have to answer the question:\nHow many attestation are you going to collect and decide if a block can be finalized or not?\nWho is eligible to participate in the consensus process?\nFor example, Orbit SSF is going to build a small set of “super committee”, and validators with higher staking amount are more likely to participate in the super committee after EIP-7251. In this post, we answer the question with Decentralized Attestation Aggregation and Quorum Certification based Finality:\nDecentralized Attestation Aggregation: We allow all validators to be an aggregator, and attestations are grouped as a Quorum. Same quorum are merged together by different aggregator in parallel, and after multiple consolidation, attestations are compressed into beacon block body. If an aggregated attestation contains more than \\frac{2}{3} of validator signatures, the block is considered finalized.\nQuorum Certification based Finality: Attestations in a finalized block receive rewards for their timely and accurate attestations. The remaining attestations are discarded. Therefore, becoming an aggregator and participating in the aggregation process can significantly increase the possibility of earning benefits.\nConclusion\nThe proposed approach of Decentralized Attestation Aggregation and Quorum Certification based Finality presents a scalable and inclusive solution for achieving SSF in Ethereum. By enabling all validators to participate in the aggregation process and leveraging a hierarchical grouping mechanism, the protocol can efficiently handle attestations from over one million validators within a 12-second slot. The introduction of Quorum Certification ensures that block finality is achieved through a supermajority consensus, aligning with Ethereum’s security and decentralization principles.\nIf you have any questions or suggestions, please feel free to reply, or contact me by [email protected]\n",
        "category": [
            "Consensus"
        ],
        "discourse": [
            "single-slot-finality"
        ]
    },
    {
        "title": "Simple guide to fast linear combinations (aka multiexponentiations)",
        "link": "https://ethresear.ch/t/simple-guide-to-fast-linear-combinations-aka-multiexponentiations/7238",
        "article": "A problem that often appears in optimizing ZK-SNARK implementations, Ethereum clients, and other cryptographic implementations is as follows. You have a large number of objects (usually elliptic curve points) P_1 ... P_n, and for each object you have a correspondting factor f_1 ... f_n. You want to compute P_1 * f_1 + P_2 * f_2 + ... + P_n * f_n, and do so quickly.\nMany people (ab-)use the term “Pippenger algorithm” to refer to a whole family of fast-linear-combination algorithms; this post takes you through the basic idea of how they work and includes a code link to a limited but simple such algorithm.\nNaively, assuming a 256-bit group order, computing a linear combination of N points takes \\approx 384 * n point additions using the square and multiply: for each value P_i, keep doubling it to generate 2P_i, 4P_i, 8P_i etc etc until you get to 2^{256}P_i, and then add together the subset of those values that correspond to 1 bits in the factor (on average half of the bits will be 1, hence add another 128 point additions to get 384). For example, if your factor is 13, then 13 in binary is 1101, so you would add P_i + 4P_i + 8P_i.\n\nBut if we have more than one point, can we do better? It turns out that yes we can. First of all, we’ll start with an algorithm that gets us a modest 3x improvement. This algorithm works by converting a linear combination problem into a “multi-subset” problem.\nThe multi-subset problem is described as follows: given a set of points Q_1 ... Q_n, compute the sum of the values in each subset S_1 ... S_k where S_i \\subset \\{Q_1 .... Q_n\\}. For example, given Q_1 ... Q_5, we might want to compute the sums of each of the subsets (\\{Q_1, Q_2, Q_4\\}, \\{Q_2, Q_3, Q_4\\}, \\{Q_1, Q_3, Q_4, Q_5\\}, \\{Q_2, Q_4, Q_5\\}). In those case we would want four sums (which we’ll denote T_1 ... T_4), one for the values in each subset, eg. T_1 = Q_1 + Q_2 + Q_4.\n\nNow here is how we convert a linear combination problem into a multi-subset problem. Take d subsets, where d is the number of bits in the highest factor in binary (often d = 256), where the i’th subset is the subset of points whose corresponding factors have an i’th bit of 1. For example, for the factors \\{5, 11, 6, 15, 12\\}, the first subset is the subset corresponding to the odd factors, so \\{Q_1, Q_2, Q_4\\}, the second subset is the subset whose 2’s bit is 1 (ie. which are 2 or 3 mod 4), so \\{Q_2, Q_3, Q_4\\}, and so on (yes, I chose the factors deliberately to match the example above).\nNow, here is how we compute our desired linear combination 5Q_1 + 11Q_2 + 6Q_3 + 15Q_4 + 12Q_5. Let T_i = sum(S_i) where S_i is the i’th subset (so eg. T_1 = Q_1 + Q_2 + Q_4). Notice that the desired linear combination exactly equals T_1 + 2T_2 + 4T_3 + 8T_4. To see why this is the case, consider the number of times any given factor appears, eg. Q_2 (with a corresponding factor f_2 = 11). 11 in binary is 1011, so its 1st, 2nd and 4th bits are turned on. Hence, it has a presence in T_1, T_2 and T_4, and so it is present 8 + 2 + 1 = 11 times in T_1 + 2T_2 + 4T_3 + 8T_4. We can repeat the same argument for every Q_i, and so every Q_i appears in the final sum the correct number of times.\nWe compute T_1 + 2T_2 + 4T_3 + 8T_4 efficiently with a double-and-add chain:\n\nWe can verify that Q_2 shows up 11 times graphically by only paying attention to what multiple of Q_2 is represented at each step:\n\nFor 256-bit factors and n points, this procedure has a cost of 256 * 2 = 512 operations for the bottom row, plus 128 operations, on average, to compute each subset, so we get a cost of 512 + 128n operations; as n gets large this approaches being three times as efficient as the naive approach of computing every product and adding them at the end separately.\nNow, let us get to more efficient ways of solving the multi-subset problem. In the example above, naively computing (\\{Q_1, Q_2, Q_4\\}, \\{Q_2, Q_3, Q_4\\}, \\{Q_1, Q_3, Q_4, Q_5\\}, \\{Q_2, Q_4, Q_5\\}) would cost 2 + 2 + 3 + 2 = 9 operations: |S_i|-1 operations for each subset S_i. But notice that there are partial computations that we can merge. If we first compute R_1 = Q_2 + Q_4, then we reduce the remaining calculation to (\\{Q_1, R_1\\}, \\{Q_3, R_1\\}, \\{Q_1, Q_3, Q_4, Q_5\\}, \\{R_1, Q_5\\}), which cuts us down to 1 + (1 + 1 + 3 + 1) = 7 operations. In general, the more subsets you have, the more mergings you can do. For example, if you have many subsets each having about half of some list of elements (which is our situation), then any randomly selected pair of elements will on average appear in about a quarter of the subsets.\nThe various more efficient ways of solving the multi-subset problem, which in turn lead to fast linear combination algorithms using the method described above, basically have to do with coming up with different heuristics for figuring out which subsets to compute first to achieve the largest reduction in total computation.\nIn https://github.com/ethereum/research/blob/420a19e4449a2a85cb81a6731d6cf4534c3a366b/fast_linear_combinations/multicombs.py I made a fairly simple implementation: search through all pairs to find the pairs that appear in the most subsets, add new elements for them, and repeat until there are no more pairs.\nThis algorithm does have the problem that it requires O(n^2 * log(n)) “bit-twiddling” work, which eventually overcomes the benefit of the O(N / log(N)) operation count for making a N-element linear combination. However, for inputs up to size ~100 this extra work is small, and at those levels you get a further ~2.5x improvement compared to the “simple” multi-subset approach (ie. ~7.5x improvement relative to multiplying each point separately), and for larger inputs you can just split them up. There are more advanced algorithms (eg. see here), but this is beyond the scope of this post; this very simple algorithm seems able to get you to within half of the theoretical optimum in most practical cases.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "Bringing privacy to EVM applications using confidential computing via co-processors",
        "link": "https://ethresear.ch/t/bringing-privacy-to-evm-applications-using-confidential-computing-via-co-processors/21217",
        "article": "By me, @rishotics and team\nSpecial thanks to Rand Hindi and the Zama team for their feedback and suggestions\nIntroduction\nBlockchain and DeFi systems promote transparency and decentralization but expose user transaction data to the public. This visibility poses significant privacy risks, deterring privacy-conscious users and opening the door to security threats like front-running attacks.\nPrivacy is a fundamental pillar of the open-source ecosystem, yet it is often overlooked in DApps. This article explains one of our approach to seamlessly integrate privacy into existing applications rather than creating isolated, fragmented ecosystems where privacy is treated as an afterthought. With billions of USD locked in ecosystems like Ethereum, Solana, and Bitcoin, privacy solutions are not even a part of their near-term roadmap. This leaves users with no choice but to engage with these systems without privacy, exposing sensitive financial data.\nSeveral privacy solutions, including shielded pools, hardware solutions, and threshold encryption, have been attempted to tackle these issues but face significant limitations. Shielded pools, while effective at concealing transaction details, create barriers to adoption due to complex user interfaces and regulatory challenges. Threshold encryption solutions suffer from complex key management and scalability issues in decentralized environments. Consequently, these solutions often sacrifice usability and compliance, limiting their effectiveness in real-world applications.\nPrivacy-enhancing technologies like FHE, TEE, and MPC offer a novel approach by enabling computations on encrypted data without decryption. Thus, they preserve privacy while addressing the scalability and regulatory challenges that have limited previous solutions. The issue is how to use these PETs with existing EVM stacks or Dapps[2].\nTo make privacy accessible to all users, we need to focus on two key areas:\nAdapting existing application: to be compatible with privacy-enhancing technologies\nIf building new application: within existing ecosystems using privacy-preserving technologies\nVery few efforts have been made to introduce privacy in existing applications. Our approach tries to tackle the above challenges and provides a generalised way of interacting with current applications.\nIn this post, we will explore how to incorporate privacy into existing Defi applications on EVM-compatible chains using PETs like FHE. The overall architecture combines off-chain co-processors with on-chain smart contracts. For these off-chain co-processors to interact effectively with on-chain states, we need a framework that enables smart contracts to work seamlessly with encrypted data.\nWe also discuss the concept of encrypted ERC20 tokens, which provide a privacy-enhanced alternative to standard ERC20 tokens. As, Recently, Circle and Inco published a report delving deeper into the topic of encrypted ERC20s. To be precise, Our framework is not tied to any specific encrypted ERC20 standard, making it adaptable for use across multiple standards.\nCurrent Transaction Flow\nThe current transaction process—from a user’s wallet → mempool →  block is entirely transparent. This transparency aligns with the core purpose of a public blockchain, where data included in the ledger should be visible to all participants. However, this openness deters many people from entering the space, as not everyone wants their data to be visible to the entire world.\nThere are various stages in the execution process where privacy can be introduced, and each stage comes with its own set of complexities. Encrypting the transaction as soon as the wallet signs it makes the most sense, as valuable information can then be hidden on the client side.\nThe challenge lies in modifying the existing infrastructure and achieving community acceptance for these changes. Solutions include encrypted mempools, encrypted solving, private RPC providers, and block building within TEEs, among others. Let’s explore some of the solutions that other teams have worked on in the past.\nSome Previous Privacy Solutions\nEncrypted Mempools\nTeams are already working on encrypted mempool solutions. Threshold-encrypted mempools use threshold encryption to protect transaction details in the mempool until they are ready to be included in a block. This prevents unauthorized parties from viewing transaction details (e.g., sender, receiver, or amount) while the transaction is still pending, addressing issues like front-running in MEV situations. Users can submit transactions with the assurance that their details will remain confidential until the block is confirmed.\nHowever, encrypted mempools has high barrier to entry due to it’s unique cryptographic (Time lock puzzle, Threshold encryption / decryption) or hardware requirements (TEEs).\nMost threshold encryption schemes require an initial setup phase that involves distributed key generation, which can be costly in terms of time and resources. In large, decentralized environments, this setup can be challenging—especially when committee members join or leave, requiring key re-shares or even a complete rerun of the setup.\nShielding Pools\nCurrent solutions that provide protection for on-chain interactions often lack user-friendliness from a UX standpoint.\nUsing shielded addresses and pools introduces significant complexity to achieving on-chain privacy. Shielded pools enable users to store and transact assets without revealing transaction details—such as the sender, receiver, or amount—on the blockchain. Zero-knowledge (ZK) proofs facilitate these shielded transactions by validating their legitimacy without disclosing any actual data. This ensures that network participants can verify the validity of a transaction without knowing who sent or received the funds or the amount transferred.\nWhen a user transfers assets into a shielded pool, those assets are “shielded,” with transaction details (amount, sender, receiver) encrypted and hidden from the public ledger. ZK proofs are then used to confirm that the user holds a valid balance and is authorized to spend it, without revealing any specifics. Users can transfer assets between shielded addresses without exposing details within the shielded pool. All transactions remain hidden, with ZK proofs ensuring compliance with transaction rules, such as maintaining balance integrity and confirming valid ownership. If a user chooses to move assets back to a transparent (non-shielded) address, they can withdraw funds. However, this typically results in a “privacy break,” as the withdrawal amount becomes visible unless transferred to another shielded pool.\nWithout proper checks, shielded pools also raise compliance and regulatory concerns, leaving users uncertain. These pools obscure transaction details, complicating the tracing of funds and the identification of involved parties. Regulators are concerned that shielded pools could facilitate money laundering by concealing illicit funds. Financial institutions and regulated entities must comply with anti-money laundering (AML) regulations, which require the detection and reporting of suspicious activities. Shielded pools limit transaction visibility, making it challenging to verify the origin of funds and adhere to AML standards.\nSome Preliminaries\nDifferential Privacy\nDifferential privacy is a mathematical framework used to quantify and ensure the privacy of individuals within a dataset [1].\nThe core idea of differential privacy is to ensure that it is difficult to determine whether any specific individual’s data is included in a dataset, even when analyzing the output of an algorithm applied to that dataset. A randomized algorithm is said to satisfy (ϵ,δ) - differential privacy if the inclusion or exclusion of an individual’s data changes the probability of any specific output only slightly.\nIn the context of differential privacy, ϵ controls the privacy loss, quantifying the maximum difference in output probabilities for neighboring datasets (datasets differing by only one individual). δ represents the probability of a small relaxation in the privacy guarantee, allowing for a slight chance of greater privacy compromise. This framework ensures that the algorithm’s output remains nearly indistinguishable for neighboring datasets, thereby limiting the information leakage about any single data point.\nDifferential privacy has become a widely adopted standard for privacy-preserving data analysis, offering robust privacy guarantees while enabling valuable statistical insights.\nTorus-based Fully Homomorphic Encryption\nTFHE is a FHE scheme optimised explicitly for fast binary gate computations. Unlike traditional FHE methods that rely on more complex lattice structures, TFHE operates over the torus, efficiently performing encrypted computations with lower noise accumulation and faster bootstrapping times.\nAs a result, TFHE has emerged as a promising solution for secure, privacy-preserving computation in real-time applications.\nEncrypted ERC20 Tokens\nEncrypted ERC20 standard for privatizing user token balances. Any token balance intended for homomorphic computation on-chain would need to be wrapped within this encrypted ERC20 standard. This approach can serve as a foundation for building various privacy-focused solutions, such as private payments, private auctions, dark pools, and more.\nThis standard implements necessary interfaces which is used to implement necessary compliance checks, which include selective disclosure of specific ciphertext requested and a few other checks.\nTo learn more about Encrypted ERC20 you can read this article by Circle [3]\nDifferential Privacy with Order Aggregation and Batch Settlements\nWe propose a solution leveraging differential privacy to enable order-solving for encrypted orders. This allows users to place encrypted orders (orders with encrypted tokens) and have them processed on-chain without revealing their details. External parties cannot determine the exact order details associated with a specific user.\nBatching is a core component of this solution. The challenge with processing a single encrypted order directly through the protocol is that once decrypted, the amount the user intended to hide becomes visible. To mitigate this, we aggregate multiple orders using the additive homomorphic properties of certain privacy-enhancing technologies (PETs), such as Fully Homomorphic Encryption (FHE). The encrypted amounts are summed and deposited as an aggregated value with a designated manager. The manager’s role is to decrypt this aggregated value via a secure wrapper (obtaining the decrypted tokens amountIn values) so that the resulting assets can interact with the appropriate solver protocol.\nBy batching encrypted orders, we introduce a level of noise into each order, effectively preserving the privacy of individual users’ order details.\nbatching flow1920×1070 77.2 KB\nThe design is inspired by Zswap DEX of Penumbra [5], which uses sealed-bid batch swaps. The price at which these orders are settled is identical, as there is only one transaction per epoch.\nOnce the order is solved, the return token amount belonging to the user is calculated homomorphically using the ratio of the input amount to the output amount (the amount received upon solving the order). This calculation is performed homomorphically in the encrypted space, ensuring that no one can fully determine how many tokens a particular user will receive, thereby preserving privacy.\nsolving flow1920×1077 54.8 KB\nEnd to End flow Order placing → Order Aggregation → Order Solving → Distribution\nEnd to end solving flow1920×766 67 KB\nMathematical Formulation\nWe are proposing two methods for mitigation for the privacy in applications:\nEncrypting Assets: Assets held by the user is encrypted via publicly verifiable encryption scheme.\nBatching Orders: Choosing a size of n of orders to batch prior execution.\nIndividually these solutions don’t provide enough privacy guarantees from an adversary POV but together it introduced differential privacy which provides probabilistic indistinguishability for a particular user’s order.\nMost DeFi action on-chain can be defined as a Tokens going in ( T_{in} ) and tokens coming out ( T_{o} ), which means that any solving action \\pi can be written as\nBy changing the domain of interaction for the user with the protocol with we can introduce a middle smart contract M which does this interaction on the users behalf. Now M has the task of receiving orders from n users and aggregating them i \\in [1,n]\nWe can write the encrypted value of  T_{in}  for a user  i  as  C^{i}  where  C^{i}  can be represented as\nThe above representation is how a lattice based homomorphically encrypted plaintext looks like.\nNow since encryption is homomorphic in nature we can simply sum the individual ciphertexts to form the aggregate ciphertext C^{\\pi_M}\nIn this process we need to perform programmable bootstrapping multiple times which reduces the noise which is getting accumulated in every addition.\nThe decrypted amount is now used further for interaction with the DeFi protocol.\nConclusion\nPrivacy in blockchain and DeFi ecosystems is becoming increasingly crucial to protect user data and secure transaction processes. While various solutions—such as shielded pools, threshold encryption, differential privacy, and fully homomorphic encryption—offer unique approaches, they also present challenges in terms of usability, compliance, and technical implementation.\nExploring these privacy-preserving techniques highlights the potential for integrating privacy into existing blockchain applications while balancing transparency and regulatory requirements. As privacy solutions continue to evolve, they promise to foster a more inclusive, secure, and user-centric blockchain ecosystem, empowering users to engage confidently in decentralized platforms.\nReferences\n[1] Differential Privacy in Constant Function Market Makers by Tarun Chitra and Guillermo Angeris and Alex Evans\n[2] Zama’s fhEVM co-processor\n[3] Unveiling the Confidential ERC-20 Framework: Compliant Privacy on Public Blockchains using FHE\n[4] TFHE: Fast Fully Homomorphic Encryption over the Torus by Ilaria Chillotti and Nicolas Gama and Mariya Georgieva and Malika Izabachène\n[5] ZSwap Penumbra\n",
        "category": [
            "Privacy"
        ],
        "discourse": [
            "transaction-privacy"
        ]
    },
    {
        "title": "Security of BLS batch verification",
        "link": "https://ethresear.ch/t/security-of-bls-batch-verification/10748",
        "article": "By JP Aumasson (Taurus), Quan Thoi Minh Nguyen, and Antonio Sanso (Ethereum Foundation)\nThanks to Vitalik Buterin for his feedback.\nbatch886×564 101 KB\nIn a 2019 post Vitalik Buterin introduced Fast verification of multiple BLS signatures. Quoting his own words this is\na purely client-side optimization that can voluntarily be done by clients verifying blocks that contain multiple signatures\nThe original post includes some preliminary security analysis, but in this post we’d like to formalize it a bit and address some specific risks in the case of:\nBad randomness\nMissing subgroup checking\nWe described several attacks that work in those cases, and provide proof-of-concept implementations using the Python reference code.\nThe batch verification construction\nThis technique works as follows, given n aggregate signatures S_i, respective public keys P_i, each over a number m_i of messages (note that each aggregate signature may cover a distinct number of messages, that is, we can have m_i\\neq m_j for i\\neq j):\nThe naive method thus consists in checking these n equalities, which involves n+\\sum_{i=1}^n m_i pairings, the most calculation-heavy operation.\nTo reduce the number of pairings in the verification, one can further aggregate signatures, as follows: the verifier generates n random scalars r_i \\geq 1, and aggregates the signatures into a single one:\nthe verifier also “updates” the signed messages (as their hashes to the curve) to integrate the coefficient of their batch, defining\nVerification can then be done by checking\nVerification thus saves n-1 pairing operations, but adds n+\\sum_{i=1}^n m_i scalar multiplications. Note that if the verification fails, then the verifier can’t tell which (aggregate) signatures are invalid.\nIn particular, if m_i=1, \\forall i, then verification requires n+1 pairings and 2n multiplications, instead of 2n pairings. Depending on the relative cost of pairings vs. scalar multiplications, the speed-up may vary (see this post for pairings implementations benchmarks)\nSecurity goals\nInformally, the batch verification should succeed if and only if all signatures would be individually successfully verified. One or more (possibly all) of the signers may be maliciously colluding. Note that this general definition implicitly covers cases where\nAttackers manipulate public keys (without necessarily knowing the private key),\nMalicious signers pick their signature/messages tuples depending on honest signers’ input.\nFor a formal study of batch verification security, we refer to the 2011 paper of Camenisch, Hohenberger,  Østergaard Pedersen.\nNote that in the Ethereum context, attackers have much less freedom than this attack model allows, but we nonetheless want security against strong attackers to prevent any unsafe overlooked scenario.\nThe importance of randomness\nThe randomness has been already discussed in the original post. Buterin points out that if coefficients are constant, then an attacker could validate invalid signature:\nthe randomizing factors are necessary: otherwise, an attacker could make a bad block where if the correct signatures are C_1 and C_2, the attacker sets the signatures to C_1+D and C_2-D for some deviation D. A full signature check would interpret this as an invalid block, but a partial check would not.\nThis observation generalizes to predictable coefficients, and more specifically to the case where \\alpha attackers collude and any subset of \\beta\\leq \\alpha coefficients are predictable among those assigned to attackers’ input.\nNote that the coefficient can’t be deterministically derived from the batch to verify, for this would make coefficients predictable to an attacker.\nButerin further discusses the possible ranges of r_i to keep the scheme safe:\nWe can also set other r_i values in a smaller range, eg. 1…2^{64} , keeping the cost of attacking the scheme extremely high (as the ri values are secret, there’s no computational way for the attacker to try all possibilities and see which one works); this would reduce the cost of multiplications by ~4x.\nA simple attack\nIf coefficients are somehow predictable, then the above trivial attack can be generalized to picking as signatures S_1=(C_1+D_1) and S_2=(C_2+D_2) such that r_1 D_1=-r_2 D_2.\nIf coefficients are uniformly random b-bit signed values, then there is a chance 1/2^{b-1} that two random coefficients satisfy this equality (1 bit being dedicated to the sign encoding), and thus that verification passes. Otherwise, the chance that r_1 D_1=-r_2 D_2 for random coefficients is approximately 2^{-n}, with n the size of  the subgroup in which lie D_1 and D_2.\nHowever, the latter attack, independent of the randomness size, will fail if signatures are checked to fall in the highest-order subgroup (see the section The importance of subgroup checks).\nSignature manipulation\nIn practice, BLS signatures are not purely used as signatures, but implementers take advantage of other informal and implicit security properties such as uniqueness and “commitment”. That is, if the private key is fixed, given a message not controlled by the signer, the signer can’t manipulate the signature. In the case of bad randomness, such use cases may be insecure.\nFor instance, let’s say there is a lottery based on the current time interval, i.e., at time interval t outside of signers’ control if S_i = \\mathsf{Sign}(sk_i, t), (S_i)_x \\mod N = 0 (where N is just some small number, e.g., the number of signers) then the signer i wins the lottery. The first two signers can collude to win the lottery as follows. The first signer chooses a random point P \\in G_1 and offline bruteforces a k such that (S_1’)_x \\mod N = (S_1 - kP)_x \\mod N = 0  where S_1 = \\mathsf{Sign}(sk_1, t). The second signer computes S_2’ = S_2 + (k r_1/r_2)P where S_2 = \\mathsf{Sign}(sk_2, t). We have\nthat is, calculating further:\nWhat it means is that the first and second signer can manipulate the signatures so that the first signer wins the lottery while making batch verification valid.\nThe importance of subgroup checks\nThe current state-of-the-art pairing curves such BLS12-381 are not prime-order, for this reason the BLS signatures IRTF document warns about it and mandates a subgroup_check(P) while performing some operations. For batch verification, subgroup check is essential, as previously commented.\nThe lack of subgroup validation appears to have a greater impact with batch verification than with standard BLS validation, where points in small subgroup are not known to trivially be exploitable, as noted in the BLS signatures IRTF document:\nFor most pairing-friendly elliptic curves used in practice, the pairing operation e (Section 1.3) is undefined when its input points are not in the prime-order subgroups of E_1 and E_2.  The resulting behavior is unpredictable, and may enable forgeries.\nand in the recent Michael Scott’s paper:\nThe provided element is not of the correct order and the impact of, for example, inputting a supposed \\mathbb{G}_2 point of the wrong order into a pairing may not yet be fully understood.\nThe difficulty of performing an actual attack is also due the fact that \\mathsf{gcd}(h_1,h_2)=1 where h_1 and h_2 are the respective cofactors of E_1 and E_2. Hence pushing the two pairing’s input to lie in the same subgroup is not possible. Let’s see an example based on BLS12-381, where\nand\nChoosing an invalid signature S_1 of order 13 and a public key P_{1,1} of order 3, a potential attack would succeed (to pass batch verification) with probability 1/39=1/3 \\times 1/13. The attack assumes an implementation that multiplies the public keys by r_i's rather than the messages (in order to gain speed when there a distinct messages signed by a same key) as described here and implemented for example by Milagro.\nAn attack then works as follows in order to validate a batch of signatures that includes a signature that would not have passed verification:\nPick S_1 of order 13, and P_{1,1} of order 3. The chance that r_1 S_1 = r_1 P_{1,1} = \\mathcal{O} is 1/39, which is the attack success rate. In such case, we have:\nS^\\star = r_2S_2 + \\cdots r_n S_n.\nSuppose, without loss of generality, that m_1=1 (namely the first batch to verify is a single signature). That is, P'_{1,1} = r_1P_{1,1} = \\mathcal{O}.\nThe right part of the verification equation becomes \\prod_{i=1}^n \\prod_{j=1}^{m_i} e(P_{i,j},M_{i,j}') that is, e(P'_{1,1},M_{1,1})\\prod_{j=2}^{m_i} e(P'_{i,j},M_{i,j})=1 \\times \\prod_{j=2}^{m_i} e(P'_{i,j},M_{i,j}).\nIt follows that verification will pass when all other signatures are valid, even if P_1's signature S_1 is not valid.\nNote that the Ethereum clients (Lighthouse, Lodestar, Nimbus, Prysm, Teku) will already have performed subgroup validation upon deserialization preventing such an attack.\nImplementations cheat sheet\nSecure implementations of batch BLS verification must ensure that:\nGroup elements (signatures, public keys, message hashes) do not represent the point to infinity and belong to their respective assigned group (BLS12-381’s \\mathbb{G}_1 or \\mathbb{G}_2).\nThe r_i coefficients are chosen of the right size, using a cryptographically random generator, without side channel leakage.\nThe r_i coefficients are non-zero, using constant-time comparison, and if zero reject it and generate a new random value (if zero is hit multiple times, verification should abort, for something must be wrong with the PRNG).\nThe number of signatures matches the number of public keys and of message tuples.\nAdditionally, implementations may prevent DoS by bounding the number of messages signed.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "STARK Round By Round Soundness and Security in Random Oracle Model",
        "link": "https://ethresear.ch/t/stark-round-by-round-soundness-and-security-in-random-oracle-model/18096",
        "article": "Hi, want to bring attention to the analysis of ethSTARK security in the random oracle model given in the latest version of the ethSTARK documentation, which is also explained at a higher level in this medium blog post, which I’ll quote the very start of here. Happy to discuss further.\nSTARK Security Explained\nA STARK proof system (Scalable Transparent Argument of Knowledge) is a powerful tool for computational integrity: it allows verifying the correctness of computations performed on public data in a trustless manner. In this blog post, we delve into the security provided by STARK proofs, defining it and exploring techniques to prove scheme security.\n(Read Section 6 in the ethSTARK documentation (version 1.2) for full details and the important and comprehensive independent work of Block et al. on the topic.)\nWhat are we trying to achieve with our security analysis? We would like to prevent a “successful attack” on the STARK system, which is given by a false statement and a STARK proof accepted by the STARK verifier for this (false) statement. Since false statements are dangerous and they can come in all sizes and shapes, we want to be secure against all false statements. Any false statement, even as trivial as 1+1=3, combined with a STARK proof accepted by a STARK verifier for this statement, is considered a successful attack on the system. (Those with a cryptographic background may be interested to know that STARKs also satisfy stronger security notions such as knowledge soundness, but for simplicity, this post focuses on the simpler case of soundness.)\nHow do we formally define the security of a STARK system? We do so by analyzing the “soundness error” which roughly measures the expected “cost” that an attacker would need to spend to construct a successful attack (i.e., find a STARK proof for a false statement that nevertheless is accepted by the STARK verifier). Mathematically speaking, the soundness error is a function e(t) that gets as input a time parameter “t”, representing the amount of computation time an attacker is willing to spend to mount the attack and outputs the success probability of the attacker in succeeding with the attack (finding a convincing proof of a false statement). As the “cost” t that the attacker is willing to spend grows, his success probability increases.\nThus far, we have defined the security of STARKs as a function e(t), which is not the way you naturally discuss security, say, on crypto Twitter. There, you probably heard statements of the form “The scheme has 96 bits of security”. How does such a statement translate to our security definition? There is no one answer to this, as people have slightly different interpretations of “x bits of security”:\nA very strict translation would mean that for any t between 1 and 2⁹⁶, the soundness error is e(t) ≤ 2⁹⁶ . This means that any attacker running time at most 2⁹⁶ has a tiny probability of success, smaller than 1/2⁹⁶, which is smaller than one in a billion times a billion times a billion.\nA more relaxed, and perhaps more common, translation is that 96 bits of security means that for any t, it holds that t/e(t) ≥ 2⁹⁶. This means that the success probability is (inverse) linear to the running time. For example, if an attacker has a running time 2⁸⁶, its success probability is at most 1/2¹⁰.\nRead the rest here.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": [
            "zk-roll-up"
        ]
    },
    {
        "title": "Proper disk i/o gas pricing via LRU cache",
        "link": "https://ethresear.ch/t/proper-disk-i-o-gas-pricing-via-lru-cache/18146",
        "article": "In this proposal, I present a method to enhance the efficiency of state management in Ethereum clients, allowing for significant gas limit increase. The proposal involves using a Least Recently Used (LRU) cache to maintain approximately 2.5GB of the most recently accessed state, aiming for a total memory usage of around 8GB. State slots accessed that are part of this cache would priced significantly less gas wise than those outside. This would allow clients like Geth or Reth to make strong assumption for their memory usage and would price disk i/o properly.\nThis would make 8GB of RAM the minimum requirement to run an ethereum full node.\nEfficient state management is crucial for Ethereum clients, particularly as the chain continues to grow in size and complexity. A common challenge is maintaining quick access to a large set of state data with limited physical memory. Currently Ethereum’s state is about 100GB, far larger than RAM access of most full nodes. This means we need to price all read/writes high enough in case they were read from disk. The proposed solution introduces  an LRU cache at the consensus level, a well-known caching strategy that ensures the most recently accessed data is kept readily available, while less frequently accessed data is relegated to slower storage. With a precise agreement on what should be cached, we can price memory i/o and disk i/o independently, making state growth significantly less of a concern.\nFrom discussion with Remco Bloemen, Georgios Konstantopoulos and Agustin Aguilar\n",
        "category": [
            "Data Science"
        ],
        "discourse": []
    },
    {
        "title": "Public Projects with Preferences and Predictions",
        "link": "https://ethresear.ch/t/public-projects-with-preferences-and-predictions/19024",
        "article": "A few years ago, I posted Governance mixing auctions and futarchy here. Also relevant are Practical Futarchy Setup and Votes as Buy Orders. My post/question eventually led to research funding from the Ethereum Foundation and now my collaborator and I have a paper proposing a new governance mechanism: Public Projects with Preferences and Predictions. I hope you find it interesting and I would love to hear your feedback and thoughts! Here is a summary.\nProblem: A group, such as members of a DAO, need to decide between one of several alternatives, such as which project to pursue (they must pick exactly one). I want to focus on two aspects of this problem:\nThey want to base the decision on an aggregation of both preferences and information. For example, one way is to first hold discussions and conduct research in order to aggregate information. Then, if consensus is not reached, hold a vote to aggregate preferences into a final decision.\nA primary problem of any organization is to avoid capture. In particular, the individual preferences of the members are generally not perfectly aligned with the mission of the organization. The above “discuss-then-vote” approach doesn’t solve this. A decisionmaking mechanism should somehow be biased by a credible estimate of the impact on the organization’s mission.\nFormalizing it: A group of agents must pick one of m alternatives. The group’s mission is quantified by what we call an “external welfare impact” of the decision. The goal is to maximize total welfare: the sum of the external welfare impact, plus the utilities of all the agents in the group. The group first wants to aggregate information, in particular about the external welfare impacts of each alternative choice it could make. We model information as predictions about the future, i.e. the group first wants to estimate the external welfare impact B_k of each alternative k=1,\\dots,m. After obtaining the estimates, the group will hold a vote. We suppose that each agent i has a preference over the alternatives, modeled as a value v_k^i for each alternative k. The group will use some sort of voting mechanism to combine the preferences as well as the external welfare impacts into a final decision.\nFor example, consider a DAO whose charter requires consideration of climate impacts of its decisions. The external welfare impact B_k of an alternative k could be measured by the amount of extra tons of CO2 produced if that alternative is chosen. The DAO may still choose an alternative that produces more CO2 if the members as a whole strongly prefer that alternative.\nProposal: We propose it the Synthetic players QUAdratic transfers mechanism with Predictions (SQUAP), and it works like this:\nWe use an information-aggregation oracle to obtain an estimate of the external welfare impact B_i of each alternative. The oracle can be implemented in many possible ways, but in particular we consider using “decision markets” (i.e. sets of conditional prediction markets, as in futarchy).\nWe use Quadratic Voting, specifically the Quadratic Transfers Mechanism studied by Eguia et al. However, the mechanism casts “extra” votes based on B_1,\\dots,B_m.\nThe extra votes are not simply the numbers B_1,\\dots,B_m for each alternative. Instead, they are the votes that “synthetic players” would cast in equilibrium of Quadratic Voting, if their total values for each alternative were B_1,\\dots,B_m. This is based on an analysis of the equilibrium, extending results of Eguia et al.\nAbout the prediction markets: We need to suppose that the external welfare impact can be predicted before the fact and measured afterward. You can think of lots of ways to use proxies and estimates for quantities that are hard to measure or have long time horizons. For simplicity, in the paper we assume that if we take any decision k, then B_k will be directly observable and measurable. So we will set up m prediction markets, predicting for each alternative k the eventual impact B_k conditioned on making decision k. When we eventually make some decision k, we cancel all the trades in every market except the market for k. But we do eventually observe B_k for that chosen alternative and we resolve the market payoffs accordingly.\nThe Quadratic Transfer Mechanism picks an alternative from a probability distribution, where the probability of taking decision k is e^{A_k} / \\sum_{\\ell} e^{A_{\\ell}} and A_k is total the number of votes cast for decision k. There is a nonzero probability of picking each alternative. That may sound odd, but it actually helps address a well-known problem with futarchy and decision markets – bad incentives around predictions about decisions that won’t actually be taken.\nFormal results: Unfortunately, we can’t quite prove (yet) good results about SQUAP itself. However, we can analyze an “Impractical Variant” that is not practical because it requires some extra knowledge by the mechanism designer. Essentially, instead of calculating our synthetic votes based on what the real players vote for, we have to first commit to our synthetic votes based just on knowing B_1,\\dots,B_m, before we see what the players do. This is impractical to calculate without good estimates of what the players will do, but it’s not totally ridiculous. In any case, we can prove something:\nMain Theorem. In the 2-alternative case, in Nash equilibrium, the Impractical Variant of SQUAP satisfies Social Welfare \\to Optimal as the “size” of the group grows large relative to the preferences of any one individual.\nThe interesting and non-obvious part of all this is that the mechanism “works” in theory even though there are apparently bad incentives across the two stages. Someone with really strong preferences about the decision could try to manipulate the prediction market prices in order to manipulate the synthetic votes. And someone who bet a lot of money in the prediction market could try to cast really outsized votes in the Quadratic Voting stage in order to make their prediction-market payoffs come good. So the interesting part is in proving that these things don’t happen, or rather, they can happen a little but not enough to influence the outcome significantly … as long as each individual’s preferences are small compared to the group total.\nOur theorem quantifies the rate at which the social welfare approaches the optimal, and it’s reasonably fast as the size of the group grows large. But this post is too long already. I’ll just clarify that Social Welfare is formalized by the sum of the values of the participants in the group, plus the external welfare impact, of whatever the mechanism chooses.\nThe paper is here: [2403.01042] Public Projects with Preferences and Predictions\n",
        "category": [
            "Economics"
        ],
        "discourse": [
            "governance"
        ]
    },
    {
        "title": "Number Duplicate Messages in Ethereum's Gossipsub Network",
        "link": "https://ethresear.ch/t/number-duplicate-messages-in-ethereums-gossipsub-network/19921",
        "article": "The ProbeLab team (probelab.io) is carrying out a study on the performance of Gossipsub in Ethereum’s P2P network. Following from our previous post on the “Gossipsub Network Dynamicity through GRAFTs and PRUNEs” in this post we investigate the number of messages and duplicated messages seen by our node, per topic. There is no public data on the overhead that broadcasting messages and control data over the network imply on each participating node.\nFor the purposes of this study, we have built a tool called Hermes, which acts as a GossipSub listener and tracer (GitHub - probe-lab/hermes: A Gossipsub listener and tracer.). Hermes subscribes to all relevant pubsub topics and traces all protocol interactions. The results reported here are from a 3.5hr trace.\nStudy Description: Gossipsub’s design is inherently allowing for message duplicates. A brief model we develop shows that it’s normal to receive each message up to 3 extra times (as a duplicate). This excludes the gossip mechanism which propagates messages through the IHAVE/IWANT control message sequence.\nTL;DR: We find that indeed duplicates through mesh stay in the order of 3 per message or below, which, however, doesn’t count for duplicates through gossip. For instance, there are edge cases where a message is requested (and responded to) through an IWANT message while the actual message is already in transit. Eventually, this results in an extra duplicate. We make two recommendations:\nReduce the number of concurrent IWANT messages we send through a limiting factor (somewhat similar to kademlia’s alpha parameter).\nLower the current heartbeat frequency (i.e., increasing the heartbeat interval) from 0.7 seconds to 1 second (as per the original protocol spec and recommendation). This would reduce the excessive IHAVE messages and reduce the chances of generating extra duplicates.\nGossipSub is a routing system that can be enabled on libp2p’s PubSub message broadcasting protocol. This protocol organizes the message broadcasting channels on what is commonly known as Topics, where peers subscribed to a given topic keep a particular subset of connected peers for that particular topic. This subset of peer connections per topic is also known as “mesh”.\nIn the case of GossipSub, the standard broadcasting mechanism of PubSub is extended with a few sets of enhancements that make it:\nmore efficient than what is commonly called flooding, reducing the protocol’s bandwidth usage\nmore resilient, as the protocol:\n\nshares metadata of seen messages over sporadic Gossip messages (for censorship or Sybil attacks)\nkeeps a local score for each mesh-connected peer to ensure healthy and useful connections, where each peer keeps connections with the highest scoring neighbours\navoids sharing a message with peers that already sent the message to us\n\n\nshares metadata of seen messages over sporadic Gossip messages (for censorship or Sybil attacks)\nkeeps a local score for each mesh-connected peer to ensure healthy and useful connections, where each peer keeps connections with the highest scoring neighbours\navoids sharing a message with peers that already sent the message to us\nThis all looks good on paper. However, there is still no public data on the overhead that broadcasting messages and control data over the network imply on each participating node. Even more importantly, how much room for improvement exists within the protocol and the implementations to make it more optimal.\nExpected Results\nMessage propagation through the GossipSub’s mesh considers some occasional duplicates that can arrive as the message might come from different peers within the mesh:\nGiven:\nn as the number of nodes in the graph\nk as the mesh degree\nl as the number of connections (links) between two nodes l = \\frac{nk}{2}\nThe number of links used to propagate a message to all nodes in the graph can be defined as n-1 ~= n. The links form a spanning tree with the message origin as root (n is big enough compared to the initial sender link, so that it can be considered negligible).\nThe number of links not used to propagate a specific message corresponds to l-n = \\frac{n(k-2)}{2}.\nThis means that on average each node will have 1 link used to receive a message, 1 to propagate it to a peer that doesn’t have it yet. And the rest k-2, to either send or receive the duplicate message.\nAssuming that \\frac{k-2}{2} links are used to send the message to peers that already have it, it means that we receive \\frac{k-2}{2} duplicate messages.\nIn the case of Ethereum, k=8, and therefore, it follows that \\frac{k-2}{2} = 3. So, the expected value is to receive 3 duplicate messages for each message.\nAs previously introduced, this report aims to provide insights on:\nthe number of duplicate messages that we receive per each shared message in the network,\nthe extra bandwidth that we are spending on duplicates,\nany existing unexpected behavior or potential optimization that could be applied on GossipSub.\nNOTES:\nThe numbers presented in the following sections belong to the same 3.5 hours run of Hermes as the previous studies, with the following extra configuration:\nThe experiment is ran on the Holesky network\nOur node was subscribed to the following topics:\n\nbeacon_block\nbeacon_aggregate_and_proof\nsync_commmittee_contribution_and_proof\nattester_slashing\nproposer_slashing\nvoluntary_exit * (check Hermes issue → Broadcasting of invalid `voluntary_exit` messages to mesh peers · Issue #24 · probe-lab/hermes · GitHub)\nbls_to_execution_change\n\n\nbeacon_block\nbeacon_aggregate_and_proof\nsync_commmittee_contribution_and_proof\nattester_slashing\nproposer_slashing\nvoluntary_exit * (check Hermes issue → Broadcasting of invalid `voluntary_exit` messages to mesh peers · Issue #24 · probe-lab/hermes · GitHub)\nbls_to_execution_change\nOverall Number of Messages\nTo give a little bit of context, the report starts by taking a look at the number of messages and the respective duplicates received over time. The following graph shows the number of HANDLED events by the libp2p-host in comparison with the DELIVERED and DUPLICATED ones.\nNOTE: In this report we will consider the DELIVER events as unique identifier of the arrival of a message. This is because the internal event tracer at the libp2p host notifies of the arrival of a unique message at multiple levels, which in turn, makes the HANDLED and DELIVER events at the arrival of a new message the exact same notification, just at different levels of the host.\noverall-number-of-events2000×1200 177 KB\nThe number of unique messages (i.e., HANDLE_MESSAGE) stays steady around the 3,000 and 3,200 unique messages per minute.\nBy looking closer into the messages per topic (not shown here), we observe that the topic with the highest message frequency is the beacon_aggregate_and_proof one, receiving over 90% of the tracked unique messages.\nThere are some duplicated spikes at the beacon_block topic that reach up to 60 duplicates  in some occasions.\nThe number of duplicates seems to vary quite wildly over time, which can be related to the number of connections per mesh (as per the analysis done further up which showed that 3 duplicates per message are expected).\nNumber of Duplicate Messages\nWhen it comes to the actual number of DUPLICATE messages, the following figures show that number of duplicates can oscillate over time.\nduplicates-per-topic1000×600 83.3 KB\nClearly, the beacon_block topic seems to be the only one generating the largest number of spikes at times.\nCDF of Duplicate Messages\nThe following graph shows the Cumulative Distribution Function (CDF) of the duplicates per message per topic. In the graph, we can see that:\nsmaller but more frequent messages like the beacon_ggregate_and_proof and sync_commitee_contributions do have fewer duplicates.\n\nbetween 32% and 45% of the messages do not have any duplicates.\n50% of the messages are received with less than 2 duplicate messages, keeping the mean lower than the theoretical target of 3 duplicates per message.\nthe upper tail shows that less than 10% of the messages get more than 4 duplicates, with a cap at 8-10 duplicates (i.e., the node’s mesh size, D).\n\n\nbetween 32% and 45% of the messages do not have any duplicates.\n50% of the messages are received with less than 2 duplicate messages, keeping the mean lower than the theoretical target of 3 duplicates per message.\nthe upper tail shows that less than 10% of the messages get more than 4 duplicates, with a cap at 8-10 duplicates (i.e., the node’s mesh size, D).\nthe case of the beacon_blocks is completely different.\n\nthere are almost no recorded messages without duplicates (1%-2%).\n54% of the messages report the expected 3  duplicates from the mesh\nTaking look at the tail of the CDF (shown in the dropdown plot further down) there are a few messages that were received up to 34 or 40 times.\n\n\nthere are almost no recorded messages without duplicates (1%-2%).\n54% of the messages report the expected 3  duplicates from the mesh\nTaking look at the tail of the CDF (shown in the dropdown plot further down) there are a few messages that were received up to 34 or 40 times.\nCDF-duplicates1000×600 29.3 KB\nCorrelation between Message Size and Number of Duplicates\nFrom the CDF above there seems to be a pattern of “the bigger the size of the message, the more duplicates it has”. So we went a step further to investigate if there is indeed a correlation. The following graph shows that the correlation between the size of a message and the number of duplicates is somewhat present but is not a norm or at least doesn’t follow any fixed pattern.\nThe figure is complemented by two auxiliary quartile plots or “boxplots”, which represent the given distribution of points of their respective axis, helping us understand that:\nsync_commmittee_contribution_and_proof messages are the smallest ones in size, which also correlates with the smallest ratio of duplicate messages.\nbeacon_aggregate_and_proof messages are the second ones in size, having also a bigger tail of duplicates on the Y concentration plot.\nbeacon_block messages, despite being the ones with the widest variation in size, do not follow any particular pattern that could correlate the message size with the number of duplicates.\nmsg-size-number-of-duplicates595×582 48.6 KB\nAs such, we conclude that there is no correlation between message size and number of duplicates.\nArrival Time of Duplicates\nReducing the number of duplicates has already been a topic of discussion in the community. There are already some proposals like gossipsub1.2  that spotted this large number of duplicated messages previously, proposing the addition of a new control IDONTWANT message that could not only notify other peers that we already got a message, but also cancel the IWANT ongoing messages.\nIn order to see how effective the IDONTWANT control message would be, we’ve computed the time between the first delivery of each message and their respective first duplicate. This is done to validate that there is enough time to send the IDONTWANT message once a new message is received (prior to the message validation) and before the duplicate starts being sent over.\nThe following graph gives the time between the delivery time of a message and the time to the first duplicated message in seconds.\narrival-cdf1000×600 48.4 KB\nResults show that 50% of the duplicated beacon blocks arrive within 73 milliseconds, roughly an entire Round Trip Time (RTT) with a well connected peer. In practice, this means that the IDONTWANT message could prevent at least the other 50% of messages that arrive between 73 milliseconds and 2 seconds of the first arrival.\nWe’ve spotted that a big part of the duplicated messages arrive from IWANT messages that we sent milliseconds before the arrival of the same message though the mesh.\nThe gossipsub1.2 proposal already contemplates this scenario, where the same IDONTWANT message could break or stop any ongoing responses to IWANT messages for that msgID.\nIn summary, we conclude that the IDONTWANT control message addition to Gossipsub will be a valuable enhancement that can indeed prevent the vast majority of duplicate messages.\nThis set of conclusions have been extracted from running the go-libp2p  implementation and, although it also involves the traces of how other implementations interact with Hermes, it might be a biased conclusion from the point of view of the Go implementation.\nWe have identified that there is no limit on the number of peers that we simultaneously send IWANT messages to for the same msgID.\nWe identify that this has some benefits:\nConcurrently fetches the message from multiple actors.\nBypasses bandwidth limitations of peer(s) we have sent IWANT messages to, since we have forwarded the IWANT message to multiple peers.\nHowever, it also has obvious downsides:\n\nWe receive multiple duplicates from the peers that respond to our simultaneous IWANT request, consuming more bandwidth on both ends.\n\n\nThe message could be already on the wire through the mesh connections, so when the IWANT message responses arrive, the message was already delivered through the mesh.\n\n\nThere is no track of who we contacted for a given message, given that Gossipsub is:\n\nforwarding the message only the first time we see it, and\nremoving the peer that sent us the message from the list of peers we’re broadcasting the message to and forgetting about that peer.\n\nThis makes the entire broadcasting process unaware of who sent us that message in IHAVEs, or who we are already contacting for a particular message - resulting in multiple duplicates.\n\nforwarding the message only the first time we see it, and\nremoving the peer that sent us the message from the list of peers we’re broadcasting the message to and forgetting about that peer.\nWe receive multiple duplicates from the peers that respond to our simultaneous IWANT request, consuming more bandwidth on both ends.\nThe message could be already on the wire through the mesh connections, so when the IWANT message responses arrive, the message was already delivered through the mesh.\nThere is no track of who we contacted for a given message, given that Gossipsub is:\nforwarding the message only the first time we see it, and\nremoving the peer that sent us the message from the list of peers we’re broadcasting the message to and forgetting about that peer.\nThis makes the entire broadcasting process unaware of who sent us that message in IHAVEs, or who we are already contacting for a particular message - resulting in multiple duplicates.\nCanceling ongoing IWANTmessages with IDONTWANT messages, which is a proposal included in gossipsub1.2 is a valuable enhancement that will limit the number of duplicates.\nRecommendation 1\nWe propose having a limiting factor (somewhat similar to kademlia’s alpha parameter), which would limit the number of concurrent IWANT messages we send for the same msgID.\nThe gossiping mechanism of Gossipsub acts as a backup mechanism to the broadcasting/mesh propagation part of the protocol for those messages that didn’t manage to reach all nodes in the network. The more frequent gossiping is, the higher its contribution becomes to message propagation (i.e., more messages are being requested through IWANT requests because they have not reached the entirety of the network).\nAn edge case that results from very frequent gossiping (i.e., small heartbeat interval) is that messages that are already in transit, but have not been downloaded completely, are being requested through an IWANT message. This inevitably results in a duplicate message once both messages arrive at their destination.\nIt is hard to quantify how often the message responses to IWANT messages are indeed future duplicates, but it is still worth pointing out that high heartbeat frequency increases the chances of those edge cases.\nRecommendation 2\nA quick and straightforward optimization is to lower the current heartbeat frequency (i.e., increasing the heartbeat interval) from 0.7 seconds to 1 second (as per the original protocol spec and recommendation). This would reduce the excessive IHAVE messages and reduce the chances of generating extra duplicates.\nWe have spotted some edge cases that may occur due to the “lack” of control over the triggered events at GossipSub (IHAVE/ IWANT).\nIt isn’t easy to judge from the logs whether those cases are just a matter of timing, as GossipSub replies to those events as interruptions (at least in the Go implementation), or if some of those cases are caused by a bug in one of the implementations.\nWe found that the number of messages where we received multiple duplicates from the same peer to just 1% of the total number of beacon_blocks received. We, therefore, conclude that this is not critical or an issue that requires further investigation.\nFor more details and weekly network health reports on Ethereum’s discv5 DHT network head over to probelab.io.\n",
        "category": [
            "Networking"
        ],
        "discourse": [
            "p2p"
        ]
    },
    {
        "title": "Exploring Sophisticated Execution Proposers for Ethereum",
        "link": "https://ethresear.ch/t/exploring-sophisticated-execution-proposers-for-ethereum/21386",
        "article": "This post contains ideas many Ethereum researchers have discussed; I transcribe them here. Thanks to Barnabé Monnot, Caspar Schwarz-Schilling, Thomas Thiery, Tim Beiko, Mike Neuder, and Justin Drake for feedback and review.\nEthereum has been designed to have a decentralized validator set. The set’s decentralization is crucial for validators’ tasks. Validators currently have the following roles:\nAttester: validators are asked to attest to consensus information like whether a block is valid and timely. The decentralization of attesters ensures that Ethereum is resilient against correlated failures, whether accidental (e.g., due to bugs taking offline a particular share of the validator set) or malicious (e.g., a share of the validator set producing a safety fault).\nBeacon block proposer: validators are asked to propose beacon blocks that contain consensus information, such as attestations. The decentralization of beacon block proposers ensures that this consensus information is (eventually) registered on-chain.\nExecution block* proposer: validators are asked to propose execution blocks that contain user transactions. A decentralized set of execution block proposers fosters resilience against cartels looking to extract rents by, e.g., censoring transactions or extracting multi-slot MEV.\nThe ecosystem may ask validators to fulfill a different set of duties in the future. In particular, Fork-Choice Enforced Inclusion Lists (FOCIL), which is proposed to be implemented in Ethereum via EIP-7805, adds a new duty to validators:\nInclusion List proposer: validators are asked to propose inclusion lists, a set of pending transactions the validator has observed. The decentralization of inclusion list proposers is crucial because it ensures that all pending transactions will be included on-chain quickly, regardless of their contents.\nThis post investigates whether it is possible to unbundle the role of execution block proposer from the other validator duties and create a new, specialized class of service providers that fulfills the role of execution proposer. Some advantages may be that Ethereum could isolate unsophisticated proposers from MEV and have higher expectations in terms of sophistication and hardware of this new class of service providers, allowing for a more performant Ethereum network. This note aims to spark a community discussion around if, when, and how Ethereum could practically separate the role of execution proposer from other validator duties. What does the ecosystem need before this separation is possible, if at all? What does the ecosystem expect from execution proposers? How does the ecosystem reason about setting expectations for protocol participants?\nFigure 1: A map of validator services as presented by Caspar, Ansgar, Francesco and Barnabé at CCE’24.865×648 206 KB\nFigure 1: A map of validator services as presented by Caspar, Ansgar, Francesco and Barnabé at CCE’24.\n* Note that we use the term execution block here and not execution payload. Currently the execution payload lives within the beacon block and is thus not a separate bock. In future designs, like Attester-Proposer Separation and ePBS EIP-7732, the execution payload is separated from the beacon block into its own execution payload. Functionally, the role of proposing an execution block and an execution payload is practically identical.\nUnbundling Design Philosophy\nThe idea of unbundling specific roles from the set of duties validators are currently tasked with is not new. In his Endgame post, Vitalik wrote about potential second tiers of stake that validate blocks, check the availability of blocks, and/or add transactions to prevent censorship. He concluded as follows:\nWhat do we get after all of this is done? We get a chain where block production is still centralized, but block validation is trustless and highly decentralized, and specialized anti-censorship magic prevents the block producers from censoring.\nThe first paragraph of Vitalik’s recent possible futures of Ethereum: The Scourge post can be interpreted as why unbundling may be necessary.\nOne of the biggest risks to the Ethereum L1 is proof-of-stake centralizing due to economic pressures. If there are economies-of-scale in participating in core proof of stake mechanisms, this would naturally lead to large stakers dominating, and small stakers dropping out to join large pools. This leads to higher risk of 51% attacks, transaction censorship, and other crises. In addition to the centralization risk, there are also risks of value extraction: a small group capturing value that would otherwise go to Ethereum’s users.\nThe primary reason for unbundling designs like Proposer-Builder Separation (PBS) and Attester-Proposer Separation (APS) has been to preserve the decentralization of the participants in the core Proof of Stake mechanism such that they can hold more centralized participants accountable. By separating the roles that benefit from economies of scale from those that do not, Ethereum can preserve the decentralization amongst the participants of the roles where there are no economies of scale.\nThe separation of concerns designs, like PBS and APS, have been criticized for leading to centralization amongst the roles where economies of scale benefit participants, i.e., the builder in PBS and the execution proposer in APS. While these works clearly show that the participants of the complicated services will be sophisticated and centralized, these works do not claim causality. The design philosophy behind these ideas is that it is inevitable that the service providers of the complicated service will be centralized. However, it is possible to preserve the decentralization of the participants of more straightforward services by unbundling these two services. More research is necessary to investigate what causes centralization. Either of the following or a combination of the two could be the cause of centralization:\nThe market mechanism that allocates the rights to fulfill a particular service amongst service providers. An example of such a market mechanism is Execution Tickets.\nThe inherent complexity of fulfilling the service. For example, building competitive blocks is known to be computationally complex and has high barriers to entry.\nA secondary reason for unbundling roles is that Ethereum can then make different assumptions about sets of participants in terms of sophistication and available hardware. Ethereum must curate this set to fulfill all roles if one service provider fulfills multiple roles. For example, Ethereum wants a decentralized attester set to ensure censorship resistance; hence, the hardware requirements for running an attester are very low. However, perhaps decentralization is not as necessary for block production if there are other censorship resistance tools. Ethereum may want to increase its hardware expectations of execution proposers once inclusion lists are deployed on mainnet.\nIncreasing bandwidth and hardware expectations for certain providers of specific services could benefit the protocol. Currently, unsophisticated proposers are a bottleneck for protocol development; more sophisticated proposers could make some protocol upgrades significantly easier. Increasing hardware expectations of proposers means they can build larger blocks and disseminate more data over the network while keeping the verification load for other nodes semi-constant.\nUnbundling roles has been explored from first principles before. Barnabé proposed the concept of rainbow staking. Rainbow staking is a conceptual framework that allows different service providers to participate in the services at which the service provider excels and shows that operator-delegator structures may appear across all these services. Notably, with rainbow staking, the protocol would no longer expect all validators to perform all tasks. Although this post is rooted in the rainbow staking conceptual framework, the goal is to practically investigate if and when either or both the execution and beacon proposer roles can be split from the other duties validators currently have.\nrainbow staking1380×912 60.3 KB\nFigure 2: A map of the unbundled protocol. Taken from the rainbow staking post.\nThis post not only builds on rainbow staking but also on an extended line of work that investigates the advantages of more sophisticated proposers. Vitalik’s Endgame post shows a more centralized block production pipeline with decentralized validation and inclusion mechanisms. Justin introduced Attester-Proposer Separation (APS), a design family of splitting execution proposers from other validator duties, which would allow for sophisticated proposers to help significantly scale Ethereum. Moreover, Mike and Justin investigated a potential implementation of APS, Execution Tickets, in which the protocol sells execution proposal rights directly to service providers. This post takes some of those ideas, combined with ongoing discussions amongst Ethereum researchers, and intends to make them legible to the community.\nThe Functional Role of Execution Proposer\nTo understand whether the role of execution proposer can be unbundled from the current validators and given to a different set of service providers, it is essential to understand what it means to be an execution proposer, what the protocol expects from execution proposers (the protocol’s desiderata), and which set of participants could be best an execution proposer.\nIn the following, we see the execution proposer and builder role as the same entity and refer to it as the execution proposer. The two roles could be split. For example, the execution proposer is in charge of proposer commitments, and the builder is in charge of ordering and inserting transactions. However, we do not do so here since there are significant synergies between the two roles. For example, an execution proposer (without a builder) could better make the proposer commit when to release its block (play timing games) if it also controls the size of the block, i.e., if the execution proposer is also the builder.\nCompetitive Execution Proposers\nThe execution proposer is expected to deliver a valid execution payload, that is an ordered list of transactions, to the Ethereum network at a specified time. To do so, at a minimum, the execution proposer must have access to transactions and the ability to pack them in a block and propagate it.\nTo be a competitive execution proposer, however, it must have access to exclusive order flow, amongst other vectors of competition such as sophisticated algorithms. This means execution proposers must acquire exclusive order flow, although projects like BuilderNet aim to create a platform on which builders can benefit from sharing order flow. Next to order flow, the execution proposer must have a fairly sophisticated block packing algorithm and minimize downtime. In practice, it is widely accepted that execution proposers (better known as builders today) are a centralized and small set of sophisticated agents with performant hardware.\nSome readers may wonder why execution proposers must be competitive or why they must compete on the dimension of MEV extraction. While it is not a design goal to have execution proposers that can maximize the extracted MEV, in my opinion, it is inevitable. Ethereum issues the right to be an execution proposer. In a permissionless market without friction, this right will be allocated to the party that values it the most, regardless of the market structure the protocol imposes.\nThe protocol can impose frictions. For example, it could allocate these rights to validators who may have some intrinsic preferences on who should produce the block. This may resemble the situation Ethereum is in today, with some validators preferring to build blocks themselves even if there is a substantial opportunity cost in doing so. Some other frictions Ethereum could rely on is that an initial allocation of rights amongst the set of execution proposers is likely to be somewhat persistent because trade amongst sophisticated execution proposers goes hand-in-hand with frictions like setting strategic reserve prices and the risk of adverse selection.\nWhat does Ethereum want from Execution Blocks and Execution Proposers?\nEthereum may want execution proposers to propose blocks that fulfill specific criteria. Some criteria of blocks may be:\nNo Censorship. One of Ethereum’s core values is censorship resistance, so each block should be censorship-free.\nNo Market Manipulation. Execution proposers have full control over the ordering of transactions in a block. This allows them to extract MEV. While the community accepts MEV extraction, there may be more sophisticated manipulations that the community sees as *breaking the fence.*\nAlthough execution proposers influence the outcome of the above criteria in the current protocol, these are criteria of blocks and not necessarily of those that make the blocks. Separately, the community may have some requirements for execution proposers themselves as well:\nNo Downtime. Execution proposers should always be available to propose valid blocks. Otherwise, slots will be missed, leading to longer transaction inclusion times.\nPerformant Machine. To have blocks with a lot of transactions (and blobs), the community may want an execution proposer that has a performant machine.\nEfficiency. Execution proposers should efficiently pack blocks to maximize the transactions included while minimizing the network resources consumed. Today, this may mean facilitating Transaction Access Lists. In the future, this may mean brokering prices for transactions.\nCensorship by builders has been a problem in Ethereum. A few large entities dominate the builder market, and since they currently control which transactions are included in a block, they can censor at will. Although the community prefers execution proposers that do not censor, it is clear that the community should not rely on execution proposers for censorship resistance. Execution proposal rights are inherently centralizing; hence, it is unreasonable to expect Ethereum’s censorship resistance from the small set of execution proposers.\nFOCIL provides a way for validators who are not the execution proposer in a slot to contribute to censorship resistance. In each slot, 16 validators each create a list of transactions that must be included in the block. If FOCIL is implemented, Ethereum will not need to rely on execution proposers for censorship resistance anymore; perhaps it is not a requirement that the set of execution proposers contributes to censorship resistance.\nMultiple concurrent block producers is a different line of research aiming to increase applications’ economic efficiency. One way it may do so is because it could prevent block producers from censoring for economic reasons. This direction, however, is largely orthogonal to whether the execution proposer role should be split from the other validator duties. Regardless of the number of execution proposers per slot, a larger, more decentralized set of participants could complement censorship resistance. Multiple concurrent block producers could be better suited to give real-time censorship resistance for certain economic use cases. In contrast, a larger set of participants could provide robust eventual censorship resistance for all transactions.\nSince the execution-proposing rights market will likely be concentrated, I believe the community should not rely on execution proposers to prevent complicated problems like multi-slot MEV. The protocol should be designed to uphold what the community considers fence-breaking without relying on execution proposers. However, before the protocol has built-in ways to prevent manipulation, curating an execution proposer set that does not break these fences may be worthwhile. Therefore, I think having built-in ways to avoid manipulations, like multi-slot MEV, is essential before moving to more sophisticated execution proposers.\nThe execution proposer requirements of No Downtime and Performant Machine can be imposed by using, for example, missed slot penalties and by requiring execution proposers to do a lot of computation, respectively. While there are clear advantages to these two requirements, it is difficult to reason about what exactly to expect from execution proposers. I.e., do we expect execution proposers to have 10x the hardware requirements validators currently have or 100x? What orders of magnitude must we consider here, and what are the trade-offs?\nThe execution proposer requirement of Efficiency can be gained by designing robust transaction fee mechanisms that allow execution proposers to compete on efficiency. Currently, execution proposers compete on extracted value. As transaction fee mechanisms become more expressive and the amount of MEV as a share of total value declines, execution proposers may be forced to compete on packing efficiently. Efficiency, which is how much output is obtained from fixed inputs, is orthogonal to how performant machines, the inputs, are. So efficiency is out-of-scope of this post.\nThere may be two ways to find out what the minimum hardware requirements should be for execution proposers:\nTarget a certain amount of execution proposers to obtain a more competitive market. By increasing the hardware requirements, the market may become less competitive. Since builders already have performant hardware, relatively high requirements would not impact market competition. However, asking a lot may increase barriers to entry.\nDecide what the community expects from execution proposers and determine the hardware requirements. For example, if the community expects execution proposers to have 128 blobs available, the bandwidth requirement should be set accordingly.\nReasoning about the requirements for execution proposers is complex and is also a second step. First, the community must decide whether it is acceptable to endow a different set of service providers with the execution proposing rights.\nSelecting Execution Proposers\nIf the community decides that not all validators should be expected to be execution proposers, the next question is how Ethereum should select its execution proposers. In the following, we discuss two potential ways to do so. Thanks to Francesco for mentioning the first method.\nOverloading MEV-Boost\nAfter FOCIL is implemented and Ethereum no longer relies on execution proposers for censorship resistance, it could quickly move to sophisticated execution proposers by simply increasing the expectations in terms of bandwidth and computing for execution proposers. This has the following consequences:\nAny validator that is still able to build blocks themselves can continue doing so.\nAny validator that does not is expected to outsource its block construction duties to a sophisticated execution proposer, for example, via MEV-Boost.\nThe advantage of this method is that it does not require the implementation of an execution proposer selection mechanism in the core protocol. Yet, the community can still enjoy the advantages of more bandwidth and computing.\nThe disadvantage is that low-performance validators must now trust relays or builders to pay validators for the right to construct their execution payload. This trust assumption is likely weak since a builder would destroy its valuable reputation if it did not pay a single validator when it should. Therefore, a validator can trust it receives its payment, not based on a trustless fair exchange but because a builder stakes its reputation.\nFinally, this method of selecting execution proposers does not address potential multi-slot MEV issues. Although no multi-slot MEV extraction is currently observed, it is possible. Validators know at least one epoch in advance when they must propose a block. Therefore, a coalition of validators could extract multi-slot MEV if they sell their slots simultaneously before the just-in-time MEV-Boost auction. MEV-Boost does not prevent such a deviation; hence, multi-slot MEV is possible.\nAttester-Proposer Separation\nAPS is a line of research that aims to find an execution proposer leader election mechanism from first principles. APS would be an in-protocol change that separates the execution proposer duties from the other duties validators have.\nAlthough there have been many proposed implementations of APS, there is very little consensus amongst ecosystem participants about what implementation is desirable. There is clearly a lot more work to do, such as formalizing the APS desiderata, understanding the mechanism design space, and comparing different proposed implementations.\nAn advantage of APS is that it could deal with potential multi-slot MEV issues from a first principles approach. This will improve the status quo, which primarily relies on beacon proposers being unsophisticated or large validators not wanting to get involved with such behavior.\nA disadvantage of APS is that it is still in the research phase and may have a long way to go before the community can agree on an implementation. Even when there is a proposed implementation, it would need to be implemented via a hard fork, which also takes more time.\nConclusion\nThis post investigates whether Ethereum should unbundle the role of execution proposer from the other validator duties. The post takes a practical and short-run stance, as opposed to more first principles work that has highlighted the same concept before. We analyze the functional role of an execution proposer to find out what it currently takes to be an execution proposer and what Ethereum may expect from execution proposers. Finally, we briefly touch on two ways to implement sophisticated execution proposers in protocol.\nThe goal of this post is to spark a community discussion about whether community members would be comfortable with more sophisticated execution proposers and, if so, what may be expected from them.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Preparing withdrawals for compromised validator or withdrawal keys. FAO Zilm",
        "link": "https://ethresear.ch/t/preparing-withdrawals-for-compromised-validator-or-withdrawal-keys-fao-zilm/10453",
        "article": "Dear Zilm & ETH Devs,\nApologies in advance if my post is deemed inappropriate for https://ethresear.ch the purpose of my post is to help find a solution for all ETH 2 nodes which may have had their validator or withdrawal keys breached.\nIt is pointless getting into the details of how it happened to me, what I think is important is trying to find a solution. I will make this attempt to share my thoughts in a manner which hopefully doesn’t get me immediately banned.\nThe purpose of a system breach may not only be for financial purposes it may be to undermine the network and the Ethereum Foundation, so being extra prudent for when withdrawals are enabled is what I would like to discuss.\nFor a large number of nodes there will be two operators, the host (server side) who holds the validator key and the Staker (client side) who holds the withdrawal key. Below I will write a list of steps which would mean the two parties would need to work together to authorise each other to request a withdrawal, thus making it much harder for a hacker/bad-actor to use a compromised withdrawal key.\nNote: This proposal is a little labor intensive and may not be for everyone, it could be optional but for extra security and peace of mind it could be worth the effort and may be worth the client paying the host a fee for example $50-$100 for each withdrawal.\nMaybe not popular, but with such large sums of money involved Host’s should use KYC type security protocols.\nNot everyone will want to withdraw funds when we reach phase 1.5 so could there be an option to “lock/unlock” the node withdrawal service, by default all nodes should be set to “locked” this way withdrawals will be impossible until the Staker chooses he/she/they is ready for a withdrawal.\nSimilar to a 3 way handshake a withdrawal could have multiple steps to protect the misuse of a key using the (server side) node key and the (client side) withdrawal key/request.\nAfter a users ID has been fully verified by the host the Staker makes a request to the host for a withdrawal/skim.\nUsing the node validator Key the host (server side) would need to send a request to the network for a withdrawal/skim.\nEach (server side) withdrawal request will trigger a unique forthcoming Epoch or Slot number to be sent to the node/host, for the purpose of this discussion we can think of an Epoch or Slot number as a type of PIN number similar to that used with a credit card (CC).\nThe PIN number could be sent to the client after full ID verification has taken place, the PIN could be sent using email, SMS, phone call or maybe even sent in the post similar to a CC PIN number?\nThe client now has their withdrawal key and a PIN, by rights the hacker only has the withdrawal key (we hope) rendering it useless without the PIN.\nThis PIN must be unique for each withdrawal request and can only be used once.\nThe client starts the withdrawal request using the PIN, withdrawal key, amount to withdraw and a wallet address to send the funds.\nThe withdrawal request will trigger an unlock request with the host (server side) and the host can confirm the withdrawal request was made by the client before unlocking the node.\nClient verifies the wallet withdrawal address and confirms the withdrawal request was initiated by the client and the node is unlocked by the host.\nThe client will have only one chance with each request, should the PIN be input incorrectly the node would stay locked the host informed of a failed withdrawal request and if it was the client who made a mistake the withdrawal process would have to be started again. This with luck should make it very hard for a validator or withdrawal Key to be misused.\nClient waits for Epoch or Slot number to be processed and with luck at this point in time a successful withdrawal/skim has taken place.\nNode goes back to default state of being “locked”.\nIt may seem laborious but by the time we get to phase 1.5 each node could have a value well in excess of $250,000 so in my view it is worth making the withdrawal process as secure as possible, which in effect renders the key useless without a PIN.\nOf course this is just a rough idea and I am by no means an expert in blockchain, however I hope there are some steps that could be helpful to anyone like myself who sadly had an ETH 2 withdrawal key breached, a similar process could also be used for withdrawal key rotation, should a Staker wish to change the withdrawal key!\nThanks kindly for reading my post, I am ever so grateful for all your efforts and time.\nTobes\n",
        "category": [
            "The Merge"
        ],
        "discourse": []
    },
    {
        "title": "Understanding Based Rollups: PGA Challenges, Total Anarchy, and Potential Solutions",
        "link": "https://ethresear.ch/t/understanding-based-rollups-pga-challenges-total-anarchy-and-potential-solutions/21320",
        "article": "Many thanks to @linoscope, @donnoh, @Brecht, @sui414, @pascalst,and @cshg for their valuable feedback and comments.\nHow to Lose $200K every Two Weeks\ntl;dr In this post, we analyze the economics of based rollups using total anarchy as a method of sequencing blocks. Focusing on the only live based rollup, Taiko, we highlight the inefficiencies of total anarchy. Specifically, we identify a critical inefficiency in L2 block building that resembles a priority gas auction (PGA), where competing proposers rush to include transactions before Taiko Labs’ proposer. This results in L2 blocks with redundant transactions being posted on-chain on L1, reducing the value of Taiko’s blocks and increasing its economic costs. As a result, Taiko Labs often incurs expenses to prove blocks with few or no profitable transactions.\nThrough a two-week analysis of block data, we observe that the market is dominated by four major proposers (including Taiko Labs). Our findings indicate that Taiko Labs faces significant losses due to consistently losing the PGA. Over this period, Taiko Labs lost approximately 83.9 ETH, which, at an average Ethereum price of $3,112, translates to a total loss of roughly $261,096 in just two weeks. This underscores the urgent need for better proposer incentives and mechanisms to mitigate these inefficiencies.\nIntroduction\nBased rollups aim to enhance Ethereum scalability by integrating L2 operations with L1 for improved data availability and security. They leverage L1 for sequencing and settlement, avoiding the need for centralized sequencers, which promotes decentralization.\nHowever, the total anarchy model used for sequencing in Taiko introduces significant inefficiencies. In this model, where block posting lacks hierarchy or coordination, any user can act as an L2 proposer and post blocks without restriction, promoting maximum permissionless participation. While this approach aligns with decentralization principles, it also introduces systemic challenges.\nVitalik described total anarchy as:\n“Total anarchy: anyone can submit a batch at any time. This is the simplest approach, but it has some important drawbacks. Particularly, there is a risk that multiple participants will generate and attempt to submit batches in parallel, and only one of those batches can be successfully included. This leads to a large amount of wasted effort in generating proofs and/or wasted gas in publishing batches to chain.”\nThese drawbacks materialize in Taiko, where multiple L2 blocks are submitted for the same L1 slot, resulting in redundant transactions. Redundant blocks consume valuable L1 space, inflate fees, and diminish economic efficiency.\nInefficiency Caused by Redundant Transactions\nIn rollups using total anarchy, redundant transactions occur when multiple L2 blocks containing the same transactions are published to L1. These blocks may be submitted within the same L1 slot or across different slots. In such cases, both blocks are submitted to L1, consuming valuable blob space and incurring L1 fees for the L2 proposer. The first block processed on L1 is executed to determine the updated L2 state. Any redundant transactions in the second block, already included in the first, are invalidated, as their state transitions have already been applied. Unique transactions in the second block remain valid and still affect the L2 state.\nThe proposer of the second block faces significant economic inefficiencies. They incur the full cost of posting and proving the block but only earn rewards for valid, non-redundant transactions. This dynamic discourages proposers from submitting redundant blocks. Additionally, posting two blocks to the same L1 slot reduces the effective throughput of the network by occupying valuable block space with redundant data blobs, increasing congestion and costs.\nTaiko’s Architecture and the Economics of redundant Blocks\nTaiko exemplifies a based rollup using total anarchy as its sequencing design, prioritizing simplicity and decentralization. In this model, anyone can collect transactions from the L2 mempool, build a bundle (which becomes the L2 block when proposed by the L1 proposer), and submit it to L1 alongside data blobs containing transaction payloads. These blocks may include transactions or remain empty (containing only a single anchor transaction) to ensure chain continuity during low-demand periods. After block submission, proposers must generate and post a validity proof to confirm the block’s correctness, which incurs additional L1 transaction costs.\nSimplified Overview of Taiko Architecture799×669 38.2 KB\nEven empty blocks must be proven to maintain the chain’s liveness and avoid slashing penalties. This requirement places a significant economic burden on fallback proposers like Taiko Labs during periods of low activity. When Taiko Labs includes profitable transactions, higher-bidding competitors often outpace it in the PGA environment, resulting in diminished rewards and economic challenges.\nPriority Gas Auction Dynamics in Taiko\nPGAs presents a recurring challenge in Taiko Labs’ operation. Competing searchers exploit Taiko Labs’ open block submission process by outbidding its proposer, using higher fees to ensure their block is executed first. Driven by economic incentives, these proposers monitor pending blocks and submit their own for the same L1 slot, offering higher transaction fees to secure inclusion.\nWhen multiple blocks overlap in content, the first valid block determines the network’s state. Redundant transactions between Taiko Labs’ block and an earlier block are excluded, forcing Taiko Labs to bear the cost of proposing and proving blocks without proportional rewards. This creates a situation where Taiko Labs incurs the full cost of sequencing blocks but receives minimal or no profit, further straining the network’s economic sustainability.\nThese inefficiencies are especially pronounced during high-demand periods, when the PGA environment is most competitive. However, during low-demand periods, the Taiko Labs proposer is forced to maintain liveness by posting and proving blocks that may contain some transactions but are not full. While these blocks may offer some rewards, in most cases they cannot cover the L1 costs, making them unprofitable. As a result, PGAs not only redirect rewards to more sophisticated proposers but also undermine the incentives necessary to maintain the network’s liveness, placing a disproportionate economic burden on fallback proposers like Taiko Labs.\nMethodology\nFor this analysis, we evaluate proposer profitability by comparing their earnings with the costs incurred. The block rewards from L2 blocks represent the earnings, while the L1 publication costs and proving costs are considered the losses. For Taiko Labs proposer, the base fee associated with each block is included in its earnings.\nTaiko Proposer Net Profit:\n\\text{(L2 Priority Fees + Base Fee)−(L1 Publication Costs + Proving Costs)}\nOther Proposers Net Profit:\n\\text{L2 Priority Fees − (L1 Publication Costs + Proving Costs)}\nThe analysis is based on blocks created between November 7, 2024, and November 22, 2024, covering Block IDs 538304 to 593793. This represents 9.34% of all blocks on the Taiko chain since genesis at the time of writing. This dataset provides insights into the economic performance of proposers who processed more than 500 blocks during this period.\nAnalysis of Proposer Rewards, Costs, and Profitability\nThe graph below presents an overview of rewards, costs, and profits for major proposers, highlighting the economic dynamics within the system. Taiko Labs, as the primary fallback proposer, is used as the baseline for evaluating profitability.\nProposer Profit Breakdown5952×2927 321 KB\nTaiko Labs Proposer Outbid by Other Proposers\nIn this section, we analyzed instances where the two top-earning proposers outbid Taiko Labs. This happens when a proposer submits a block faster than Taiko Labs and secures its execution on L1 first.\nTaiko Labs' outbid by Proposer A1920×1055 138 KB\nThis graph illustrates each instance where Proposer A outpaced the Taiko proposer in posting a block.\nY-Axis: Represents the reward associated with each block (sum of L2 transaction fees).\nX-Axis: Represents the size of the posted block.\nTimeframe: Over the two-week period analyzed, this occurred 4,621 times.\nIn our analysis, we examined instances where blocks proposed by Proposer A were immediately followed by blocks proposed by Taiko Labs’ proposer.\nProfitability Comparison:\nProposer A:\n\nIn blue on the graph, we observe all 4,285 profitable blocks proposed (92.7% profitability), while the not profitable blocks are shown in ligthblue.\n\n\nIn blue on the graph, we observe all 4,285 profitable blocks proposed (92.7% profitability), while the not profitable blocks are shown in ligthblue.\nTaiko Proposer:\n\nIn red, the graph shows that the Taiko Labs proposer achieved only 103 profitable blocks (2.2% profitability with 4,518 blocks resulting in 97.8% of blocks being unprofitable), with the not profitable blocks shown in pink.\n\n\nIn red, the graph shows that the Taiko Labs proposer achieved only 103 profitable blocks (2.2% profitability with 4,518 blocks resulting in 97.8% of blocks being unprofitable), with the not profitable blocks shown in pink.\nEconomic Impact on Taiko:\n\nThe total loss incurred by the Taiko Labs proposer, as a result of consistently being outbid by Proposer A, amounted to 18.37 ETH.\n\n\nThe total loss incurred by the Taiko Labs proposer, as a result of consistently being outbid by Proposer A, amounted to 18.37 ETH.\nTaiko Labs' outbid by Proposer B1920×1064 139 KB\nSimilarly, we analyzed cases where blocks proposed by Proposer B preceded those proposed by the Taiko Labs proposer. This occurred 4,870 times during the observation period.\nProfitability Comparison:\nProposer B:\n\nProposed 4,333 profitable blocks (89.0% profitability) in blue, with not profitable blocks shown in lightblue.\n\n\nProposed 4,333 profitable blocks (89.0% profitability) in blue, with not profitable blocks shown in lightblue.\nTaiko Proposer:\n\nAchieved 132 profitable blocks (2.7% profitability with 4,738 blocks resulting in 97.3% of blocks being unprofitable) in red, with not profitable blocks in pink.\n\n\nAchieved 132 profitable blocks (2.7% profitability with 4,738 blocks resulting in 97.3% of blocks being unprofitable) in red, with not profitable blocks in pink.\nEconomic Impact on Taiko:\n\nThe total loss incurred by the Taiko Labs proposer in these cases was 18.25 ETH.\n\n\nThe total loss incurred by the Taiko Labs proposer in these cases was 18.25 ETH.\nTransaction Distribution Analysis\nTo further investigate proposer behavior, we analyzed the distribution of transactions per block using a Kernel Density Estimation (KDE) graph. This visualizes how proposers allocate transactions across blocks, highlighting differences in their strategies.\nTransactions Count KDE1920×1020 103 KB\nAnalysis of Block Profitability by Major Proposers\nWe continue our analysis by evaluating the number of profitable blocks proposed by each proposer and examining the distribution of these results in Taiko.\nProposers profitability (1)4453×2317 266 KB\nThis graph illustrates the profitability of blocks published by major proposers (processing more than 500 blocks) during the analyzed period. It categorizes blocks into two groups: profitable blocks (green) and unprofitable blocks (red), highlighting the proportion of each for individual proposers.\nInsights\nThis analysis reveals how competing proposers, driven by their own economic interests, create challenges for Taiko Labs. A critical issue arises when Taiko Labs posts blocks on L1 with low-priority fees, enabling more sophisticated actors to outbid them in the PGA environment. Our findings indicate that over 80% of Taiko Labs’ posted blocks were unprofitable, and being outbid occurred in more than half of the blocks proposed by Taiko. This highlights the economic inefficiencies Taiko Labs faces as it strives to maintain network liveness in an environment where competing proposers exploit its fallback role.\nPossible Solutions\nUsing total anarchy for sequencing requires guarantees of execution to prevent redundant transactions. This approach can be challenging because, from the L1 perspective, transactions are executing correctly.\nOne potential solution is to add the L2 block ID field in the L2 block proposal function, causing the block proposal to revert if the target is missed due to competition from other proposers. While this still incurs a transaction cost for proposing, it avoids the expense of proving the block. Taiko Labs could potentially use revert protection to prevent conflicting blocks from getting on-chain. By doing this, they could avoid wasting transaction fees. However, it’s worth noting that revert protection introduces a trust assumption on the builder. Another problem might be when you have blocks with the same ID that don’t have redundant transactions.\nAnother possible solution is execution preconfirmations. However, ensuring execution guarantees on the L2 side adds complexity to the preconfirmation process. Having a single preconfer can provide guarantees that they will not publish conflicting blocks for the same slot, as doing so could result in slashing penalties. This mechanism can significantly reduce redundant submissions and lower L1 fee wastage. However, it also introduces execution complexity, posing challenges that must be addressed to ensure efficient implementation.\nThe solution that might be the easiest to implement involves the use of execution tickets. Execution Tickets, or others leader election mechanisms like based preconfirmation, provide a deterministic system to elect a single block proposer per slot. This approach minimizes conflicts and redundancy by ensuring that only one proposer is responsible for block submission at any given time.\nExecution tickets have several advantages. By eliminating redundant block submissions, they reduce wasted resources and align proposer incentives with the system’s overall efficiency. However, implementing such a system introduces challenges to ensure fair and reliable leader election.\nDiscussion and Conclusion\nWhile total anarchy encourages permissionless participation, it struggles to meet the efficiency demands of based rollups due to redundant blocks and the competitive PGA environment. Taiko serves as a compelling case study, illustrating the economic costs associated with inefficient block space utilization on L1.\nPotential solutions such as execution preconfirmations could address these inefficiencies but add system complexity. Alternatively, introducing a leader election mechanism could reduce redundant blocks by adding structure, though it might also introduce centralization risks. A balanced approach could retain permissionless participation while penalizing harmful behavior, aligning decentralization with practical efficiency.\nAcknowledgments\nI would like to express my sincere gratitude to Flashbots for awarding the grant that made this work possible and for supporting my ongoing research on this topic. I also extend my thanks to the PBS Foundation for their initial support of this research.\nHow does Taiko Labs post blocks on L1?\nCurrently, the Taiko proposer operates openly by observing the public L2 mempool and publishing their blocks on the L1 mempool. Since everything is done publicly, outpacing the Taiko Labs sequencer by submitting blocks faster is relatively straightforward, provided you can generate the proof for the blocks you publish or find a prover willing to generate the proof on your behalf.\nData Collection?\nTo collect data, we listened to events from the contract responsible for proving and proposing: 0x06a9Ab27c7e2255df1815E6CC0168d7755Feb19a. From these events, we extracted the Taiko block ID in which the L1 block was recorded and the L1 transaction hash.\nUsing the transaction hash, it was straightforward to check the transaction fees associated with each transaction via RPCs. For L2 transaction fees and the L2 base fee, we used the L2 block ID and calculated the results based on the block reward. While this method might not be the fastest, acquiring data for Taiko has proven to be challenging and relatively slow.\nIn future posts, we aim to find a faster way to collect data for all chains.\nEncrypted Mempool as a Solution?\nAn encrypted mempool wouldn’t solve the problem, as blocks with redundant transactions would still occur. Over time, this could lead to a monopoly where the most competitive and sophisticated searcher consistently posts blocks faster than others.\nAre Proposer A and Proposer B Outbidding Each Other?\nWe found only 57 occurrences where these two proposers published blocks in immediate succession, indicating that direct PGA-style competition between them is relatively rare. Proposer A published first in 31 instances, making all of those blocks 100% profitable for Proposer A but only 54.8% profitable for Proposer B. Conversely, Proposer B published before Proposer A in 26 instances, and in those cases, both proposers’ blocks were profitable 80.8% of the time. Further analysis will be conducted for other proposers in a subsequent post.\nHow Can I Identify Blocks Impacted by Outbidding?\nYou can view it by simply checking TaikoScan. Often, when blocks are empty or contain fewer than 100 transactions, it suggests the proposer was outbid in the PGA environment. Even blocks with a higher number of transactions might have been affected; in such cases, comparing the block’s costs to its rewards is the only way to confirm. For a more in-depth analysis, decoding the blob is the most reliable approach.\nWas Taiko Labs ever profitable?\nTo answer this question definitively, further analysis is needed. However, the intuition suggests that Taiko Labs becomes profitable under specific conditions. For other proposers, profitability occurs when  \\text{L2  Priority Fees − (L1 Publication Costs + Proving Costs) > 0}. If this condition is not met, they avoid publishing blocks that would result in a loss.\nIn contrast, the Taiko Labs proposer earns an additional Base Fee, making its profitability condition:\n\\text{(L2  Priority Fees + Base Fee) − (L1 Publication Costs + Proving Costs) > 0}\nWhen this condition holds, Taiko Labs is profitable, as the Base Fee offsets the publication and proving costs that would otherwise make the block unprofitable for other proposers.\n",
        "category": [
            "Economics"
        ],
        "discourse": [
            "mev",
            "based-sequencing"
        ]
    },
    {
        "title": "Ahead-of-Time Block Auctions To Enable Execution Preconfirmations",
        "link": "https://ethresear.ch/t/ahead-of-time-block-auctions-to-enable-execution-preconfirmations/21345",
        "article": "Thanks to Burak Oz, Bo Du, Ladislaus, Domothy, Alejandro Ranchal-Pedrosa, Justin Drake for discussion or review. Final work is the author’s and reviews are not an endorsement! Also thank you to the teams who participated in Sequencing Week.\nThis post assumes familiarity with preconfirmations. This post is opening a potential design direction for discussion and not an endorsement or necessarily a reflection of any team’s roadmap.\nSummary\nExecution preconfirmations have two problems in order to ensure they are more profitable for the proposer than today’s existing PBS pipeline:\nAdversarial flow: how to prevent searchers from submitting transactions via execution preconfirmations, which extract money from the builder and proposer\nPricing: how to price blockspace and mitigate the futures risk of the block becoming more valuable later\nThere are two approaches, one where the gateway is an unsophisticated entity, and the other where the gateway is a sophisticated builder.\nThe unsophisticated gateway addresses this issue by having pricers compete to take on the futures risk on behalf of the gateway. This avoids further gateway centralization.\nThe sophisticated builder gateway addresses this issue by designating block builders as the preconfirmation gateway. Builders can detect and block adversarial top of block ToB activity, preventing searchers from exploiting MEV opportunities. Moreover, builders are uniquely positioned to accurately price slots within the block.\nPricer System\nIn this system, the gateway acts as an unsophisticated auction house. It holds a bidding process where pricers can bid to underwrite the futures risk of a preconfirmation request, in exchange for receiving the preconfirmation bid. Under this design, for every incoming transaction, a bidding process will occur in which a pricer agrees to take on its futures risk in exchange for a fee.\nphoto_2024-12-31 20.01.44993×503 30.1 KB\nEthereum Sequencing Call #13\nThe pricer will be exposed to the futures risk of the contract, in exchange for accepting the tip, they will be exposed to the future risk of block space value. The key issue with the pricer system is: how to compare the value of a block with an execution constraint versus a block without the execution constraint. It could be possible to check bundles that affect a certain state in a BuilderNet-style TEE, and check the value of the block with and without the constraint to figure out the opportunity cost of transaction inclusion. This enables appropriately moving the futures risk onto the pricer.\nCurrent Design\nThis is a sketch of the components of the preconfirmation pipeline.\nCreated During Sequencing Week1280×759 50.2 KB\nDesign suggested at Sequencing Week\nIn this design, the proposer delegates the right to issue a preconfirmation transaction to an external party, the gateway, a block builder, which is issued the right to add transactions to the top of block ToB, along with an inclusion list constraint on Rest of Block RoB. The gateway submits an ordered list that must be added to ToB and an unordered list that must be included in RoB.\nThis protects the proposer from DDOS attacks and allows the gateway to have more complicated and computationally expensive logic. Research by Burak Oz and others [1]  have shown that roughly 60-70% of block builder profits is earned by off-chain agreements including searcher flow and exclusive order flow. Therefore, pricing the preconfirmation is a challenging task, as only 30–40% of the data required to price the transaction is on-chain. Certain proxies like cex-dex volatility can be used to attempt to predict the off-chain MEV; however, it requires the gateway to perform complex decision-making.\nThe gateway must:\nsuccessfully predict the on-chain MEV extracted in the block\nsuccessfully predict the off-chain MEV extracted via off-chain agreements\ntake on the futures risk of selling blockspace now that may increase in value later on\n\nexecution preconfirmations are particularly difficult because the gateway must predict which contentious state is MEV-valuable, and therefore deny transactions that affect certain pieces of contentious state\n\n\nexecution preconfirmations are particularly difficult because the gateway must predict which contentious state is MEV-valuable, and therefore deny transactions that affect certain pieces of contentious state\navoid being gamed by searchers and other entities that may seek to buy blockspace for less than its value, thereby extracting value from the proposer\nThe entity with the best data on the value of the upcoming block is the builder themselves. We propose merging the role of the builder and the preconfer into the gateway, we ask block builders who reap high MEV rewards to also take on the role of predicting the value of the upcoming block, and therefore pricing the preconfirmation.\nExecution preconfirmations are especially valuable because they allow for synchronous composability between L2s and the L1 in advance of a block, they also allow the gateway to act as a shared block builder between the L2s and L1. They allow following transactions to act on the output state of the first transaction, creating continuous block building, massively improving UX.\nAhead of Time Slot Auction\nThe gateway can buy the rights to build the top of the block either Just In Time JiT just ahead of the slot start time, or up to 32 slots in advance. We believe it’s better to hold an auction closer to the slot time, as the gateway has more data, and can bid higher.  Meanwhile the existing PBS pipeline is preserved, the gateway can auction off space in the RoB that has not yet been auctioned.\nThe block can be sold in the following positions:\nbid for gateway rights (ToB and RoB inclusion constraint) 32 slots in advance (potentially even traded every slot)\n\nthe issue with trading the block far in advance is there will likely be a discount to the block’s value vs a JIT auction. Therefore, we suspect a JIT auction slightly ahead of the slot start time is preferable.\nJIT auctions in advance of every slot prevent multi-slot MEV\n\n\nthe issue with trading the block far in advance is there will likely be a discount to the block’s value vs a JIT auction. Therefore, we suspect a JIT auction slightly ahead of the slot start time is preferable.\nJIT auctions in advance of every slot prevent multi-slot MEV\nbid for gateway rights just before the upcoming block\nsell the rest of the slot via existing pbs pipeline.\nInstead of selling the block at the end of the block, as is done currently, we propose auctioning the block JIT before the current slot\nIn exchange for taking on the responsibility of issuing preconfirmations, the builder earns additional fees for preconfirmation risk.\nConcerns\nCentralization of the Gateway\n\nBy moving the auction process just ahead of the block, we simply move the existing process that occurred at the end of the block to in advance of the block, this adds minimal changes beyond what exists today. We expect the builder market to further decentralize, we can reuse these innovations to further decentralize the gateway.\n\n\nBy moving the auction process just ahead of the block, we simply move the existing process that occurred at the end of the block to in advance of the block, this adds minimal changes beyond what exists today. We expect the builder market to further decentralize, we can reuse these innovations to further decentralize the gateway.\nBidding in an ahead of time auction vs JIT, where as a builder you would need to discount the expected MEV given there’s uncertainty. So depending on how long in advance you have to bid, it may not be optimal for the proposer.\n\nWe believe the additional profits earned from preconfirmation tips may be greater than the blockspace futures risk, so the builder and proposer will be net more profitable.\n\n\nWe believe the additional profits earned from preconfirmation tips may be greater than the blockspace futures risk, so the builder and proposer will be net more profitable.\nReferences\n[1] Burak Öz, Danning Sui, Thomas Thiery, Florian Matthes, Who Wins Ethereum Block Building Auctions and Why?\n[2] Preconfirmations Call #13\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "sequencing",
            "based-sequencing",
            "preconfirmations"
        ]
    },
    {
        "title": "No minimum ETH Staking without the LSD",
        "link": "https://ethresear.ch/t/no-minimum-eth-staking-without-the-lsd/16065",
        "article": "Existing ETH staking-as-a-service providers typically require users to deposit a full balance of 32 ETH or deposit funds into inherently centralized liquid staking derivative (LSD) protocols. However, relying on a third party in LSD protocols introduces certain counterparty risks, including the potential centralization of node operators, the vulnerability of validator signing keys to ransom attacks, and the management of stake balances leading to potential ETH being stuck or lost. Furthermore, the concentration of ETH within these centralized entities amplifies the potential impact of exploited risks.\nMore significantly, as additional opportunities in the Ethereum ecosystem grow, such as Eigenlayer. Single purpose LSDs are too inflexible to provide the optionality that ETH stakers have and will continue to have. It will be important to have a UI that allows users to natively choose where they want their staked ETH to go. By removing the middleman, this direct staking process opens up possibilities for stakers to earn additional rewards and maximize their returns.\nFor these reasons we’re building Casimir SelfStake to empower stakers with greater control over their assets and the ability to tap into emerging opportunities within the Ethereum ecosystem.\nCasimir SelfStake offers a different approach where stakers can directly deposit any amount of ETH to highly capable Ethereum operators through a factory smart contract model. This approach minimizes counterparty risk for users and enhances the decentralization of Ethereum staking. Validators’ duties are carried out by openly registered and collateralized operators using distributed validator technology (DVT). Trustless key management is achieved through zero-coordination distributed key generation (DKG). Automated actions, such as compounding stake or handling a slash, are executed by a decentralized oracle network (DON). Furthermore, the user experience is improved through the use of account abstraction to wrap staking contract actions.\nWe’re looking for feedback, discussion, and mentorship from the broader community on this project. We believe this methodology can help enable a more decentralized and trustless approach to Ethereum staking without sacrificing scalability, usability, or staking yield.\nYou can follow our work here: GitHub - consensusnetworks/casimir: 🌊 Decentralized staking and asset management\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Supporting decentralized staking through more anti-correlation incentives",
        "link": "https://ethresear.ch/t/supporting-decentralized-staking-through-more-anti-correlation-incentives/19116",
        "article": "Content note: preliminary research. Would love to see independent replication attempts.\nCode: https://github.com/ethereum/research/tree/master/correlation_analysis\nOne tactic for incentivizing better decentralization in a protocol is to penalize correlations. That is, if one actor misbehaves (including accidentally), the penalty that they receive would be greater the more other actors (as measured by total ETH) misbehave at the same time as them. The theory is that if you are a single large actor, any mistakes that you make would be more likely to be replicated across all “identities” that you control, even if you split your coins up among many nominally-separate accounts.\nThis technique is already employed in Ethereum slashing (and arguably inactivity leak) mechanics. However, edge-case incentives that only arise in a highly exceptional attack situation that may never arise in practice are perhaps not sufficient for incentivizing decentralization.\nThis post proposes to extend a similar sort of anti-correlation incentive to more “mundane” failures, such as missing an attestation, that nearly all validators make at least occasionally. The theory is that larger stakers, including both wealthy individuals and staking pools, are going to run many validators on the same internet connection or even on the same physical computer, and this will cause disproportionate correlated failures. Such stakers could always make an independent physical setup for each node, but if they end up doing so, it would mean that we have completely eliminated economies of scale in staking.\nSanity check: are errors by different validators in the same “cluster” actually more likely to correlate with each other?\nWe can check this by combining two datasets: (i) attestation data from some recent epochs showing which validators were supposed to have attested, and which validators actually did attest, during each slot, and (ii) data mapping validator IDs to publicly-known clusters that contain many validators (eg. “Lido”, “Coinbase”, “Vitalik Buterin”). You can find a dump of the former here, here and here, and the latter here.\nWe then run a script that computes the total number of co-failures: instances of two validators within the same cluster being assigned to attest during the same slot, and failing in that slot.\nWe also compute expected co-failures: the number of co-failures that “should have happened” if failures were fully the result of random chance.\nFor example, suppose that there are ten validators with one cluster of size 4 and the others independent, and three validators fail: two within that cluster, and one outside it.\n\nThere is one co-failure here: the second and fourth validators within the first cluster. If all four validators in that clusters had failed, there would be six co-failures, one for each six possible pairs.\nBut how many co-failures “should there” have been? This is a tricky philosophical question. A few ways to answer:\nFor each failure, assume that the number of co-failures equals the failure rate across the other validators in that slot times the number of validators in that cluster, and halve it to compensate for double-counting. For the above example, this gives \\frac{2}{3}.\nCalculate the global failure rate, square it, and then multiply that by \\frac{n * (n-1)}{2} for each cluster. This gives (\\frac{3}{10})^2 * 6 = 0.54.\nRandomly redistribute each validator’s failures among their entire history.\nEach method is not perfect. The first two methods fail to take into account different clusters having different quality setups. Meanwhile, the last method fails to take into account correlations arising from different slots having different inherent difficulties: for example, slot 8103681 has a very large number of attestations that don’t get included within a single slot, possibly because the block was published unusually late.\n882×227 11.8 KB\nSee the “10216 ssfumbles” in this python output.\nI ended up implementing three approaches: the first two approaches above, and a more sophisticated approach where I compare “actual co-failures” with “fake co-failures”: failures where each cluster member is replaced with a (pseudo-) random validator that has a similar failure rate.\nI also explicitly separate out fumbles and misses. I define these terms as follows:\nFumble: when a validator misses an attestation during the current epoch, but attested correctly during the previous epoch\nMiss: when a validator misses an attestation during the current epoch and also missed during the previous epoch\nThe goal is to separate the two very different phenomena of (i) network hiccups during normal operation, and (ii) going offline or having longer-term glitches.\nI also simultaneously do this analysis for two datasets: max-deadline and single-slot-deadline. The first dataset treats a validator as having failed in an epoch only if an attestation was never included at all. The second dataset treats a validator as having failed if the attestation does not get included within a single slot.\nHere are my results for the first two methods of computing expected co-failures. SSfumbles and SSmisses here refer to fumbles and misses using the single-slot dataset.\nFor the first method, the Actual row is different, because a more restricted dataset is used for efficiency:\nThe “expected” and “fake clusters” columns show how many co-failures within clusters there “should have been”, if clusters were uncorrelated, based on the techniques described above. The “actual” columns show how many co-failures there actually were. Uniformly, we see strong evidence of “excess correlated failures” within clusters: two validators in the same cluster are significantly more likely to miss attestations at the same time than two validators in different clusters.\nHow might we apply this to penalty rules?\nI propose a simple strawman: in each slot, let p be the current number of missed slots divided by the average for the last 32 slots. That is, p[i] = \n\\frac{misses[i]}{\\sum_{j=i-32}^{i-1}\\ misses[j]}. Cap it: p \\leftarrow min(p, 4). Penalties for attestations of that slot should be proportional to p. That is, the penalty for not attesting at a slot should be proportional to how many validators fail in that slot compared to other recent slots.\nThis mechanism has a nice property that it’s not easily attackable: there isn’t a case where failing decreases your penalties, and manipulating the average enough to have an impact requires making a large number of failures yourself.\nNow, let us try actually running it. Here are the total penalties for big clusters, medium clusters, small clusters and all validators (including non-clustered) for four penalty schemes:\nbasic: Penalize one point per miss (ie. similar to status quo)\nbasic_ss: the same but requiring single-slot inclusion to not count as a miss\nexcess: penalize p points with p calculated as above\nexcess_ss: penalize p points with p calculated as above, requiring single-slot inclusion to not count as a miss\nHere is the output:\nWith the “basic” schemes, big has a ~1.4x advantage over small (~1.2x in the single-slot dataset). With the “excess” schemes, this drops to ~1.3x (~1.1x in the single-slot dataset). With multiple other iterations of this, using slightly different datasets, the excess penalty scheme uniformly shrinks the advantage of “the big guy” over “the little guy”.\nWhat’s going on?\nThe number of failures per slot is small: it’s usually in the low dozens. This is much smaller than pretty much any “large staker”. In fact, it’s smaller than the number of validators that a large staker would have active in a single slot (ie. 1/32 of their total stock). If a large staker runs many nodes on the same physical computer or internet connection, then any failures will plausibly affect all of their validators.\nWhat this means is: when a large validator has an attestation inclusion failure, they single-handedly move the current slot’s failure rate, which then in turn increases their penalty. Small validators do not do this.\nIn principle, a big staker can get around this penalty scheme by putting each validator on a separate internet connection. But this sacrifices the economies-of-scale advantage that a big staker has in being able to reuse the same physical infrastructure.\nTopics for further analysis\nFind other strategies to confirm the size of this effect where validators in the same cluster are unusually likely to have attestation failures at the same time\nTry to find the ideal (but still simple, so as to not overfit and not be exploitable) reward/penalty scheme to minimize the average big validator’s advantage over little validators.\nTry to prove safety properties about this class of incentive schemes, ideally identify a “region of design space” within which risks of weird attacks (eg. strategically going offline at specific times to manipulate the average) are too expensive to be worth it\nCluster by geography. This could determine whether or not this mechanism also creates an incentive to geographically decentralize.\nCluster by (execution and beacon) client software. This could determine whether or not this mechanism also creates an incentive to use minority clients.\nMini-FAQ\nQ: But wouldn’t this just lead to staking pools architecturally decentralizing their infra without politically decentralizing themselves, and isn’t the latter what we care about more at this point?\nA: If they do, then that increases the cost of their operations, making solo staking relatively more competitive. The goal is not to single-handedly force solo staking, the goal is to make the economic part of the incentives more balanced. Political decentralization seems very hard or impossible to incentivize in-protocol; for that I think we will just have to count on social pressure, starknet-like airdrops, etc. But if economic incentives can be tweaked to favor architectural decentralization, that makes things easier for politically decentralized projects (which cannot avoid being architecturally decentralized) to get off the ground.\nQ: Wouldn’t this hurt the “middle-size stakers” (wealthy individuals who are not big exchanges/pools) the most, and encourage them to move to pools?\nA: In the table above, the “small” section refers to stakers with 10-300 validators, ie. 320-9600 ETH. That includes most wealthy people. And as we can see, those stakers suffer significantly higher penalties than pools today, and the simulation shows how the proposed adjusted reward scheme would equalize things between precisely those validators and the really big ones. Mathematically speaking, someone with 100 validator slots would only have 3 per slot, so they would not be greatly affecting the penalty factor for a round; only validators that go far above that would be.\nQ: Post-MAXEB, won’t big stakers get around this by consolidating all their ETH into one validator?\nA: The proportional penalty formula would count total amount of ETH, not number of validator IDs, so 4000 staked ETH that acts the same way would be treated the same if it’s split between 1 validator or 2 or 125.\nQ: Won’t adding even more incentives to be online create further pressure to optimize and hence centralize, regardless of the details?\nA:The parameters can be set so that on average, the size of the incentive to be online is the same as it is today.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Based rollups—superpowers from L1 sequencing",
        "link": "https://ethresear.ch/t/based-rollups-superpowers-from-l1-sequencing/15016",
        "article": "TLDR: We highlight a special subset of rollups we call “based” or “L1-sequenced”. The sequencing of such rollups—based sequencing—is maximally simple and inherits L1 liveness and decentralisation. Moreover, based rollups are particularly economically aligned with their base L1.\nDefinition\nA rollup is said to be based, or L1-sequenced, when its sequencing is driven by the base L1. More concretely, a based rollup is one where the next L1 proposer may, in collaboration with L1 searchers and builders, permissionlessly include the next rollup block as part of the next L1 block.\nAdvantages\n\nliveness: Based sequencing enjoys the same liveness guarantees as the L1. Notice that non-based rollups with escape hatches suffer degraded liveness:\n\n\nweaker settlement guarantees: Transactions in the escape hatch have to wait a timeout period before guaranteed settlement.\n\ncensorship-based MEV: Rollups with escape hatches are liable to toxic MEV from short-term sequencer censorship during the timeout period.\n\nnetwork effects at risk: A mass exit triggered by a sequencer liveness failure (e.g. a 51% attack on a decentralised PoS sequencing mechanism) would disrupt rollup network effects. Notice that rollups, unlike the L1, cannot use social consensus to gracefully recover from sequencer liveness failures. Mass exists are a sword of Damocles in all known non-based rollup designs.\n\ngas penalty: Transactions that settle through the escape hatch often incur a gas penalty for its users (e.g. because of suboptimal non-batched transaction data compression).\n\n\n\nweaker settlement guarantees: Transactions in the escape hatch have to wait a timeout period before guaranteed settlement.\n\ncensorship-based MEV: Rollups with escape hatches are liable to toxic MEV from short-term sequencer censorship during the timeout period.\n\nnetwork effects at risk: A mass exit triggered by a sequencer liveness failure (e.g. a 51% attack on a decentralised PoS sequencing mechanism) would disrupt rollup network effects. Notice that rollups, unlike the L1, cannot use social consensus to gracefully recover from sequencer liveness failures. Mass exists are a sword of Damocles in all known non-based rollup designs.\n\ngas penalty: Transactions that settle through the escape hatch often incur a gas penalty for its users (e.g. because of suboptimal non-batched transaction data compression).\n\ndecentralisation: Based sequencing inherits the decentralisation of the L1 and naturally reuses L1 searcher-builder-proposer infrastructure. L1 searchers and block builders are incentivised to extract rollup MEV by including rollup blocks within their L1 bundles and L1 blocks. This then incentivises L1 proposers to include rollup blocks on the L1.\n\nsimplicity: Based sequencing is maximally simple; significantly simpler than even centralised sequencing. Based sequencing requires no sequencer signature verification, no escape hatch, and no external PoS consensus.\n\n\nhistorical note: In January 2021 Vitalik described based sequencing as “total anarchy” that risks multiple rollup blocks submitted at the same time, causing wasted gas and effort. It is now understood that proposer-builder separation (PBS) allows for tightly regimented based sequencing, with at most one rollup block per L1 block and no wasted gas. Wasted zk-rollup proving effort is avoided when rollup block n+1 (or n+k for k >= 1)  includes a SNARK proof for rollup block n.\n\n\n\nhistorical note: In January 2021 Vitalik described based sequencing as “total anarchy” that risks multiple rollup blocks submitted at the same time, causing wasted gas and effort. It is now understood that proposer-builder separation (PBS) allows for tightly regimented based sequencing, with at most one rollup block per L1 block and no wasted gas. Wasted zk-rollup proving effort is avoided when rollup block n+1 (or n+k for k >= 1)  includes a SNARK proof for rollup block n.\n\ncost: Based sequencing enjoys zero gas overhead—no need to even verify signatures from centralised or decentralised sequencers. The simplicity of based sequencing reduces development costs, shrinking time to market and collapsing the surface area for sequencing and escape hatch bugs. Based sequencing is also tokenless, avoiding the regulatory burden of token-based sequencing.\n\nL1 economic alignment: MEV originating from based rollups naturally flows to the base L1. These flows strengthen L1 economic security and, in the case of MEV burn, improve the economic scarcity of the L1 native token. This tight economic alignment with the L1 may help based rollups build legitimacy. Importantly, notice that based rollups retain the option for revenue from L2 congestion fees (e.g. L2 base fees in the style of EIP-1559) despite sacrificing MEV income.\n\nsovereignty: Based rollups retain the option for sovereignty despite delegating sequencing to the L1. A based rollup can have a governance token, can charge base fees, and can use proceeds of such base fees as it sees fit (e.g. to fund public goods à la Optimism).\nDisadvantages\n\nno MEV income: Based rollups forgo MEV to the L1, limiting their revenue to base fees. Counter-intuitively, this may increase overall income for based rollups. The reason is that the rollup landscape is plausibly winner-take-most and the winning rollup may leverage the improved security, decentralisation, simplicity, and alignment of based rollups to achieve dominance and ultimately maximise revenue.\n\nconstrained sequencing: Delegating sequencing to the L1 comes with reduced sequencing flexibility. This makes the provision of certain sequencing services harder, possibly impossible:\n\n\npre-confirmations: Fast pre-confirmations are trivial with centralised sequencing, and achievable with an external PoS consensus. Fast pre-confirmations with L1 sequencing is an open problem with promising research avenues including EigenLayer, inclusion lists, and builder bonds.\n\nfirst come first served: Providing Arbitrum-style first come first served (FCFS) sequencing is unclear with L1 sequencing. EigenLayer may unlock an FCFS overlay to L1 sequencing.\n\n\n\npre-confirmations: Fast pre-confirmations are trivial with centralised sequencing, and achievable with an external PoS consensus. Fast pre-confirmations with L1 sequencing is an open problem with promising research avenues including EigenLayer, inclusion lists, and builder bonds.\n\nfirst come first served: Providing Arbitrum-style first come first served (FCFS) sequencing is unclear with L1 sequencing. EigenLayer may unlock an FCFS overlay to L1 sequencing.\nNaming\nThe name “based rollup” derives from the close proximity with the base L1. We acknowledge the naming collision with the recently-announced Base chain from Coinbase, and argue this could be a happy coincidence. Indeed, Coinbase shared two design goals in their Base announcement:\n\ntokenlessness: “We have no plans to issue a new network token.” (in bold)\n\ndecentralisation: “We […] plan to progressively decentralize the chain over time.”\nBase can achieve tokenless decentralisation by becoming based.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": []
    },
    {
        "title": "Towards Attester-Includer Separation",
        "link": "https://ethresear.ch/t/towards-attester-includer-separation/21306",
        "article": "Screenshot 2024-12-17 at 10.44.051556×1562 318 KB\n^Rare picture of an includer, chilling and effortlessly collecting rewards for improving Ethereum’s censorship resistance.\nby Thomas Thiery - December 17th, 2024\nThanks to Julian Ma and Barnabé Monnot, Terence Tsao and Jacob Kaufmann for feedback and discussions on this post.\nLately I’ve been stumbling upon more and more discussions and research around incentives for transaction inclusion via FOCIL EIP-7805, state vs. inclusion preconfirmations, and further separating protocol roles and duties.  It has become increasingly clear to me that thinking from first principles about state and inclusion as two orthogonal dimensions could be useful in guiding future protocol development.\nDisclaimer: In this post, I’ll use simplified, somewhat caricature-like definitions to differentiate inclusion transactions from state transactions, acknowledging that in practice there is much more nuance.\nInclusion transactions\nBy inclusion transactions, I mean order-invariant transactions (h/t James Prestwich) whose outcome remains the same regardless of the state on which they are executed; the only important factor is that they are included somewhere in the block. Examples include making a payment at a coffee shop or transferring tokens to a friend.\nBecause of their order-invariant property, inclusion transactions are typically sent to the public mempool. Publicly disclosing transaction information before inclusion is acceptable since no one can exploit these transactions (e.g., by frontrunning), as they don’t intrinsically carry Maximal Extractable Value (MEV). To be considered valid for inclusion, these transactions must pay the base fee for each unit of gas consumed.\nIn a post-EIP-7805 world, inclusion transactions would mostly be included in Ethereum blocks by multiple IL proposers via FOCIL (using inclusion rules like time pending in the mempool, or priority fee ordering). By being publicly broadcast to the mempool, these transactions benefit from increased chances that one of multiple IL proposers will include them via their inclusion lists.\nState transactions\nBy state transactions, I mean order-dependent transactions whose outcomes change based on the state at the time of execution. For example, a transaction performing a token swap on an automated market maker like Uniswap.\nState transactions originating from regular users potentially carry Maximal Extractable Value (MEV) and can be exploited by searchers—for example, through frontrunning or sandwich attacks—if their content is publicly available before inclusion. These transactions are often time-sensitive and benefit from being sent through private channels to ensure MEV protection and/or receive rebates. When state transactions carry MEV and are made available to sophisticated parties like searchers, whether willingly or unwillingly, they are usually bundled together with other transactions to extract the MEV opportunity they create. Note that state transactions can also originate from sophisticated parties themselves; for example, arbitrageurs who take advantage of price differences between different exchanges to make a profit. In both cases, when state transactions generate MEV opportunities, they are usually accompanied by tips to validators (in the form of priority fees or by using coinbase transfers) in addition to the base fee.\nBecause FOCIL does not provide any guarantees on transaction ordering or inclusion during network congestion (i.e., when blocks are full), we can assume that the market structure for state transactions wouldn’t change much in a post-EIP-7805 world.\nHere’s a brief recap of the main differences between inclusion and state transactions:\nInclusion transactions:\n- Pay base fees to be considered for inclusion in the block.\n- Do not use priority fees to signal order preferences.\n- Benefit from being publicly broadcast to a large number of parties.\n- Do not have to rely on sophisticated actors for inclusion.\nState transactions:\n- Pay base fees to be considered for inclusion in the block.\n- Add priority fees to express preferences regarding the specific state on which they would like to be executed, which corresponds to a particular position in the block.\n- Benefit from being sent privately to one or a few sophisticated parties.\n\nNote: An inclusion transaction might pay priority fees for reasons unrelated to order preferences. For example, it may need to compensate the block producer for additional resource usage—such as the extra propagation time required by blobs. Priority fees can also signal a desire for faster inclusion rather than a specific placement in the block. In other words, even if a transaction does not care about its position, it may be willing to pay more to reduce delays and be included sooner (e.g., blob transactions, fraud proof transactions).\nBase and Priority Fees\nSince EIP-1559 was implemented, all transactions—both inclusion and state transactions—must pay base fees to be considered for inclusion in a block. Priority fees, on the other hand, can serve different purposes depending on whether there is network congestion:\nNo Congestion: When there is enough space in the block to include all pending transactions:\n\nInclusion transactions do not generally need to pay priority fees since they will be included as long as they pay the base fee and do not care about being inserted at a specific position in the block.\nState transactions will pay priority fees to incentivize the block producer to execute them on a particular state.\n\n\nInclusion transactions do not generally need to pay priority fees since they will be included as long as they pay the base fee and do not care about being inserted at a specific position in the block.\nState transactions will pay priority fees to incentivize the block producer to execute them on a particular state.\nCongestion:\n\nInclusion transactions might then choose to add priority fees to increase their chances of being included in the next block—for example, ahead of other inclusion transactions— rather than waiting until the network is no longer congested.\nState transactions will use priority fees regardless of whether there is network congestion.\n\n\nInclusion transactions might then choose to add priority fees to increase their chances of being included in the next block—for example, ahead of other inclusion transactions— rather than waiting until the network is no longer congested.\nState transactions will use priority fees regardless of whether there is network congestion.\nThe interesting takeaway here is that the boundary between state and inclusion transactions blurs during periods of congestion. When there isn’t enough space for every transaction, simply wanting to be included anywhere in a given block conceptually becomes very similar to wanting to be executed on a specific state. Priority fees can thus be thought of as a one-size-fits-all mechanism to incentivize block producers and secure an advantageous position in a block.\nHowever, there is another obvious reason a transaction might not be included in a block even without congestion: censorship.\nCosts of censorship\nIn a post-EIP-7805 world, there are still a couple of ways to censor a transaction:\n\nBlock Stuffing: Given FOCIL’s conditional and anywhere-in-block properties, the proposer can stuff its block up to the gas limit in order to exclude the transaction and still satisfy the IL conditions. To estimate the cost of stuffing multiple consecutive blocks, I used the following formula:\n\n\\text{Block Stuffing Cost} = (\\text{Gas Limit} - \\text{Gas Used}) \\times \\text{Base Fees} \\times \\left( \\frac{1}{0.125} \\times 1.125^N - 1 \\right)\n\nwhere N is the number of consecutive blocks.\nThe figure below estimates the average costs of block stuffing over the past three months (based on this Dune query), highlighting how market conditions and base fees influence them, and illustrating how stuffing multiple blocks in a row becomes exponentially more expensive for the attacker.\nDec 9 Screenshot from Attester-Includer Separation1986×586 66.9 KB\n\n\nMissing slot: Alternatively, the proposer can choose to skip block proposal for their assigned slot, causing them to forgo both consensus (issuance) and execution (MEV) layer rewards, which amount to approximately 0.04 ETH combined per block on average during the past three months.\nScreenshot 2024-12-18 at 18.21.351986×588 37.7 KB\n\nBlock Stuffing: Given FOCIL’s conditional and anywhere-in-block properties, the proposer can stuff its block up to the gas limit in order to exclude the transaction and still satisfy the IL conditions. To estimate the cost of stuffing multiple consecutive blocks, I used the following formula:\nwhere N is the number of consecutive blocks.\nThe figure below estimates the average costs of block stuffing over the past three months (based on this Dune query), highlighting how market conditions and base fees influence them, and illustrating how stuffing multiple blocks in a row becomes exponentially more expensive for the attacker.\nDec 9 Screenshot from Attester-Includer Separation1986×586 66.9 KB\nMissing slot: Alternatively, the proposer can choose to skip block proposal for their assigned slot, causing them to forgo both consensus (issuance) and execution (MEV) layer rewards, which amount to approximately 0.04 ETH combined per block on average during the past three months.\nScreenshot 2024-12-18 at 18.21.351986×588 37.7 KB\nNote that in both cases above, increasing a transaction’s priority fees makes it more costly for the proposer to exclude it.\nIL committee bribing:  Lastly, an obvious way to censor a transaction is to convince all IL proposers not to include it in their ILs. Under EIP-7805, IL proposers are not rewarded for including transactions in their ILs. In practice, convincing all IL proposers in a committee to act dishonestly and against the ethos of the Ethereum network might be difficult. With FOCIL, we only need one member of the committee to act honestly and include all transactions without censoring for the mechanism to work as expected. However, in theory, an attacker could offer a very small bribe to all 16 committee members to exclude a given transaction. If the IL proposers are rational, they might accept any bribe greater than zero.\nBy relying on the altruistic behavior of IL proposers, there is no way to control the cost they incur when censoring a transaction.\nInclusion fees\nGiven the distinct properties and life cycles of state and inclusion transactions—and the imbalance where users can tip the proposer but cannot affect the cost incurred by IL proposers when censoring transactions in protocol—one option is introducing an independent inclusion fee (IF) and reward mechanism to increase inclusion guarantees and cost of censorship, while preserving the role of priority fees (PF) as proposer tips.\nThis approach allows users to craft their transactions based on network conditions (base fees) while controlling how much they are willing to pay for (1) Being executed on a specific state via PFs and (2) Increasing their inclusion guarantees via IFs, or both. In the diagram below, you can see how transactions are sent either privately to the block producer or to the public mempool, and are specifying both priority and inclusion fees. We assume that transactions are added to inclusion lists (ILs) and sorted in descending order based on inclusion fees (more on this in the next section). The block producer then orders the full payload—by default according to priority fees or in any other order depending on MEV opportunities—incorporating transactions from ILs and those they received privately.\nNov 21 Screenshot from Attester-Includer Separation1872×786 101 KB\nReward mechanism\nA simple way to distribute inclusion fees among IL proposers is to allocate them proportionally based on their contributions, rewarding only those who included the transactions in their ILs (i.e., conditional tips). This leads to greater incentives to include transactions no one else wants to include (e.g., “censorable transactions”).\nIn the example above, IL Proposer 1 included all pending transactions from the mempool. Their rewards would thus be calculated as follows:\nTransaction g: Inclusion fee of 6 divided by 4 proposers = 6 ⁄ 4 = 1.5\nTransaction d: Inclusion fee of 4 divided by 3 proposers = 4 ⁄ 3 ≈ 1.333\nTransaction e: Inclusion fee of 1 divided by 3 proposers = 1 ⁄ 3 ≈ 0.333\nTransaction f: Inclusion fee of 0 divided by 4 proposers = 0 ⁄ 4 = 0\nAdding these up, IL Proposer 1 would receive approximately  1.5 + 1.333 + 0.333  + 0 = 3.166 in rewards for including these transactions in its IL.\nAlternative approaches to rewarding IL proposers include using issuance rather than fees,  weighting rewards based on past performance. It is also important that any such reward mechanism be independent of the existing Transaction Fee Mechanisms (TFMs)—in other words, separate from both the base fee and the priority fee. Attempting to repurpose the base fee to reward IL proposers is not incentive-compatible because EIP-1559’s economic design relies on burning the base fee to prevent block producers from manipulating transaction inclusion and inflating fees for personal gain. By ensuring that the base fee is never directly redistributed, the system maintains a balanced incentive structure. Similarly, relying on a model that redirects priority fees to IL proposers fails under network congestion, as block producers would then have a greater incentive to include non-IL transactions for higher direct rewards.\nRoles and participants\nIs it possible—and desirable— to go further and separate protocol participants who are tasked with transaction inclusion (IL committee members) from those who focus on valuable state transactions (proposers)? Let’s now imagine a post-FOCIL, post-APS (Attester-Proposer Separation) world. In this scenario, we still have IL proposers including transactions from the public mempool in their inclusion lists and being rewarded via inclusion fees. However, there’s now a separation between attesters/the beacon proposer, the execution proposer and the builder (whether PBS is enshrined in the protocol or not is not relevant for this part of the discussion).\nHere’s a quick overview of each participant’s responsibilities:\nNov 19 Screenshot from Attester-Includer Separation1041×954 72.6 KB\nInterestingly, by separating attesters from proposers, APS also effectively separates execution proposers from IL committee members. However, in the next section, we argue that IL committee members should be considered a separate class of participants. Because their responsibilities are limited in complexity and they are not directly involved in the system’s economic security, they don’t need to be subject to the same capital requirements as attesters.\nAttester-Includer Separation\nAttester Includer Separation2000×1461 219 KB\nh/t Barnabé\nBuilding on the idea of unbundling roles to better align them with protocol duties, and drawing inspiration from tiered staking models like Rainbow Staking and validator selection mechanisms such as Orbit, we propose further separating attesters from IL committee members (includers).\nThinking from first principles, we would want both attesters and includers to be geographically decentralized and unsophisticated. However, there are some crucial differences between these sets of participants regarding capital requirements and the importance of their roles in securing the network:\nAttesters:\n\nSecurity: Overall, attesters play an extremely crucial role in securing the network by participating in consensus and ensuring liveness and finality. This critical role comes with some constraints. For example, it is important to avoid rotating attesters too quickly, as it might not be optimally secure from a consensus perspective. Similarly, we do not want validators to enter or exit the active set of attesters too rapidly, which is why we have withdrawal and deposit queues.\nCapital requirements: We want attesters to consolidate by maximizing the balance of a single validator (e.g., up to 2,048 ETH) instead of running multiple instances with lower balances (e.g., 32 ETH). This consolidation enables us to achieve high levels of economic security with a manageable number of participants, and facilitates moving towards faster finality (e.g., 3SF). Additionally, attesters must have at least some amount of ETH at stake to allow for slashing in cases where they do not fulfill their duties, whether intentionally or not. This means their capital needs to be staked and locked upfront (i.e., heavy).\n\n\nSecurity: Overall, attesters play an extremely crucial role in securing the network by participating in consensus and ensuring liveness and finality. This critical role comes with some constraints. For example, it is important to avoid rotating attesters too quickly, as it might not be optimally secure from a consensus perspective. Similarly, we do not want validators to enter or exit the active set of attesters too rapidly, which is why we have withdrawal and deposit queues.\nCapital requirements: We want attesters to consolidate by maximizing the balance of a single validator (e.g., up to 2,048 ETH) instead of running multiple instances with lower balances (e.g., 32 ETH). This consolidation enables us to achieve high levels of economic security with a manageable number of participants, and facilitates moving towards faster finality (e.g., 3SF). Additionally, attesters must have at least some amount of ETH at stake to allow for slashing in cases where they do not fulfill their duties, whether intentionally or not. This means their capital needs to be staked and locked upfront (i.e., heavy).\nIL Committee Members:\n\nSecurity: IL committee members are not involved with consensus and don’t play a role in securing the network. They are only tasked to improve censorship resistance by including transactions in ILs using their local view of the public mempool. Moreover, we only need one-out-of-n IL proposers to honestly build its IL for FOCIL to be effective and impose constraints on what transactions builders have to include in their blocks.\nCapital Requirements: Ideally, we want very low barriers to entry so that anyone wishing to contribute to Ethereum’s censorship resistance can easily do so with 0.01 ETH for example, or just enough to ensure Sybil resistance and prevent participants to “just signing up” but then being offline. The IL committee also rotates every slot, so there is potentially no need for queues or penalties other than missing rewards if an inclusion fee of sorts exists.\n\n\nSecurity: IL committee members are not involved with consensus and don’t play a role in securing the network. They are only tasked to improve censorship resistance by including transactions in ILs using their local view of the public mempool. Moreover, we only need one-out-of-n IL proposers to honestly build its IL for FOCIL to be effective and impose constraints on what transactions builders have to include in their blocks.\nCapital Requirements: Ideally, we want very low barriers to entry so that anyone wishing to contribute to Ethereum’s censorship resistance can easily do so with 0.01 ETH for example, or just enough to ensure Sybil resistance and prevent participants to “just signing up” but then being offline. The IL committee also rotates every slot, so there is potentially no need for queues or penalties other than missing rewards if an inclusion fee of sorts exists.\nThe question is: Do these differences justify moving towards two independent sets of participants, each fulfilling a specific duty, or should it remain the same set of participants?\nDec 9 Screenshot from Attester-Includer Separation (1)1526×992 66.6 KB\nWe argue that it does. By allowing anyone to join and contribute to Ethereum’s censorship resistance as an “includer”—with minimal hardware requirements (e.g., a smartwatch), a simple, low-friction user experience (no queues), as well as light and minimal capital requirements—and by rewarding them with an independent transaction fee mechanism (inclusion fees), the network can self-regulate based on the level of censorship. If many transactions are being censored, users can raise inclusion fees, thereby increasing the cost of censorship. As these higher fees get distributed among includers, more individuals will be incentivized to participate in creating inclusion lists (ILs), ultimately improving Ethereum’s censorship resistance. Lastly, includers should also be able to participate in improving the network’s censorship-resistant properties and uphold chain neutrality without publicly revealing their preferences via the specific transactions included in their lists. To this end, we can leverage anonymous ILs, using a combination of linkable ring signatures and anonymous broadcast protocols to protect their identities.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Timeliness detectors and 51% attack recovery in blockchains",
        "link": "https://ethresear.ch/t/timeliness-detectors-and-51-attack-recovery-in-blockchains/6925",
        "article": "Summary\nI propose a construction, based on Lamport’s 99% fault tolerant consensus ideas, that I call timeliness detectors. Timeliness detectors allows online clients (ie. clients, aka users, that are connected to other clients with latency \\le \\delta) to detect, with guarantees of correctness and agreement, whether or not blocks were published “on time”. In the event of a 51% attack, this allows at least the subset of clients that are online to come to agreement over (i) whether or not a “sufficiently bad” 51% attack happened, and (ii) what is the “correct” chain to converge on and potentially even (iii) which validators to “blame” for the attack. This reduces the ability of 51% attacks to cause chaos and speeds up recovery time from an attack, as well as increasing the chance that a successful attack costs money.\nTimeliness detectors\nThe most basic construction for a timeliness detector is as follows. For every block that a client receives, the client maintains a “is it timely?” predicate, which states whether or not the client thinks the block was received “on time”. The goal of this will be to try to distinguish the attacking chain from the “correct” chain in a 51% attack:\n\n51attack701×295 10.3 KB\n\nOur model will be simple: each block B has a self-declared timestamp t (in real protocols, the timestamp would often be implicit, eg. in the slot number). There is a commonly agreed synchrony bound \\delta. The simplest possible timeliness detector is: if you receive B before time t + \\delta, then you see the block as timely, and if you receive it after t + \\delta, then you do not. But this fails to have agreement:\n\nWe solve the problem as follows. For each block, we randomly select a sample of N “attesters”, v_1 ... v_n. Each attester follows the rule: if they see a block B with a timestamp t along with signatures from k attesters before time t + (2k+1)\\delta, they re-broadcast it along with their own signature. And the rule that a client follows is: if they see a block B with a timestamp t along with signatures from k attesters before time t + 2k\\delta, they accept it as timely. If they see B but it never satisfies this condition, they see B as not timely.\nLet us see what happens when even one client sees some block B as timely, though others may not see it as timely at first because of latency discrepancies. We will at first assume a single, honest, attester.\n\nThis diagram shows the basic principle behind what is going on. If a client sees a block before for deadline T, then (at least because they themselves can rebroadcast it) that block will get into the hands of an attester before the attester deadline T + \\delta, and the attester will add their signature, and they will rebroadcast it before time T + \\delta, guaranteeing that other nodes will see the block with the signature before time T + 2\\delta. The key mechanic is this ability for one additional signature to delay the deadline.\nNow, consider the case of n-1 dishonest attesters and one honest attester. If a client sees a timely block with k signatures, then there are two possibilities:\nOne of those k signatures is honest.\nNone of those k signatures are honest (so one attester who has not yet signed still remains)\nIn case (1), we know that the attester is honest, and so the attester broadcasted B with j \\le k signatures before time T + (2j-1)\\delta, which means that (by the synchrony assumption) every client saw that bundle before time T + 2j\\delta, so every client accepted B as current.\nIn case (2), we know that the honest attester will see the bundle before time T + (2k+1)\\delta, so they will rebroadcast it with their own signature, and every other client will see that expanded bundle before the k+1 signature deadline T + (2k+2)\\delta.\nSo now we have a “timeliness detector” which a client can use to keep track of which blocks are on time and which blocks are not, and where all clients with latency \\le \\delta to attesters will agree on which blocks are timely.\nThe Simplest Blockchain Architecture\nCome up with any rule which determines who can propose and who attests to blocks at any slot. We can define a “99% fault tolerant blockchain” as follows: to determine the current state, just process all timely blocks in order of their self-declared timestamp.\nThis actually works (and provides resistance to both finality-reversion and censorship 51% attacks), and under its own assumptions gives a quite simple blockchain architecture! The only catch: it rests everything on the assumption that all clients will be online and the network will never be disrupted. Hence, for it to work safely, it would need to have a block time of perhaps a week or longer. This could actually be a reasonable architecture for an “auxiliary chain” that keeps track of validator deposits and withdrawals and slashings, for example, preventing long-run 51% attacks from censoring new validators coming in or censoring themselves getting slashed for misbehavior. But we don’t want this architecture for the main chain that all the activity is happening on.\nA more reasonable alternative\nIn this post, however, we will focus on architectures that satisfy a somewhat weaker set of security assumptions: they are fine if either one of two assumptions is true: (i) network latency is low, including network latency between validators and clients, and (ii) the majority of validators is honest. First, let us get back to the model where we have a blockchain with some fork choice rule, instead of just discrete blocks. We will go through examples for our two favorite finality-bearing fork choice rules, (i) FFG and (ii) LMD GHOST.\nFor FFG, we extend the fork choice rule as follows. Start from the genesis, and whenever you see a block with two child chains which are both finalized, pick the chain with the lower-epoch timely finalized block. From there, proceed as before. In general, there will only ever be two conflicting finalized chains in two cases: (i) a 33% attack, and (ii) many nodes going offline (or censoring) leading to a long-running inactivity leak.\nCase (i):\n\nCase (ii), option 1 (offline minority finalizing later):\n\n51attack5726×291 9.95 KB\n\nCase (ii), option 2 (offline majority, later reappearing with finalized chain):\n\n51attack6761×291 10.2 KB\n\nHence, in all cases, we can prevent 51% attacks from breaking finality, at least past a certain point in time (T + 2k\\delta, the time bound after which if a client has not accepted a block as timely then we know that it will never accept it as timely). Note also that the above diagram is slightly misleading; what we care about is not the timelines of the finalized block, but rather the timeliness of a block that includes evidence that proves that the block is finalized.\nFor clients that are offline sometimes, this does not change anything as long as there is no 51% attack: if the chain is not under attack, then blocks in the canonical chain will be timely, and so finalized blocks will always be timely.\nThe main case where this may lead to added risk is the case of clients that have high latency but are unaware that they have high latency; they could see timely blocks as non-timely or non-timely blocks as timely. The goal of this mechanism is that if the non-timeliness-dependent fork choice and the timeliness-dependent fork choice disagree, the user should be notified of this, so they would socially verify what is going on; they should not be instructed to blindly accept the timeliness-dependent fork choice as canonical.\nDealing with censorship\nWe can also use timeliness detectors to automatically detect and block censorship. This is easy: if a block B with self-declared time t is timely, then any chain that does not include that block (either as an ancestor or as an uncle) before time t + (2k+2)\\delta is automatically ruled non-canonical. This ensures that a chain that censors blocks for longer than (2k+2)\\delta will automatically be rejected by clients.\n\nThe main benefit of using timeliness detectors here is that it creates consensus on when there is “too much” censorship, avoiding the risk of “edge attacks” that are deliberately designed to appear “sufficiently bad” to some users but not others, thereby causing the community to waste time and energy with arguments about whether or not to fork away the censoring chain (instead, most users would in all cases agree on the correct course of action).\nNote that this requires an uncle inclusion mechanism, which eg. eth2 does not have. Additionally, it requires a mechanism by which transactions inside of uncles get executed, so that the censorship resistance extends to transactions and not just the raw bodies of blocks. This requires care to work well with stateless clients.\nOne additional nitpick is that care is required to handle the possibility of many blocks being published and gaining timeliness status and needing to be included as uncles at the same time. This could happen either due to delayed publication or due to a single proposer maliciously publishing many blocks in the same slot. The former can be dealt with via a modified rule that blocks must include either all timely blocks that are older than (2k+2)\\delta or the maximum allowed number (eg. 4) of uncles. The latter can be dealt with with a rule that if one block from a particular slot is included, all other blocks from that slot can be validly ignored.\nNote that in the Casper CBC framework, censorship prevention and de-prioritization of chains containing non-timely or censoring blocks by itself suffices to provide the same finality guarantees as we saw for the FFG framework above.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Resistance",
        "link": "https://ethresear.ch/t/overpass-channels-horizontally-scalable-privacy-enhanced-with-independent-verification-fluid-liquidity-and-robust-censorship-resistance/19756",
        "article": "This research paper presents a novel decentralized payment network model that leverages zero-knowledge proofs (ZKPs) to ensure transaction validity and balance consistency without relying on validators or traditional consensus mechanisms. The network features a fixed token supply, airdropped to participants at inception, eliminating the need for mining and associated costs. The core design focuses on direct mutual verification between sender and receiver, with an extensive exploration of the underlying mathematical foundations, formal proofs, algorithms, and data structures underpinning this system.\nThe proposed payment network aims to address key challenges faced by existing decentralized payment systems, such as high transaction costs, scalability limitations, and privacy concerns. By employing ZKPs and a unilateral payment channel architecture, the network enables efficient, secure, and privacy-preserving transactions without the need for intermediaries or complex consensus protocols. The paper provides a comprehensive analysis of the system’s security, privacy, and scalability properties, along with detailed comparisons to alternative approaches. The underlying mathematical framework and formal proofs are rigorously defined, ensuring the robustness and correctness of the proposed model.\nDecentralized payment systems have garnered significant attention due to their potential to provide secure, transparent, and efficient financial transactions without intermediaries. However, existing solutions often face challenges related to high transaction costs, scalability limitations, and privacy concerns. This research introduces a novel decentralized payment network that leverages zero-knowledge proofs (ZKPs) and unilateral payment channels to address these issues.\nThe proposed network architecture is designed to address specific challenges faced by existing decentralized payment systems:\nThe absence of mining and associated costs solves the issue of high transaction fees and energy consumption in traditional proof-of-work-based systems.\nThe elimination of validators and consensus mechanisms tackles the scalability limitations and potential centralization risks in proof-of-stake and delegated systems.\nThe use of storage partitioning and off-chain payment channels addresses the scalability and privacy concerns associated with storing all transactions on-chain.\nBy distributing a fixed token supply to participants at the network’s inception, the system eliminates the need for mining and its associated costs. The network focuses on enabling direct mutual verification of transactions between senders and receivers, ensuring the validity of transactions and the consistency of account balances without relying on validators or consensus mechanisms. By leveraging zk-SNARKs, the network allows for direct proof of validity between sender and receiver, as the succinct zero-knowledge proofs inherently prove the correctness of transactions.\nTo enhance efficiency and scalability, the network uses a multi-tier Merkle tree system with Merkle proofs, ensuring that only a constant succinct size (O(1)) of data is submitted to the blockchain. This design minimizes on-chain storage requirements and ensures data availability.\nAt the core of this novel payment network lies a comprehensive mathematical framework that leverages zero-knowledge proofs, particularly zk-SNARKs, to validate transactions and generate wallet state proofs. These proofs enable efficient verification of transaction validity and balance updates while preserving user privacy.\nThe network’s architecture is composed of several key components, including unilateral payment channels, hierarchical smart contracts, and partitioned storage nodes. These components work together to enable scalable, secure, and privacy-preserving transactions, while minimizing on-chain storage requirements and ensuring data availability.\nTo ensure the robustness and correctness of the proposed model, the paper presents formal algorithms, theorems, and proofs for crucial aspects of the system, such as the Balance Consistency Theorem and the dispute resolution mechanism. These mathematical formalisms provide a solid foundation for the security and reliability of the payment network.\nFurthermore, the paper includes an in-depth analysis of the network’s security, privacy, and scalability properties, highlighting its advantages over alternative approaches, such as traditional blockchain-based payment systems and centralized payment networks. The analysis also acknowledges potential limitations and challenges, such as the complexity of zk-SNARK implementations and the need for ongoing optimizations.\nThe main contributions of this research can be summarized as follows:\nA comprehensive mathematical framework for ensuring transaction validity and balance consistency using zero-knowledge proofs, particularly zk-SNARKs.\nA detailed description of the network’s architecture, including unilateral payment channels, hierarchical smart contracts, and partitioned storage nodes.\nFormal algorithms, theorems, and proofs for key components of the system, such as the Balance Consistency Theorem, zk-SNARK proof generation, smart contract verification, and dispute resolution.\nAn in-depth analysis of the network’s security, privacy, and scalability properties, along with detailed comparisons to alternative approaches.\nAn exploration of promising use cases that leverage the enhanced privacy features of the proposed system.\nThe proposed decentralized payment network presents a promising approach to enabling secure, private, and scalable transactions in a decentralized setting, paving the way for more efficient and accessible financial services on the blockchain. The extensive mathematical formalism and rigorous analysis provided in this paper contribute to the growing body of research on decentralized payment systems and demonstrate the potential of zero-knowledge proofs in enhancing the security, privacy, and scalability of blockchain-based financial applications.\nDALL·E_2024-06-05_20.49.33_-_A_high-level_overview_of_a_decentralized_payment_network_architecture._The_diagram_includes_Off-Cha1117×970 218 KB\nThis section introduces the key concepts and technologies used in the proposed decentralized payment network, providing a solid foundation for understanding the system’s design and functionality.\nZero-Knowledge Proofs (ZKPs)\nZero-knowledge proofs (ZKPs) are cryptographic protocols that enable one party (the prover) to prove to another party (the verifier) that a statement is true without revealing any information beyond the validity of the statement itself. ZKPs have numerous applications in blockchain technology, particularly in privacy-preserving transactions and scalable off-chain solutions.\nOne prominent type of ZKP is zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge). zk-SNARKs enable the generation of succinct proofs that can be verified quickly, making them well-suited for blockchain applications where proofs need to be stored on-chain and verified by multiple parties.\nDefinition: A zero-knowledge proof for a statement S is a protocol between a prover P and a verifier V such that:\nCompleteness: If S is true, V will be convinced by P with high probability.\nSoundness: If S is false, V will not be convinced by P except with negligible probability.\nZero-Knowledge: If S is true, V learns nothing beyond the fact that S is true.\nIn the proposed decentralized payment network, zk-SNARKs are employed to prove the validity of transactions and generate wallet state proofs, ensuring the privacy and security of user balances while enabling efficient verification.\nThe proposed decentralized payment network consists of several key components: off-chain payment channels, hierarchical smart contracts, and partitioned storage nodes. This section provides a detailed description of each component and their interactions within the overall system.\nCertainly! Here is the document reformatted in Markdown with low-level LaTeX math included:\nUnilateral Payment Channels\nPayment channels are a key component of scalable blockchain solutions, enabling off-chain transactions between parties without the need to record every transaction on the main blockchain. In the proposed network, each user has a unilateral payment channel associated with their wallet contract, which holds their tokens off-chain. This design choice simplifies channel management and enables cross-partition transactions.\nDefinition: A unilateral payment channel between a user U and their wallet contract W is a tuple (B, T_1, T_2, \\ldots, T_n), where:\nB is the initial balance of U in the payment channel.\nT_1, T_2, \\ldots, T_n are the transactions executed within the payment channel.\nThe final state of the payment channel is determined by the cumulative effect of all transactions T_1, T_2, \\ldots, T_n on the initial balance B.\nTo set up a unilateral payment channel, a user creates a wallet contract on the blockchain and transfers the desired amount of tokens to the contract. The wallet contract manages the user’s off-chain balance and state updates through zk-SNARK proofs. When a user wants to transfer tokens to another user, they generate a zk-SNARK proof that verifies the validity of the transaction and includes the necessary metadata for the recipient to generate the next transaction proof. This design enables instant transaction finality and eliminates the need for on-chain confirmation.\nExample 1: Unilateral Payment Channel Setup and Transactions\nSuppose Alice wants to set up a unilateral payment channel with an initial balance of 100 tokens. She creates a wallet contract W_A on the blockchain and transfers 100 tokens to it. The wallet contract initializes Alice’s off-chain balance to 100 tokens.\nLater, Alice decides to send 30 tokens to Bob. She generates a zk-SNARK proof \\pi_1 that verifies the validity of the transaction, including the availability of sufficient funds and the correctness of the updated balances. Upon generating the proof \\pi_1, the wallet contract immediately locks 30 tokens, reducing Alice’s available balance to 70 tokens. Alice sends the transaction details and the proof \\pi_1 to Bob.\nBob verifies the proof \\pi_1 to ensure the transaction’s validity. If the proof is valid, Alice and Bob update their local off-chain balances accordingly. Alice’s balance remains 70 tokens, while Bob’s balance increases by 30 tokens. Both parties sign the proof \\pi_1 to authorize the future rebalancing of their respective payment channels.\nIf Bob does not accept the proof within a specified timeout period, the smart contract automatically releases the locked funds back to Alice’s available balance, ensuring no funds are indefinitely locked.\nThis example demonstrates how unilateral payment channels enable secure, off-chain transactions between users while preserving privacy and scalability.\nOff-Chain Payment Channel Operations\nAs introduced in Section 2, off-chain payment channels form the foundation of the proposed network’s scalability. Each user has a unilateral payment channel associated with their wallet contract, which holds their tokens off-chain. The channel setup and transaction process can be formally described as follows:\nChannel Setup\nHere, U represents the user, W is the wallet contract, and B is the initial balance in the payment channel.\nOff-Chain Transactions\nHere, S represents the sender, R is the receiver, and T is the transaction. The zk-SNARK proof \\pi verifies the validity of the transaction, including the availability of sufficient funds and the correctness of the updated balances. If the proof is valid, both parties update their local off-chain balances and sign the proof to authorize the future rebalancing of their payment channels. If the proof is not accepted within a specified timeout period, the smart contract automatically releases the locked funds back to the sender’s available balance.\nHierarchical Smart Contracts\nThe hierarchical smart contract structure is a key component of the proposed network, enabling efficient rebalancing of payment channels and management of cross-partition transactions. The structure consists of three layers: root contracts, intermediate contracts, and wallet contracts.\n\nRoot Contracts:\nResponsibilities:\n\nServe as the entry point for users and maintain a mapping of intermediate contracts.\nAggregate Merkle roots from intermediate contracts and submit a single, final aggregated Merkle root.\nSubmit the final Merkle root to the blockchain at regular intervals, ensuring the global state is verifiable on-chain with minimal frequency and cost being a constant size O(1).\n\n\nServe as the entry point for users and maintain a mapping of intermediate contracts.\nAggregate Merkle roots from intermediate contracts and submit a single, final aggregated Merkle root.\nSubmit the final Merkle root to the blockchain at regular intervals, ensuring the global state is verifiable on-chain with minimal frequency and cost being a constant size O(1).\n\nIntermediate Contracts:\nResponsibilities:\n\nManage liquidity pools for specific transaction types or user groups.\nMaintain a mapping of wallet contracts and are responsible for rebalancing payment channels based on the transaction proofs submitted by users.\nCollect Merkle roots from wallet contracts and aggregate them into a single Merkle tree within their partition.\nPeriodically submit the aggregated Merkle root to the root contract.\nEnsure the state within their partition is verifiable on-chain with minimal frequency and cost.\n\n\nManage liquidity pools for specific transaction types or user groups.\nMaintain a mapping of wallet contracts and are responsible for rebalancing payment channels based on the transaction proofs submitted by users.\nCollect Merkle roots from wallet contracts and aggregate them into a single Merkle tree within their partition.\nPeriodically submit the aggregated Merkle root to the root contract.\nEnsure the state within their partition is verifiable on-chain with minimal frequency and cost.\n\nWallet Contracts:\nResponsibilities:\n\nRepresent individual user payment channels and hold the users’ off-chain balances.\nGenerate zk-SNARK proofs for their state and submit these proofs to the storage nodes.\nStore proofs to the storage nodes for data availability.\n\n\nRepresent individual user payment channels and hold the users’ off-chain balances.\nGenerate zk-SNARK proofs for their state and submit these proofs to the storage nodes.\nStore proofs to the storage nodes for data availability.\nRoot Contracts:\nResponsibilities:\nServe as the entry point for users and maintain a mapping of intermediate contracts.\nAggregate Merkle roots from intermediate contracts and submit a single, final aggregated Merkle root.\nSubmit the final Merkle root to the blockchain at regular intervals, ensuring the global state is verifiable on-chain with minimal frequency and cost being a constant size O(1).\nIntermediate Contracts:\nResponsibilities:\nManage liquidity pools for specific transaction types or user groups.\nMaintain a mapping of wallet contracts and are responsible for rebalancing payment channels based on the transaction proofs submitted by users.\nCollect Merkle roots from wallet contracts and aggregate them into a single Merkle tree within their partition.\nPeriodically submit the aggregated Merkle root to the root contract.\nEnsure the state within their partition is verifiable on-chain with minimal frequency and cost.\nWallet Contracts:\nResponsibilities:\nRepresent individual user payment channels and hold the users’ off-chain balances.\nGenerate zk-SNARK proofs for their state and submit these proofs to the storage nodes.\nStore proofs to the storage nodes for data availability.\nThe hierarchical structure allows for efficient liquidity management and reduced on-chain transaction costs, as rebalancing operations are performed at the intermediate level, and the root contracts only need to process periodic updates.\nExample 3: Hierarchical Smart Contract Interaction\nContinuing with the previous examples, suppose Alice, Bob, Carol, and David belong to the same user group managed by an intermediate contract IC_1. The intermediate contract IC_1 is mapped to a root contract RC.\n\nWhen Alice sends 30 tokens to Bob (transaction T_1), she generates a zk-SNARK proof \\pi_1. Upon generating the proof \\pi_1, Alice’s wallet contract immediately locks 30 tokens, reducing her available balance accordingly. The transaction proof \\pi_1 is then submitted to the intermediate contract IC_1. The intermediate contract verifies the proof and updates the balances of Alice’s and Bob’s wallet contracts accordingly.\n\n\nSimilarly, when Alice receives 50 tokens from Carol (transaction T_2), Carol generates a zk-SNARK proof \\pi_2. Upon generating the proof \\pi_2, Carol’s wallet contract immediately locks 50 tokens, reducing her available balance. The transaction proof \\pi_2 is then submitted to IC_1, which verifies the proof and updates the balances of Alice’s and Carol’s wallet contracts.\n\n\nPeriodically, the intermediate contract IC_1 submits a summary of the balance updates to the root contract RC, which maintains a global view of the network’s state by submitting a single aggregated Merkle root to the blockchain.\n\nWhen Alice sends 30 tokens to Bob (transaction T_1), she generates a zk-SNARK proof \\pi_1. Upon generating the proof \\pi_1, Alice’s wallet contract immediately locks 30 tokens, reducing her available balance accordingly. The transaction proof \\pi_1 is then submitted to the intermediate contract IC_1. The intermediate contract verifies the proof and updates the balances of Alice’s and Bob’s wallet contracts accordingly.\nSimilarly, when Alice receives 50 tokens from Carol (transaction T_2), Carol generates a zk-SNARK proof \\pi_2. Upon generating the proof \\pi_2, Carol’s wallet contract immediately locks 50 tokens, reducing her available balance. The transaction proof \\pi_2 is then submitted to IC_1, which verifies the proof and updates the balances of Alice’s and Carol’s wallet contracts.\nPeriodically, the intermediate contract IC_1 submits a summary of the balance updates to the root contract RC, which maintains a global view of the network’s state by submitting a single aggregated Merkle root to the blockchain.\nThis hierarchical structure, with the immediate balance locking mechanism, ensures that all transactions are secure and funds are not double spent, even if there are delays in transaction acceptance or verification.\nStorage Nodes and Blockchain Interaction\nTo ensure data availability and scalability, the proposed network employs storage nodes that store the off-chain transaction history and wallet state proofs. Each storage node maintains a copy of the entire off-chain data, ensuring redundancy and decentralization.\nStorage Node Operations:\nStoring Proofs: Storage nodes store zk-SNARK proofs for individual wallet states. Each wallet maintains its own Merkle tree that includes these proofs.\nAggregating Data: At regular intervals, storage nodes aggregate the off-chain data into a single Merkle root, representing the state of all payment channels they manage. This Merkle root is then submitted to the intermediate contracts.\nThe blockchain acts as a secure and immutable ledger, storing the Merkle roots submitted by the root contract. This allows for efficient\nverification of the network’s global state, as any discrepancies between the off-chain data and the on-chain Merkle roots can be easily detected and resolved.\nThis hierarchical structure enables efficient verification of individual payment channels and the entire network state without storing the full transaction history on-chain. By leveraging the security and immutability of the blockchain while keeping the majority of the data off-chain, the proposed network achieves a balance between scalability, data availability, and security.\nExample 4: Storage Node Operation and Blockchain Interaction\nFollowing the previous examples, suppose storage node SN_1 is responsible for storing the transaction proofs and wallet state proofs for Alice, Bob, Carol, and David.\nWhen Alice generates a wallet state proof \\pi_s after transactions T_1, T_2, and T_3, she submits the proof to the storage node SN_1. The storage node stores the proof and updates its local Merkle tree with the new proof.\nSimilarly, when Bob, Carol, and David generate their wallet state proofs, they submit them to SN_1, which stores the proofs and updates its local Merkle tree accordingly.\nAt the end of each epoch, SN_1 generates a Merkle root R that represents the state of all payment channels it manages. The storage node then submits the Merkle root R to the intermediate contract, providing a compact and tamper-evident snapshot of the network’s state.\nThe intermediate contract aggregates the Merkle roots from all storage nodes within its partition and submits a single final Merkle root to the root contract.\nThe root contract aggregates the Merkle roots from all intermediate contracts and submits a single final Merkle root to the blockchain.\nThe blockchain stores the submitted Merkle root, allowing for efficient verification of the network’s global state. If any discrepancies arise between the off-chain data and the on-chain Merkle roots, they can be easily detected and resolved using the dispute resolution mechanism described in the following section.\nThis hierarchical structure, combined with immediate balance locking and zk-SNARK proofs, ensures secure, efficient, and scalable off-chain transactions, maintaining the integrity and security of the overall network.\nTo ensure the validity of transactions and the consistency of account balances, the proposed payment network employs a combination of zero-knowledge proofs and formal mathematical proofs. This section presents the core theorems and algorithms that underpin the system’s security and correctness. (as stated in the abstract, we have a fixed supply released in full at genesis)\nTransaction Validity\nEach transaction in the proposed network is accompanied by a zk-SNARK proof that verifies the following conditions:\nThe sender has sufficient balance to cover the transaction amount.\nThe sender’s updated balance is correctly computed.\nThe receiver’s updated balance is correctly computed.\nLet T_i be a transaction in which sender S transfers \\Delta_i tokens to receiver R. The accompanying zk-SNARK proof \\pi_i ensures the following conditions:\nwhere B_S and B_R are the initial balances of S and R, respectively, and B'_S and B'_R are the updated balances after the transaction.\nBalance Consistency\nTo prove the consistency of account balances in the presence of valid transactions, we present the following theorem:\nTheorem (Balance Consistency): Given a series of valid transactions T_1, T_2, \\ldots, T_n between two parties S and R, the final balances B'_S and B'_R satisfy:\nwhere B_S and B_R are the initial balances of S and R, respectively.\nProof: We prove the theorem by induction on the number of transactions n.\nBase case: For n = 1, we have a single transaction T_1 with amount \\Delta_1. The updated balances after the transaction are:\nAdding the above equations yields:\nInductive step: Assume the theorem holds for n = k transactions. We prove that it also holds for n = k + 1 transactions.\nLet B^{(k)}_S and B^{(k)}_R be the balances after the first k transactions. By the induction hypothesis, we have:\nNow, consider the (k+1)-th transaction T_{k+1} with amount \\Delta_{k+1}. The updated balances after this transaction are:\nAdding the above equations and substituting the induction hypothesis yields:\nTherefore, the theorem holds for n = k + 1 transactions.\nBy the principle of mathematical induction, the theorem holds for any number of valid transactions\nn \\geq 1. \\blacksquare\nThe Balance Consistency theorem ensures that the total balance of the system remains constant throughout a series of valid transactions, providing a fundamental property for the security and correctness of the proposed payment network.\nThe proposed decentralized payment network integrates multiple layers of fraud prevention mechanisms through its hierarchical smart contract system and the use of zk-SNARKs. These measures ensure the integrity and consistency of transaction states, inherently preventing the submission of outdated or fraudulent states. This section outlines how these mechanisms work in detail.\nZK-SNARK Proofs and State Updates\nThe network leverages zk-SNARKs to validate each transaction. The key elements include:\n\nProof of Validity: Each transaction within the network must be accompanied by a zk-SNARK proof. This proof verifies several critical aspects:\n\nThe sender has sufficient balance to cover the transaction.\nThe sender’s updated balance after the transaction is correctly computed.\nThe receiver’s updated balance after the transaction is correctly computed.\n\n\nThe sender has sufficient balance to cover the transaction.\nThe sender’s updated balance after the transaction is correctly computed.\nThe receiver’s updated balance after the transaction is correctly computed.\n\nConsistent State Management: Each user’s wallet contract maintains a Merkle tree of state proofs. Each state update (i.e., each transaction) is validated through zk-SNARKs, ensuring it is consistent with the previously recorded state. This cryptographic validation prevents unauthorized or incorrect state changes.\n\nProof of Validity: Each transaction within the network must be accompanied by a zk-SNARK proof. This proof verifies several critical aspects:\nThe sender has sufficient balance to cover the transaction.\nThe sender’s updated balance after the transaction is correctly computed.\nThe receiver’s updated balance after the transaction is correctly computed.\nConsistent State Management: Each user’s wallet contract maintains a Merkle tree of state proofs. Each state update (i.e., each transaction) is validated through zk-SNARKs, ensuring it is consistent with the previously recorded state. This cryptographic validation prevents unauthorized or incorrect state changes.\nPrevention of Old State Submission\nThe design of the proposed network inherently prevents the submission of outdated or fraudulent states through the following mechanisms:\n\nProof Consistency: Each zk-SNARK proof submitted for a state update must be consistent with the latest recorded state. Intermediate contracts aggregate data from wallet contracts, and root contracts submit these aggregated roots to the blockchain. Any attempt to submit an old state would be detected as it would not match the current aggregated Merkle root.\n\n\nOn-Chain Verification: The final aggregated Merkle root submitted by the root contract is stored on the blockchain, providing a tamper-evident record of the global state. During dispute resolution, the submitted state proofs are verified against this on-chain Merkle root to ensure only the most recent valid state is considered.\n\nProof Consistency: Each zk-SNARK proof submitted for a state update must be consistent with the latest recorded state. Intermediate contracts aggregate data from wallet contracts, and root contracts submit these aggregated roots to the blockchain. Any attempt to submit an old state would be detected as it would not match the current aggregated Merkle root.\nOn-Chain Verification: The final aggregated Merkle root submitted by the root contract is stored on the blockchain, providing a tamper-evident record of the global state. During dispute resolution, the submitted state proofs are verified against this on-chain Merkle root to ensure only the most recent valid state is considered.\nMitigated Need for Watchtowers\nDue to the robust fraud prevention mechanisms built into the proposed system, the traditional need for watchtowers entities that monitor the blockchain for malicious activities and act on behalf of users is significantly reduced. The hierarchical structure and the use of zk-SNARKs ensure that:\nEach state update is cryptographically verified, preventing unauthorized changes.\nThe aggregated Merkle roots provide a consistent and tamper-evident record of the network’s state.\nDispute resolution is handled efficiently and fairly based on the most recent valid state proofs.\nThe comprehensive fraud prevention mechanisms of the proposed decentralized payment network ensure high levels of security and integrity without the need for external monitoring entities like watchtowers. The hierarchical smart contract system and zk-SNARKs work together to maintain consistent and verifiable transaction states, providing a secure and efficient framework for decentralized financial transactions.\nRole of the DAO\nWhile the built-in mechanisms provide robust security and minimize the need for watchtowers, there are scenarios where manual involvement might be necessary. To address these situations, a Decentralized Autonomous Organization (DAO) can be implemented to manage and oversee the network’s operations. The DAO would play a crucial role in:\nHandling Exceptional Cases: Situations that require manual intervention beyond the automated fraud prevention and dispute resolution mechanisms.\nBalancing Automation and Trust: Ensuring the right mix of automated processes, cryptographic proofs, and trust mechanisms to maintain network integrity.\nDemocratic Decision-Making: Leveraging community governance to make decisions on critical issues, such as protocol upgrades, handling disputes that the automated system cannot resolve, and other governance matters.\nThe combination of zk-SNARKs, hierarchical smart contracts, and the DAO creates a robust framework for fraud prevention and network governance. The minimized need for watchtowers is achieved through advanced cryptographic verification and efficient dispute resolution mechanisms. However, the DAO ensures that any issues requiring manual involvement are handled with a balance of automation, trust, rigorous mathematical verification, and democratic decision-making. This comprehensive approach provides a secure, scalable, and trustworthy decentralized payment network.\nDispute Resolution\nIn the event of a dispute between parties, the proposed network employs a dispute resolution mechanism based on the submitted zk-SNARK proofs and Merkle roots. The dispute resolution process can be formally described as follows:\nHere, S and R represent the disputing parties, and \\pi_S and \\pi_R are their respective final state proofs. The dispute resolution mechanism verifies the submitted proofs against the Merkle roots stored on-chain and resolves the dispute based on the validity of the proofs. This ensures that the resolution is based on the most recent valid state of the payment channel, preventing fraud and maintaining the integrity of the system.\nThe dispute resolution process follows these steps:\nDispute Initiation: Either party can initiate a dispute by submitting a dispute request to the relevant smart contract (e.g., the intermediate contract managing their user group).\nEvidence Submission: Both parties are required to submit their final state proofs (\\pi_S \\text{ and } \\pi_R) within a predefined timeframe (e.g., 24 hours). These proofs represent the latest state of their respective payment channels and include the relevant transaction history.\nProof Verification: The dispute resolution mechanism verifies the submitted proofs against the Merkle roots stored on-chain. This verification process ensures that the proofs are valid and consistent with the global state of the network.\nResolution: The dispute is resolved based on the validity of the submitted proofs:\n\nIf \\pi_S is valid and \\pi_R is invalid, the dispute is resolved in favor of party S.\nIf \\pi_R is valid and \\pi_S is invalid, the dispute is resolved in favor of party R.\nIf both proofs are valid, the dispute is resolved based on the most recent valid state proof, determined by the timestamp or sequence number associated with the proofs.\nIf neither proof is valid or if one party fails to submit their proof within the required timeframe, the dispute can be escalated to a higher-level contract (e.g., the root contract) or a trusted third party for manual review and resolution.\n\n\nIf \\pi_S is valid and \\pi_R is invalid, the dispute is resolved in favor of party S.\nIf \\pi_R is valid and \\pi_S is invalid, the dispute is resolved in favor of party R.\nIf both proofs are valid, the dispute is resolved based on the most recent valid state proof, determined by the timestamp or sequence number associated with the proofs.\nIf neither proof is valid or if one party fails to submit their proof within the required timeframe, the dispute can be escalated to a higher-level contract (e.g., the root contract) or a trusted third party for manual review and resolution.\nOutcome Enforcement: Once the dispute is resolved, the smart contracts automatically enforce the outcome by updating the balances of the involved parties according to the resolution decision. This may involve redistributing tokens between the parties’ payment channels or applying penalties for fraudulent behavior.\nTo incentivize honest behavior and discourage frivolous disputes, the network can implement additional mechanisms:\nDispute Bond: Parties initiating a dispute may be required to post a bond (in the form of tokens) that is forfeited if their submitted proof is found to be invalid or if they fail to submit their proof within the required timeframe. This bond serves as a deterrent against malicious actors and ensures that disputing parties have a stake in the resolution process.\nReputation System: The network can maintain a reputation score for each user based on their history of successful transactions and dispute resolutions. Users with a high reputation score may be given preference in case of ambiguous disputes or may enjoy reduced dispute bond requirements. Conversely, users with a history of fraudulent behavior or frivolous disputes may face higher bond requirements or even temporary suspension from the network.\nBy combining cryptographic proofs, smart contract automation, and economic incentives, the proposed dispute resolution mechanism ensures that conflicts are resolved fairly and efficiently while maintaining the integrity of the payment network.\nExample 5: Dispute Resolution\nSuppose a dispute arises between Alice and Bob regarding the final state of their payment channel. Alice claims that her final balance is 100 tokens, while Bob claims that Alice’s final balance is 80 tokens.\nDispute Initiation: Alice initiates a dispute by submitting a dispute request to the intermediate contract IC_1 that manages their user group. She deposits the required dispute bond of 10 tokens.\nEvidence Submission: Alice and Bob submit their respective final state proofs, \\pi_A and \\pi_B, to the dispute resolution mechanism within the 24-hour timeframe. Alice’s proof \\pi_A shows her balance as 100 tokens, while Bob’s proof \\pi_B shows Alice’s balance as 80 tokens.\nProof Verification: The dispute resolution mechanism verifies the submitted proofs against the Merkle roots stored on-chain. It finds that Alice’s proof \\pi_A is consistent with the on-chain state, while Bob’s proof \\pi_B is invalid.\nResolution: As Alice’s proof \\pi_A is valid and Bob’s proof \\pi_B is invalid, the dispute is resolved in favor of Alice. The resolution confirms that Alice’s final balance is indeed 100 tokens.\nOutcome Enforcement: The intermediate contract IC_1 automatically updates the balances of Alice and Bob’s payment channels according to the resolution decision. Alice’s balance remains at 100 tokens, while Bob’s balance is adjusted based on the discrepancy. Additionally, Bob’s dispute bond of 10 tokens is forfeited and distributed as a reward to Alice for submitting a valid proof.\nThis example demonstrates how the dispute resolution mechanism ensures the integrity of the payment network by resolving conflicts based on the validity of the submitted zk-SNARK proofs and the Merkle roots stored on-chain, while also incentivizing honest behavior through the use of dispute bonds.\nComparison to Alternative Approaches\nThe proposed decentralized payment network offers several advantages over alternative approaches, such as traditional blockchain-based payment systems and centralized payment networks.\nCompared to traditional blockchain-based payment systems, the proposed network provides higher scalability and privacy. The use of off-chain payment channels and zk-SNARKs enables faster and more private transactions, while the hierarchical smart contract structure and partitioned storage nodes enable more efficient processing and storage of transaction data.\nCompared to centralized payment networks, the proposed system offers greater security, transparency, and censorship resistance. By leveraging the security and immutability of blockchain technology and the privacy-preserving properties of zk-SNARKs, the network can provide a more secure and transparent payment infrastructure that is resistant to censorship and control by central authorities.\nExample 6: Comparison to Centralized Payment Networks\nSuppose a centralized payment network relies on a single trusted entity to process transactions and manage user balances. While this approach may offer high transaction throughput, it also presents several risks and limitations:\nSingle point of failure: If the central entity experiences technical issues or becomes compromised, the entire payment network may become unavailable or vulnerable to fraud.\nLack of transparency: Users must trust the central entity to manage their funds honestly and securely, without the ability to independently verify the state of their balances or the validity of transactions.\nCensorship risk: The central entity may choose to block or reverse transactions based on their own criteria, censoring users or restricting access to the payment network.\nIn contrast, the proposed decentralized payment network addresses these issues through its use of blockchain technology, zk-SNARKs, and a decentralized architecture:\nDecentralization: The network is maintained by a distributed network of storage nodes and smart contracts, eliminating the single point of failure and ensuring the availability and resilience of the system.\nTransparency and verifiability: Users can independently verify the state of their balances and the validity of transactions using the zk-SNARK proofs and the Merkle roots stored on-chain, providing a high level of transparency and trust in the system.\nCensorship resistance: The decentralized nature of the network and the use of zk-SNARKs ensure that transactions cannot be easily censored or reversed by any single entity, preserving the freedom and autonomy of users.\nThis example highlights the significant advantages of the proposed decentralized payment network over centralized alternatives, providing a more secure, transparent, and censorship-resistant payment infrastructure for users.\nThis section provides a comprehensive analysis of the security, privacy, and scalability properties of the proposed decentralized payment network, and compares it to alternative approaches.\nWe delve into the technical details of the zk-SNARK implementation, discuss potential challenges and trade-offs, explore additional privacy-enhancing techniques, and consider the governance aspects of the system.\nSecurity Analysis\nThe security of the proposed network relies on the soundness and completeness of the zk-SNARK proofs, as well as the integrity of the hierarchical smart contract structure. We employ the state-of-the-art zk-SNARK construction proposed by Groth, which offers succinct proofs and efficient verification. The zk-SNARK scheme is built upon the q-Power Knowledge of Exponent (q-PKE) assumption and the q-Decisional Diffie-Hellman (q-DDH) assumption in bilinear groups.\nLet \\mathcal{G}_1, \\mathcal{G}_2, and \\mathcal{G}_T be cyclic groups of prime order p, and let e : \\mathcal{G}_1 \\times \\mathcal{G}_2 \\rightarrow \\mathcal{G}_T be a bilinear map. The q-PKE assumption states that for any polynomial-size adversary \\mathcal{A}, the following probability is negligible in the security parameter \\lambda:\nThe q-DDH assumption states that for any polynomial-size adversary \\mathcal{A}, the following probability is negligible in the security parameter \\lambda:\nUnder these assumptions, the zk-SNARK construction ensures that the proofs are sound and complete, meaning that a prover cannot create a valid proof for a false statement (soundness) and that a valid proof always verifies successfully (completeness). Consequently, transactions are guaranteed to be valid, and balances are correctly updated, preventing double-spending and other fraudulent activities.\nThe hierarchical smart contract structure, combined with the storage nodes, ensures that the network’s global state remains consistent and verifiable, even in the presence of malicious actors. The smart contracts are implemented using the Solidity language and are formally verified using the Oyente and Zeus tools to ensure their correctness and security.\n1. Collusion during the trusted setup ceremony:\nMitigated by the use of secure multi-party computation (MPC) protocols like ZEXE, ensuring a distributed setup process.\nInvolvement of diverse participants reduces the likelihood of successful collusion.\n2. Collusion among users:\nPrevented by the use of unforgeable and computationally binding zk-SNARK proofs (PLONK), making it infeasible for users to create valid proofs for fraudulent transactions.\nSmart contracts verify proofs before executing transactions, ensuring only legitimate transactions are processed.\n3. Collusion among storage nodes:\nMitigated by the distributed storage architecture with multiple nodes maintaining data copies, making it difficult for nodes to collude and provide false data without detection.\nThe use of Merkle trees and hash-based commitments allows smart contracts to verify data authenticity.\n4. Smart contract vulnerabilities:\nAddressed by formal verification tools, independent security audits, secure coding practices, access controls, and error handling mechanisms.\nUpgradability and emergency stop mechanisms allow for deploying security patches and freezing contracts in case of severe vulnerabilities.\n5. Privacy leaks:\nMitigated by the use of zk-SNARKs, ensuring transaction privacy.\nMixing techniques, anonymity networks, metadata obfuscation, and regular security assessments further enhance privacy protection.\n6. Sybil attacks:\nInherently resistant due to the use of zk-SNARK proofs, smart contract verification, and the underlying blockchain’s consensus mechanism.\nThe system’s design, including proof validity and economic disincentives, makes it infeasible for attackers to create and manipulate multiple identities or payment channels.\nThe requirement of fees to set up payment channels and execute transactions further discourages Sybil attacks by making them financially costly for attackers.\n7. Denial-of-Service (DoS) attacks:\nInherently mitigated by the computational cost of generating zk-SNARK proofs for each transaction, making it impractical for attackers to flood the network with a large number of transactions.\nThe decentralized architecture and the resilience of the underlying Ethereum blockchain provide additional protection against DoS attacks.\nThe proposed decentralized payment network exhibits significant scalability potential due to its innovative use of zero-knowledge proofs (ZKPs), particularly zk-SNARKs, and the absence of traditional consensus mechanisms, which together enable instant finality. In this section, we will provide a detailed mathematical assessment and real-world benchmarks to validate the network’s scalability potential.\nMathematical Formalism of TPS Scalability\nLet us define the total time per transaction T_{tx} as the sum of the time for proof generation T_{pg}, network latency T_{nl}, and the overhead for contract execution and state updates T_{oh}. Given that we aim for high scalability, we will leverage parallel processing capabilities of nodes to handle multiple channels efficiently.\nAssuming average values for these times, such as:\nThe total time per transaction can be approximated as:\nThus, the transactions per second (TPS) per node can be calculated as:\nIf we consider the network scaling linearly with the number of nodes, the total TPS for n nodes can be expressed as:\nFor example, with 100 nodes, the network could achieve:\nTPS Within Channels\nTo further detail the TPS within individual payment channels, consider that each node can manage multiple channels. Let c denote the number of channels a node can handle, and let T_{channel} represent the time to process a transaction within a channel.\nThus, the TPS per channel:\nIf each node can handle c channels, the total TPS per node considering channels would be:\nAssuming a node can handle 10,000 channels:\nFor a network with n nodes, the total TPS could be:\nReal-World Micro Benchmarks\nTo validate these theoretical calculations, we consider benchmarks from existing state channel implementations:\nCeler Network: Claims to handle up to 15,000 TPS per node.\nRaiden Network: Aims for several thousand TPS per node.\nLightning Network: Estimates around 1,000 TPS per node in practical scenarios.\nGiven these benchmarks, our assumption of handling 10,000 channels per node with approximately 14.29 TPS per channel, resulting in 142,900 TPS per node, is ambitious but within a reasonable range for a highly optimized implementation leveraging zk-SNARKs and efficient contract management.\nPotential Bottlenecks\nDespite the promising scalability, several bottlenecks could impact performance:\nProof Generation and Verification: While zk-SNARKs are efficient, the complexity of proofs can increase with advanced use cases.\nNetwork Latency: Global transactions can introduce delays that affect overall throughput.\nSmart Contract Efficiency: Inefficiencies in smart contracts can create processing delays.\nStorage and Data Management: Managing large numbers of channels and associated data could become challenging.\nNode Reliability and Security: Ensuring the reliability and security of each node is critical.\nAddressing these bottlenecks through ongoing optimization and robust infrastructure will be crucial to achieving the theoretical TPS and ensuring the network’s scalability and robustness.\nSummary\nThe proposed decentralized payment network, leveraging zk-SNARKs and instant finality mechanisms, exhibits significant scalability potential. The mathematical formalism and real-world benchmarks indicate that the network can achieve high TPS by efficiently managing multiple channels per node. Continuous optimization and addressing potential bottlenecks will be essential to realizing this potential in practice.\nExisting Layer 2 Solutions\nState Channels (e.g., Lightning Network, Raiden Network):\nScalability: High throughput off-chain.\nFinality: Near-instant off-chain finality.\nChallenges: Requires channel monitoring and on-chain closures.\nPlasma:\nScalability: High throughput with off-chain child chains.\nFinality: Periodic on-chain commitments.\nChallenges: Complex exit management and data availability.\nOptimistic Rollups:\nScalability: Batches transactions off-chain.\nFinality: Delayed due to fraud proof periods.\nChallenges: Requires fraud proof monitoring.\nZK-Rollups:\nScalability: High throughput with off-chain transaction bundling.\nFinality: Near-instant with zk-SNARKs.\nChallenges: Complex proof generation.\nComparative Analysis\nThroughput and Finality:\nThe proposed network achieves high throughput and instant finality, comparable to state channels and ZK-Rollups and superior to Optimistic Rollups.\nEfficiency and Cost:\nMore cost-efficient by reducing on-chain transactions and eliminating mining, outperforming state channels and Plasma.\nData Management:\nEfficient off-chain data management through partitioned storage nodes, similar to Plasma and rollups.\nSecurity and Privacy:\nRobust security and privacy with zk-SNARKs, comparable to ZK-Rollups and superior to solutions relying on fraud proofs.\nThe proposed decentralized payment network is implemented using a combination of Rust, TypeScript, and Solidity. The core components, such as the zk-SNARK proof generation and verification, are written in Rust for performance and security reasons. The smart contracts are developed using Solidity, while the frontend and client-side interactions are built with TypeScript.\nSpecific zk-SNARK Construction\nThe system employs the PLONK zk-SNARK construction, which offers universality, updatability, and efficient proof generation and verification. PLONK allows for the creation of a universal and updateable structured reference string (SRS) that can be reused across multiple circuits or applications, reducing the complexity and coordination overhead associated with repeated trusted setups.\nThe PLONK circuits are designed using the arkworks library in Rust, which provides a set of tools and primitives for building zk-SNARK circuits compatible with the PLONK proving system. The library supports efficient constraint generation, witness computation, and proof generation, making it well-suited for the development of the decentralized payment network.\nChallenges and Optimizations\nOne of the main challenges in implementing PLONK is the complexity of designing and optimizing the circuits to take advantage of the universal SRS. This requires a deep understanding of the PLONK framework and the techniques for constructing efficient and secure circuits.\nTo address this challenge, the implementation leverages various optimization techniques, such as:\nConstraint system optimization: Minimizing the number of constraints in the circuit by using efficient gate design and layout techniques, such as gate aggregation and constant folding.\nWitness compression: Reducing the size of the witness by using compact data representations and eliminating redundant information.\nProof aggregation: Batching multiple proofs together to reduce the overall proof size and verification cost.\nThese optimizations help to improve the performance and scalability of the PLONK-based zk-SNARK circuits, ensuring that the decentralized payment network can handle a high volume of transactions efficiently.\nIntegration with Ethereum\nThe smart contracts for the payment network are implemented using Solidity and deployed on the Ethereum blockchain. The contracts interact with the PLONK proofs generated by the Rust components through a verification contract that is optimized for the PLONK proving system.\nThe verification contract is designed to be gas-efficient and supports batch verification of PLONK proofs, allowing multiple proofs to be verified in a single transaction. This helps to reduce the overall cost and improve the throughput of the system.\nTrusted Setup Ceremony\nAs PLONK requires a trusted setup for the universal SRS, a multi-party computation (MPC) ceremony is conducted to generate the SRS. The ceremony involves multiple participants from different organizations and backgrounds, ensuring that no single party has control over the entire setup process.\nThe MPC ceremony is organized and facilitated using secure computation frameworks, such as the ZEXE library, which provides a set of tools and protocols for conducting distributed key generation and parameter setup.\nIn a private asset transfer, Alice can transfer assets to Bob without revealing the transaction details to the public. Using PLONK, Alice generates a proof π that verifies the validity of the transfer and her sufficient balance without disclosing the transaction amount Δ.\nTransfer T = (A, B, \\pi) where \\pi is the PLONK proof\n\\pi \\leftarrow \\text{generateProof}(A, B, \\Delta)\nSubmit (A, B, \\pi) to the transfer contract\nContract verifies \\pi\n\nIf \\pi is valid, execute transfer from A to B\nElse, reject transfer\n\n\nIf \\pi is valid, execute transfer from A to B\nElse, reject transfer\nThe proof \\pi ensures the following conditions:\nB_A \\geq \\Delta (Alice’s balance is sufficient)\nB_A' = B_A - \\Delta (Alice’s updated balance)\nB_B' = B_B + \\Delta (Bob’s updated balance)\nThe smart contract executes the transfer only if the proof is valid, ensuring the transfer’s legitimacy without revealing the transaction details.\nBy leveraging PLONK, the proposed decentralized payment network achieves a balance between privacy, scalability, and ease of implementation. The universal and updateable nature of PLONK, combined with the optimization techniques and secure trusted setup ceremony, provides a solid foundation for building a privacy-focused and efficient payment system.\nConfidential Voting Systems\nConfidential voting systems are a critical use case for enhanced privacy in decentralized networks. Voting systems must ensure that each vote is anonymous and secure while maintaining the integrity and transparency of the election process. By leveraging zk-SNARKs, our network can provide a solution that guarantees the confidentiality of votes while allowing for public verification of election results.\nIn a confidential voting system built on the proposed network, voters would cast their votes through private transactions, with zk-SNARKs proving that each vote is valid and belongs to an eligible voter without revealing the voter’s identity. The votes would be tallied through a series of confidential transactions, with the final result verifiable through the aggregated Merkle roots stored on-chain. This approach ensures that the voting process is transparent and auditable while preserving the privacy of individual voters.\nPrivate Asset Transfers\nPrivate asset transfers are another significant use case for enhanced privacy in a decentralized network. These transfers require confidentiality to protect the financial privacy of users, ensuring that transaction details remain private while the integrity of the transfer is verifiable.\nWith the proposed network, users can transfer assets through confidential payment channels, with zk-SNARKs proving the validity of the transactions without revealing the amounts or the identities of the parties involved. This feature is particularly valuable for businesses and individuals who wish to keep their financial transactions private, such as in the case of sensitive business deals, wealth management, or personal remittances.\nSecure Health Records Management\nSecure health records management is an essential use case for enhanced privacy, where sensitive health information must be kept confidential while ensuring that authorized parties can verify the records. Using zk-SNARKs, the proposed network can enable the secure storage and sharing of health records while maintaining patient privacy.\nIn this use case, health records would be stored off-chain, with zk-SNARKs proving the authenticity and integrity of the records without revealing their contents. Patients can grant access to their records to authorized parties, such as healthcare providers or insurance companies, through private transactions. The authorized parties can then verify the records’ authenticity using the zk-SNARK proofs, ensuring that the records have not been tampered with while preserving patient confidentiality.\nGlobal Payment System\nA global payment system is perhaps the most scalable and impactful use case for a decentralized network with enhanced privacy. Such a system must provide sufficient privacy to protect user transactions while ensuring transparency and scalability to facilitate mass adoption. By leveraging zk-SNARKs, the proposed network can achieve a balanced privacy level that ensures transaction confidentiality without hindering scalability or regulatory compliance.\nIn a global payment system built on the proposed network, users can transact through confidential payment channels, with zk-SNARKs proving the validity of transactions without revealing the amounts or the identities of the parties involved. This privacy level can be customized based on the specific requirements of different jurisdictions, ensuring compliance with local regulations while still preserving user privacy.\nTo facilitate cross-border transactions and enable seamless interoperability with existing payment systems, the network can integrate with traditional financial institutions and payment processors through secure off-chain communication channels. These channels can leverage zk-SNARKs to prove the authenticity of transactions and balances without revealing sensitive information, enabling a hybrid approach that combines the benefits of decentralized privacy with the reach and stability of established financial networks.\nBy leveraging zk-SNARKs in these use cases, the proposed decentralized payment network can provide enhanced privacy and scalability, making it suitable for a wide range of applications. These examples illustrate how the network can achieve a balance between privacy and transparency, facilitating mass adoption while maintaining the necessary confidentiality.\nThe proposed decentralized payment network offers:\nHigher Throughput: Comparable to or exceeding state channels and rollups.\nInstant Finality: Superior to Optimistic Rollups.\nCost Efficiency: Reduces on-chain interactions and eliminates mining.\nEnhanced Privacy: Matches or surpasses ZK-Rollups.\nThe unique combination of features in the proposed network makes it a potentially more scalable and private solution compared to existing Layer 2 systems.\nBy leveraging zk-SNARKs in these use cases, we can provide enhanced privacy and scalability, making our decentralized network suitable for a wide range of applications. These examples illustrate how the network can achieve a balance between privacy and transparency, facilitating mass adoption while maintaining the necessary confidentiality.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "On the gas efficiency of the WHIR polynomial commitment scheme",
        "link": "https://ethresear.ch/t/on-the-gas-efficiency-of-the-whir-polynomial-commitment-scheme/21301",
        "article": "Joint post with @WizardOfMenlo\nTLDR; We developed an open-sourced and MIT licensed prototype EVM verifier for the WHIR polynomial commitment scheme (PCS). For a multivariate polynomial of 22 variables and 100 bits of security, verification costs are 1.9m gas. With a more aggressive parameter setting, we reach even lower costs, below the 1.5m gas mark. This makes WHIR a serious post-quantum PCS candidate for teams using or looking to leverage STARKs in production.\nWHIR\nWHIR is a multilinear PCS requiring only a transparent setup and guaranteeing post-quantum security, while achieving high verification speeds. We wanted to estimate how high verification speed would translate in terms of gas usage, when verifying WHIR proofs on the EVM. To this end, we developed a prototype WHIR verifier and benchmarked it at various parameters levels.\nOur prototype implementation supports various settings, with a folding factor of at most 4 - this is an implementation idiosyncrasy stemming from gas optimizations and can be easily modified. While we present results for a specific set of parameters, our verifier is open-sourced and MIT-licensed, letting anyone test it with different configuration requirements.\nGas saving strategies\nWHIR is equipped with a variety of parameters and lets users choose how much work the prover should carry out in the benefit of the verifier. We quickly review here four important ones:\nThe chosen code rate \\rho will impact the argument size and hence verifier time, at the expense of prover time. In concrete terms, with WHIR, the number of queries (i.e. requests for evaluations of a polynomial) a verifier makes tends to 0 as \\rho approaches 0. This means that each saved query translates into saved gas costs.\nThe folding factor k determines the number of rounds between the prover and verifier. A higher k means fewer rounds, but additional prover work.\nThe amount of proof of work grinding[1] lets the verifier decrease the number of authentication paths it will demand from the prover. Higher grinding levels translates into reducing both calldata and verifier execution costs.\nWe can also tune WHIR’s security parameter \\lambda directly. In practice, this means that higher \\lambda values will result in increased verifier queries and gas costs.\nIndependently from WHIR, one final technique we can leverage is masking. It allows to save calldata by masking the output of the last bytes of the merkle tree hash function[2]. For the sake of completeness, we included an implementation of this verifier working with a (naive) version of this industry-standard technique. We recall that masking is specifically beneficial when achieving \\lambda \\le 128, as to get to a \\lambda security level, we need our hash function digest size to be at most 2*\\lambda.\nResults\nIn this work, we show how the WHIR verifier performs with different starting rates \\rho, folding factors k, and \\lambda bits of security, for randomly sampled polynomials of 16, 20 and 22 variables and instantiated with the capacity bound conjecture. We will plot how each of those parameters impact gas costs relative to the chosen level of pow grinding and show how to reach low gas costs when using WHIR.\nWe chose our experiment parameters to be in line with what we consider to be useful levels of practicality and security. Our gas results are obtained from computing an average transaction gas cost using foundry forge and anvil at version 0.2.0 - commit e10ab3d.\nWe start with adjusting for the code rate and plot the impact of the chosen rate on the verifier’s gas usage. We can see the impact of choosing the correct starting code rate \\rho is quite large. For 20 variables and pow grinding at 30 bits, we can cut gas costs in half when going from \\rho=2^{-1} to \\rho=2^{-6}. Hence, for the rest of our experiment, we set a code rate \\rho=2^{-6}. Also, we omitted for the sake of clarity results for polynomials with 22 variables. We will now start to plot results for it as well.\n\nOur next parameter of interest will be the folding factor k. In the case where we have a polynomial composed of 22 variables and pow grinding set to 30 bits, we end up with a x1.75 increase in gas costs when using a lower folding factor k=2 instead of k=4. Hence, we will now set k=4.\n\nNext, we tune our \\lambda parameter to a more aggressive level of security. We plot how setting \\lambda=80 further decreases our gas costs. Clearly, removing 20 bits of security results in large additional savings. In the case of 22 variables and 30 bits of grinding, each removed bit of security translates into more than 10k of saved gas.\n\nFinally, we apply a naive masking strategy for our calldata. Since we set \\lambda = 80, we mask commitments to 160 bits - a similar level to what starkware runs (or has been running) its verifier. Combined with \\rho=6, k=4, we get to our final and rather competitive gas costs for our WHIR verifier:\nFor 16 variables, our naive implementation of this strategy saves us an average additional 25k gas. We end with an average total gas cost of 1091720 gas, just above the 1m mark.\nIn the case where our multivariate  polynomials are of size 20 and 22, we can save an additional ~35k gas. We end up with average total gas costs of respectively 1388230 and 1493552 gas, below the 1.5m gas mark.\nFor comparison, verifying groth16 and fflonk proofs costs around 300k gas today. However, WHIR is transparent and post-quantum. Such gas costs illustrate why we think WHIR is a pretty good option when not only compared to protocols with similar assumptions like FRI, but also to trusted setup based ones.\nFuture directions\nFirst, we developed a prototype verifier. We are confident that additional optimizations lie ahead. One of the low hanging fruit being that there still are parts of our code that would benefit from assembly re-writes.\nFor 22 variables, with \\lambda=80, \\rho=1/2^6, k=4, 30 bits of grinding and masking, our average gas cost is almost 1.5m gas. However, we expect a total optimized gas cost averaging around 1.2m-1.3m gas. Indeed, our current (average) costs breakdown consists into roughly: 700k gas for calldata, 250k gas for merkle proving, 80k gas for computing STIR challenges, 65k gas for the sumcheck iterations, 60k gas for fiat shamir utilities (pop from transcript, pow checking, …) and 25k gas for various uni/multivariate evaluations. Hence, we estimate remaining glue code (paid in memory expansions, overheads from using abstractions, …) to take around 300k - 250k gas, which an assembly rewrite would help reduce.\nAlso, our implementation of masking has been somewhat naive, as it simply consisted into zeroing the last 12 bytes of our  merkle decommitments. We would be happy to integrate an improved version of this masking strategy to not pay anything for masked values. On a related front, we carried out a bespoke multi merkle proof assembly implementation hailing from Recmo’s initial solidity version. Multi-merkle proofs make up for ~20% of gas costs of the verifier. We would be happy to see further algorithmic improvements here as we suspect our version to be far from perfect.\nFinally, WHIR isn’t limited to a particular field. We would be curious to see how those numbers fare in the context of different, smaller fields. In particular, we expect our field operations to bear less gas costs compared to when working with the BN254 scalar field.\nAcknowledgements\nThanks to @alxkzmn for helping with the merkle and pow modules in an early version of this work.\nFor a primer on grinding, see section 3.11.3 of the ethSTARK paper. ↩︎\nGas savings obtained from this approach might be subject to change in the future, see EIP 7623. ↩︎\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "Parallelizing Ethereum: A Novel Approach Using Historical Data and Transactional Data",
        "link": "https://ethresear.ch/t/parallelizing-ethereum-a-novel-approach-using-historical-data-and-transactional-data/20590",
        "article": "Currently at max, Ethereum TPS is maxed out 12-15 TPS  compared to VISA(Visa does 25k up to 65k per seconds btw) that’s like way too low, now of course one of the main reasons this is happening is due to lack of parallelization on Ethereum but what if there was a way we could parallize Ethereum, in my current research which is still in work, I propose two ways to parallelize Ethereum transactions using two methods:\n1. Historical data\n2. Transactional data\nHistorical data: when you think about historical data, think about branch prediction your compiler uses. This would involve analysing past transactions from popular contracts that are not proxies or dynamic in nature. For example, think about Uniswap v3 contracts, non-upgradeable and highly predictable due to the number of times it has been executed there’s no reason we can’t predict possible states a popular or highly executed non-upgradeable smart contract can interact with, this is how it works: based on how highly executed contracts have behaved previously (which states they accessed), the system could predict which states future transactions will likely interact with.\nTransactional data: 3 methods and 3 ways, ever heard of static analysis, using a transaction method, the calldata and the contract bytecode there is way we can predict the expected states a transaction will cause effect\nHow the parallelization will work:\n\nBlock Builders can perform this analysis before proposing blocks. Once the transactional or historical data is analysed, the block builder can organize the transactions into groups or clusters that can be parallelized. Each cluster represents transactions that do not conflict with each other.\n\n\nBlock Proposal: After grouping transactions into parallelizable clusters, the block builder can propose a block in a way that minimizes execution time, potentially allowing multiple processors or threads to handle different clusters in parallel.\n\nBlock Builders can perform this analysis before proposing blocks. Once the transactional or historical data is analysed, the block builder can organize the transactions into groups or clusters that can be parallelized. Each cluster represents transactions that do not conflict with each other.\nBlock Proposal: After grouping transactions into parallelizable clusters, the block builder can propose a block in a way that minimizes execution time, potentially allowing multiple processors or threads to handle different clusters in parallel.\nDependent Transactions:  let’s assume we have two transactions (y1, y2) that depend on each other, parallelization of this will be extremely difficult, a popular example of this would be a transaction to approve a number of tokens for spending(y1) and another one to actually transfer the token by a smart contract, trying to parallelize this will throw an error. A common way to solve this is not to parallelize transactions or use a single processor to process these transactions, how do we identify these transactions? quite simple, we can use Tx.origin and msg.sender to identify these types of transactions\nInternal transactions that are inter and intra dependent: The only way to solve these types is to implement a type of algorithm I call TGA (Transactional Graph Analysis). What’s TGA,\nIndependent transactions: These can be executed in parallel across different processors.\nDependent transactions: These must be executed sequentially, based on their dependencies.\nExample:\nLet’s say a block has 5 transactions:\nTx1: Transfers ETH from Alice to Bob.\nTx2: Approves a DAI transfer for a DeFi contract.\nTx3: Transfers DAI from Alice to the contract (depends on Tx2).\nTx4: Changes a parameter in a governance contract.\nTx5: Reads a balance from the governance contract (depends on Tx4).\nUsing TGA:\nTx1, Tx2, and Tx4 are independent (no edges between them), so they can be parallelized.\nTx3 depends on Tx2 and must be executed after it.\nTx5 depends on Tx4, so it must be executed after Tx4.\nOf course, a better method will be to implement timestamps for each transaction \nI also think this is more suitable to a L2 \n",
        "category": [
            "EVM"
        ],
        "discourse": [
            "layer-2"
        ]
    },
    {
        "title": "Tasklist for post-quantum ETH",
        "link": "https://ethresear.ch/t/tasklist-for-post-quantum-eth/21296",
        "article": "Background\nCryptographically relevant quantum computer, if built, could enable Shor’s algorithm and Grover’s algorithm. These completely break ECDSA / ECDH, and reduce hash function (& cipher) strength from 2^n to 2^\\frac{n}{2}\nThere are some non-obvious parts of ETH which would need to be upgraded.\nWhat is still OK\nbip39 (pbkdf2-sha512) seems just fine\neip2333 validator withdrawal keys are also fine!\nFinal thoughts\nI’m confident all of these problems can be solved even in limited time. Let’s start solving them.\nIf such computer appears soon, it’s possible to do Vitalik’s trick (How to hard-fork to save most users’ funds in a quantum emergency): freeze all accounts and leverage BIP39 with ZK-proofs to recover funds into new pq scheme.\n",
        "category": [
            "Architecture"
        ],
        "discourse": [
            "post-quantum",
            "signature-aggregation"
        ]
    },
    {
        "title": "On Increasing the Block Gas Limit: Technical Considerations & Path Forward",
        "link": "https://ethresear.ch/t/on-increasing-the-block-gas-limit-technical-considerations-path-forward/21225",
        "article": "Authored by: Toni, Marek, Pari, Jacek, Paul, Tim and Alex.\nAuthors’ Note:\nThe core development community is committed to continuous improvement of the network’s scalability and user experience. With recent community-driven initiatives, such as pumpthegas.org, there has been a growing call to increase Ethereum’s block gas limit with some proposals approaching 60 million. While this enthusiasm reflects the shared goal of expanding Ethereum’s capacity, it is important to proceed deliberately and in harmony with the technical realities of the protocol and its clients. Before encouraging the community to actively signal for limits beyond 36 million, we may want to deepen our understanding of the potential consequences—conducting more analysis, collecting empirical data, and examining results of upcoming protocol changes in the greatest detail possible—so that adjustments are made with both confidence and caution.\nContext\nThe consensus-layer (CL) clients currently implement certain constraints, as specified by the formal specifications. These constraints include a maximum acceptable uncompressed block size for gossip propagation, currently set to 10 MiB. In practice, this indirectly influences the maximum feasible block gas limit. Today, raising the gas limit to 60 million gas, as proposed by some community members, would generate blocks that exceed this gossip constraint—leading to missed slots and overall network instability.\nUntil these client-level assumptions can be revisited and improved, the network should move forward with caution when considering increases beyond certain thresholds.\nThese constraints are not arbitrary; they are in place to safeguard the network. Extremely large blocks can facilitate potential DoS vectors by forcing nodes to handle unwieldy amounts of data. Without practical use cases for such large blocks—and with the risk of malicious actors exploiting them—the core developers have designed limits to mitigate negative effects and protect the network’s health.\nFunctionality up to ~40M gas:\nBlocks at or below this level remain within the acceptable size range, allowing clients to propagate them and maintain consensus stability. This ensures that validators do not see unexpected missed slots due to overly large blocks which would be prevented from being propagated because of gossip limits.\nBeyond ~40M gas:\nValid blocks larger than 10 MiB could fail to propagate as expected. This results in some validators missing their slots despite producing otherwise valid blocks. The gossip limits, which cannot be easily circumvented today, create a bottleneck. In addition, without further empirical data, the initial analyses guiding the blob count increases may not fully reflect the increased complexities of operating under a significantly higher gas limit.\nWhy Wait for Pectra?\nThe core developers have been planning the Pectra network upgrade that reduces worst-case block sizes and create the headroom needed to safely increase capacity. Two notable upcoming changes are:\n\nEIP-7623 (Included in Pectra):\nThis proposal aims to reduce worst-case block sizes. By increasing the cost of calldata for calldata-heavy transactions, it opens pathways to safely handle more capacity—be that additional blobs or a higher gas limit. Reducing worst-case scenarios mitigates potential DoS vectors and helps ensure that the network remains stable and resilient under heavier loads.\n\n\nEIP-7691 (Included in Pectra):\nThis proposal will increase the target/maximum number of blobs per block from 4/6 to 6/9. By observing the network’s performance under increased blob counts, we can gather data on propagation behavior, storage demands, and client resource usage. This empirical evidence will guide safer adjustments in block composition and size.\n\nEIP-7623 (Included in Pectra):\nThis proposal aims to reduce worst-case block sizes. By increasing the cost of calldata for calldata-heavy transactions, it opens pathways to safely handle more capacity—be that additional blobs or a higher gas limit. Reducing worst-case scenarios mitigates potential DoS vectors and helps ensure that the network remains stable and resilient under heavier loads.\nEIP-7691 (Included in Pectra):\nThis proposal will increase the target/maximum number of blobs per block from 4/6 to 6/9. By observing the network’s performance under increased blob counts, we can gather data on propagation behavior, storage demands, and client resource usage. This empirical evidence will guide safer adjustments in block composition and size.\nBy first deploying the Pectra hardfork and analyzing the outcomes of EIP-7623 and EIP-7691 in a production environment, we stand to gain critical empirical evidence. This data will inform both core developers and the broader Ethereum community on how the network responds to changes in block composition and size. Armed with this understanding, the community can make more informed decisions on how to increase the gas limit while maintaining Ethereum’s robustness and security.\nFuture upgrades, such as PeerDAS, will build on these insights, further refining parameters and scaling capabilities as the network evolves.\nA Call for Patience and Collaboration\nThe Ethereum community’s proactive approach and passion for scaling solutions is commendable. Core developers are keenly aware of this momentum and, in general, are supportive of finding a responsible path to increasing the gas limit. However, moving too quickly—especially beyond 36M gas—risks unintended consequences and network instability.\nWe encourage all stakeholders—users, validators, researchers, and client developers—to remain patient and work together through this transition.\nBy deferring significant capacity increases until after the Pectra hardfork, monitoring the real-world effects of EIP-7623 and EIP-7691, and carefully reviewing the results, we can ensure that these increases are implemented responsibly and sustainably.\nWhile many sympathize with the desire to see Ethereum’s gas limit significantly increase over a short period, a more incremental approach might be sounder. For instance, starting with a moderate increase to around 36M gas would allow us to carefully monitor the network’s response, assess client performance, and ensure that no unforeseen issues arise. If the data supports further increases, we could then proceed more confidently to higher limits while maintaining the network’s stability and security.\nFinally, we may also anticipate further updates and guidance from core developers in the coming days/weeks as they work towards resolving these issues.\n",
        "category": [
            "Execution Layer Research"
        ],
        "discourse": []
    },
    {
        "title": "Block Arrivals, Home Stakers & Bumping the blob count",
        "link": "https://ethresear.ch/t/block-arrivals-home-stakers-bumping-the-blob-count/21096",
        "article": "Thanks to all the community members that are selflessly sharing their data that allowed this analysis to be possible, to MigaLabs for their validator entity data and to ProbeLab for an early version of their Mainnet bandwidth report.\nSpecial thanks to Parithosh, Toni, Mikel, Andrew, and Matty for their feedback and help throughout this analysis.\nIntro\nThe ethPandaOps team has recently started receiving data from members of the community. Home stakers are one of the Ethereum network’s most valuable assets, and this scheme is starting to shine a light on how they see the network. A sidecar binary is run alongside a beacon node, and records the events happening on the node via the beacon API event stream. These events are sent to the ethPandaOps team, who then publishes the data (with a small delay & some data scrubbing).\nFor more info:\nData collection\nAccessing the data\nThe team has been collecting community data for around 6 weeks, and we now have enough data to make some interesting observations that were previously not possible.\nBackground\nWith the arrival of EIP4844, a block is only considered valid once the block & the blobs referenced in the block are received by a node. This block/blob bundle has until 4s in the slot to be received by the majority of the network otherwise it runs the risk of being re-orged.\nSophisticated operators (and now MEV-Relays) play block proposal timing games. These operators submit their blocks as late as possible to maximise their profit while minimizing their risk of being re-orged. These timing games have historically obfuscated block arrival timing data.\nUnreleased, upcoming research from ProbeLab indicates that the 50th percentile of non-cloud nodes on the network have 20Mbps of upload bandwidth.\nBlob usage on Mainnet continues to grow.\nProblem statement\nEthereum’s decentralization is paramount to it’s success. EIP7691 aims to increase the blob count, and runs the risk of unintentionally excluding some node operators from it’s set if the parameters are too high.\nWe need to:\nEnsure that a blob count increase is safe for home stakers, as this group of actors is the “worst case” for a blob count increase as they have the lowest available bandwidth.\nEnsure that the network has enough data throughput to support Layer 2 growth.\nGiven the existing landscape, we can make some assumptions with regards to a potential blob count increase:\nLeast at risk:\nCounterintuitively, if you looked at block arrival data, operators playing timing games would appear to be the most at risk of being impacted by a blob count increase. Being reorged out for proposing late is bad for business, and we can assume they’ll adjust their block proposal timings accordingly. A blob count increase is unlikely to be problematic here.\nMost at risk:\nA solo staker building a block locally (no mev-relay) and being attested to by other home stakers. In this scenario:\nThe proposer:\n\nneeds to publish their block and all blobs to the network. This node needs to publish the block (~100KB), and then all of the blobs (128KB each) to all of its mesh peers as fast as possible.\n\nwhen building blocks locally, the proposer does not have the help of the MEV Relay gossiping the block/blobs bundle to its own peers.\n\n\n\n\nneeds to publish their block and all blobs to the network. This node needs to publish the block (~100KB), and then all of the blobs (128KB each) to all of its mesh peers as fast as possible.\n\nwhen building blocks locally, the proposer does not have the help of the MEV Relay gossiping the block/blobs bundle to its own peers.\n\n\nwhen building blocks locally, the proposer does not have the help of the MEV Relay gossiping the block/blobs bundle to its own peers.\nThe attesters:\n\nneed to download the block/blobs bundle from the p2p network before 4s in to the slot.\n\n\nneed to download the block/blobs bundle from the p2p network before 4s in to the slot.\nThis analysis will ask the following questions:\nQuestion 1: How is 3/6 performing on Mainnet?\nQuestion 2: Does arrival time scale with block/blob bundle size?\nQuestion 3: How much more can we support on Mainnet today?\nWe’ll answer these questions from the perspective of a home staker as this is our most at-risk group of operators.\nCheck out the juypter notebook here.\nThe timing data was captured by Xatu Sentry, a sidecar binary that runs alongside a beacon node and records many events via the beacon API event stream. These events can be considered worst case for block arrival times as there is additional processing time and network overhead for the event to be recorded. From analysis, this overhead ranges between 50-300ms but we have kept the timing data as-is in the interest of safety.\nEach beacon node implementation emits block, head, and blob_sidecar events in different orders. To address this, we define a block/blob bundle as “arrived” only after all associated events for the slot have been recorded from each beacon node. This is once again a worst case scenario.\nQuestion 1\nHow is 3/6 performing on Mainnet?\nTL;DR: Pretty well!\n1166×804 115 KB\nThis chart shows block/blob bundle seen arrival times against the combined size of the bundle. When looking at locally built blocks proposed by solo stakers and seen by home users, a lot of block/blob bundles are seen before 4s.\n1166×804 118 KB\nWe should also look at blocks provided by MEV Relay as the reality is that a lot of blocks are proposed via this route. We can see an increase in the min, as there are additional round trips involved in this process, but things still look healthy!\nOutcome: Block/blob bundles are arriving well within the 4s deadline for our home users.\nQuestion 2\nDoes arrival time scale with block/blob bundle size?\nTL;DR: Yes\n3530×2411 650 KB\nThe trend lines show the 99th, 95th, and 50th percentiles of arrival times - meaning what percentage of blocks arrive were seen faster than that line.\nThese percentile trend lines answer our question: as bundle sizes increase, arrival times also increase. This suggests bandwidth is the primary bottleneck for these actors.\n3530×2411 651 KB\nAgain looking at blocks provided via MEV Relay, we see a similar story.\nOutcome: Yes, arrival times scale with block/blob bundles size\nQuestion 3:\nHow much more can we support on Mainnet today?\nTLDR: 4/8 and 6/9 are both achievable\nTo answer this question we need to check how big the block is. The 99th percentile of compressed beacon block size through our time period is 101KB. Our blobs are fixed at a size of 128KB.\nUsing these parameters, we can overlay the block/blob count:\n3530×2411 918 KB\nWe can very naively plot a trend line to see when would cross the 4s attestation deadline.\n3530×2411 1 MB\nThis trend line assumes a linear relationship between blob count and arrival time. Under this assumption, we can support up to 14 blobs per block while maintaining 95% of block/blob bundles arriving within the deadline. The 95% target provides margin for the 50-300ms processing overhead in our measurements, while also accounting for outliers.\n3530×2411 1020 KB\nWhen looking at MEV Relay blocks specifically, the data shows an even more optimistic picture - the 95th percentile trend line indicates support for up to 20 blobs per block. This improved performance can be attributed to MEV Relays being high-bandwidth nodes that help distribute blocks and blobs across the network in parallel with the proposer.\nEIP7623 improves the worst case compressed block size to ~720KB - about 7x larger than our average historical block. Let’s analyze if we can still support more blobs with this increased block size.\n3530×2411 967 KB\nEven with an absolute worst case block size with EIP7623 we still support a blob increase. Note that the current maximum compressed block size on Mainnet is 1.79MB (and we’re seemingly going ok!), so take this data point with a grain of salt.\n1175×804 208 KB\nThe trend for MEV Relay blocks again supports a much higher blob count compared to locally built blocks.\nOutcome: The data supports a blob count increase, especially if EIP7623 is included at the same time. 4/8 or 6/9 are both safe to apply. There is potential for a higher blob count, but we’ll need to see how the network performs with an initial bump first.\nConclusion\nEthereum’s decentralization is fundamental, and home stakers play a crucial role in this picture. The network is a delicate and intricate system that demands thoughtful and deliberate consideration.\nOur analysis indicates that block arrival performance surpasses initial expectations for nodes with limited bandwidth. The community-contributed data offers valuable real-world insights into the network’s capabilities, and we would like to once again thank those who are contributing their data.\nWhile we naively assumed a linear relationship between blob count and arrival time, this is a simplified view of a highly complex distributed system. Additionally, there are ongoing work streams that could either improve or worsen bandwidth requirements over time. Our analysis is focused on the data available to us now, based on observations from the past six weeks of network performance.\nBased on block/blob arrival metrics alone, increasing the blob count from (target 3/max 6) to (target 4/max 8) or (target 6/max 9) appears to be viable. However, this is just one of many factors to evaluate when deciding on blob count adjustments.\n",
        "category": [
            "Sharding"
        ],
        "discourse": [
            "scaling",
            "data-availability"
        ]
    },
    {
        "title": "On solo staking, local block building and blobs",
        "link": "https://ethresear.ch/t/on-solo-staking-local-block-building-and-blobs/20540",
        "article": "791×785 121 KB\nIn recent weeks, discussions about a potential increase in blob throughput in Pectra have intensified, with two distinct groups emerging. One advocates for the increase, while the other is hesitant, preferring to wait for clear data supporting such a change.\nFrom my perspective, one sentiment is overwhelmingly clear within the community:\nWhile there hasn’t been a consensus on the minimum requirements for validators (see sassal.eth’s tweet on that), the Ethereum community has made one thing clear:\nIn my view, this reflects a healthy direction for Ethereum and underscores the community’s view on the importance of viable solo staking.\nHowever, it raises an important question: “Where is the line?”\nSpecifically, at what point does the contribution of a weaker, lower-bandwidth staker to decentralization no longer justify the limitations it imposes on Ethereum’s ability to scale?\nIn this piece, I aim to provide additional data points to help the community make an informed decision on whether we want to pursue a blob throughput increase in Pectra.\nAs Potuz, a core developer from Prysm, aptly stated, the real question is not “Do we want to scale, and how?” but rather, “Are we ready to do so now?”\nWho is being reorged today?  (Oct 2023 - Oct 2024)\n1000×500 33.8 KB\nOn average ~0.2% of blocks are reorged (=reorged ⊆ missed).\nProfessional node operators (NOs) such as Lido, Kiln, Figment, and EtherFi are reorged less often than the average.\nLess professional NOs such as solo stakers, Rocketpool operators, or the unidentified category which likely includes many solo stakers that couldn’t be identified, are more frequently reorged.\nAs shown in an earlier analysis, the reorg rate has been trending down since the Dencun hardfork.\nIn the following chart, we can see that this effect was different for different entities:\nreorgs_entites_over_time (3)1000×500 53.4 KB\nThe reorg rate decreased for solo stakers and unidentified since Dencun.\nThe same applies to Rocketpool operators, as well as larger operators such as Lido, Coinbase, Figment, and OKX.\nWhat about local block building? (Oct 2023 - Oct 2024)\n1000×500 17 KB\nLocal builders have a reorg rate of approximately 1.02%.\nMEV-Boost builders have a reorg rate of approximately 0.20%.\nLocal builders are approximately 5 times more likely to be reorged than MEV-Boost builders.\n\n1000×500 32.9 KB\nThe reorg share for local block builders seems to have remained constant or even increased after the Dencun hardfork.\nFor MEV-Boost users, reorgs have been trending down since Dencun.\nNotably, previous analysis showed that local builders included on average more blobs into their blocks. Furthermore we have seen that right after the Dencun hardfork blocks with 6 blobs struggled a bit, but this eventually stabilized again. This might explain why the reorg rate didn’t decrease for local builders.\nWho are the local builders? (Oct 2023 - Oct 2024)\n1200×300 13.7 KB\nSolo stakers (here labeled as “solo staker” but with many solo stakers in the unidentified category) are the largest entity within the “local builder” category.\nFurthermore there are Lido NOs that are not using MEV-Boost at all or use the min-bid flag.\nMultiple factors can lead to reorgs, making it challenging to pinpoint exactly why certain validators, like solo stakers, experience them more frequently than others.\n",
        "category": [
            "Sharding"
        ],
        "discourse": [
            "data-availability",
            "mev"
        ]
    },
    {
        "title": "Zero-knowledge proofs of identity using electronic passports",
        "link": "https://ethresear.ch/t/zero-knowledge-proofs-of-identity-using-electronic-passports/19263",
        "article": "282155110-514ae671-3c02-434f-ac6a-31ce20eec24d1792×401 181 KB\nMany applications need to verify their user’s identity online, whether it is nationality, age, or simply uniqueness. Today, this is hard. They are stuck between shady heuristics like tracking IP addresses and technologies like Worldcoin that need to deploy their infrastructure widely.\nFortunately, UN countries in association with the International Civil Aviation Organization have built a great tool for us to piggyback on: electronic passports. They are issued by more than 172 countries and include an NFC chip with a signature of the person’s information, including name, date of birth, nationality and gender. Issuing countries make their public keys accessible in online registries, enabling the verification of signatures.\nA circuit for passport verification\nFor someone to prove their identity using a passport, they will have to do two things. First, read the content of their passport’s chip. This can be done easily with any NFC-enabled phone. Then, show a verifier that their passport has been correctly signed. Instead of sending all of their personal data for the verification to happen, they can generate a zero-knowledge proof that redacts some of their inputs.\nOur circuit will have to checks two things:\nThe disclosed attributes have been signed correctly\nThe corresponding public key is part of the public key registry of UN countries\nA simple circuit compliant with the electronic passport specs would look something like this:\nHere is roughly what happens:\nEach datagroup stored in the passport contains some of the person’s information. The datagroups we are most interested in are the first one (nationality, age, etc) and the second one (photo). The circuit takes them as inputs along with the signing public key.\nDatagroups are hashed, concatenated and hashed again.\nThe final result is formatted, hashed and signed by the country authority. We can use the public key to check this signature.\nThis makes the following attributes disclosable: name, passport number, nationality, issuing state, date of birth, gender, expiry date, photo.\nSome countries also provide additional data like place of birth, address, phone number, profession and a person to notify. Biometrics like fingerprint and iris are sometimes included but can’t be retrieved, as they require a special access key.\nIn practice, we want our circuit to have a few other features:\nInstead of passing the country’s public key directly, we want the user to prove that the public key that signed their passport is part of the registry published by the ICAO. This can be done by passing a merkle proof of inclusion and having only the merkle root as a public input.\nTo allow for selective disclosure of any attribute, we pass a bitmap as a public input that will redact some of the attributes.\nWe want specific modules for age disclosure and nationality list inclusion. A range check can guarantee someone is above a certain age without disclosing the precise age, and an inclusion check can be done over a set of countries to prove someone is or is not a citizen of any country in a list.\nFor applications like minting an SBT or voting, we want to check that the passport is not expired. This can be done by passing the current date and doing a range check over the date in the circuit. We can then check that the current date is correct using the block timestamp in a smart contract or server-side in offchain verification.\nFor applications that need sybil-resistance, we want to store a nullifier that prevents using the same passport twice. The simplest approach involves storing a hash of the government’s signature, though this does not render the individual anonymous from the government’s perspective. There are other approaches, see here for a discussion of the tradeoffs.\nA map of a more complete circuit can be found here.\nOne of the challenges is the number of signature algorithms used. Most countries use common ones like RSA with SHA256, but the ICAO specifications are quite permissive and some countries chose to use hash functions like SHA512 or unusual padding formats. We currently support the most common one and we are working on adding support for more.\nApplications\nApplications roughly fall into three categories: proof of humanity, selective disclosure and authentication.\nProof of humanity can be used in general for sybil resistance. This includes voting, fair airdrops, quadratic funding and helping social media fight bots. If passports can’t be construed as a general solution today, they can be integrated into wider systems like Gitcoin Passport or Zupass.\nSelective disclosure has applications like privacy preserving age check. Some countries restrict buying alcohol, drugs or entering casinos for minors, and zk could help bringing better privacy to those controls.\nAnother example of selective disclosure is proving one is not a citizen of any country in a set of forbidden countries. This could help creating an intermediate level of compliance between KYC-gated traditional finance and fully permissionless DeFi.\nUsing passport signatures for authentication, one can build a ERC-4337 recovery module that asks for a proof from a specific passport as one of the conditions for recovery. Some passports also support Active Authentication, meaning they have their own private key and the ability to sign data. This would make them suitable for direct transaction signing, either for small transactions or in a multisig setup with other signers.\nLimitations\nThe most obvious limitations of using passport signatures are the following:\nThe passport does not do any kind of biometric check when the chip is read. Therefore there is no straightforward way to know if the passport has not been borrowed or stolen.\nMost of the world population does not have a passport. Even in the US, only around 50% of the population owns a passport.\nIssuing authorities can create an arbitrary number of passports and cheat in systems that require passports for sybil resistance.\nPassports can be lost or revoked. Some countries allow citizen to keep their previous passport when they are issued a new one. Some people have dual citizenship. All those cases are hard to mitigate, as the signatures stay valid.\nThose limitations are all quite fundamental to the way passports work today. They can be addressed by aggregating attestations from multiple sources, which will be covered in a future post.\nCurrent state\nProof of Passport is fully open source, from mobile app to circuits. If you are interested in contributing, please check open issues.\nWhile performance would have been a bottleneck a few years ago, work from teams like Polygon ID, arkworks and mopro have made client-side proving on smartphones quite fast. Generating a proof with the current circuit takes ~4 seconds on a recent iPhone.\nWe are currently focused on shipping the mobile app for the first integrations. It allows users to mint an Soulbound Token disclosing only specific attributes they chose, or none at all other than the validity of their passport. Contact us to try out the beta release.\nThanks to Rémi, Andy, Aayush, Youssef and Vivek for contributing ideas and helping build this technology!\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "SOLO: Liquid Staking for Solo Validators",
        "link": "https://ethresear.ch/t/solo-liquid-staking-for-solo-validators/21157",
        "article": "By Cairo & Diego\nMany thanks to Vitalik Buterin, Alexander Herrmann, Philogy, William X, Thomas Thiery, and Julian Ma for feedback and discussions.\n1024×1024 106 KB\nSolo validators are vital to Ethereum’s security, decentralization, and censorship resistance. They operate independent nodes distributed around the world that cannot be easily coerced. However, they face key challenges:\n\nHigh entry barriers: The 32 ETH staking requirement is prohibitively high for many potential participants.\n\n\nOpportunity costs: Significantly more capital is locked up than what’s likely to be lost in cases of validator misbehavior.\n\n\nLimited liquidity: Solo validators can’t easily borrow against their stake and obtain leverage while still operating the validator.\n\nHigh entry barriers: The 32 ETH staking requirement is prohibitively high for many potential participants.\nOpportunity costs: Significantly more capital is locked up than what’s likely to be lost in cases of validator misbehavior.\nLimited liquidity: Solo validators can’t easily borrow against their stake and obtain leverage while still operating the validator.\nTo address these issues, we propose SOLO—a protocol that enables a permissionless liquid staking token (LST) minted against the portion of a validator’s stake that’s unlikely to be lost, even in cases of penalties and slashing. Following the Pectra update, this could be as much as 96.1% of the stake, barring significant concurrent mass slashing events and long inactivity leaks. This would reduce entry costs for solo validators by a factor of 25.6, to just 1.25 ETH. The protocol would also enable solo validators to borrow efficiently against their otherwise illiquid stake.\nOur mechanism achieves a single, fungible LST across all validators without relying on governance, trusted hardware (e.g., SGX), or permissioned operator sets. The protocol leverages EIP-7002 (Execution Layer Triggerable Withdrawals) and EIP-7251 (“MaxEB”) from Pectra. Validators need only point their withdrawal credentials to the protocol.\nTo counter a potential decrease in the cost of a 51% attack on Ethereum and discourage dominance by single validators, the protocol dynamically limits the achievable leverage using an economic anti-concentration mechanism that disproportionately discourages larger validators. This aims to prevent any single entity from gaining excessive control over Ethereum through the protocol.\nBackground\nEthereum’s protocol uses two types of negative incentives for validators: penalties and slashing.\nPenalties are applied for missed attestations and sync committee duties. They’re relatively mild—a validator can recover from one day of downtime with about one day of uptime.\nSlashing, however, is more severe. It’s applied for serious protocol violations, such as proposing or attesting to multiple blocks for the same slot. This punishment consists of four parts:\n\nAn initial penalty: Post-Pectra, this would be 1 ETH per 4,096 ETH of effective balance. For a 32-ETH validator, the loss would be 0.00781 ETH.\n\n\nA correlation penalty for mass slashing events: Typically zero, this penalty could potentially wipe out a validator’s entire balance if a third or more of the total stake is slashed within the 18 days before and after the validator’s slashing. In a scenario where 1% of the total stake is slashed, the correlation penalty would be at most 0.960 ETH for a 32-ETH validator.\n\n\nPenalties for missed attestations: Ethereum automatically invalidates all attestations from the slashed validator until they become withdrawable, which occurs after 8,192 epochs. This penalty would amount to 0.0564 ETH for a 32-ETH validator today.\n\n\nAn inactivity leak penalty: If the chain is not finalizing, slashed validators incur additional costs. A 32-ETH validator would suffer a loss of 0.0157 ETH for a leak spanning 128 epochs.\n\nAn initial penalty: Post-Pectra, this would be 1 ETH per 4,096 ETH of effective balance. For a 32-ETH validator, the loss would be 0.00781 ETH.\nA correlation penalty for mass slashing events: Typically zero, this penalty could potentially wipe out a validator’s entire balance if a third or more of the total stake is slashed within the 18 days before and after the validator’s slashing. In a scenario where 1% of the total stake is slashed, the correlation penalty would be at most 0.960 ETH for a 32-ETH validator.\nPenalties for missed attestations: Ethereum automatically invalidates all attestations from the slashed validator until they become withdrawable, which occurs after 8,192 epochs. This penalty would amount to 0.0564 ETH for a 32-ETH validator today.\nAn inactivity leak penalty: If the chain is not finalizing, slashed validators incur additional costs. A 32-ETH validator would suffer a loss of 0.0157 ETH for a leak spanning 128 epochs.\nIf up to 1% of the total stake is slashed and inactivity leaks amount to no more than 128 epochs, slashing a 32-ETH validator would result in a loss of at most approximately 1.04 ETH—just 3.25% of the balance.\n2482×1104 131 KB\nThis means that, outside of extraordinary mass slashing events, most of the ETH held by a single independent validator is not at risk even if that validator can misbehave arbitrarily.\nPrior Work\nSeveral approaches have explored LSTs in this context:\n\nJustin Drake proposed an LST mechanism for solo validators that relies on trusted hardware (SGX) to prevent slashing penalties.\n\n\nDankrad Feist proposed a two-tiered staking system with separate slashable and non-slashable capital tiers.\n\n\nLido allows ETH holders to “lend” their ETH to node operators without giving those node operators the ability to steal their funds. Their Community Staking Module supports permissionless participation by solo stakers who put down a bond.\n\n\nRocket Pool enables solo validators to “borrow” up to 24 ETH for each 8 ETH they provide. However, they must also stake RPL tokens—at least 10% of the borrowed amount—and share validator rewards proportionally.\n\n\nfrxETH also allows solo anonymous validators to borrow up to 24 ETH for each 8 ETH they provide, but the borrowed ETH can only be used to create new validators.\n\nJustin Drake proposed an LST mechanism for solo validators that relies on trusted hardware (SGX) to prevent slashing penalties.\nDankrad Feist proposed a two-tiered staking system with separate slashable and non-slashable capital tiers.\nLido allows ETH holders to “lend” their ETH to node operators without giving those node operators the ability to steal their funds. Their Community Staking Module supports permissionless participation by solo stakers who put down a bond.\nRocket Pool enables solo validators to “borrow” up to 24 ETH for each 8 ETH they provide. However, they must also stake RPL tokens—at least 10% of the borrowed amount—and share validator rewards proportionally.\nfrxETH also allows solo anonymous validators to borrow up to 24 ETH for each 8 ETH they provide, but the borrowed ETH can only be used to create new validators.\nSOLO, however, offers solo validators unique advantages:\n\nIt allows them to create validators with as little as 1.25 ETH.\n\n\nIt enables minting of a single, fungible LST for any validator without relying on trusted hardware, governance, a permissioned operator set, or staking of an additional token.\n\n\nIt compensates LST holders for the time value of their ETH and the risk of bad debt through a market-based funding rate.\n\n\nIt requires no changes to Ethereum, except for EIP-7002 and EIP-7251, both in Pectra.\n\n\nIt dynamically counterbalances the potential risk it poses to Ethereum of a 51% attack, using a mechanism that disproportionately discourages larger validators from accumulating too much stake.\n\nIt allows them to create validators with as little as 1.25 ETH.\nIt enables minting of a single, fungible LST for any validator without relying on trusted hardware, governance, a permissioned operator set, or staking of an additional token.\nIt compensates LST holders for the time value of their ETH and the risk of bad debt through a market-based funding rate.\nIt requires no changes to Ethereum, except for EIP-7002 and EIP-7251, both in Pectra.\nIt dynamically counterbalances the potential risk it poses to Ethereum of a 51% attack, using a mechanism that disproportionately discourages larger validators from accumulating too much stake.\nProtocol Overview\nThe mechanism draws inspiration from synthetic stablecoin systems like RAI. Node operators act as borrowers, minting an LST called SOLO against validators’ staked ETH, while SOLO holders serve as lenders. If a validator’s SOLO “debt” becomes too high relative to their stake, they’re liquidated and force-withdrawn. A dynamic funding rate compensates SOLO holders for the time value of the underlying ETH and the risk of bad debt, making the token trade close to 1 ETH.\nThe protocol defines the loan-to-value (LTV) of active validator i, whose withdrawal credentials point to the protocol, as follows:\ndebt_i is the amount of the SOLO minted against validator i, including any unpaid funding, and collateral_i is validator i's effective balance in Ethereum’s consensus layer.\nA validator’s LTV can increase due to SOLO minting, funding, penalties, or partial withdrawals, raising their liquidation risk. Conversely, their LTV can decrease due to validator rewards and additional deposits, lowering this risk.\nLTV_{max} is the maximum allowed LTV after SOLO minting and partial withdrawals. It is strictly less than 100% and also caps the system’s achievable leverage by limiting how little operators can put forward relative to the validator’s stake size.\nLTV_{liq} is the liquidation threshold—the highest LTV before a position becomes eligible for liquidation and forced withdrawal—and must be at least LTV_{max}. Its value should account for potential losses during delays between liquidation eligibility and execution (e.g., if the validator is a proposer and censors the liquidation) and during the exit queue following the forced withdrawal.\nH_i is the health factor of validator i, indicating how close it is to liquidation eligibility, if it has any debt:\nA value of H_i below 1 makes the validator eligible for liquidation.\n1788×1368 84.9 KB\nFollowing the analysis in the Appendix, we could estimate values of 96.1% for LTV_{max} and 96.4% for LTV_{liq}.\nMinting\nTo mint SOLO, the withdrawal credentials of the validator must point at a protocol-controlled surrogate contract. Surrogate contracts are used to attribute withdrawals from the consensus layer. If the validator’s withdrawal credentials haven’t been migrated to 0x01 yet, the operator should migrate them and set them to the surrogate contract.\nIf its withdrawal credentials point at the contract, a validator can be registered by the operator by calling register. The operator can then call mint to mint SOLO against the validator.\nThe protocol should also offer a method deploy that atomically transfers 32 ETH from the caller, deploys a new validator via Ethereum’s deposit contract, with withdrawal credentials set to the surrogate contract, registers it, and mints SOLO.\nWithdrawing\nOperators initiate the full withdrawal of the validator by triggering a voluntary exit in the consensus layer or by calling a function withdraw, which triggers the exit of the validator. Partial withdrawals can only be triggered through withdraw, which ensures that the resulting LTV would be less than LTV_{max}. These functionalities, as with liquidation, depend on EIP-7002.\nOnce the withdrawal completes, the operator calls a method claim to receive the ETH from the stake. If this were a full withdrawal, the operator must have first paid for any outstanding debt against the validator using method repay.\nLiquidating\nA validator becomes eligible for liquidation when its health factor drops below 1, which occurs when its LTV exceeds the liquidation threshold. Once eligible, anyone can trigger the liquidation process by calling method liquidate on the validator. This call then triggers the withdrawal of the validator.\nAfter the withdrawal completes, the contract auctions the received ETH for SOLO to pay back the validator’s debt. Any excess ETH can be returned to the validator (or kept as a liquidation fee by the protocol). As LTV_{max} accounts for slashing penalties, the received ETH should suffice to cover incurred losses. In the event that this is not enough, SOLO holders would incur the costs from the bad debt.\nSlashing\nAt the end of the slashing process, the protocol receives any remaining stake of the validator after penalties have been applied. The protocol then auctions this ETH for SOLO in order to cover the slashed validator’s debt, following a similar process as a liquidation.\nFunding Rates\nA mechanism is needed to compensate SOLO holders for the time value of their ETH, as well as for the risk of any bad debt (e.g., from a significant mass slashing event or prolonged inactivity leak). Otherwise, SOLO wouldn’t trade close to 1:1 with ETH, which is essential for the protocol.\nWe propose using a dynamic, market-based funding rate. SOLO minters (debtors) pay this rate to SOLO holders (lenders). It works by proportionally increasing debts, with SOLO holders benefiting through continuous token rebasing (akin to Lido’s stETH daily rebasing). If the SOLO price falls below 1 ETH, the funding rate rises, making SOLO holding more appealing. If the SOLO price rises above 1 ETH, the funding rate falls, making SOLO holding less attractive. In extreme cases where the SOLO price persistently remains below 1 ETH, the funding rate would eventually rise so high that all positions would be liquidated.\nThis funding rate can be implemented in various ways. For real-world examples, see the funding rate used in RAI and Squeeth. However, unlike with those mechanisms, the SOLO funding rate can be floored at 0%, avoiding the need for negative rebasing. To prevent the possibility of a sustained depeg in which SOLO trades at higher than 1 ETH even when rates are 0%, the protocol can allow unlimited minting of SOLO against native ETH 1:1 whenever the funding rate is at 0%. Anyone can then redeem SOLO for this ETH at a 1:1 ratio. This reserve of ETH must be emptied before the funding rate can rise above 0% again.\nIn practice, any increase in protocol returns would likely trigger a surge in SOLO demand, prompting a rise in the funding rate. The reverse holds as well. This self-balancing mechanism becomes even more efficient due to the low entry costs. Nevertheless, SOLO enables solo validators to deploy their borrowed funds into higher-yielding ventures while still operating the validator—a potentially profitable strategy if those returns exceed the funding cost when combined with the validator rewards.\nLowering Capital Requirements\nThe protocol leverages flash loans to significantly lower the capital requirements for solo validators.\nCreating a validator\nSolo validators would call a special function, flashDeploy, with an amount of ETH, S, as little as  32 (1-LTV_{max}), which does the following:\n\nBorrows (32 - S) ETH from a fee-free flash-loan provider to reach 32 ETH with S;\n\n\nCalls deploy with 32 ETH, along with validator information, to mint (32 - S) of SOLO;\n\n\nExchanges (32 - S) of SOLO for (32 - S) ETH in a decentralized exchange (assuming no price impact);\n\n\nRepays the (32 - S) ETH debt with the flash-loan provider.\n\nBorrows (32 - S) ETH from a fee-free flash-loan provider to reach 32 ETH with S;\nCalls deploy with 32 ETH, along with validator information, to mint (32 - S) of SOLO;\nExchanges (32 - S) of SOLO for (32 - S) ETH in a decentralized exchange (assuming no price impact);\nRepays the (32 - S) ETH debt with the flash-loan provider.\n2042×1302 88.7 KB\nSolo validators must then maintain a health factor of at least 1 to avoid liquidation, as this validator would accumulate funding while it’s active.\nFor an LTV_{max} of 96.1%, the minimum required amount would be approximately 1.25 ETH (i.e., 32  (1 - LTV_{max})), reducing the entry cost for solo validators by a factor of 25.6.\nWithdrawing a validator\nTo withdraw the initial amount deposited, S, and rewards, solo validators only need to cover any additional debt accrued, \\Delta debt —not the full debt of the validator for (32 - S) SOLO. They’d initiate this process by triggering the full withdrawal of the validator in the consensus layer or through function withdraw. After the withdrawal is completed, they’d call function flashClaim, which does the following:\n\nTransfers \\Delta debt SOLO from the solo validator to the protocol.\n\n\nBorrows (32 - S) ETH from a fee-free flash-loan provider;\n\n\nExchanges (32 - S) ETH for (32 - S) SOLO in a decentralized exchange (assuming no price impact);\n\n\nCalls claim to settle the validator’s debt of (32 - S + \\Delta debt) SOLO;\n\n\nRepays (32 - S) ETH debt with the flash-loan provider;\n\n\nReturns any ETH left, (S + rewards - penalties), to the solo validator.\n\nTransfers \\Delta debt SOLO from the solo validator to the protocol.\nBorrows (32 - S) ETH from a fee-free flash-loan provider;\nExchanges (32 - S) ETH for (32 - S) SOLO in a decentralized exchange (assuming no price impact);\nCalls claim to settle the validator’s debt of (32 - S + \\Delta debt) SOLO;\nRepays (32 - S) ETH debt with the flash-loan provider;\nReturns any ETH left, (S + rewards - penalties), to the solo validator.\n2042×1640 105 KB\nIf the withdrawn amount is less than the validator’s debt, the protocol incurs bad debt. This scenario, resulting from potential significant validator losses during mass slashing events or prolonged inactivity leaks, would render flashClaim inoperable as there wouldn’t be enough ETH to repay the flash-loan. However, since solo validators only receive the ETH left after debt repayment, their expected return in this scenario would be zero, meaning that no ETH remains locked for them.\nAnti-Concentration: Discouraging Large Validators\nBy reducing the cost of creating a validator, the protocol fosters decentralization and the proliferation of small validators, but it also may reduce the costs of accumulating very large amounts of stake and even potentially executing a successful 51% attack on Ethereum. While the protocol’s benefits are theoretically available to both attackers and honest users, coordinated attackers may be more likely to exploit this advantage effectively.\nTo counterbalance this risk, we propose an anti-concentration mechanism: a dynamic leverage adjustment that automatically decreases LTV_{max} as the protocol’s share of the total ETH staked grows. One possible formula for this adjustment would be the following:\nwhere LTV_{max}^{t_0} is the initial maximum LTV (e.g., 96.1%).\nThis mechanism serves two purposes.\nFirst, it helps ensure that SOLO does not dominate Ethereum staking. If SOLO ETH becomes too large a share of all ETH staked, then SOLO will stop providing significant leverage.\nBut an additional benefit of the mechanism is that it helps ensure that SOLO itself is not dominated by a few large stakers. It does this by disproportionately discouraging large validators from adding more stake.\nConsider the “marginal LTV ” for an individual validator—the ratio of the amount of SOLO that they can mint for each new unit of ETH they put in as collateral. For anyone who is not yet using the protocol, their marginal LTV is equal to LTV_{max}. But for anyone who is already a staker using the protocol—particularly a large one—the calculus is different. Because of the anti-concentration mechanism, each additional unit of collateral they deposit increases the LTV for all of their stake, requiring them to deposit additional collateral to support all of their existing SOLO debt. This means the marginal LTV for this large validator is actually lower than the marginal LTV of a small validator.\nTo calculate the marginal LTV, we can start with the formula for the maximum amount of debt that a user, Alice, can take out, debt_{max}, as a function of the amount she has staked through SOLO, C_{Alice}. This is expressed partly in terms of the total amount of ETH that other stakers are staking through SOLO:\nThe user’s marginal LTV is just the derivative of that formula with respect to the user’s collateral, which works out to:\nThe upshot of this formula is that users who already have a large amount of collateral being used in the system face a lower marginal LTV than smaller users.\nThe chart below shows both effects of the anti-concentration mechanism. The red line shows the marginal LTV for a small staker considering using SOLO, as a function of the percent of total stake that is using SOLO. The blue line shows the marginal LTV for a staker who already controls half of the stake in SOLO.\n1208×1130 75.8 KB\nImportantly, this mechanism does not depend on any kind of identity-based Sybil resistance. Attackers can’t avoid it by splitting their holdings across multiple validators or by consolidating large amounts of ETH into single validators. This anti-concentration mechanism provides an economic disincentive for large stakers to accumulate stake through SOLO.\nConclusion\nSOLO addresses key challenges faced by solo validators: high entry barriers and limited liquidity for their stake. All in all, we believe this mechanism could make solo validating significantly more attractive.\nA 32-ETH validator would incur a maximum loss of 0.00867 ETH if offline for an entire exit queue lasting up to 5.6 days as of today. If the validator’s liquidation was delayed by a day, the maximum loss would increase to 0.0102 ETH. Should inactivity leaks occur during this process for up to 128 epochs, the loss would rise to 0.0259 ETH. Moreover, if the validator is slashed during the withdrawability delay period following the exit queue, incurring at most penalties for 1.04 ETH, the total loss would reach 1.07 ETH.\nTo prevent immediate liquidation when funding accrues, we introduce a buffer between LTV_{max} and LTV_{liq}. This buffer, set at 0.3% of the borrowed amount, would mean that LTV_{liq}=1.003\\cdot  LTV_{max}.\nThe maximum loss LTV_{liq} can tolerate without causing bad debt, as a function of LTV_{max} is 32 (1-LTV_{max}). Thus,\nNow, potential\\,loss=penalties + buffer= 1.07 ETH +\\,0.003 \\cdot borrowed\\, amount. Since borrowed\\, amount is 32\\cdot LTV_{max} ETH, the buffer is 0.003( 32\\cdot LTV_{max}).\nConsequently, substituting back into the equation gives the following:\nSolving this inequality yields maximum values of \t\\approx 96.1% for LTV_{max} and \\approx 96.4% for LTV_{liq}.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": []
    },
    {
        "title": "Custard - Improving UX for Super TXs",
        "link": "https://ethresear.ch/t/custard-improving-ux-for-super-txs/21273",
        "article": "Joint work with Kubi. Thanks to Drew, Justin, Ladislaus, Conor, Lin for their review and feedback. Feedback is not necessarily an endorsement.\nCustard  (Constraining User State To Avoid Reneging Downstream) is a technique for enabling atomically composable super transactions on based rollups through careful state management. The key insight is that by constraining specific parts of L1 state that a super transaction depends on, we can enable validators to safely issue L1 execution preconfirmations ahead-of-time without requiring control over the whole L1 block. This reduces the validator coordination needed to bootstrap preconf protocols and allows based rollups to offer super transactions sooner with fewer participating validators. This post explores three ways to implement Custard: through EIP-7702, smart contracts, and exclusion preconfirmations.\nWhy does this matter?\nA primary benefit of based sequencing is that it enables synchrony and atomicity between the L1 and based rollups (BRU). This means operations on both layers can be combined into what we call a “super transaction” - a bundle of transactions that works across both layers as if they were one.\nA Real-World Example\nLet’s consider a practical example: L1→L2→L1 arbitrage. Imagine Alice wants to atomically:\nMove her tokens from L1 to a BRU\nMake a trade on the BRU\nMove her tokens and profit back to L1\nCurrently, these would be separate steps with delays between them (e.g., normally 7 days between steps 2-3 for optimistic rollups). Applying Custard, Alice can bundle all of these actions into one super transaction that completes within a single Ethereum block.\nTo make super transactions widely adoptable, we need to address three key challenges:\nReal-Time Proving: We need a way to settle the BRU’s state quickly enough to withdraw within a single L1 block\nGuaranteeing Atomicity: We need mechanisms to ensure either all parts of the super transaction complete successfully or none of them happen\nValidator Availability: We need sufficient validator participation to make super transactions reliably available for users\nThe rest of this post examines each of these challenges and existing approaches to solving them, before introducing Custard as a way to bring them together.\nChallenge #1: Real-Time Proving\nTraditional Approach and Its Limitations\nUntil now, it was commonly believed that (trustless) instant withdrawals from rollups required “real-time proving” - essentially, the rollup’s state must first be settled via validity proof. For Alice’s super transaction to complete in one L1 block (12 seconds), the validity proof would need to be generated even faster. However, the technology to generate these proofs so quickly (real-time SNARKs) isn’t deployed in the market yet but there many amazing efforts in progress ().\nCurrent Workarounds\nSome projects (UniFi, Gwyneth, T1) have turned to Trusted Execution Environments (TEEs) as an alternative to SNARKs for proving the rollup’s state transition function. While TEEs can generate proofs much faster than SNARKs, making real-time proving possible, they come with a significant drawback: they require trusting the hardware manufacturer and prover. This additional trust assumption introduces new risks to based rollups that traditional SNARK-based systems don’t have.\nA New Solution: The Solver Approach\nNethermind recently proposed a solution that achieves the UX of instant withdrawals without needing real-time proving. Their approach:\nUses solvers to immediately provide users withdrawal liquidity on the L1 before the BRU state is settled\nMaintains atomicity (solvers and BRU bridge are protected from reorgs)\nMaintains trustlessness (no need to trust the solver or TEEs)\nProvides a practical path forward while we wait for real-time proving technology to mature (at the cost of capital efficiency)\nChallenge #2: Guaranteeing Atomicity\nFor Alice’s super transaction to work seamlessly, we need to ensure that either all sub-transactions complete successfully or none of them happen at all. This is where execution preconfirmations (EPs) come in.\nThe Role of Execution Preconfs\nTo guarantee the super transaction’s success, we need Ethereum validators to provide four specific guarantees:\nL1 Deposit Guarantee: Confirms Alice’s funds will successfully move from L1 → BRU\nL2 Swap Guarantee: Ensures Alice’s trade on the rollup will execute as expected\nL2 Withdrawal Guarantee: Confirms Alice’s request to move funds back from BRU → L1 will be processed\nL1 Solver Guarantee: Ensures a solver transfers Alice funds on the L1\nWhy Ethereum Validators Matter\nA crucial insight is that these guarantees must come from Ethereum validators themselves. They are uniquely positioned to make these guarantees because they can be proposers for both layers:\nOn Ethereum, they have a write-lock over the L1 since they have sole authority to propose the next block\nOn the BRU, they can be configured to be the only ones with permission to sequence transactions during their slot\nThis dual write-lock is what makes based sequencing special - only Ethereum validators can credibly commit that a super transaction will execute exactly as planned. Based preconfs are the mechanism through which validators make these promises binding - by staking capital, validators become preconfers and face economic penalties if they fail to honor their commitments.\nChallenge #3: Validator Availability\nL1 EP constraints\nA critical limitation of L1 EPs is their “just-in-time” nature. Validators can only safely issue these L1 EPs when they’re the current block proposer. Why? Because a future validator doesn’t have a write-lock on the L1 and earlier validators could change L1 state in ways that break their L1 EPs.\nThis is different from L2 EPs on the rollup, where validators can safely make commitments “ahead-of-time” because the rollup’s smart contracts ensure only the designated preconfer can write to the state prior and during their turn.\nimage1920×738 80.5 KB\nThe Bootstrap Challenge\nThis just-in-time constraint creates two significant problems:\nLimited Availability: Super transactions can only happen during blocks where the\nL1 validator has opted in as a preconfer\nUX Issues: Unless every single Ethereum validator participates in the system, there will be slots where super transactions aren’t available\nGetting 100% of Ethereum validators to participate is an enormous BD challenge. Therefore, we need an alternative solution: finding a way for validators to safely issue L1 EPs ahead-of-time.\nWhat is Custard?\nCustard offers a solution to our timing problem by making a key observation: we don’t always need to control the entire L1 block to make safe guarantees. Instead, we can selectively lock just the specific pieces of L1 state that our super transaction needs to work with.\nThis insight is powerful because it means we can issue some types of L1 EPs ahead-of-time, as long as we can guarantee that the specific state we care about won’t change. By only locking what we need, rather than requiring control over everything, we can significantly reduce the number of validators needed as preconfers.\nNote: The implementations we’ll describe next are intentionally simplified to clearly illustrate the mechanisms. In practice, these can be optimized for better capital efficiency and generality.\nCustard with EIP-7702\nEIP-7702 enables user accounts (EOAs) to set their own custom code based on any smart contract, effectively turning it into a smart account. We can use this to create time-locked guarantees about a user’s account state.\nHow It Works\nLet’s walk through how Alice could execute her super transaction using EIP-7702:\nInitial Lock (Slot S)\n\nAlice locks her account’s nonce until a future slot S'\nThis prevents any changes to her account until the designated slot\n\n\nAlice locks her account’s nonce until a future slot S'\nThis prevents any changes to her account until the designated slot\nSetup (Slot S + 1)\n\nAlice requests her super transaction from a preconfer who will propose at a future slot S'\nThe transaction includes:\n\nDepositing B ETH to the rollup\nExecuting the arbitrage trade\nWithdrawing B + ε - f  ETH (original amount plus profit minus fee)\nHaving a solver complete the withdrawal on L1\n\n\n\n\nAlice requests her super transaction from a preconfer who will propose at a future slot S'\nThe transaction includes:\n\nDepositing B ETH to the rollup\nExecuting the arbitrage trade\nWithdrawing B + ε - f  ETH (original amount plus profit minus fee)\nHaving a solver complete the withdrawal on L1\n\n\nDepositing B ETH to the rollup\nExecuting the arbitrage trade\nWithdrawing B + ε - f  ETH (original amount plus profit minus fee)\nHaving a solver complete the withdrawal on L1\nVerification (Slot S + 1)\n\nThe preconfer checks that Alice’s account is properly locked\nIf verified, the preconfer issues all necessary preconfs\n\n\nThe preconfer checks that Alice’s account is properly locked\nIf verified, the preconfer issues all necessary preconfs\nExecution (Slot S')\n\nThe preconfer executes the entire transaction:\n\nDeposits Alice’s B ETH to the BRU\nCommits the BRU blob to L1 containing Alice’s trade\nCompletes the solver’s transfer of B + ε - f ETH to Alice on L1\n\n\n\n\nThe preconfer executes the entire transaction:\n\nDeposits Alice’s B ETH to the BRU\nCommits the BRU blob to L1 containing Alice’s trade\nCompletes the solver’s transfer of B + ε - f ETH to Alice on L1\n\n\nDeposits Alice’s B ETH to the BRU\nCommits the BRU blob to L1 containing Alice’s trade\nCompletes the solver’s transfer of B + ε - f ETH to Alice on L1\nSettlement (Slot S' + Δ)\n\nAfter Δ blocks, the BRU state is proven\nThe solver can recover B + ε + f ETH by withdrawing from BRU\n\n\nAfter Δ blocks, the BRU state is proven\nThe solver can recover B + ε + f ETH by withdrawing from BRU\nKey Insight\nBy locking her account, Alice guarantees she’ll have sufficient funds (B ETH) when the super transaction executes. Preconfers can safely issue L1 EPs ahead-of-time if they require accounts to be first locked, solving our timing problem.\nCustard with Smart Contracts\nWhile waiting for EIP-7702 to be released, we can achieve similar results using smart contracts. The key difference is that instead of modifying account behavior directly, users must first deposit their assets into an escrow contract that enforces the same guarantees:\nAssets are locked until the target slot\nThe funds can only be deposited into the rollup\nNo decreases in asset balance are allowed until then\nimage1920×916 86.9 KB\nThe execution flow mirrors the EIP-7702 approach, but with one notable advantage: the escrow contract naturally accumulates a pool of locked assets, enabling potential capital efficiency optimizations in protocol design.\nCustard with Exclusion Preconfs\nExclusion preconfs represent a different kind of validator promise: instead of guaranteeing what they will do, validators commit to what they won’t do. Specifically, they promise to prevent certain state changes by disallowing specific account actions to take place. While exclusion typically goes against Ethereum’s values, when used carefully in this context, it serves a constructive purpose: locking specific account states to preserve ahead-of-time L1 EP validity. Importantly, these types of preconfs would only be permitted if explicitly authorized by the account owner to avoid censorship.\nHow It Works\nLet’s walk through how Alice could execute her super transaction using exclusion preconfs:\nIssuing Execution Preconfs\n\nAlice gets exclusion preconfs from the validators ahead of her target super transaction slot\nEach exclusion preconf promises not to:\n\nInclude transactions that would increase Alice’s nonce\nInclude transactions that would decrease Alice’s ETH balance\n\n\n\n\nAlice gets exclusion preconfs from the validators ahead of her target super transaction slot\nEach exclusion preconf promises not to:\n\nInclude transactions that would increase Alice’s nonce\nInclude transactions that would decrease Alice’s ETH balance\n\n\nInclude transactions that would increase Alice’s nonce\nInclude transactions that would decrease Alice’s ETH balance\nExecution\n\nWhen the target slot arrives, Alice’s EOA is guaranteed to have the required ETH\nThe super transaction can proceed safely\n\n\nWhen the target slot arrives, Alice’s EOA is guaranteed to have the required ETH\nThe super transaction can proceed safely\nimage1920×881 88.7 KB\nAn advantage of this approach is that all execution preconfs are issued off-chain, reducing gas costs. However, it introduces several complexities. Super transactions still require all earlier slot validators to be L1 exclusion preconfers - while easier than previous approaches, this remains a significant BD challenge. Additionally, paying for exclusion preconfs becomes tricky since nothing lands on-chain in the happy case and the collateral requirements and slashing conditions need to be carefully considered when assessing the risk of reneging.\nLimitations of this approach\nA key distinction in Nethermind’s solver approach is that withdrawal requests have a simple “L1 output condition” - they only need to verify that tokens arrive at a specific L1 address. This simplicity is what enables atomic withdrawals without real-time proving. However, more sophisticated super transactions might require L1 output conditions to depend on complex L2 state changes, in which case we may require real-time proving of L2 state. While Custard’s principles for managing L1 state dependencies still apply, implementations will either need to wait for real-time SNARKs to mature or accept the additional risks of TEE-based proving solutions.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": [
            "based-sequencing"
        ]
    },
    {
        "title": "Efficient ECDSA signature verification using Circom",
        "link": "https://ethresear.ch/t/efficient-ecdsa-signature-verification-using-circom/13629",
        "article": "Context\nAn ECDSA signature verification circuit written in Circom results in an R1CS with approximately 1.5 million constraints, a proving key that is close to a GB large, and a proving time that is non-trivial.\nThe point of doing an ECDSA signature verification using Circom is to verify that the prover owns a certain address without revealing the address. Although it is easier to check privKey * G == pubKey, most wallets don’t expose the private key of a wallet, therefore we are restricted to checking signatures. Signature verification without revealing the address has applications in, for example, zero-knowledge proof of membership. (Which is why we want to have a way to prove ownership of an address anonymously)\nIn this post, I will decribe a method that could improve the efficiency of ECDSA signature verification written in Circom.\nI focus on implementation using Circom to avoid ambiguity. However, the method is not completely dependent on Circom. You can swap “Circom” with “zkp”, “zk-snark”, or other high-level arithmetic circuit languages.\nOverview of the method\nThe essence of the method is, that in order to do as less computations as possible in a circuit, we offload some signature verification calculations from the circuit.\nThe method\nThis credit for this technique goes to the answerer of this stack exchange post. Although, the method is not formally verified. If this method lacks correctness, soundness, or zero-knowledge-ness, then the entire scheme I describe in this post will not work. That being a possibility, I’m writing this post hoping it to be useful at least in some way,  as a source of ideas.\nVerify an ECDSA signature without revealing the s part of the signature leads to reducing the required calculation that needs to be kept private (i.e. needs to be SNARKified)\nFirst, you produce a signature with your private key as usual.\nR = k * G\ns = k^-1(m + da * r)\nThe signature = (r, s)\nwhere\n\nG: The generator point of the curve\n\nk: The signature nonce\n\nR: k * G\n\n\nQa: The public key\n\nm: The message to sign\n\nr: x-coordinate of R\n\nThe signature can be verified by checking the following equality\nR = s^{-1} * m * G+ s^{-1}r * Qa\nor\ns * R = m * G + r * Qa.\nThis equation can be perceived as s being the private key, R being the generator point, and m * G + r * Qa being the public key.\nNow we can prove the knowledge of s without revealing s itself, by generating a signature!\nWe calculate the signature as follows:\nR’ = k’ * R\ns’ = k^-1(m’ + s * r')\nwhere\n\nk’: The signature nonce\n\nm’: The message to sign\n\nr’: x-coordinate of R'\n\nWe verify the signature (s’, r')  by checking:\ns’ * R’ = m’ * R + r' * Qa’\nThis equation itself doesn’t reveal Qa (the public key).  So it can be checked publicly, without using Circom.\nWe also need to check that Qa’ actually comes from Qa. This can be done by checking:\nQa’ = m * G + r * Qa\nSince we don’t want to reveal Qa (the public key), this equality check is done using Circom. Moreover, it is required to keep m a secret. If m is revealed, Qa will be recoverable. That is, in zero-knowledge proof of membership, the public keys that are members of a set are publicly known; someone can just check which public key matches Qa’, by filling in m and r .\nSummary and benchmarks\nTo sum up, the circuit will take Qa and m as the private input, and r’ and Qa’ as the public input. I constructed the outline of the circuit here. The circuit is not complete. The purpose of it is to demonstrate the outline.\nThe benchmarks:\nconstraints: \\approx 200,000\nproving key size: \\approx 134MB\nproving time (witness generation + proving): \\approx 15s on a MacBook Pro\nWhich is a meaningful improvement from the original circuit.\nAnd that is it.\nFeedback will be appreciated.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "Lookup singularity via MMR",
        "link": "https://ethresear.ch/t/lookup-singularity-via-mmr/18704",
        "article": "Introduction\nWhat I will show is that if we have a public merkle mountain range where existence within it implies correctness, then this primitive can be used to achieve the lookup singularity. What I mean is you can enable complex lookup tables for sha256 hash, merklizing leaves, or potentially even EVM storage proofs.\nThere are optimistic and immediate finality variations of this scheme which balance certain trade offs.\nBackground\nLookup tables are an incredible innovation in the world of ZKPs. They allow for much faster proving for complex computation like a Sha256 hash. Barry Whitehat coined the term the lookup singularity (Lookup Singularity - General - zk research) to represent how they can drastically improve the performance of SNARKs.\nStandard lookup tables require pre-computing an entire table, and committing it during circuit compilation. Therefore these tables cannot be updated after the keys are generated. The cost of committing the table is amortized over all the lookups used during proving. There is a negligible per-lookup cost for these types of constructions (eg plonk-lookup). The obvious limitation here is that it’s impractical for lookup tables that are very large, say 32-bit float arithmetic (2^64 rows).\nJustin Thaler wrote papers for a new system called Jolt/Lasso which sacrifices the low per-lookup cost in order to get more complex lookups. This is done by using multivariate polynomials to represent the lookup table as a  polynomial evaluation. This makes it so you don’t need the full table in memory to use in a circuit, but increases the per-lookup cost. See Lasso/src/subtables/lt.rs at 823da601422d17f7c150631947b33a9db1ad5b98 · a16z/Lasso\nThese constructions don’t practically allow for lookup tables of truly complex operations like a sha256 hash. Say I had 100 leaves and I wanted to merklize them to compute the root. No existing lookup table construction can help you with this.\nTrusted lookup table oracle\nOne very easy way to increase the performance and utility of a lookup table is to use a trusted source. Imagine I wanted to multiply 2 floating point numbers, I can submit the lookup via an API to a service that returns the lookup table with a snark-friendly signature. In practice we would batch all the look ups together with one signature.\nThe beauty of this approach is you can enable truly arbitrary lookup operations, which would dramatically improve the performance of your circuits. You also get a custom lookup table for each proof you generate. This approach would drastically decrease circuit complexity.\nOne thing to note is that for any sensitive parts of the circuit you may wish to do it without the table oracle to preserve privacy. Alternatively you could include extra unnecessary lookups to obfuscate which were used. This approach lets you offload any parts of the circuit which can be public into a lookup table. Essentially a selectively disclosed computation ZKP.\nHowever, the issue here is that the trusted party can secretly sign incorrect lookups in order to forge a proof and attack a system. The idea I want to present is to solve this malicious lookup oracle problem via crypto-economic methods.\nWhat if there was a better way?\nSecuring with crypto-economic security\nThe obvious place to start is have the lookup oracle stake a large amount of ETH/USDC/etc. If I can coerce the oracle to give me a malicious result, I can generate a fraud proof (ZKP or smart contract) in order to claim a reward.\nThis works well to prevent the oracle from colluding with someone else. This however fails in the situation where the oracle is also the one requesting the lookup. There is no economic incentive to claim your own stake.\nThe only way to solve this problem is to force the oracle to publicly disclose every lookup table that it generates. This can be done with a merkle mountain range (MMR). The key insight you need to see is that existence within the public MMR directly implies correctness. We then construct the circuits to check for existence within a MMR, and we then compare with the root of the trusted, public MMR.\nOverall Solution\nFirst you use a merkle mountain range w/ a zk friendly hash to commit tables into a smart contract. A MMR is advantageous because:\nScales virtually infinitely\nShort inclusion proofs\nQuick to update proofs\nIn order to avoid confusion, I want to emphasize the MMR does not merklize a full lookup table. Instead each leaf in the MMR can be a custom lookup table(s) for a particular proof generated. Each leaf can be multiple tables, as a circuit might offload multiple operations to a table. The MMR may include duplicate lookups.\nThe lookup table used during proving only needs to include the entries needed. This is the key advantage of this approach. The table(s) can be generated locally by the user before being generating the proof. The tables then can be sent to be added to the global MMR, at which point the proof can be updated to have inclusion in trusted MMR.\nTo save on gas, the actual lookup tables do not need to be put on-chain, we can use an optimistic approach where just the commitments of the tables are stored. The details around off-chain data availability will not be discussed here.\nIn situations where we want quick finality, you can have a contract verify table validity before adding it to the global MMR. The trade off is it will cost more. The optimistic approach actually allows for much more complicated look ups (eg call a smart contract w/ some input at a block height)\nThere are a few variations these tables can be utilized, but the custom lookup table is always an input into the primary circuit. This is nice because in a mobile app setting this enables us to generate a snark and its corresponding tables very quickly. What remains is proving the table used is in the global lookup tree. The mobile app can submit the zkp + lookup table to an infrastructure provider which will finalize the snark.\nThe infrastructure provider can verify the table validity, and submit it for inclusion into the trusted MMR. Once included, an inclusion proof can be generated. The original snark can be recursively updated to verify the table used exists within the global MMR. The root of the MMR then becomes a public input of the resulting ZKP. The location of the table can optionally be disclosed as well.\nThe overall trade-off is pretty simple. For the price of waiting a bit to submit/verify the proof, you get fast and memory efficient snarks client-side. The waiting to submit can be handled for the user with infrastructure.\nComplex fraud proofs are not required for the optimistic variation. A smart contract can be used to check validity of entries when prompted.\nConclusion\nI am very confident the above would work, and yield dramatic performance gains to client-side snark proving.\nFor the optimistic variation we can enable truly complex lookups that simplify ZKPs around EVM storage proofs. The tradeoff for very complex look ups would be a longer settlement time. The settlement time can be decreased if you limit the lookup operations in complexity (sha256, floating point operations etc).\nIn the immediate finality variation, there will be a larger financial cost for each addition to the MMR. I suspect the EVM would be prohibitively expensive, especially at scale. To optimize this approach it would be best to build a new chain. As of the time of writing, Solana is doing 3500 TPS at a tx fee around $0.0002. If you reduced the complexity of the execution environment you would get more TPS for cheaper.\nIt hit me after my initial post that there is an alternative use case for the technique described above where a MMR is unnecessary.\nFrom the perspective of the prover, generating the main ZKP is done in two parts\nCompute lookup table(s) for circuit\nUse table(s) as a public input to generate the ZKP\nFrom here we have a ZKP and a list of assumptions the ZKP is made on. The other use case is the prover can offload some of the heavy parts of the circuit to infrastructure to validate the assumptions, and recursively update the ZKP.\nPretty neat\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "Bandwidth Availability in Ethereum: Regional Differences and Network Impacts",
        "link": "https://ethresear.ch/t/bandwidth-availability-in-ethereum-regional-differences-and-network-impacts/21138",
        "article": "This study was done by @cortze and @yiannisbot from the ProbeLab team (probelab.io), with the feedback of the EF and the PeerDAS community.\nThe Dencun upgrade was an important milestone for Ethereum’s roadmap. It introduced Blob transactions and Blob sidecars (through EIP-4844). The upgrade included major updates for the underlying network, which now has to share over GossipSub a target of three blob sidecars of 128KBs each per slot, caped to six of them as a maximum (often referenced as “3/6”, referring to target and max blob sidecars, respectively).\nThe current network hasn’t experienced any major outage or finalization difficulty since the upgrade. Nodes in the network could get along with the current blob target and max values with the blob sidecars propagating through the network well within the 4 second propagation window, as can be seen in ProbeLab’s weekly report on Block Arrivals: Week 2024-47 | ProbeLab.\nHowever, with increased demand from L2s for more capacity, questions come up with regard to “what is the maximum “target/max” blob count that the network can comfortably handle?”, especially for home stakers.\nThe motivation and target of this study is to help define how much extra bandwidth is available in the network to support a blob target and max value increase for the next hard fork.\nIn this post, we:\nintroduce the measurement methodology and framework we’ve prepared to estimate the available upload bandwidth of nodes in the network.\npresent a comprehensive bandwidth measurement study that we carried out, where we measured the bandwidth availability of most online and reachable nodes in the Ethereum network over a period of 6 days from 2024-11-23 to 2024-11-28.\nWe’ve built a tool called net-probe that:\npulls node information from the Nebula crawler, which crawls the Ethereum discv5 network every 2 hours.\nconnects to each node discovered during the crawl through Hermes.\ndownloads a carefully chosen volume of data from each node Hermes connected to.\nBefore executing the full experiment, we had to find out what’s the right amount of data we should be pulling from each node, in order to: i) saturate the node’s uplink and as a result find out how much bandwidth it’s got available, but at the same time, ii) avoid disrupting the node’s operation. Note that these are Ethereum mainnet nodes, so we had to proceed with care to avoid any disruption.\nWe tested the following parameters:\nBlock-by-range RPC calls requesting:  1, 5 10, 20, 30 and 40 blocks,\n10 retries for the RPCs with  1, 5, 10, and 20 blocks,\n5 retries for the RPCs with 30 and 40 blocks, to avoid spamming those peers.\nSetting the RPC size\nThe following plot shows the bandwidth we could measure (x-axis) for the nodes we managed to connect to for different RPC size requests, i.e., different number of blocks requested per RPC.\nWe extract the following conclusions from the comparison of the different test studies:\nSending RPC calls for 1, 5 and 10 blocks gives widely different results, with higher RPC size requests showing more bandwidth availability. This is a clear indicator for increasing the “blocks requested per RPC” value, as we don’t seem to be saturating the nodes’ bandwidth with smaller RPC calls.\nThe CDF plot shows that the BW measurement barely changes when we request 20, 30  or 40 blocks per RPC, indicating that this RPC size seems to generate enough traffic on the TCP connection to saturate the uplink bandwidth of nodes.\nimage1000×600 65.5 KB\nNOTE: the mean size of the 54,463 downloaded blocks was 101.30 KB of serialized data and 47.76 KB of compressed data.\nSetting RPC retries\nAs expected, there are also some differences in the bandwidth availability (y-axis) observed after consecutive RPC retries (x-axis in the following plot):\nThe first 2 RPC responses do not max out the available bandwidth in the TCP connection (see upward trend of line-plots).\nAfter the 3rd one, the measurements become pretty stable for most RPC sizes, i.e., the trend is flattening.\nimage1000×600 54.7 KB\nThe above plot gives the impression that larger RPC-sized requests result in higher BW availability.\nHowever, taking into account the ratio of requests that are successful at each RPC retry, we see that requesting a larger number of blocks does have an impact on the success rate of the responses. For example, requesting 30 or 40 blocks starts failing after the third retry. Furthermore, requesting consecutively larger RPCs also generates a faster decrease in the success rate over the RPC retries.\nimage1000×600 56.9 KB\nFinal parameters\nGiven the above observations, we have chosen the following values for our production study:\nRetries: 6 sequential RPC Requests\nRPC size:  20 blocks per retry\nRequest concurrency of 2 nodes at a time\nDates: 2024-11-23 to 2024-11-28\nInfrastructure: AWS EC2 instance from the following regions:\n\nus-west-1  - California\nus-east-2 - Virginia\neu-central-1 - Frankfurt\nap-southeast-1 - Sydney\n\n\nus-west-1  - California\nus-east-2 - Virginia\neu-central-1 - Frankfurt\nap-southeast-1 - Sydney\nResulting statistical sample:\n\nThe results we present in the following were obtained from a total of 13,023 unique peers from the Nebula Ethereum database for 6 days (unique online peers over those dates).\nWe collected BW measurements from 9,179 nodes, representing 70,48% of the total online and reachable nodes.\n\n\nThe results we present in the following were obtained from a total of 13,023 unique peers from the Nebula Ethereum database for 6 days (unique online peers over those dates).\nWe collected BW measurements from 9,179 nodes, representing 70,48% of the total online and reachable nodes.\nNOTE: The tool can measure 3 different types of BW measurements:\nthe effective BW: the bytes of serialized data we could measure,\nthe compressed BW: the bytes of compressed data we could measure,\nthe wire BW: all the bytes shared over the wire.\nFor simplicity and completeness, the rest of the report will refer to the “wire BW”.\nThe following plot shows the CDF of the mean wire BW we measured from the 9,179 unique nodes net-probe could successfully connect over these 6 days. The graph shows the BW measurements experienced from each region net-probe was running on, so the figure can be read as follows:\nThe BW measurements from our net-probe deployments in the US and Europe show that 40% of the network peers served blocks at a rate below ~20Mbps. This also means that the remaining ~60% of nodes had an upload capacity of more than ~20Mbps.\nMeanwhile, our net-probe deployment in Sydney could only achieve 20Mbps upload speed with 20% of network nodes (see 0.8 mark on the y-axis for the ap-southeast-2 region). This is not surprising, given that the geographical distribution of nodes we observe at ProbeLab shows that almost 65% of the network nodes are deployed in the US and Europe.\nimage1000×600 71 KB\nCDF of the wire bandwidth for each of the nodes 9,179 successfully connected. Please note that the graph is zoomed within the 0 to 150 Mbps range.\nBandwidth Availability per Infrastructure Deployment (cloud vs non-cloud)\nIn the following plot, we present the bandwidth observed for different types of node deployments, namely, those that are hosted on public cloud infra versus those that are not.\nWe observe that nodes operating in cloud providers provide blocks at a higher upload BW rate, having approximately 5Mbps of extra bandwidth available, compared to non-cloud deployments.\nThis marginal difference in bandwidth availability between cloud and non-cloud deployments shows that solo stakers (most likely to use non-cloud infra), are putting enough resources into their node deployment.\nimage1000×600 75.9 KB\nCDF of the wire bandwidth segregated by the nodes’ hosting type ( 4,409 cloud vs 4,770 non-could nodes).\nBandwidth Availability per Client Implementation\nComparing the upload bandwidth by the client implementation of the remote nodes, the following image shows that most clients share similar distributions until the 70th percentile, with the exception of Lodestar, which we discuss below. From that point on, the differences in the distribution’s tail become slightly more visible.\nOur takeaway points from this measurement are:\nNo major differences between different client implementations in terms of bandwidth availability.\nLodestar seems to be outperforming the rest by at least 10Mbps at the 60th percentile.\nLodestar’s improved performance is very likely due to the fact that more than 80% of its nodes are based in the US or EU (link), which as we’ve seen earlier present higher bandwidth availability.\nimage1000×600 68.9 KB\nCDF of the wire bandwidth segregated by the nodes’ client.\nBandwidth Availability per Region\nBandwidth measurements, as well as latency measurements, are highly subjected to the location from where the data is being generated. Thus, we’ve chosen our net-probe deployments to represent: the most popular regions for Ethereum node deployments (US and EU), and the one geolocated the furthest (AUS). The following image provides the CDF of the wire BW of nodes from the four regions aggregated by continents. The measurements indicate that, as expected, regions further away present lower bandwidth availability. In this case, we observe that:\nOceania and Africa are at least 8 to 10 Mbps behind the rest of the regions, providing a non-desirable 90th percentile below the 20 Mbps mark.\nNodes in Asia and South America achieve 15 to 25 Mbps between the 50th and 80th percentile.\nNodes in Europe match the BW distribution of NA nodes until the 40th percentile, which later diverted towards a median of 24 Mbps up to the 90th percentile of 60 Mbps.\nimage1000×600 65.4 KB\nCDF of the wire bandwidth segregated by the nodes’ continents\nBandwidth Availability per Slot Window\nEach Ethereum slot is split into different parts each of which is allocated to different duties over the 12 seconds slot duration. Trying to correlate the available wire BW across the slot time, the following image shows the mean wire BW availability of nodes for each node hosting type at each second within the slot. We observe the following:\nCloud-hosted nodes have a higher mean of available BW, but both cloud and non-cloud nodes’ portion of available bandwidth fluctuates visibly throughout the slot.\nThere is clearly less bandwidth availability from the 1st to the 4th second of the slot, which is the window when blocks and blobs are broadcasted to the network. The bandwidth drop we were able to measure represents a 9,5% to 10% drop for non-cloud users, while it was a 13% of bandwidth reduction for nodes located in data centers.\nDespite this drop in the mean amount of bandwidth available, there is still ~19Mbps for non-cloud hosted nodes and ~23Mbps for cloud hosted nodes available. This is a strong indication that a slight increase in the blob count target and max values should not be disruptive to the network.\nimage1000×600 71.4 KB\nMean wire bandwidth availability per deployment type over time within the slot.\nRPC Success Rates Throughout the Slot\nAs a byproduct of our study, we captured the number of successful RPC replies that we got throughout the slot, presented by the “Number of data points” available to us. There is a dip in the number of data points (i.e., successful RPCs) between the 2nd and 7th/8th second of the slot when blocks, blobs and attestations are getting broadcasted, propagated and processed.\nimage1000×600 74.1 KB\nHistogram of the successful data points at each slot-second.\nCurrent blob count per block\nHaving seen node bandwidth availability from several perspectives, it is important to understand how many blob transactions have been included in blocks during the measurement period. This aspect is important in order to asses what are the implications of a blob count increase, i.e., how many blobs resulted in the bandwidth availability that we’ve seen and how much space is there for extra blobs.\nThe following graph shows the number of blobs that were included in each slot throughout the 6 days of study. Here are our observations:\n35% of the slots had no blobs at all.\n42% of the slots included 5 to 6 blobs.\nGiven the current 3/6 target/max blob values, 42% of the bandwidth measurements we did were performed at almost the blob max capacity.\nAssuming demand will keep up when the blob count target and max increase to 6/9, we can project that in ~42% of the cases there will be 9 blobs per block and in ~50% there will be 6 blobs per block, or less.\nGiven the above, our take is that we don’t see any critical objection to the current desired blob target and max values of 6/9 discussed in the All Core Dev CL Call on Thursday 28th of November.\nimage1000×600 26.7 KB\nCDF of the number of blobs per slot.\nFrom this extensive set of measurements, we extract the following conclusions:\nRepresenting the perspective of at least 65% of the network (nodes in NA and EU), we’ve measured that only 30% to 40% of the measured peers reported an upload link below the 20 Mbps mark.\nOn the other hand, when nodes are located in more remote locations like Sydney, only 20% of connections went above 20 Mbps.\nWe need to keep in mind that this available BW is effectively doubled if we consider that 80% of the blocks achieve at least a snappy compression rate of 2.\nNodes deployed in cloud infra seem to have at least 5 Mbps more bandwidth available than non-cloud nodes.\nThe measured bandwidth throughout the slot time shows that there is, on average, a 9% to 13% drop in bandwidth availability between seconds 1 and 4.. The precise moment where the block plus blobs are meant to be broadcasted to the network.\nWe could spot a slight decrease in the success rate of the RPC requests we sent during the “busy” moments of the slot, which could affect, to some degree, the sampling of blobs in PeerDAS.\nWith the current discussion around increasing the blob count from a target and max values of 3/6 to 6/9, the presented metrics align with the EthPandaOps recent post (link) that it should be a reasonable increase with the current network state. The network already has 50% of slots at the targeted blob count goal or beyond, while 35% of slots don’t even have associated blobs.\n",
        "category": [
            "Networking"
        ],
        "discourse": []
    },
    {
        "title": "Agent-based Simulation of Execution Tickets",
        "link": "https://ethresear.ch/t/agent-based-simulation-of-execution-tickets/21254",
        "article": "by Pascal Stichler (ephema labs)\nMany thanks to Julian, Jonah, Marc and Chris for valuable feedback and special thanks to Barnabé for prompting the research in the first place and guiding it.\nPlease note: This is only a summary of the findings. The complete research report can be found here and the code is available on Google Colab and Github. Instructions on how to run the simulation are shared here. Simulation results are available in this folder and a recording of a presentation at Devcon 2024 on the topic can be found here.\nExecution Tickets are currently discussed as a promising next evolutionary step for enhancing Ethereum’s block space allocation mechanism. With Execution Tickets the protocol sells the right for execution block proposing. This is done by offering tickets, which allow ticket holders to participate in a lottery to be drawn as an execution block proposer in the future. It separates consensus rewards such as priority tips paid by users from execution rewards such as Maximum Extractable Value (MEV). It aims to foster decentralization among beacon chain validators by reducing the sophistication requirements for validators by removing optimization shenanigans like timing games [1]. Further, it enables protocol-level capture of MEV. Thereby, it essentially aims to tackle two problems: the “allocation” problem that currently MEV rewards leak to block proposers and secondly the “centralization” problem of the MEV supply chain [2].\nExecution Tickets were initially introduced by Justin Drake here outlining the motivation. He explains the general mechanism design of separating consensus from execution layer rewards and selling Execution Tickets for participating in a lottery to be chosen as the execution block proposer. Mike Neuder formalized and more clearly outlined the mechanism design here and also collected several open questions. We provide a more colloquial introduction on how and why protocol mechanisms evolved here. In a good economic analysis here and here by Jonah Burian, Davide Crapis and Fahad Saleh it is shown that when priced correctly, Execution Tickets can internalize all value generated MEV rewards at protocol level. However, with the important limitation that “the protocol must be capable of selling tickets at their intrinsic value”. Further, Barnabé Monnot provides a good overview of protocol considerations over time and how Execution Auction (EA) as a special implementation of Execution Tickets with a fixed 32-slot advance period could function here. The idea of Execution Auctions is further extended by Thomas Thiery here by introducing a randomization element (randEA).\nAs the previous literature often focuses on the bigger picture and leaves out the knits and grits of the mechanism design we plan to focus on the details of the mechanism design. For example, what is the best pricing mechanism for selling Execution Tickets? Should there be a variable or fixed amount of tickets [3]? Shall tickets be expiring and resaleable? Or maybe returnable to the protocol?\nTo provide insights into answering these questions, we developed a theoretical framework identifying the primary objectives of an Execution Ticket mechanism design, metrics how to measure them, and propose desirable price characteristics. Further, we outlined the main mechanism design parameters and their possible expressions. Based on a theoretical evaluation and an agent-based simulation, we draw primary conclusions on the mechanism design.\nThe approach to validate potential mechanism designs for Execution Tickets is twofold.\nIn the first step, we conduct a theoretical analysis of the objectives that the mechanism aims to optimize and propose several metrics to measure the achievement of these objectives. Next, we outline the design space of possible mechanism attributes and their potential values. This includes properties of the tickets as well as potential pricing and allocation mechanisms (e.g. auction-based formats vs. quoted-price formats). Based on a preliminary theoretical analysis, potential concrete mechanism designs are proposed and evaluated using a theoretical framework.\nIn the second step, these findings are verified using an agent-based simulation. Simulations (e.g. EIP-1559 simulation) have proven to be a suitable tool to estimate the impact of potential mechanism design choices. The simulation emulates the allocation, trading, and redemption processes of Execution Tickets. The scope of the simulation is to run the previously designed configurations and compare them based on the objectives. Furthermore, conclusions can be drawn from the simulation about each parameter to determine favorable choices. The simulation is implemented in Python using existing industry standard frameworks (radCAD). For brevity reasons the simulation assumptions and specifications are not outlined here, but can be found in the research report in Chapter 5.1.\nTo outline the possible design mechanisms, we first outline the desired mechanism behavior and the solution space of different configurations before evaluating them.\nTable 1: Summary of important objectives\nExecution Tickets aim to optimize two key objectives: fostering decentralization among beacon chain proposers and capturing Maximum Extractable Value (MEV) at the protocol level. Thereby it addresses the two key goals of the Ethereum roadmap segment “The Scourge”: (i) Minimize centralization risks at Ethereum’s staking layer and (ii) Minimize risks of excessive value extraction from users.\nDecentralization is a key aspect, as it prevents several unfavorable dynamics. In the context of Execution Tickets it can be divided into beacon chain validator and execution chain decentralization. Execution chain centralization can happen on ticket holder or on block builder level [2]. Generally, decentralization ensures liveness in the sense that not a single actor can voluntarily or involuntarily halt the chain and impair liveness. Further, it contributes to censorship resistance [4]. For these reasons, beacon chain validator decentralization is paramount. Execution chain proposer decentralization is less critical under the assumption that beacon chain validators can force certain transactions into the block, for example with a version of inclusion lists and JIT top-of-block auctions. In this case, execution chain proposer decentralization is mainly relevant to avoid liveness risk and to foster competitive bidding for ETs.\nCapturing MEV at the protocol level is essential as it removes MEV rewards from beacon chain validator rewards and most likely burning the rewards is the most neutral way to do so. Additionally, from a game theory perspective the mechanism should adhere to certain criteria outlined in [5]. Firstly, block producers must be incentivized to participate and propose non-empty blocks, described as Block Producer Incentive Compatible (BPIC) [6]. Secondly, it should be resistant to Off-Chain Agreements (OCA-proof), meaning that participants cannot mutually benefit from making off-chain agreements. Lastly, it should be Dominant-Strategy Incentive Compatible (DSIC), meaning for each participant there is a dominant strategy they can apply regardless of the behavior of other participants. For example, in a sealed-bid first price auction participants need to theorize about the intrinsic valuations of other participants and their bidding strategies to calculate their bid, making it not DSIC and thereby making it more complicated for participants and potentially not incentivizing to reveal their true intrinsic valuation.\nWe propose to measure decentralization through three metrics: Nakamoto coefficient, Herfindahl-Hirschman Index (HHI) [7], and market share of the largest ticket holder. Further, MEV capture can be assessed by the share of MEV rewards of ticket holders captured at the protocol level.\nExecution Ticket price behavior we see of secondary importance, however still worthwhile to consider. Thereby we focus on three aspects: price predictability, smoothness, and accuracy. Price predictability is crucial for validators to participate in auctions and plan long-term. As summarized in [8], volatility can be a measure for price predictability in financial markets, following methods like the Garman-Klass (GK) measure [9]. The Garman-Klass measure is traditionally used in financial markets to measure volatility by including the daily opening, low, high and closing price. For our purpose the time interval needs to be adjusted, e.g. to epoch-based intervals. Price smoothness ensures stability during market fluctuations, reducing risk for ticket holders, with the variance of consecutive price changes (V(Δp), essentially being autocorrelation of prices) proposed as a measurement. Lastly, price accuracy reflects the true value of ETs, aiming to capture the maximum share of MEV while remaining attractive to participants, measured similarly to MEV capture.\nIn table 2 we outline the design space of possible configurations of Execution Tickets.\nTable 2: Outline of possible execution ticket configurations\nWhile most attributes are straightforward, we will provide some background on the pricing mechanisms. Unlike MEV-Boost, where rewards may go to block producers, Execution Ticket earnings are intended to benefit protocol token holders by being burned, thereby increasing social welfare [10]. Fixed pricing mechanisms are deemed inefficient for maximizing social welfare [11], so the focus is on dynamic pricing mechanisms.1\nThe pricing mechanisms are categorized into two main categories being auction-based and adaptive quoted price formats:\nFirst-Price Auctions (FPA): Bidders submit bids without knowing others’ bids, and the highest bidder wins and pays their bid amount. FPAs often lead to bid shading, where bidders underbid their true valuations, resulting in inefficiencies and high volatility. Sealed-bid first price auctions are not DSIC (Dominant Strategy Incentive Compatible) [5]. Open ascending-bid first price auctions can be DSIC (h/t to Julian for pointing this out!). As they behave similarly to SPAs, we focused on sealed-bid FPAs.\nSecond-Price Auctions (SPA): Also known as Vickrey auctions, bidders submit sealed bids, and the highest bidder wins but pays the second-highest bid. This format encourages truthful bidding since bidders pay less than or equal to their true intrinsic value. While SPAs are almost OCA-proof (Off-Chain Agreement proof), they may be susceptible to manipulation through fake bids [13]. However, since Execution Ticket earnings are burned rather than rewarded to validators, this risk is mitigated but might make them more susceptible to off-chain agreements.\nAdapted EIP-1559 Pricing: An adapted version of EIP-1559 for Execution Tickets involves the protocol quoting a price that adjusts similarly to EIP-1559. However, while for EIP-1559 the adjustment is based on the gas usage, for Execution Tickets it needs to be based on the number of outstanding tickets relative to a target amount. Tickets could either be sold on a continuous basis where ticket holders can always buy a ticket from the protocol if they desire or in a batch process where at each slot between zero and a specified maximum of tickets are sold. While EIP-1559 has been effective in maintaining gas usage near the target [14], its retroactive price adjustments may lag during MEV spikes making it more challenging for Execution Tickets.\nAdapted AMM-like Pricing: The adapted version of an AMM-like pricing entails the protocol dynamically updating the price of the tickets based on a bonding curve and the amount of outstanding tickets. Here as well a target amount of outstanding tickets needs to be defined and the bonding curve function needs to be adapted and carefully designed. In the research paper we outline three options how this might be adapted and implement one in the simulation. However, this remains the scope of future research on how to best adapt it.\nTo substantiate the parameters, several possible mechanism designs are outlined. Given that based on the categorial parameters alone already 512 configurations are possible2, only sample mechanism designs are evaluated. In more detail, the following configurations are evaluated:\nTable 3: Overview of possible mechanism design configurations\nTable 4: Simulation results on selected mechanism designs1256×956 95.3 KB\nGenerally, the simulation results of over 300 simulation runs show that in all configurations decentralization remains a challenge. None of the configuration scores particularly well on the decentralization metrics. This is driven by the diverse abilities of ticket holders (based on [15], [16]) and the fact that in most scenarios the bids are based on expected future valuations which leaves out specialization factors. It shows that in cases with a secondary market enabled, the centralization forces are reduced. This derives from specialized ticket holders being able to more accurately estimate the true value of MEV for a slot in just-in-time (JIT) auctions and thereby winning the auction. With regards to MEV capture, we can see different attributes emerge. The auction formats generally score well, similarly the AMM-style pricing scores well. The 1559-style pricing is capturing less MEV due to a step-wise and less dynamic price adaptation mechanism. With regards to the price predictability, smoothness and accuracy we can observe that the auction formats that operate with a longer lookahead are very predictable and smooth, while JIT auctions and a 1559-style pricing are less smooth.\nFirst Price Auctions\nWith regards to first price auctions, we saw a “winner’s curse” play out, in the terms that assuming that bidders have differing intrinsic value expectations for a ticket which are following a normal distribution, the most optimistic bidders wins. And the most optimistic bidder with the highest valuation overestimates the value the most and thereby makes a loss on the trade. This is a known problem of auctions (e.g. [17]). However, noteworthy to point out, as this leads to higher “risk-adjustments” by bidders which in turn could lead to reduced MEV capture by the protocol.\nWith regards to the simulation it shows that first price auctions generally perform well, however two things need to be critically challenged here. Firstly, we have implemented it as a sealed bid auction, as we assume the operational communication overhead for a leaderless auction with ascending bids is too high. Therefore, holding sealed-bid on-chain auctions must be feasible. As outlined in the research report, several proposals for this are currently being discussed, however still in the earlier stages [18], [19]. Secondly, as sealed-bid first price auctions are not DSIC, no single dominant bidding strategy exists. Hence, in the simulation the bidding is based on a heuristical bidding strategy where bidders have no information on the intrinsic valuations of other bidders. This assumption will not hold true in multi-round scenarios of Execution Ticket selling. So more sophisticated bidding strategies based on historical bids of competitors might emerge that might potentially reduce the captured MEV. So it is unclear yet if first price auctions can be actually designed in this scenario in a way to behave differently than second price auctions.\nSecond Price Auctions\nFor second price auctions we observed that the MEV capture highly depends on the competitiveness of the specific simulation. In cases with at least two similarly strong ticket holders, the MEV capture was high. However, on average it was only medium, given the missing competition.\n1000×600 53 KB\nFigure 1: Example simulation results for second price auctions with two similarly capable ticket holders (Source: 2024-09-24_10-52 UTC, runs: 10, time steps: 1000)\nEIP-1559 Style Pricing\nAs outlined above, the EIP-1559 pricing needs to be adapted to work with Execution Tickets and we have implemented it as a batch process. However, we observe that this leads to self-reinforcing oscillating ticket prices. Even adjusting the price adjustment factor does not lead to better outcomes in our simulations. This leads to the conclusion that a batch update process is not sufficient. How a continuous price update process can be technically implemented in a decentralized setting remains an open question. Overall, the pricing mechanism needs to be carefully designed to achieve the desired price behavior.\n1000×600 35.8 KB\nFigure 2: Price curve for EIP-1559 style pricing\nFurther, in certain simulations4 we have observed that if one ticket holder has a significantly higher willingness to pay, the prices stabilize at a point where only this ticket holder is able to purchase tickets, leading to a high centralization.\nAMM-style Pricing\nFor the AMM-style pricing as outlined above it needs to be adapted to be suitable for Execution Tickets. Running configurations with AMM–style pricing shows that the pricing mechanism can be sensitive to the adjustment factor. A too slow adoption does not accurately capture the demand, a too large adaption factor is not granular enough to differentiate the expected valuations and would lead to a latency race.\nHowever, the simulations show promising results that this mechanism is able to capture a high level of MEV. From an operational perspective it remains to be investigated how this could be implemented to suit the selling process needed for Execution Tickets.\nConclusion on Auction Formats\nTaking into consideration the different observations, based on the simulation results we conclude that an auction based format, most probably second price auction, is the most feasible format. It leads to a high captured MEV, is DSIC and leads to favorable price properties in the simulation. An AMM-style pricing seems also to be a promising solution, however more open design questions on mechanism and implementation remain.\nOne relevant question remains open around OCA-proofness, in case the ET earnings are burned. There might be a sybil attack vector where block builders bribe the actor / committee defining the winning bid and thereby being able to achieve a lower price. E.g. if the winning bid is 10 ETH, the block builder however pays the committee members 5 ETH to artificially set the winning bid price at 1 ETH, there could be a 4 ETH profit margin. To avoid this, bids or prices would need to be on-chain which is not feasible given the time horizon of the auction. Another option could be a leaderless auction as outlined by [20].\nFindings on Ticket Attributes\nWe observe that the question of a fixed vs. a variable amount of tickets is closely related to the pricing mechanism. For certain mechanisms, fixed amounts of tickets make more sense (auctions) while for others (EIP-1559 and AMM-style) a flexible amount is better. Hence, we see this more as a secondary attribute that is deducted from the pricing mechanism.\nRegarding expiring tickets, we observe in the simulation that especially for short expiry times the MEV capture is impaired, as ticket buyers need to discount the value of a ticket on the primary market and on the secondary market since the possibility of a ticket expiry without redemption needs to be priced in. This leads to generally lower captured MEV values. Further, we observe that it has secondary complications as the pricing of each actor becomes more sophisticated as the expiry period, outstanding tickets etc. need to be factored in. This leads to the conclusion that non-expiring tickets seem to be the favorable configuration.\nRegarding refundability, we only observe limited effects on the market dynamics with the tested discount (around 20 %). It leads to more security for ticket holders. However, this depends on the discount. Further it is closely related with the secondary market. In case a secondary market exists, this option is often more attractive to dispose of tickets. It shows that allowing for refundability does not influence the mechanism in a substantial way and complicates the design choices as well as the decisions for ticket holders. Hence, the preliminary analysis leads to the conclusion that tickets shall not be refundable.\nRegarding the secondary market, an interesting finding is that this increases decentralization. Due to the ability of more specialized ticket holders to buy tickets just-in-time in periods where they are able to capture higher MEV due to specialization. Further, it leads to overall higher MEV captured due to reduced risks for the primary ticket holders. Additionally, we observe that in some configurations with discrete pricing (e.g. AMM-style pricing) it leads to arbitrage opportunities, if the AMM-pricing is not adapting fine granularly enough and tickets can be bought by the ticket holder with the lowest latency and then be resold at a higher price at the secondary market. Given that also from a technical perspective it is difficult to prevent a secondary market, a preliminary recommendation is rather embrace the benefits of it and try to foster it.\nOur research did not focus on the beacon round attestation and the secondary effects ETs might have on it.\nAdditionally, we did not focus on the specific details of inclusion lists. They are briefly discussed in the research report as a potential mechanism to ensure liveness, but in the simulation and configurations it will not be a focus of the work. This ties closely to multi-block MEV. As we have shown in our previous work, it has historically not been structurally observed, however might be a concern for Execution Tickets. The topic is hence briefly discussed in the research report, but not in-depth evaluated and not implemented in the simulation. Further, timing games are not included in the simulation. Additionally, assuming sealed-bid auctions, we work with static demand functions of the ticket holders that do not take into consideration the bids of other ticket holders. In addition, considerations around private order flow are not modeled in the simulation. Furthermore, the role of relays is left out and we don’t simulate missed blocks and missed block penalties.\nRegarding the pricing mechanisms, we propose initial versions of how they can be designed, however leave the verification and formal definition to future research. This includes the more in-depth research of specific parameters such as adjustment steps for EIP-1559-style pricing and others. We only look at this from an exploratory perspective.\nFurther, we exclude a more in-depth analysis around the burning mechanism of the earnings from the Execution Ticket sales. As outlined in [21], burning mechanisms usually impair the OCA-proofness of mechanisms.\nExecution Tickets present a promising next evolutionary step for enhancing Ethereum’s block space allocation mechanism. It separates consensus rewards from execution rewards and sells the execution rights in an effective manner. It aims to foster decentralization among beacon chain validators and enables protocol-level capture of Maximum Extractable Value (MEV).\nWe developed a theoretical framework identifying three primary objectives of an Execution Ticket mechanism design: decentralization, MEV capture, and Block Producer Incentive Compatibility (BPIC). Further, we propose metrics on how to measure the objectives. For decentralization we propose to use the highest market share, Nakamoto coefficient, and Herfindahl-Hirschman Index, while for MEV capture we propose to measure the MEV share of the protocol from Execution Ticket holder earnings. Further, the three price characteristics of price predictability, smoothness and accuracy are identified as desired attributes.\nTo evaluate the parameters and configurations, we implemented an agent-based simulation and based on over 300 simulation runs several findings are concluded. Results indicate that while none of the mechanisms scores particularly well on decentralization, enabling a secondary market reduces centralization by allowing specialized ticket holders to purchase tickets just-in-time. Regarding MEV capture, auction formats and AMM-style pricing performed well, whereas EIP-1559-style pricing captures less MEV and has stronger price fluctuations. Auction formats with longer lookahead periods demonstrated favorable price predictability and smoothness while scoring slightly less favorable on price accuracy.\nBased on this, a second-price auction format seems most promising as it achieves high MEV capture, adheres to Dominant-Strategy Incentive Compatibility (DSIC) and exhibits favorable price characteristics. First-price auctions and AMM-style pricing formats show promising results in the simulation as well, however leave more questions open from a theoretical mechanism perspective. Non-expiring tickets score better as they avoid impairing MEV capture due to discounted valuations from expiry risk. Refundability was found to have limited impact on market dynamics and adds complexity; thus, non-refundable tickets are suggested. Embracing a secondary market seems favorable, as it enhances decentralization and increases overall MEV capture. Nevertheless, in line with [22] and [23] we observe that the decentralization of the builder market is challenging and highly depends on the MEV extraction capabilities of the top builders.\nOverall, inline with previous theoretical work [2], [22], [24] we conclude that Execution Tickets pose a promising mechanism to foster beacon chain validator decentralization and capture MEV at protocol level. However, questions remain around block builder centralization, proneness to off-chain agreements and multi-block MEV.\n[1] C. Schwarz-Schilling, F. Saleh, T. Thiery, J. Pan, N. Shah, and B. Monnot, “Time is Money: Strategic Timing Games in Proof-of-Stake Protocols,” May 2023, [Online]. Available: [2305.09032] Time is Money: Strategic Timing Games in Proof-of-Stake Protocols\n[2] J. Burian, D. Crapis, and F. Saleh, “MEV Capture and Decentralization in Execution Tickets,” Aug. 21, 2024, arXiv: arXiv:2408.11255. Accessed: Oct. 14, 2024. [Online]. Available: [2408.11255] MEV Capture and Decentralization in Execution Tickets\n[3] C. Schlegel, “Inelastic vs. Elastic Supply: Why Proof of Stake Could Be Less Centralizing Than Execution Tickets - Research,” The Flashbots Collective. Accessed: Oct. 14, 2024. [Online]. Available: Inelastic vs. Elastic Supply: Why Proof of Stake Could Be Less Centralizing Than Execution Tickets - Research - The Flashbots Collective\n[4] J. Lee, B. Lee, J. Jung, H. Shim, and H. Kim, “DQ: Two approaches to measure the degree of decentralization of blockchain,” ICT Express, vol. 7, no. 3, pp. 278–282, Sep. 2021, doi: 10.1016/j.icte.2021.08.008.\n[5] T. Roughgarden, “Transaction Fee Mechanism Design,” ACM SIGecom Exch., vol. 19, no. 1, pp. 52–55, 2021, doi: 10.1145/3476436.3476445.\n[6] M. Bahrani, P. Garimidi, and T. Roughgarden, “Transaction Fee Mechanism Design with Active Block Producers,” 2023, [Online]. Available: [2307.01686v2] Transaction Fee Mechanism Design with Active Block Producers\n[7] L. Heimbach, L. Kiffer, C. Ferreira Torres, and R. Wattenhofer, “Ethereum’s Proposer-Builder Separation: Promises and Realities,” Proc. ACM SIGCOMM Internet Meas. Conf. IMC, pp. 406–420, May 2023, doi: 10.1145/3618257.3624824.\n[8] S.-H. Poon and C. W. J. Granger, “Forecasting Volatility in Financial Markets: A Review,” J. Econ. Lit., vol. 41, no. 2, pp. 478–539, Jun. 2003, doi: 10.1257/002205103765762743.\n[9] S.-K. Tan, J. S.-K. Chan, and K.-H. Ng, “On the speculative nature of cryptocurrencies: A study on Garman and Klass volatility measure,” Finance Res. Lett., vol. 32, p. 101075, Jan. 2020, doi: 10.1016/j.frl.2018.12.023.\n[10] A. Kiayias, P. Lazos, and J. C. Schlegel, “Would Friedman Burn your Tokens?,” Papers, 2023, [Online]. Available: Would Friedman Burn your Tokens?\n[11] V. Buterin, “Blockchain Resources Pricing.” 2019. Accessed: Mar. 21, 2024. [Online]. Available: research/papers/pricing/ethpricing.pdf at 139e3dd83b06fae918792c495b8ccd0d1635b0d4 · ethereum/research · GitHub\n[12] M. Neuder, P. Garimidi, and T. Roughgarden, “On block-space distribution mechanisms - Proof-of-Stake / Block proposer,” Ethereum Research. Accessed: Oct. 28, 2024. [Online]. Available: On block-space distribution mechanisms\n[13] M. Akbarpour and S. Li, “Credible Auctions: A Trilemma,” Econometrica, vol. 88, no. 2, pp. 425–467, 2020, doi: 10.3982/ECTA15925.\n[14] Y. Liu, Y. Lu, K. Nayak, F. Zhang, L. Zhang, and Y. Zhao, “Empirical Analysis of EIP-1559: Transaction Fees, Waiting Time, and Consensus Security,” Proc. ACM Conf. Comput. Commun. Secur., pp. 2099–2113, 2022, doi: 10.1145/3548606.3559341.\n[15] S. Yang, K. Nayak, and F. Zhang, “Decentralization of Ethereum’s Builder Market,” May 2024, [Online]. Available: [2405.01329v3] Decentralization of Ethereum's Builder Market\n[16] B. Öz, D. Sui, T. Thiery, and F. Matthes, “Who Wins Ethereum Block Building Auctions and Why?,” Jul. 18, 2024, arXiv: arXiv:2407.13931. Accessed: Oct. 02, 2024. [Online]. Available: [2407.13931] Who Wins Ethereum Block Building Auctions and Why?\n[17] M. H. Bazerman and W. F. Samuelson, “I Won the Auction But Don’t Want the Prize,” http://dx.doi.org/10.1177/0022002783027004003, vol. 27, no. 4, pp. 618–634, 1983, doi: 10.1177/0022002783027004003.\n[18] H. S. Galal and A. M. Youssef, “Verifiable Sealed-Bid Auction on the Ethereum Blockchain,” 2018, 2018/704. Accessed: Oct. 14, 2024. [Online]. Available: Verifiable Sealed-Bid Auction on the Ethereum Blockchain\n[19] P. Momeni, S. Gorbunov, and B. Zhang, “FairBlock: Preventing Blockchain Front-Running with Minimal Overheads,” in Security and Privacy in Communication Networks, vol. 462, F. Li, K. Liang, Z. Lin, and S. K. Katsikas, Eds., in Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, vol. 462. , Cham: Springer Nature Switzerland, 2023, pp. 250–271. doi: 10.1007/978-3-031-25538-0_14.\n[20] D. White, D. Robinson, L. Thouvenin, and K. Srinivasan, “Leaderless Auctions,” Paradigm. Accessed: Oct. 04, 2024. [Online]. Available: Leaderless Auctions - Paradigm\n[21] T. Roughgarden, “Transaction Fee Mechanism Design for the Ethereum Blockchain: An Economic Analysis of EIP-1559,” 2020, [Online]. Available: [2012.00854v1] Transaction Fee Mechanism Design for the Ethereum Blockchain: An Economic Analysis of EIP-1559\n[22] M. Bahrani, P. Garimidi, and T. Roughgarden, “Centralization in Block Building and Proposer-Builder Separation,” Jan. 2024, [Online]. Available: [2401.12120] Centralization in Block Building and Proposer-Builder Separation\n[23] M. Pan, A. Mamageishvili, and C. Schlegel, “On sybil-proof mechanisms,” Jul. 22, 2024, arXiv: arXiv:2407.14485. Accessed: Oct. 28, 2024. [Online]. Available: [2407.14485] On sybil-proof mechanisms\n[24] J. Burian, “The Future of MEV - An Analysis of Ethereum Execution Tickets,” 2024. [Online]. Available: [2404.04262] The Future of MEV\n1 Note that further pricing mechanism proposals exist with winning changes being proportional to the paid price as e.g. outlined in [12]\n2 2 (Amount of Tickets) * 2 (Expiring Tickets) * 4 (Refundability) * 4 (Resalability) * 2 (Enhanced Lookahead) * 4 (Pricing Mechanisms)\n3 Results based on 10 runs with 1000 timesteps for each configuration. Color coding of results based on literature and subjective judgment\n4 E.g. see simulation results 2024-05-14_18-09_1_1000_EIP-1559 for details\n",
        "category": [
            "Economics"
        ],
        "discourse": [
            "mev"
        ]
    },
    {
        "title": "PeerDAS with significantly less bandwidth consumption",
        "link": "https://ethresear.ch/t/peerdas-with-significantly-less-bandwidth-consumption/20932",
        "article": "Authors: pop\ntldr; this proposal reduces the bandwidth consumption of PeerDAS by 56.2%\nSince this proposal is an improvement to PeerDAS. Familiarity with PeerDAS is required.\nTopic observation is a building block of this design so it would be helpful if you read it first.\nCurrently GossipSub imposes an amplification factor on the bandwidth consumption to PeerDAS, since more than one peers can send you the same columns. In fact, you need only one copy, so this amplification wastes your bandwidth.\nPreviously we have IDONTWANT implemented which reduces the number of copies you will receive, but it doesn’t guarantee exactly how many.\nThis proposal enables nodes to receive only one copy of most of their sampled columns.\n(For simplicity, let’s assume that DATA_COLUMN_SIDECAR_SUBNET_COUNT and NUMBER_OF_CUSTODY_GROUPS are equal to NUMBER_OF_COLUMNS)\nLet S be SAMPLES_PER_SLOT, C be the size of a column, and D be the amplification factor of GossipSub (aka the mesh degree).\nNodes are required to subscribe to and sample S columns, so each node has to consume the bandwidth about D*S*C bytes per slot.\nPreviously, we have each node subscribe to S GossipSub topics. Now, we subscribe to fewer topics than that. We have each node subscribe to K=2 topics which is lower than S. Nodes will still receive or forward D copies in these K topics, but they will receive only one copy and forward no copy for the remaining S-K topics.\nThe reason that we still need to subscribe to K topics is because we need to provide backbones for the topics as required by topic observations (aka stability of the topics).\nThe bandwidth consumption of K subscriptions is D*K*C bytes per slot.\nslot-breakdown1589×485 41.2 KB\nNow, the remaining question is how the node can get the remaining S-K columns that it needs.\nFirstly, you start observing the topic at the beginning of the slot (shown as a blue line).\nAfter that, your peers will notify you when there is a new message in the topic. Orange lines show when your peers notify you. Notice that peer 2 is the first one who gets the message (column) and notifies you first.\nSince peer 2 notifies you first, you request the actual column from peer 2 with the timeout T (400 ms). After the timeout, if you don’t get it from peer 2, you request it from the peer that notifies you second which is peer 4. If you still don’t get it, you keep going on. Red lines show when you request the column from each peer. The further lines are lighter to indicate that it’s less probable. Consecutive lines are 400ms apart indicating the timeout.\nIt looks like timeouts will delay the column reception a lot because with the current PeerDAS you will get the column right at the orange lines which are faster. In fact, it’s not that bad for the following reasons.\nIt saves a lot of bandwidth. Imagine that you get a copy of the column at each orange line. That looks very wasteful. With this proposal, you get only one copy at one of the red lines.\nTimeouts are rare. You don’t expect to get many timeouts for the following reasons.\n\nThe network condition is already good. If not, how could your peer notify you that it gets a message?. If you could notify me, so you could also send me the column. If it doesn’t, you can probably de-score it.\nYour peer can send you an early rejection without waiting for the timeout. For instance, if your peer is overloaded and doesn’t want to waste the bandwidth sending you the column, it can just send a rejection to you and you can move forward to another peer quickly.\n\n\nThe network condition is already good. If not, how could your peer notify you that it gets a message?. If you could notify me, so you could also send me the column. If it doesn’t, you can probably de-score it.\nYour peer can send you an early rejection without waiting for the timeout. For instance, if your peer is overloaded and doesn’t want to waste the bandwidth sending you the column, it can just send a rejection to you and you can move forward to another peer quickly.\nThe total bandwidth consumption would be (D*K+2*(S-K))*C bytes per slot.\nAssign the parameters with the current assignments in the spec: D = 8, K = 2, and S = 8.\nThe bandwidth consumption of the current PeerDAS is 64*C.\nThe bandwidth consumption of the new one is 28*C which is 56.2% reduction.\nThe reason I assign K=2 is because, with 8k nodes and the number of columns of 128, there will be at least 100 nodes in each topic.\nPessimistically, if you think K=2 doesn’t make the topics stable enough, we can go to K=4 and the bandwidth consumption would be 40*C which is still 37.5% reduction.\nYou can note that the analysis in the previous sections assumes that you will receive or forward exactly D copies of messages when subscribing to topics.\nThis is not true with IDONTWANT since it can reduce the number of copies you will receive by sending IDONTWANT to your peers before they send you a copy.\nThere is a still corner case that IDONTWANT doesn’t help reduce the number of copies at all. Imagine that all of your peers send you the message at the same time (the orange lines are very close to each other), so you don’t have a chance to send IDONTWANT to them in time. So, in this case, you still receive the same number of copies as before. While in this proposal, it’s guaranteed that you will receive only one copy.\nHowever, we can combine this proposal with IDONTWANT to get an even better protocol. Since nodes still subscribe to K topics. IDONTWANT can reduce a lot of bandwidth consumption there.\nPeer sampling is a process that after all the columns are disseminated through the network, you request a column from your peer that’s supposed to have it. If you get the column back, it means that column is available. If not, you may either request another peer or decide that the column is not available.\nYou can see that you always request for a column no matter what which is different from this proposal. In this proposal, you will request a column only if your peer notifies you that it has one. So peer sampling and this proposal are fundamentally different.\nAnother difference is, in peer sampling, you aren’t sure when to request a column. In other words, you don’t know when the column dissemination is finished so that you can start requesting the column. What you can do is to set an exact timestamp that you will assume the dissemination is already finished and start requesting. This sometimes waste you some time since the dissemination is finished far before the timestamp. In this proposal, you don’t get this problem since you’re notified when you can request.\n",
        "category": [
            "Networking"
        ],
        "discourse": [
            "p2p",
            "scaling",
            "data-availability"
        ]
    },
    {
        "title": "Can Ethereum Distribution System reduce disk space usage & enable object passing?",
        "link": "https://ethresear.ch/t/can-ethereum-distribution-system-reduce-disk-space-usage-enable-object-passing/20560",
        "article": "Im looking for a developer community feedback for Ethereum Distribution System project: a generalized, semver enabled fully-on chain distribution system (generalized factory)\nPlease reach back in thread or leave comments in discussion tab on github! \nGitHub - peeramid-labs/eds: Ethereum Distribution System\nEthereum Distribution System \nTLDR, hypotesis I want to validate:\nHypotesis 1\nThe current landscape of smart contract distribution on the Ethereum network is fragmented and inefficient.\nMostly, projects generate numerous deployment artifacts that cannot be optimized in database(?) because of built metadata affects code hashes.\nThis practice results in a increase in blockchain size for node operators, far beyond what is necessary.\nThis likely is much below what storage / tx takes, however, contrary to tx data, bytecode is not something nodes can prune, meaning in the long run it may become more actual?\nI am not familiar with how execution clients store contract bytecode, but I doubt they split metadata objects in separate table, and even if, the smart contract devs are not incentivised to re-use heavily and likely anyway produce overhead number of artefacts\nHypotesis 2\nNewer ecosystems such as Sui claim competitive advantage over Ethereum, for example, for being able to pass objects between contracts.\nFrom my understanding, the EVM is eventually right concept because it allows to build similar abstraction model on top, as it is just a computer architecture.\nWith stateless Distributions proposed in EDS, it is possible to write software that will enable functionality similar as Sui proposes on inside evm. ( e.g,: Two applications distributed trough same distribution / distributor do not need for user explicit approvals as they are by design distributed together. )\nHypotesis 3\nDistributions that are stateless chunks of code, in principle could be used to provision execution layer migration scripts for the whole protocol.\nEIP 7702 discussion is exemplary for this, it’s a breaking change for many security assertions.\nIt could be presented as “Ethereum 3.0”, a network within a network, where upon fork day, Beacon chain promises to migrate all of infrastructure that community added and then do user states (assets) migration ad-hoc whenever users reach out to designated migration contract.\nOpen questions\nHow dumb am I  any of these hypothesises can hold their truth?\nCan bytecode size on Ethereum become a problem in foreseeable future?\nI’ve encapsulated proposal to enshrine use of bytecode hashes in ERC7744 however If this proves to be very helpful, perhaps it’s worth to move in EIPs?\nIf EDS proves to have positive perception, this implies that there will be extensive use of proxies instead of new deployments, perhaps an EIP needed to bake native proxy support for this (even if not, proxies are very popular) ?\n",
        "category": [
            "Execution Layer Research"
        ],
        "discourse": []
    },
    {
        "title": "Prover time comparison of GKR+Groth16 vs. Groth16 for proving MiMC hashes",
        "link": "https://ethresear.ch/t/prover-time-comparison-of-gkr-groth16-vs-groth16-for-proving-mimc-hashes/8373",
        "article": "Olivier Begassat, Alexandre Belling, Gautam Botrel, Nicolas Liochon, Thomas Piellard\nFollowing our proposal here we present the results of our implementation of GKR+Groth16 for MiMC7. This post contains a comparison of proving times for 2^23 (~8M) MiMC hashes using two approaches:\nstraightforward hash circuit in Groth16 using Gnark (extrapolated from the proving time for 2^17 hashes)\nour GKR+Groth16 approach: generate a GRK proof, feed it to a GKR verifier  embedded a in a Groth16 SNARK circuit and generate the associated SNARK proof with Gnark.\nThe latter is 27 times faster than the former, i.e. 3 minutes for GKR+Groth16 vs. 1 hour and 24 minutes for Groth16 only.\nThe circuit\nThe GKR circuit performs a check on the MiMC permutation BN256 scalar field (as opposed to our initial proposition to do it with gMiMC7). It consists of 91 layers with an identical structure but using layer specific round constants. Every layer is made up of gates of the following types:\nCopy gates that output the values of their left entry (ignoring the value on the right entry). \\text{copy}(v_l, v_r) = v_l.\nNonlinear gate \\text{nonlin}(v_l, v_r) = v_l + (v_r + c)^7\n\nFinal round nonlinear gates \\text{fnonlin}(v_l, v_r) = (v_r + c)^7\n\n(To be precise: every layer except the last contains copy and nonlinear gates while the last layer contains only final round nonlinear gates.)\nThe base-circuit is made up of 91 layers, each of which has two inputs, let’s say v = [v_0, v_1], and produces two outputs (except for the final layer, producing only one). There are 2^{b_N} (e.g. 2^{23}) parallel copies of the base-circuit. At every intermediary layer the outputs of the base-circuit are [\\text{nonlin}(v_1, v_0), \\text{copy}(v_1, v_0)]. In the final (output) layer the base-circuit outputs [\\text{fnonlin}(v_l, v_r)] only. Notice that the MiMC round function as described in the paper is of the form x \\rightarrow (x + c + k)^7. Pipelining the operations x \\rightarrow x^7, x \\rightarrow x + c and x \\rightarrow x + k results in \\text{nonlin} being of degree 1 in v_r. This saves a lot of time in the prover. It is, however, important to pre-add v_0 = v_0 + v_1, in the first layer and to use the final round nonlinear gate in the last layer.\nThe results\nWe benchmarked our implementation on a 32 core AWS c5.24xlarge instance. Our benchmarks include the time needed to generate the GKR proof and the time needed to build the SNARK proof of a circuit verifying the GKR proof. It also includes the assignment time for both of those steps. For the benchmark we set bN = 23 (ie: we prove 8M hashes)\nFor the baseline, we benchmark the running time for the assignment and the prover time of a gnark circuit which verifies MIMC. Since SNARK circuits verifying 8M hashes are impractical over the curve bn256, we extrapolated results obtained with fewer hashes. We benchmarked 2^17 hashes and scaled the results up to 2^23.\nWe implemented the following circuit to measure Gnark’s performance.\nWe implemented the GKR prover and the Groth16 circuit of proof of the proof verification.\nWith 2^22 (~4M) hashes:\nWhich is a 27-fold improvement compared to the baseline for 8M hashes.\nConstraints per second\nGnark performances for MiMC hashes are far better than the ones for the GKR proof checker when we look at the metric “number of constraints per second”. The table below shows that there are many more wires in the GKR proof verification circuit than in the circuit verifying the MiMC hashes. In other words, the number of constraints is an imperfect indicator for performance to be expected.\nImpact on the number of hashes on the proving time\nWith a simple Groth16 prover proving time grows linearly with the number of hashes. Groth16 + GKR has a sublinear (logarithmic) overhead, subsequently the more hashes to verify the more interesting this approach is.\nFuture work\nIt is possible to apply this approach to other types of hash functions (e.g. Poseidon) and altogether different purposes (e.g. signature verification). The GKR path is not fully optimized. Specializing the implementation, plus various optimizations should lead to a >30\\times improvement.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": [
            "zk-roll-up"
        ]
    },
    {
        "title": "Perfect Mathematical Composability - Theorem-Like Primitives",
        "link": "https://ethresear.ch/t/perfect-mathematical-composability-theorem-like-primitives/21191",
        "article": "Brandon “Cryptskii” Ramsay  \t\t\t\t\t\t\t\t\t\tDec 6 2024\nAbstract\nPROBLEM: The Infamous blockchain Trilema\nSOLUTION:\n\nIntroduction: Reimagining Blockchain Scalability\nOverpass introduces a revolutionary paradigm we call “perfect mathematical composability.” Think of it like discovering a new mathematical universe where the normal rules of complexity don’t apply.\nThe Traditional Bottleneck\nImagine traditional layer-2 solutions as a city’s transportation system where every vehicle must pass through a central checkpoint. As traffic (transactions) increases, congestion becomes inevitable. Each new route added to the system (composition) requires more complex traffic management and slower verification times.\nOverpass’s Novel Architecture\nInstead, Overpass creates what we might call “mathematical wormholes” - cryptographically secured channels that allow instant state transitions without sacrificing security. The brilliance lies in its mathematical construction:\nThe Core Mathematical Innovation:\nFor components A and B, Overpass guarantees:\nThis seemingly simple equation represents a profound breakthrough. Traditional systems suffer from what I call the “verification tax” - each composition adds overhead. Overpass eliminates this tax through its self-proving state mechanism.\n\nRollups require global consensus and inherit base layer latency\n\n\nPayment Channels demand complex challenge periods and watchtowers\n\n\nPlasma chains depend on complex exit games and data availability assumptions\n\nRollups require global consensus and inherit base layer latency\nPayment Channels demand complex challenge periods and watchtowers\nPlasma chains depend on complex exit games and data availability assumptions\nConsider Alice’s high-frequency trading platform: with traditional layer-2 solutions, each new trading pair she adds increases system complexity quadratically. Integration with new financial primitives requires careful analysis of intricate interactions and potential failure modes. It’s like a juggler trying to keep an ever-increasing number of balls in the air - eventually, the complexity becomes unmanageable.\nWe formally define this traditional scaling limitation:\nTheorem (Traditional Composition Costs)\nFor a system S composed of n components (C_1,...,C_n):\n\\text{Cost}_\\text{verify}(S) = \\sum_{i=1}^n \\text{Cost}_\\text{verify}(C_i) + O(n^2)\n\\text{Security}_\\text{total}(S) \\leq \\min_{i=1}^n \\text{Security}(C_i)\nEven in systems built with strong mathematical properties, we observe:\n\nVerification costs grow quadratically with component count\n\n\nSecurity guarantees degrade to the weakest link\n\n\nProof obligations expand exponentially\n\n\nState transitions require global coordination\n\nVerification costs grow quadratically with component count\nSecurity guarantees degrade to the weakest link\nProof obligations expand exponentially\nState transitions require global coordination\nPerfect Mathematical Composability: A New Paradigm\nLet’s explore the practical applications and cryptographic foundations that make Overpass’s perfect mathematical composability possible.\nLet me help convert this academic paper to markdown format with proper LaTeX math notation.\nDefinition 2.1 (Perfect Mathematical Composability). A system exhibits perfect mathematical composability if:\n\\forall \\text{ components } A, B:\nCost_{verify}(A \\oplus B) = O(1)\nSecurity(A \\oplus B) = Security(A) \\cdot Security(B)\nWhere \\oplus represents composition.\nConsider Alice operating a high-frequency trading system:\nExample 2.2 (Traditional State Update).\nAlice submits update u_1\nSystem verifies current state s_0\nSystem computes new state s_1 = f(s_0, u_1)\nSystem verifies s_1 is valid\nCost grows with state size\nExample 2.3 (Overpass State Update).\nAlice constructs update u_1\nAlice generates proof \\pi_1 that:\ns_1 = f(s_0, u_1) \\land Valid(s_1)\nSystem verifies \\pi_1 in O(1) time\nState transition is immediate\nThe protocol achieves perfect composability through:\nDefinition 3.1 (Self-Proving State). A state S is self-proving if there exists a proof \\pi such that:\nValid(S) \\iff Verify(\\pi) = 1\nAlgorithm 1 State Update Protocol\nWe prove key security properties:\nTheorem 4.1 (State Transition Security). For any adversary A:\nPr[A \\text{ creates valid } \\pi \\text{ for invalid } \\Delta] \\leq 2^{-\\lambda}\nProof. By the soundness property of the underlying zk-SNARK system:\nAny proof \\pi for invalid transition fails verification with probability \\geq 1 - 2^{-\\lambda}\nThe adversary cannot do better than random guessing\nTherefore probability of successful forgery is bounded by 2^{-\\lambda} ■\nTheorem 4.2 (Composable Security). For states S_1, S_2 with proofs \\pi_1, \\pi_2:\nPr[Break(S_1 \\oplus S_2)] \\leq 2^{-2\\lambda}\nThe protocol achieves:\nThroughput = O(n \\cdot m) for n wallets, m channels\nCost_{verify} = O(1) regardless of depth\nTime_{finality} = O(1) (instant)\nCore system components:\n$$System = {Prover, Verifier, Storage, L1Interface}$$\nWith parameters:\n\\lambda = 128 (security parameter)\nd = 32 (tree depth)\nm = 2^{16} (max channels)\nn = 2^{20} (max states)\nImagine traditional DeFi protocols as a city’s financial district where every transaction, no matter how small, requires approval from multiple banks. Each new financial product (like a derivative or lending protocol) adds another layer of bureaucracy. Overpass transforms this into something more akin to a mathematical proof system - once you prove a theorem, it remains valid regardless of how many other theorems build upon it.\nThe Cryptographic Foundation: Self-Proving States\nAt the heart of Overpass lies the concept of self-proving states, formalized as:\nThis elegant equivalence tells us something profound: the validity of a state becomes a mathematical property rather than a computational one. Like a mathematical theorem, once proven, it never needs to be re-verified.\nConsider a practical DeFi application:\nTraditional Lending Protocol:\nOverpass-Enhanced Protocol:\nThe Zero-Knowledge Magic\nOverpass leverages zk-SNARKs in a novel way. Rather than just proving computation, it creates what we might call “computational theorems.” Each state transition generates a proof π that:\nProves the transition is valid\nComposes perfectly with other proofs\nMaintains constant verification cost\nThe mathematical beauty emerges in the security analysis:\nThis is analogous to multiplying the security of a lock - instead of being as weak as the weakest component, the system becomes as strong as the product of its components’ security.\nPractical Applications:\nHigh-Frequency DeFi\nOverpass enables:\nParallel state updates\nLocal verification\nConstant composition costs\nThe System Parameters:\nThese parameters support real-world financial applications while maintaining mathematical guarantees that were previously thought impossible in distributed systems.\nTraditional Flow:\nSubmit update\nWait for verification\nCompute new state\nWait for validation\nFinalize\nOverpass Flow:\nConstruct update\nGenerate proof π where: s₁ = f(s₀, u₁) ∧ Valid(s₁)\nInstant verification and finality\nThe Security Mathematics\nFor any adversary A:\nThis bound is achieved through zk-SNARK properties, but what’s truly revolutionary is how security compounds through composition:\nIn practical terms, this means that combining two secure channels actually creates a stronger system, not a weaker one - a property previously thought impossible in distributed systems.\nPerformance Implications\nThe system achieves:\nThroughput: O(n·m) for n wallets, m channels\nVerification Cost: O(1) regardless of depth\nFinality: Instant O(1)\nWith concrete parameters:\nλ = 128 (security parameter)\nd = 32 (tree depth)\nm = 2¹⁶ (max channels)\nn = 2²⁰ (max states)\nThis allows for theoretical throughput in the millions of transactions per second while maintaining instant finality - a combination previously thought to violate the blockchain trilemma.\nThe broader implications for distributed systems theory are profound. Overpass demonstrates that perfect mathematical composability isn’t just a theoretical construct - it’s achievable in practice. This opens new horizons for building complex financial instruments and DeFi protocols that can scale without compromising security.\nDefinition (Perfect Mathematical Composability)\nA system exhibits perfect mathematical composability if for all components A and B:\n\\text{Cost}_\\text{verify}(A \\oplus B) = O(1)\n\\text{Security}(A \\oplus B) = \\text{Security}(A) \\cdot \\text{Security}(B)\n\\text{Time}_\\text{finalize}(A \\oplus B) = \\max(\\text{Time}_\\text{finalize}(A), \\text{Time}_\\text{finalize}(B))\nWhere \\oplus represents composition.\nThis seemingly impossible property emerges from a novel combination of zero-knowledge proofs and state channel techniques. Let’s see how this plays out in practice through Alice’s trading system:\nExample (Traditional State Update Process)\nAlice operates a trading platform with 1000 active pairs:\n\nAlice submits update u_1 to modify BTCETH pair\n\n\nSystem must verify current state s_0 of entire system\n\n\nSystem computes new state s_1 = f(s_0, u_1)\n\n\nSystem verifies s_1 validity across all pairs\n\n\nCost scales with total pairs: O(1000)\n\n\nOther pairs blocked during verification\n\n\nFront-running possible during delay\n\n\nFailure in any pair affects all trades\n\nAlice submits update u_1 to modify BTCETH pair\nSystem must verify current state s_0 of entire system\nSystem computes new state s_1 = f(s_0, u_1)\nSystem verifies s_1 validity across all pairs\nCost scales with total pairs: O(1000)\nOther pairs blocked during verification\nFront-running possible during delay\nFailure in any pair affects all trades\nExample (Overpass State Update Process)\nAlice’s same platform with Overpass:\n\nAlice constructs local update u_1 for BTCETH\n\n\nAlice generates proof \\pi_1 proving:\n\nAlice constructs local update u_1 for BTCETH\nAlice generates proof \\pi_1 proving:\ns_1 = f(s_0, u_1) \\land \\text{Valid}(s_1)\n\nSystem verifies \\pi_1 in constant time: O(1)\n\n\nState transition completes instantly\n\n\nOther pairs continue operating independently\n\n\nFront-running mathematically impossible\n\n\nPerfect isolation between pairs\n\n\nSecurity guarantees multiply\n\nSystem verifies \\pi_1 in constant time: O(1)\nState transition completes instantly\nOther pairs continue operating independently\nFront-running mathematically impossible\nPerfect isolation between pairs\nSecurity guarantees multiply\nMathematical Foundations\nThe key insight enabling perfect mathematical composability is treating financial primitives as mathematical theorems rather than engineering components. Just as mathematical proofs can be composed while maintaining their truth value, Overpass enables composition of financial operations while preserving their security properties.\nSelf-Proving States\nThe foundation of Overpass is the concept of self-proving states. Like a mathematical proof that carries its own verification, each state in Overpass contains inherent evidence of its correctness.\nDefinition (Self-Proving State)\nA state S is self-proving if there exists a proof \\pi such that:\n\\text{Valid}(S) \\iff \\text{Verify}(\\pi) = 1\nWhere \\pi must satisfy:\n\nSuccinctness: |\\pi| = O(\\log n) where n is state size\n\n\nEfficient Verification: \\text{Time}_\\text{verify}(\\pi) = O(1)\n\n\nNon-interactivity: No additional information needed\n\n\nComposability: Proofs can be combined while maintaining properties\n\nSuccinctness: |\\pi| = O(\\log n) where n is state size\nEfficient Verification: \\text{Time}_\\text{verify}(\\pi) = O(1)\nNon-interactivity: No additional information needed\nComposability: Proofs can be combined while maintaining properties\nProtocol Design\nThe Overpass protocol operates like a self-proving mathematical system, where each operation carries its own verification. Think of it like a chain of mathematical theorems, where each new proof builds upon and strengthens previous results.\nState Transition Mechanism\nThe core protocol implements state transitions through a novel combination of zero-knowledge proofs and state channels:\nAlgorithm: Overpass State Transition Protocol\nConsider Bob operating a decentralized exchange. With traditional systems, each trade requires:\nGlobal state verification\nConsensus among participants\nChallenge period delays\nComplex failure recovery\nWith Overpass, Bob’s exchange operates like a mathematical proof machine:\nExample (DEX Operation)\nBob executes trade T between Alice and Carol:\n\\text{State}_\\text{old} = \\{A: 100\\text{ ETH}, C: 5000\\text{ DAI}\\}\nT = \\text{Swap}(10\\text{ ETH}, 500\\text{ DAI})\n2. New state with proof:\n\\text{State}_\\text{new} = \\{A: 90\\text{ ETH}, C: 5500\\text{ DAI}\\}\n\\text{Proof} = \\pi\nAnyone can verify instantly:\n\\text{Verify}(\\pi, \\text{State}_\\text{old}, T, \\text{State}_\\text{new}) = 1\nHierarchical State Management\nThe protocol organizes state in a hierarchical structure:\nDefinition (State Hierarchy)\n\\mathcal{H} = \\{\\text{Root} \\rightarrow \\text{Wallet} \\rightarrow \\text{Channel}\\}\nWhere:\n\nRoot: Global state anchor\n\n\nWallet: User-specific state collection\n\n\nChannel: Individual interaction context\n\nRoot: Global state anchor\nWallet: User-specific state collection\nChannel: Individual interaction context\nThis hierarchy enables local operation with global consistency:\nTheorem (Hierarchical Consistency)\nFor any valid state transition \\Delta at level l:\n\\text{Valid}(\\Delta@l) \\implies \\text{Valid}(\\Delta@\\text{Root})\nSecurity Analysis\nThe security of Overpass reduces to fundamental cryptographic primitives, much like how physical security reduces to the laws of physics. We prove several key properties:\nTheorem (Perfect Isolation)\nFor any two channels C_1, C_2:\n\\text{Compromise}(C_1) \\not\\implies \\text{Compromise}(C_2)\nProof\nBy contradiction:\n\nAssume compromise of C_1 affects C_2\n\n\nThis implies information flow between channels\n\n\nBut channels only interact through proofs\n\n\nProofs are independently verifiable\n\n\nTherefore, no compromise propagation possible\n\nAssume compromise of C_1 affects C_2\nThis implies information flow between channels\nBut channels only interact through proofs\nProofs are independently verifiable\nTherefore, no compromise propagation possible\nEven more remarkably, security guarantees strengthen through composition:\nTheorem (Security Amplification)\nFor channels C_1, C_2 with security parameters \\lambda_1, \\lambda_2:\n\\text{Security}(C_1 \\oplus C_2) = 2^{-(\\lambda_1 + \\lambda_2)}\nPerformance Characteristics\nThe protocol achieves remarkable scaling properties:\nTheorem (Scaling Characteristics)\nFor a system with n participants and m channels:\n\\text{Throughput} = O(n \\cdot m)\n\\text{Latency} = O(1)\n\\text{Cost} = O(\\log d) \\text{ where } d = \\log_2(n \\cdot m)\nConsider Alice’s high-frequency trading platform:\nExample (Production Scaling)\nAlice’s platform handles:\n\n100,000 trades/second\n\n\n1,000 trading pairs\n\n\n10,000 active users\n\n100,000 trades/second\n1,000 trading pairs\n10,000 active users\nTraditional system requirements:\n\\text{Cost}_\\text{traditional} = O(100000 \\cdot 1000) = O(10^8)\nOverpass system requirements:\n\\text{Cost}_\\text{overpass} = O(\\log_2(100000 \\cdot 1000)) = O(24)\nProof Generation Systems and Scale-Out Architecture\nLike a modern automotive assembly line where quality checks are integrated into the manufacturing process rather than performed at the end, Overpass transforms proof generation from a bottleneck into a streamlined, parallel process. This section explores the sophisticated architecture enabling scalable proof generation without compromising our perfect mathematical composability guarantees.\nProof Generation Complexity\nThe computational complexity for generating proofs follows a distinctive pattern:\nDefinition (Proof Generation Complexity)\nFor a state transition \\Delta with n components:\n\\text{Cost}_{\\text{prove}}(\\Delta) = O(n \\log n) // Circuit evaluation\n\\text{Size}_{\\text{witness}}(\\Delta) = O(n) // Witness generation\n\\text{Setup}_{\\text{cost}} = O(1) // One-time ceremony\nHowever, three key properties make this complexity highly manageable in practice:\nTheorem (Proof Generation Properties)\nPerfect Parallelization:\nThe Proof Factory Architecture\nOverpass introduces a novel ”Proof Factory” architecture that transforms proof generation from a potential bottleneck into a scalable pipeline:\nDefinition (Proof Factory Pipeline)\nThis creates a hierarchical proof generation system:\nTheorem (Proof Generation Hierarchy)\nFor any transaction t:\nScale-Out Performance Analysis\nConsider Alice’s high-frequency trading platform operating at 100,000 TPS:\nExample (Production Proof Generation)\nThe system achieves remarkable efficiency through what we term ”proof economics”:\nTheorem (Proof Economics)\nFor a system processing m transactions per second:\nMathematical Guarantees Under Load\nMost importantly, the proof generation system maintains perfect mathematical composability regardless of load:\nTheorem (Proof Generation Composability)\nFor any two proof generation processes P_1, P_2:\nThis means that even under maximum load:\nVerification remains constant-time\nSecurity guarantees multiply\nComposition remains perfect\nImplementation Architecture\nThe proof generation system comprises four core components:\nProof Generation Cluster:\nThis architecture achieves remarkable performance characteristics:\nTheorem (Performance Bounds)\nFor a system with n provers:\nWhere:\nProverCapacity ≈ 10,000 proofs/second/GPU\nVerificationSpeed = O(1) constant time\nNetworkBandwidth scales linearly with infrastructure\nReal-World Performance Modeling\nProduction deployments demonstrate the system’s capabilities:\nExample (Enterprise Deployment)\nThis creates what we term ”proof-generation economies of scale”:\nDefinition (Proof Economics)\nFuture Optimizations\nKey areas for continued optimization include:\nRecursive Proof Generation:\nThis proof generation architecture demonstrates that while proof generation is computationally intensive, it can be optimized to support global-scale financial applications without compromising Overpass’s perfect mathematical composability guarantees.\nImplementation Architecture\nThe system comprises four core components working in harmony:\nProver Subsystem\nGenerates zero-knowledge proofs for state transitions:\n\nParallel proof generation\n\n\nGPU acceleration\n\n\nProof caching and reuse\n\n\nAdaptive circuit optimization\n\nParallel proof generation\nGPU acceleration\nProof caching and reuse\nAdaptive circuit optimization\nVerifier Subsystem\nValidates state transition proofs:\n\nConstant-time verification\n\n\nHardware acceleration\n\n\nBatch verification\n\n\nProof aggregation\n\nConstant-time verification\nHardware acceleration\nBatch verification\nProof aggregation\nStorage Subsystem\nManages system state:\n\nSparse Merkle trees\n\n\nState compression\n\n\nPruning strategies\n\n\nArchival policies\n\nSparse Merkle trees\nState compression\nPruning strategies\nArchival policies\nL1 Interface\nHandles settlement layer interaction:\n\nBatched settlements\n\n\nProof aggregation\n\n\nGas optimization\n\n\nFallback strategies\n\nBatched settlements\nProof aggregation\nGas optimization\nFallback strategies\nEconomic Analysis\nThe protocol’s economic model provides strong incentives for efficient operation:\nTheorem (Economic Efficiency)\nFor any state update u:\n\\text{Cost}_\\text{total}(u) = \\alpha \\cdot \\log(n) + \\beta \\cdot \\text{Size}_\\text{state}(u) + \\gamma \\cdot \\mathbb{1}_\\text{settlement}\nWhere:\n\n\\alpha: Circuit computation coefficient\n\n\n\\beta: Storage coefficient\n\n\n\\gamma: L1 settlement coefficient\n\n\n\\mathbb{1}_\\text{settlement}: Settlement indicator\n\n\\alpha: Circuit computation coefficient\n\\beta: Storage coefficient\n\\gamma: L1 settlement coefficient\n\\mathbb{1}_\\text{settlement}: Settlement indicator\nThis enables precise cost prediction and optimization:\nExample (Cost Analysis)\nBob’s payment channel network:\n\nSimple transfer: $\\approx 0.001$ (proof + storage)\n\n\nComplex update: $\\approx 0.005$ (proof + storage)\n\n\nL1 settlement: $\\approx 5$ (when needed)\n\nSimple transfer: $\\approx 0.001$ (proof + storage)\nComplex update: $\\approx 0.005$ (proof + storage)\nL1 settlement: $\\approx 5$ (when needed)\nLet me explore how Overpass’s perfect mathematical composability framework could revolutionize protocol governance. Traditional governance resembles a parliamentary system where changes require debates, votes, and often contentious forks. Mathematical governance would transform this into something more akin to a formal proof system - where updates prove their own correctness.\nConsider what I call ”Perfect Governance Composability”:\nSelf-Proving Protocol Updates\nTraditional governance suffers from what I call the ”implementation uncertainty principle” - we can never be fully certain how code changes will affect complex systems. Mathematical governance transforms this:\nThink of it like a mathematical journal where new theorems must prove not only their own correctness but their harmonious interaction with all existing theorems.\nUpdate Correctness Framework\nConsider the update correctness function:\nThis creates what I call ”Mathematical Update Certainty”:\nUpdates prove their own correctness\nIntegration is mathematically guaranteed\nSecurity compounds rather than degrades\nPerfect Governance Composition\nThe system enables governance primitives previously thought impossible:\nSystem parameters support governance at scale:\nPractical Implications\nThis framework transforms protocol governance:\nUpdate Safety\nReal-world applications:\nSelf-proving protocol upgrades\nMathematically guaranteed backwards compatibility\nPerfect cross-protocol update coordination\nConsider Alice’s DeFi protocol upgrade:\nThis creates what I call ”Mathematical Protocol Evolution” - where updates become theorems building upon a formal foundation rather than risky code changes.\nThe broader transformation:\nThrough mathematical governance, protocols can evolve with the same certainty that mathematical knowledge expands - each addition proved correct and harmoniously integrated with the whole.\nFuture Research Directions\nKey areas for continued research include:\nRecursive Proofs\nEnabling unbounded scaling through proof composition:\n\\pi_\\text{recursive} : \\text{Prove}(\\pi_1 \\land \\pi_2 \\land ... \\land \\pi_n)\n\\text{Size}(\\pi_\\text{recursive}) = O(1) \\text{ regardless of } n\nPrivacy Enhancements\nPreserving confidentiality while maintaining verifiability:\n\\text{State}_\\text{hidden} = \\text{Commit}(\\text{State}_\\text{real})\n\\pi_\\text{private} : \\text{State}_\\text{hidden} \\rightarrow \\text{State}'_\\text{hidden}\nCross-Chain Integration\nEnabling seamless interaction between different blockchains:\nConclusion\nOverpass represents a fundamental breakthrough in blockchain scaling by achieving perfect mathematical composability. Rather than engineering approximations, it builds with mathematical theorems that maintain their certainty through composition. This enables a new paradigm where:\n\nProofs replace consensus\n\n\nUnilateral replaces bilateral\n\n\nMathematics replaces game theory\n\n\nSimplicity replaces complexity\n\nProofs replace consensus\nUnilateral replaces bilateral\nMathematics replaces game theory\nSimplicity replaces complexity\nThrough its perfect mathematical composability, Overpass achieves what has been a holy grail in computer science: the ability to compose complex systems while maintaining or even strengthening their security and efficiency guarantees. This breakthrough has profound implications for the future of financial technology and distributed systems.\nMATH-FI\n(mathematical-finance)\nLet me paint a picture of how Overpass’s perfect mathematical composability could fundamentally transform DeFi applications. Think of traditional DeFi as a city where every financial transaction must travel through congested intersections - each new protocol adds traffic lights that slow everything down. Overpass creates mathematical expressways where transactions flow freely while maintaining perfect security.\nAutomated Market Makers (AMMs)\nCurrent State:\nOverpass-Enhanced AMM:\nImagine a Uniswap-like protocol where:\nPrice updates propagate instantly\nMultiple pools compose without verification overhead\nArbitrage becomes mathematically atomic\nThe mathematical beauty emerges in concentrated liquidity positions:\nConsider a lending protocol composition:\nThe practical implications are profound:\nFlash loans become mathematically atomic\nCross-chain collateral becomes instantly verifiable\nLiquidation cascades become mathematically impossible\nThis enables:\nOptions contracts with instant settlement\nPerpetual futures with zero verification overhead\nComplex derivatives that compose without security loss\nReal-world implications:\nInstant strategy switching\nZero-knowledge yield composition\nMathematical yield optimization\nThe system parameters support these applications beautifully:\nThis creates what I call “mathematical finance”:\nwhere security and efficiency emerge from mathematical properties rather than computational constraints.\nOverpass’s perfect mathematical composability enables entirely new DeFi primitives that were previously impossible. Imagine traditional DeFi as a two-dimensional financial chessboard - pieces can only move in predefined patterns. Overpass creates a mathematical hypercube where financial instruments can interact across multiple dimensions simultaneously.\nMathematical Liquidity Networks\nThis enables what I call “hyperliquidity” - pools that exist in a superposition of states until mathematically resolved. Consider:\nPractical implications:\nCapital efficiency approaches theoretical maximum\nSlippage becomes mathematically bounded\nImpermanent loss can be perfectly hedged\nThis creates entirely new primitives:\nPrivacy-preserving options with mathematical guarantees\nComposable insurance products with instant settlement\nCross-chain synthetic assets with perfect price correlation\nThis enables:\nSelf-executing financial contracts\nMathematically guaranteed margin positions\nAtomic cross-protocol arbitrage\nReal-world applications:\nMulti-dimensional yield farming\nCross-timeframe liquidity provision\nPerfect mathematical hedging\nThis enables entirely new primitives:\nSelf-validating financial products\nComposable risk management systems\nMathematical guarantee of solvency\nThe system parameters support these advanced primitives:\nTraditional markets resemble a network of interconnected pipes - a blockage anywhere can cascade through the system. Overpass creates what I call a “mathematical liquid space” where market forces flow through provably secure channels.\nMarket Efficiency Transformation\nPerfect Mathematical Price Discovery\nThis creates what I call “Strong-Form Mathematical Efficiency”:\nThis creates mathematical firewalls against systemic collapse:\nReal-world implications:\nFlash crashes become mathematically impossible\nLiquidity crunches cannot propagate\nSystem-wide solvency is provable in O(1) time\nTraditional Market Hypothesis becomes the “Mathematical Market Hypothesis”:\nThis transforms market microstructure:\nPrice discovery becomes a mathematical property\nMarket making approaches perfect efficiency\nSystemic risk becomes mathematically bounded\nThe system parameters support these guarantees:\nThis creates what I call “Mathematical Market Completeness” - a state where:\nAll risks are perfectly hedgeable\nAll prices are instantly discoverable\nAll positions are mathematically secured\nLet me explore how Overpass’s perfect mathematical composability transforms monetary policy and central banking. Traditional central banking resembles conducting an orchestra with delayed feedback - policy changes propagate slowly through complex transmission mechanisms. Overpass creates what I call “mathematical monetary policy” - where effects are instant, measurable, and perfectly composable.\nMonetary Policy Innovation\nPerfect Policy Transmission\nOverpass transforms international monetary coordination from a complex diplomatic dance into what I call “mathematical monetary harmony.” Traditional international finance resembles a network of independent central banks trying to coordinate through imperfect communication channels - like an orchestra where each section plays with a different conductor. Overpass creates a mathematical framework where coordination emerges naturally from protocol properties.\nThis creates “Mathematical Currency Stability”:\nConsider the stability function across borders:\nTraditional reserve systems suffer from what I call the “Triffin dilemma squared” - competing national interests create inherent instabilities. Overpass enables:\nThis transforms international reserves:\nReserve adequacy becomes mathematically provable\nCurrency crises become protocol-impossible\nGlobal imbalances are automatically bounded\nThe system parameters support global coordination:\nConsider what I call “Mathematical Currency Stability”:\nReal-world implications:\nCurrency attacks become mathematically impossible\nGlobal liquidity is protocol-guaranteed\nSystemic stability emerges from composition\nThe broader transformation:\nThis creates “Mathematical Monetary Control”:\nConsider the policy effectiveness function:\nTraditional central banking operates under uncertainty. Overpass enables:\nThis transforms monetary operations:\nMoney supply becomes mathematically precise\nVelocity is continuously measurable\nPolicy effects are instantly verifiable\nConsider the stability framework:\nThe system enables what I call “Mathematical Monetary Federalism”:\nSystem parameters support these capabilities:\nReal-world implications:\nPerfect policy implementation\nInstant economic measurement\nMathematical stability guarantees\nBeyond Traditional Central Banking:\nThis creates what I call “Mathematical Monetary Science” - where policy becomes a precise, measurable discipline rather than an art of educated guesswork.\n“Mathematical monetary harmony.” Traditional international finance resembles a network of independent central banks trying to coordinate through imperfect communication channels - like an orchestra where each section plays with a different conductor. Overpass creates a mathematical framework where coordination emerges naturally from protocol properties.\nThis creates “Mathematical Currency Stability”:\nConsider the stability function across borders:\nTraditional reserve systems suffer from what I call the “Triffin dilemma squared” - competing national interests create inherent instabilities. Overpass enables:\nThis transforms international reserves:\nReserve adequacy becomes mathematically provable\nCurrency crises become protocol-impossible\nGlobal imbalances are automatically bounded\nThe system parameters support global coordination:\nConsider what I call “Mathematical Currency Stability”:\nReal-world implications:\nCurrency attacks become mathematically impossible\nGlobal liquidity is protocol-guaranteed\nSystemic stability emerges from composition\nThe broader transformation:\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "Fake GLV: You don't need an efficient endomorphism to implement GLV-like scalar multiplication in SNARK circuits",
        "link": "https://ethresear.ch/t/fake-glv-you-dont-need-an-efficient-endomorphism-to-implement-glv-like-scalar-multiplication-in-snark-circuits/20394",
        "article": "Introduction\nP-256, also known as secp256r1 and prime256v1, is a 256-bit prime field Weierstrass curve standardized by the NIST. It is widely adopted in internet systems, which explains its myriad use cases in platforms such as TLS, DNSSEC, Apple’s Secure Enclave, Passkeys, Android Keystore, and Yubikey. The key operation in elliptic curves based cryptography is the scalar multiplication. When the curve is equipped with an efficient endomorphism it is possible to speed up this operation through the well-known GLV algorithm. P-256 does unfortunately not have an efficient endomorphism (see parameters) to enjoy this speedup.\nVerifying ECDSA signatures on Ethereum through precompiled contracts, i.e. smart contracts built into the Ethereum protocol (there are only 9) is only possible with the secp256k1 curve and not the P-256.\nVerifying ECDSA signatures on P-256 requires computing scalar multiplications in Solidity and is especially useful for smart-contract wallets, enabling hardware-based signing keys and safer, easier self-custody. Different solutions can bring P-256 signatures on-chain. There are primarily three interesting approaches: (zk)-SNARK based verifiers, smart contract verifiers (e.g. [Dubois23], Ledger/FCL (deprecated), smoo.th/SCL and daimo/p256verifier), and native protocol precompiles (EIP/RIP 7212).\nUsing SNARK (succinctness) properties, provides a great way to reduce gas cost for computation on Ethereum (e.g. ~232k gas for Groth16, ~285k gas for PLONK and ~185k gas for FFLONK). This is very competitive with (and sometimes better that) the currently gas-optimal smart contract verifier. Moreover one can batch many ECDSA verifications in a single proof, amortizing thus the gas cost. However verifying P-256 signatures in a SNARK circuit can be very expensive i.e. long proving time. This is because the field where the points on the P-256 curve lie is different than the field where the SNARK computation is usually expressed. To be able to verify the proof onchain through the procompile the SNARK field needs to be the BN254 scalar field. Different teams tried to implement the ECDSA verification on P-256 in a BN254 SNARK circuit efficiently. Among these: zkwebauthn/webauthn-halo2, https://github.com/zkwebauthn/webauthn-circom and PSE/circom-ecdsa-p256.\nIf P-256 had an efficient endomorphism we could have optimized the proving time a great deal!\nIn this note we show a way to implement a GLV-like scalar multiplications in-circuit without having an efficient endomorphism.\nOther applications\nThis technique can be applied to any elliptic curve without an efficient endomorphism (e.g. Curve25519, P-384, MNT-753 (k=4, k=6), STARK curve, \\mathcal{B} of “cycle5”, …). See this database for other curves.\nThis would question the choice of Bandersnatch (an embedded endomorphism-equipped curve over BLS12-381) over Jubjub (an embedded curve over BLS12-381 without endomorphism) for Ethereum Verkle trees.\nThis can speedup ECDSA verification in Starknet and Cairo (through the STARK curve).\nThis can speedup natively the folding step (à la Nova) of Ed25519 signatures through the 2-cycles proposed here by Aurore Guillevic.\nStandard scalar multiplication\nLet E be an elliptic curve defined over the prime field \\mathbb{F}_p and let r be a prime divisor of the curve order \\#E(\\mathbb{F}_p) (i.e. the number of points).\nLet s \\in \\mathbb{F}_r and P(x,y) \\in E(\\mathbb{F}_p), we are interested in proving scalar multiplication s\\cdot P over the r-torsion subgroup of E, denoted E[r] (i.e. the subset of points of order r).\nThe simplest algorithm is the standard left-to-right double-and-add:\nIf/else branching is not possible in SNARK circuits so this is replaced by constant window table lookups inside the circuit. This can be achieved using polynomials which vanish at the constants that aren’t being selected, i.e. a 1-bit table lookup Q ← s_i * Q + (1 - s_i) * (Q+P). Hence this double-and-add algorithm requires t doublings, t additions and t 1-bit table lookup.\nThis can be extended to windowed double-and-add, i.e. scanning more than a bit per iteration using larger window tables, but the multiplicative depth of the evaluation increases exponentially. We use affine coordinates for doubling/adding points because inverses cost as much as multiplications, i.e. instead of checking that 1/x is y we provide y out-circuit and check in-circuit that x\\cdot y = 1. However since we start with Q ← ∞ it is infeasible to avoid conditional branching since affine formulas are incomplete. Instead, we scan the bits right-to-left and assume that the first bit s_0 is 1 (so that we start at Q ← P), we double the input point P instead of the accumulator Q in this algorithm and finally conditionally subtract (using the 1-bit lookup) P if s_0 was 0.\nGLV scalar multiplication\nHowever it is well known that if the curve is equipped with an efficient endomorphism then there exists a faster algorithm known as [GLV].\nExample 1 : suppose that E has Complex Multiplication (CM) with discrimant -D=-3, i.e. E is of the form y^2=x^3+b, with b \\in \\mathbb{F}_p. This is the case of BN254, BLS12-381 and secp256k1 elliptic curves used in Ethereum. There is an efficient endomorphism \\phi: E \\rightarrow E defined by (x,y)\\mapsto (\\omega x,y) (and \\mathcal{O} \\mapsto \\mathcal{O}) that acts on P \\in E[r] as \\phi(P)=\\lambda \\cdot P. Both \\omega and \\lambda are cube roots of unity in \\mathbb{F}_p and \\mathbb{F}_r respectively, i.e. \\omega^2+\\omega+1 \\equiv 0 \\pmod p and \\lambda^2+\\lambda+1 \\equiv 0 \\pmod r.\nExample 2 : suppose that E has Complex Multiplication (CM) with discrimant -D=-8, meaning that the endomorphism ring is \\mathbf{Z}[\\sqrt{−2}]. This is the case of the Bandersnatch elliptic curves specified in Ethereum Verkle trie. There is an efficient endomorphism \\phi: E \\rightarrow E whose kernel is generated by a 2-torsion point. The map can be found by looking at 2-isogeneous curves and applying Vélu’s formulas. For Bandersnatch it is defined by (x,y)\\mapsto (u^2\\cdot \\frac{x^2+wx+t}{x+w},u^3\\cdot y\\cdot \\frac{x^2+2wx+v}{(x+w)^2}) for some constants u,v,w,t (and \\mathcal{O} \\mapsto \\mathcal{O}) that acts on P \\in E[r] as \\phi(P)=\\lambda \\cdot P where \\lambda^2+2 \\equiv 0 \\pmod r.\nThe GLV algorithm starts by decomposing s as s = s_0 + \\lambda s_1 and then replacing the scalar multiplication s \\cdot P by s_0 \\cdot P + s_1 \\cdot \\phi(P). Because s_0 and s_1 are guaranteed to be \\leq \\sqrt{r} (see Sec.4 of [GLV] and Sec.4 of [FourQ] for an optimization trick), we can halve the size of the for loop in the double-and-add algorithm. We can then scan simultaenously the bits of s_0 and s_1 and apply the Strauss-Shamir trick. This results in a significant speed up but only when an endomorphism is available. For example the left-to-right double-and-add would become:\nUsing the efficient endomorphism in-circuit is also possible (see [Halo, Sec. 6.2 and Appendix C] or [gnark implementation] for short Weierstrass curves and [arkworks] and [gnark] implementations for twisted Edwards). But one should be careful about some extra checks of the decomposition s = s_0 + \\lambda s_1 \\mod r (not the SNARK modulus). The integers s_0, s_1 can possibly be negative in which case they will be reduced in-circuit modulo the SNARK field and not r.\nThe fake GLV trick\nRemember that we are proving that s\\cdot P = Q and not computing it. We can “hint” the result Q and check in-circuit that s\\cdot P - Q = \\mathcal{O}. Now, if we can find u,v \\leq \\sqrt{r} such that v\\cdot s = u \\pmod r then we can check instead that\n(v\\cdot s)\\cdot P - v\\cdot Q = \\mathcal{O}\nwhich is equivalent to\n u\\cdot P - v\\cdot Q = \\mathcal{O}\nThe thing now is that u and v are “small” and we can, similarly to the GLV algorithm, halve the size of the double-and-add loop and apply the Strauss-Shamir trick.\nSolution: running the half-GCD algorithm (i.e. running GCD half-way) is sufficient to find u and v. We can apply the exact same trick for finding the lattice basis as in the GLV paper (Sec. 4). For completeness we recall the algorithm hereafter.\nWe apply the extended Euclidean algorithm to find the greatest common divisor of r and s (This gcd is 1 since r is prime.) The algorithm produces a sequence of equations\nw_i \\cdot r + v_i \\cdot s = u_i\nfor i = 0, 1, 2, \\dots  where w_0 = 1, v_0 = 0, u_0 = r, w_1 = 0, v_1 = 1, u_1 = s, and u_i \\geq 0 for all i. We stop at the index m for which u_m \\geq \\sqrt{r} and take u = u_{m+1} and v = -v_{m+1}.\nNote: By construction u is guaranteed to be a positive integer but v can be negative, in which case it would be reduced in-circuit modulo the SNARK modulus and not r. To circumvent this we return in the hint u, v and a \\texttt{b}=1 if v is negative and \\texttt{b}=0 otherwise. In-circuit we negate Q instead when \\texttt{b}=1.\nImplementation\nA generic implementation in the gnark library is available at gnark.io (feat/fake-GLV branch). For Short Weierstrass (e.g. P256) look at the scalarMulFakeGLV method in the emulated package and for twisted Edwards (e.g. Bandersnatch/Jubjub) look at the scalarMulFakeGLV method in the native package.\nThe best algorithm to implement scalar multiplication in a non-native circuit (i.e. circuit field ≠ curve field) when an efficient endomorphism is not available is an adaptation of [Joye07] (implemented in gnark here).\nNext we compare this scalar multiplication with our fake GLV in a PLONKish vanilla (i.e. no custom gates) circuit (scs) over the BN254 curve (Ethereum compatible). We also give benchmarks in R1CS.\nNote here that the old ECDSA verification uses Strauss-Shamir trick for computing [s]P+[t]Q while the new version is merely two fake GLV multiplications and an addition.\np256wallet.org is an ERC-4337 smart contract wallet that leverages zk-SNARKs for WebAuthn and P-256 signature verification. It uses PSE/circom-ecdsa-p256 to generate the webAuthn proof, and underneath PSE/circom-ecdsa-p256 to generate the ECDSA proof on P-256 curve. The github README reports 1,972,905 R1CS. Compiling our circuit in R1CS results in 195,266 R1CS. This is more than a 10x reduction, which is not only due to the fake GLV algorithm but also to optimized non-native field arithmetic in gnark.\nSimilar results are noticed for other curves in short Weirstrass, e.g. P-384 and STARK curve:\nand also in twisted Edwards e.g. Jubjub vs. Bandersnatch:\nEDIT: Thanks to Ben Smith for reporting that a similar idea was proposed in [SAC05:ABGL+] for ECDSA verification. We note that, in our context, the trick applies to a single scalar multiplication and that the half GCD is free through the hint.\nAcknowledgement\nI would like to thank Arnau Cube, Aard Vark, Holden Mui, Olivier Bégassat, Thomas Piellard and Ben Smith for fruitful discussions.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "Censorable Tornado Cash",
        "link": "https://ethresear.ch/t/censorable-tornado-cash/20920",
        "article": "I’m currently researching Tornado Cash, mainly because I believe it’s a proven application of ZK technology and has broad privacy-oriented uses for community members. I’ve created this thread to discuss it with everyone.\nTornado Cash (also stylized as TornadoCash ) is an open source, non-custodial, fully decentralized cryptocurrency tumbler that runs on Ethereum Virtual Machine-compatible networks. It offers a service that mixes potentially identifiable or “tainted” cryptocurrency funds with others, so as to obscure the trail back to the fund’s original source. This is a privacy tool used in EVM networks where all transactions are public by default.\nTornado Cash\n\n\n Tornado Cash (also stylized as TornadoCash) is an open source, non-custodial, fully decentralized cryptocurrency tumbler that runs on Ethereum Virtual Machine-compatible networks. It offers a service that mixes potentially identifiable or \"tainted\" cryptocurrency funds with others, so as to obscure the trail back to the fund's original source. This is a privacy tool used in EVM networks where all transactions are public by default.\n In August 2022, the U.S. Department of the Treasury blacklist...\nNocturne is a protocol enabling private accounts on Ethereum. Imagine a conventional Ethereum account but with built-in asset privacy. Nocturne allows users to deposit or receive funds to private, stealth addresses within the Nocturne contracts. Then, in the future, a user can prove ownership of assets in zero knowledge for use in arbitrary transactions or confidential transfers.It is currently abandoned.\nhttps://nocturne-xyz.gitbook.io/nocturne\nAt the core of Tornado Cash’s privacy capability is ZK technology, which enables proof of ownership without revealing user identities or transaction details. Tornado Cash’s main contracts, known as pools, are designed for deposit and withdrawal operations. Users deposit funds into a pool contract and receive an anonymous proof to use later for withdrawal, thereby obscuring the original source of funds.\nThis anonymity makes Tornado Cash a favored tool for money laundering. Several documented cases illustrate how malicious actors have leveraged Tornado Cash’s anonymity to launder stolen funds, often evading regulatory scrutiny. Criminals have effectively obscured the money trail, making it difficult for law enforcement to track illicit transactions.\nIn August 2022, the U.S. Treasury’s Office of Foreign Assets Control (OFAC) sanctioned Tornado Cash, adding its associated USDC and ETH addresses to the Specially Designated Nationals (SDN) list, barring U.S. residents from using the service. The Treasury cited Tornado Cash’s role in numerous decentralized finance (DeFi) hacks, where individuals and groups allegedly laundered over $7 billion worth of cryptocurrency through the platform since its inception in 2019.\nFuture Evolution of Privacy Transactions: Selective Auditing as a Path Forward\nAs privacy solutions evolve, selective auditing features may become standard, enabling both anonymity for users and transparency for regulators. For example, Japan’s recent crackdown on a Monero laundering operation involving over 100 million yen highlights the global regulatory push for compliance in privacy-preserving systems.\nZero-knowledge proofs (ZKPs) are central to maintaining anonymity in the cryptocurrency space. By proving information without revealing it, ZKPs provide a basis for private transactions. However, purely anonymous systems can pose regulatory challenges. Recent innovations in ZK technology, like “partially decryptable zero-knowledge proofs” or Selectively Auditable Zero-Knowledge Proofs (SA-ZKPs), offer a promising balance between privacy and auditability.\nThe SA-ZKP algorithm comprises the following components:\nCommitment Scheme C=(CKeygen,Commit,COpen)C = (CKeygen, Commit, COpen)C=(CKeygen,Commit,COpen): Establishes a commitment to private data, allowing it to be used in proofs without revealing it.\nZero-Knowledge Proof Σ=(K,P,V)\\Sigma = (K, P, V)Σ=(K,P,V): Allows verifiable proof of commitment without disclosing the committed data.\nTrapdoor Generation: Creates a cryptographic “trapdoor” to enable selective auditability.\nSelective Decryption Process: Allows authorized entities to selectively decrypt committed data for regulatory auditing.\nRegulated Tornado Cash Workflow with SA-ZKP\nApplying the SA-ZKP algorithm to a regulated version of Tornado Cash could create a privacy-compliant framework with selective auditability:\nRegulator Registration (Trapdoor Generation): Regulators register with the network to gain access to audit permissions through a cryptographic trapdoor.\nTransaction Flow: Users deposit funds anonymously, with cryptographic commitments created for auditing if necessary.\nAudit Process (Selective Decryption): In cases of suspicious activity, regulators can selectively decrypt transaction data to investigate without compromising the privacy of all users.\nBy integrating SA-ZKP with Tornado Cash’s core operations, we can achieve a dual objective: respecting user privacy while empowering regulatory authorities with necessary oversight capabilities.\nIf I were to launch a new version of a mixer, where I would decrypt specific transaction proofs for law enforcement using a trapdoor key when requested, would you still use this mixer? Why or why not?\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "zkCasper: A SNARK based protocol for verifying Casper FFG Consensus",
        "link": "https://ethresear.ch/t/zkcasper-a-snark-based-protocol-for-verifying-casper-ffg-consensus/15559",
        "article": "In this research article I present a SNARK based protocol for verifying Ethereum’s Casper FFG consensus proofs.\nWith this scheme on/offchain light clients can benefit from the crypto economic security provided by the ETH 17m ($34b) at stake.\nzkCasper\nIn this research article, I present a protocol for efficiently verifying the Ethereum Beacon chain's Casper FFG consensus proofs using a SNARK based scheme.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "Verify ed25519 signatures cheaply on Eth using ZK-Snarks",
        "link": "https://ethresear.ch/t/verify-ed25519-signatures-cheaply-on-eth-using-zk-snarks/13139",
        "article": "Prepared by : Garvit Goel, Rahul Ghangas, Jinank Jain\nIn this article, we will discuss how you can verify Ed25519 signatures on Ethereum today in a very gas-efficient way, without the use of any pre-compiles such as the proposed EIP665. We will use the same principles as used by many zk-rollups. We have already shipped the code for this, albeit not yet audited. Let’s get to it.\nFor dApps that want to verify Ed25519 signatures on Ethereum, rather than verifying the signature(s) directly on Ethereum, (and performing the curve operations inside a solidity smart contract), one can construct a zk-proof of signature validity and verify the proof on-chain instead.\nGas cost for verification of a single Ed25519 signature is about ~500K gas (when the Ed25519 curve is implemented directly in Solidity). On the other hand, the gas cost for verifying a zk-snark on-chain is about ~300k gas. These gas savings become significant when you want to verify a large number of signatures in one batch, then you can just create a single ZK-proof for the entire batch.\nAt Electron Labs, we have built a circom-based library that allows you to generate a zk-snark proof for a batch of Ed25519 signatures.\nYou can check out the details of our mathematical approach here. You can even test proof generation today using the APIs given on the previous link.\nCheck out the complete code base here\nThe Performance of a Single Ed25519 is as below:\nAll metrics were measured on a 16-core 3.0GHz, 32G RAM machine (AWS c5a.4xlarge instance).\nFurthermore, for batching we support:\nMax batch size supported = 99 signatures\nProof generation time for a batch of 99 signatures = ~16 minutes.\nWhile these metrics are good for a PoC, we need to do a lot better. Hence, as next steps, we are planning to integrate recursive snarks. This will increase the max batch size and reduce proof generation time by multiple orders of magnitude.\nUse Cases:\nWe believe one of the best use cases for this tech is extending light client bridges to Ethereum. This includes Polygon Avail, IBC and Rainbow Bridge. One could also build zk-rollups that use Ed25519 for user accounts.\nEnding Note:\nWe would love to work with teams that want to use this tech. We are also looking for research groups who are working on recursive snark technology to help us scale our tech.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "L2 sequencer proving on weak hardware; parallelization and decentralization",
        "link": "https://ethresear.ch/t/l2-sequencer-proving-on-weak-hardware-parallelization-and-decentralization/20313",
        "article": "Linea’s sequencer proves a 30m gassed block of transactions in 5 minutes. Here’s its setup:\nSo is it possible to reduce the proving time and, at the same time, obtain decentralization guarantees? We have an idea.\nOverview\nAlmost all of the L2 sequencers are closed-source, intellectual property, and thus protected behind centralized setups. To cram that much power into an entity requires a great deal of justification today. To decentralize the flow, on the other hand, one has to accept certain amounts of delay and noise usually found in decentralized compute networks.\nAny zkVM toolset puts a certain upper bound on the maximum number of cycles(roughly speaking 1 cycle equals 1 operation) it can prove in one go. This is usually done for efficiency reasons. For Risc0, a RISC-V general zkVM, it is 2^24 ~ 16.78m cycles. With recursion, proving infinitely sized programs are made possible. So the solution is to divide a large program into individual sub-programs(called segment in Risc0 jargon) and have them proved one by one and aggregate the proofs into a final proof as if the whole program was proved in one go. For example, consider proving a 1b cycles program. With 16.78M maximum segment size limit, one ends up proving 60 segments. The upper bound for segment size limit is not the end of story however and one can customize it into a well-known range of [2^13 - 2^24]. Each segment limit size needs specific memory requirements shown on Table 1:\nScreenshot from 2024-07-18 14-37-341366×768 32.9 KB\nExtrapolating Table 1’s values, we get 50m cycles for a program that needs 384gb of memory, in order to be proved in Risc0. Recall that Linea’s prover uses 384gb of memory to generate proofs. This is a naive 1-1 translation, but we can treat it as baseline for further testing. So, with this assumption, should one write Linea’s sequencer logic in Risc0, she would end up with a program that is 50m cycles long. Doubling cycles to ~90m, to account for aggregation won’t hurt here.\nRecursion is a powerful idea in zkVM proving. With recursion once can get to prove seemingly large programs very quickly assuming she has a prove-ready network of machines. Table 2 shows a segmented prove session for a 90m cycles program on a pretty weak machine(8+ years old, Intel core i7 5500U(2C 4T), 16gb memory):\nScreenshot from 2024-07-18 14-47-061288×439 64.4 KB\nAs you can see, different segment size limits result in varied proving regimes. In Table 2, two columns are colored in green, 2^18 and 2^19. Consulting Table 1, we would get 2gb and 4gb of required memory to prove them respectively. These columns are sweet spots for any zkVM proving network whose nodes are presumably weak. Focusing on the 2^19 segment size limit, to prove a 90m cycles program, one would need at least 168 nodes in order to prove the program in 4 minutes and 9 seconds. But 168 nodes is a faulty assumption. In reality, if a p2p network is to undertake the proving job, it needs to have redundancy values of 1:4 and above. The redundancy accounts for noise that is a feature of any p2p network. With 1:4 redundant nodes, 1 in every 5 nodes is assumed to be honest and the rest are time wasters. So, a 1:4 redundant p2p network needs at least of 840 nodes to get the job done.\nAssuming that the proving network is p2p, one can expect to obtain decentralized guarantees en route.\nConclusion\nHere we introduced an imaginary setup to decentralize and improve L2 sequencer proving times. If the claim turns out to be legit, we would expect to improve the overall proving time for any zkVM application area. In addition, the setup provides decentralization guarantees as a side effect. While everything looks nice, we, at Wholesum network would like to put this setup to test and see if it works in action. If successful, a p2p verifiable compute network of 10,000 weak nodes can handle up to 10 Linea like L2s.\nA somewhat more expanded version of this post is also available here.\nWe appreciate your feedback.\n",
        "category": [
            "Layer 2",
            "ZK Rollup"
        ],
        "discourse": []
    },
    {
        "title": "Cheon's attack and its effect on the security of big trusted setups",
        "link": "https://ethresear.ch/t/cheons-attack-and-its-effect-on-the-security-of-big-trusted-setups/6692",
        "article": "Thanks to Ariel Gabizon and Zac Williamson for collaborating on the post, and the authors of Marlin for highlighting the attack and its importance.\nThe attack\nCheon shows that if you’re given g, g^\\alpha and g^{\\alpha^d}, where g is an element of a group of order p and d | p -1, then it’s possible to find \\alpha in 2\\left(\\left\\lceil\\sqrt{\\frac{p - 1}{d}}\\right\\rceil + \\left\\lceil\\sqrt{d}\\right\\rceil\\right)\\cdot \\left(\\mathsf{Exp}_{\\mathbb{G}}(p) + \\log{p} \\cdot \\mathsf{Comp}_{\\mathbb{G}}\\right) operations, where \\mathsf{Exp}_{\\mathbb{G}}(n) means the cost of one exponentiation of an element in \\mathbb{G} by a positive integer less than n amd \\mathsf{Comp}_{\\mathbb{G}} means the cost to determine if two elements of \\mathbb{G} are identical. By assuming that \\mathsf{Exp}_{\\mathbb{G}}(p) dominates \\mathsf{Comp}_{\\mathbb{G}} and that the \\log{p} factor can be ignored when using a hash table, the cost formula can be simplified to be approximately 2\\left(\\left\\lceil\\sqrt{\\frac{p - 1}{d}}\\right\\rceil + \\left\\lceil\\sqrt{d}\\right\\rceil\\right)\\cdot \\mathsf{Exp}_{\\mathbb{G}}(p). The storage cost is \\max\\left\\{{\\left\\lceil\\sqrt{\\frac{p-1}{d}}\\right\\rceil}, \\left\\lceil\\sqrt{d}\\right\\rceil\\right\\} elements of \\mathbb{G}.\nFor more intuition on how the attack works, check out Ariel’s write-up.\nCheon uses Baby-step Giant-step as the main part of the attack, and it’s possible to use Pollard’s Rho instead.\nWhen using Pollard’s Rho algorithm, we can either use a large memory or a constant memory version, as mentioned in [3]. For the large memory version, i.e. which requires saving around 1.25\\left(\\sqrt{\\frac{p-1}{d}} + \\sqrt{d}\\right) elements of \\mathbb{G}, the expected number of evaluations (which roughly mean exponentiations) is 1.25\\left(\\sqrt{\\frac{p-1}{d}} + \\sqrt{d}\\right). For the constant memory version, the expected number of evaluations is 3.09\\left(\\sqrt{\\frac{p-1}{d}} + \\sqrt{d}\\right) and 1.03\\left(\\sqrt{\\frac{p-1}{d}} + \\sqrt{d}\\right) comparisons.\nThe Marlin authors also noticed that if you’re given g, g^\\alpha and g^{\\alpha^d} and h, h^\\alpha and h^{\\alpha^d} where g is a generator of \\mathbb{G}_1 and h is a generator of \\mathbb{G}_2, it’s also possible to use the pairing to transfer the problem into \\mathbb{G}_T: e(g^{\\alpha^m}, h^{\\alpha^n}) = e(g,h)^{\\alpha^{m+n}}.\nThe impact\nThis is particularly relevant for trusted setups that have been performed in the past and are being performed at the moment. Solving for \\tau allows for the possibilty of breaking soundness.\n\nZcash Powers of Tau - Sapling - BLS12-381 - we have up until g^{\\tau^{2^{22} - 1}} in \\mathbb{G}_1 and g^{\\tau^{2^{21}}} in \\mathbb{G}_2\n\n\nAZTEC PLONK setup - BN254 - we have g^{\\tau^{3 \\cdot 2^{25}}}  in \\mathbb{G}_1\n\n\nPerpetual Powers of Tau - BN254 - we have up until g^{\\tau^{2^{29} - 1}} in \\mathbb{G}_1 and g^{\\tau^{2^{28}}} in \\mathbb{G}_2\n\n\nFilecoin Powers of Tau - BLS12-381 - we have up until g^{\\tau^{2^{28} - 1}} in \\mathbb{G}_1 and g^{\\tau^{2^{27}}} in \\mathbb{G}_2\n\nLet’s take the biggest one to show the potential impact - Perpetual Powers of Tau. By the Cheon method with Pollard’s Rho, we can solve DLP in \\mathbb{G}_1 for \\tau in 1.25\\left(\\sqrt{\\frac{2^{254}}{2^{28}}} + \\sqrt{2^{28}}\\right) \\approx 2^{114}, so at most 2^{114} exponentiations, or 114-bit security. For BN254, the impact is not severe, since there are other NFS-based attacks that lower the security to around 110-bit security. You could also transfer the method to \\mathbb{G}_T, and get 1.25\\left(\\sqrt{\\frac{2^{254}}{2^{29}}} + \\sqrt{2^{29}}\\right)  \\approx 2^{114}, but the operations in \\mathbb{G}_T are significantly more expensive.\nFor BLS12-381 setups, the impact might be more meaningful. The goal was to design a curve with 128-bit security, and the trusted setup lowers is. In the Filecoin parameters, this translate to 1.25\\left(\\sqrt{\\frac{2^{255}}{2^{27}}} + \\sqrt{2^{27}}\\right) \\approx 2^{114}, so at most 2^{114} exponentiations.\nThis is also relevant to other projects which will perform a trusted setup:\nProjects that are using curves mentioned in Zexe, such as Celo and possibly EYBlockchain\n\n\nCoda that uses MNT4753 and MNT6753\nProjects that are using curves mentioned in DIZK\n\nConclusion\nFuture projects that target 128-bit security should also consider this attack, which has become relevant because of the growing size of circuits.\nThis might also be a benefit of updatable setups, such as can be done for PLONK, Marlin and Sonic - you can estimate the amount of time it would take to solve for \\tau and make sure the SRS is updated before that.\nReferences\n[1] Cheon, Jung Hee. “Discrete logarithm problems with auxiliary inputs.” Journal of Cryptology 23.3 (2010): 457-476.\n[2] Kozaki, Shunji, Taketeru Kutsuma, and Kazuto Matsuo. “Remarks on Cheon’s algorithms for pairing-related problems.” International Conference on Pairing-Based Cryptography. Springer, Berlin, Heidelberg, 2007.\n[3] Bai, Shi, and Richard P. Brent. “On the efficiency of Pollard’s rho method for discrete logarithms.” Proceedings of the fourteenth symposium on Computing: the Australasian theory-Volume 77. Australian Computer Society, Inc., 2008.\n[4] Chiesa, Alessandro, Yuncong Hu, Mary Maller, Pratyush Mishra, Noah Vesely, and Nicholas Ward. Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS. Cryptology ePrint Archive, Report 2019/1047, 2019.\n[5] Gabizon, Ariel, Zachary J. Williamson, and Oana Ciobotaru. PLONK: Permutations over Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge. Cryptology ePrint Archive, Report 2019/953, 2019.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "The Application of ZK-SNARKs in Solidity - Privacy Transformation, Computational Optimization, and MEV Resistance",
        "link": "https://ethresear.ch/t/the-application-of-zk-snarks-in-solidity-privacy-transformation-computational-optimization-and-mev-resistance/17017",
        "article": "Authors: Mirror Tang , Shixiang Tang , Shawn Chong ,Yunbo Yang\nEthereum is a blockchain-based open platform that allows developers to build and deploy smart contracts. Smart contracts are programmable codes on Ethereum that enable the creation of various applications. With the development of Ethereum, certain issues and challenges have emerged, including privacy concerns in applications. Defi applications involve a large amount of address information and user funds. Protecting the privacy of transactions is crucial for users in certain application scenarios. By utilizing privacy-preserving technologies, transaction details can be made visible only to involved parties and not to the public. Through the use of ZK-SNARKs (Zero-Knowledge Succinct Non-Interactive Argument of Knowledge), we can implement transformations on existing applications on Ethereum. This includes adding features such as private transfers, private transactions, private orders, and private voting to existing projects on Ethereum, as well as optimizing computations and addressing MEV (Maximal Extractable Value) challenges in Ethereum application-layer projects.\nThrough this research, our aim is to promote privacy in the Ethereum application layer and address issues related to privacy transformation, computational optimization (such as Rollup), and MEV resistance on Ethereum.\nCompetitor Analysis: Smart contracts without privacy features are susceptible to competitor analysis and monitoring. Competitors can gain sensitive information about business operations and strategies by observing and analyzing transaction patterns and data of the contract, thereby weakening competitive advantages.\nTransaction Traceability: In the absence of privacy features, contract transactions are traceable, and the participants and contents of the transactions can be tracked and identified. This exposes transaction intentions and participants for certain transactions, such as anonymous voting or sensitive transactions.\nData Security: Data in smart contracts has become a primary target for attackers. Contracts without privacy features are at risk of data leakage, tampering, or malicious attacks. Attackers often exploit and manipulate contract data through analysis to carry out malicious activities, causing harm to users and contracts.\nEVM Smart Contract Multithreading: Currently, direct implementation of multithreading is not possible in Ethereum smart contracts. Ethereum adopts an account-based execution model, where each transaction is executed sequentially in a single thread. This is because Ethereum’s consensus mechanism requires sequential verification and execution of transactions to ensure consensus among all nodes. Smart contracts in Ethereum face performance bottlenecks when dealing with large-scale data. To run a significant number of smart contracts containing zero-knowledge proofs on-chain, optimizations such as asynchronous calls, event-driven programming, and delegation splitting need to be implemented to achieve concurrent execution.\nAuditable Zero-Knowledge (ZK): Auditable ZK refers to the ability of verifiers to provide received zero-knowledge proofs to third parties (usually the public) for verifying their validity without going through the entire proof process again. This means that third parties can verify the correctness of the proof without knowing the specific details of the statement. Auditable ZK requires more computation and storage operations compared to general ZK implementations, especially in the verification phase. This may have an impact on the performance and resource consumption of smart contracts, placing higher demands on the performance optimization of Solidity and corresponding ZK circuits.\nProof System Scalability: Existing proof systems suffer from scalability issues, making it difficult to support large-scale circuits, such as proving LLM circuits. Current potential scalability solutions include recursive proofs and distributed proofs, which have the potential to enhance the scalability of proof systems and provide solutions for proving large-scale circuits.\nProof System Security Risks: Some proof systems, such as Groth16 and Marlin, rely on a trusted setup (also known as toxic waste) that is privately generated. Once made public, the security of the entire proof system cannot be guaranteed.\nGroth16 (Used by Zcash currently)\nIn the case where the adversary is restricted to only linear/affine operations, Groth constructed a LIP with a communication cost of only 3 elements based on QAP. Based on this LIP, it constructed a zk-SNARK with a communication cost of 3 group elements and a verifier computational cost of only 4 pairing operations (known as Groth16).\nAdvantages: Small proof size, currently the fastest verification speed.\nDisadvantages: Trusted setup bound to the circuit, meaning that a new trusted setup is required for generating proofs for a different circuit, and the trusted setup cannot be dynamically updated.\nMarlin\nTo address the inability of zk-SNARKs schemes to achieve global updates, Groth et al. based on QAP, proposed a zk-SNARK with a global and updatable Common Reference String (updatable universal CRS), denoted as GKMMM18.Building on this, Maller et presented Sonic scheme that utilized the permutation argument, grand-product argument, and other techniques to achieve a globally updatable CRS with a size of O(|C|), concise NIZKAoK without additional preprocessing, under the algebraic group model.\nMarlin is a performance-improved scheme of Sonic (as is Plonk), primarily optimizing the SRS preprocessing and polynomial commitment, thereby reducing the proof size and verification time of the proof system.\"\nAdvantages: Support for globally updatable trusted setup, achieving succinct verification in an amortized sense.\nDisadvantages: High complexity in the proof process, less succinct proof size compared to Groth16.\nPlonk\nPlonk is also an optimization of the Sonic scheme, introducing a different circuit representation called Plonkish, which is different from R1CS (Rank-1 Constraint System) and allows for more scalability, such as lookup operations. Plonk optimizes permutation arguments through “Evaluation on subgroup rather than coefficient of monomials” and leverages Lagrange basis polynomials.\nAdvantages: Support for globally updatable trusted setup, fully succinct verification, and a more scalable circuit representation in Plonkish.\nDisadvantages: Marlin may perform better in cases with frequent large addition fan-in; less succinct proof size compared to Groth16.\nHALO2\nTo reduce proving complexity and the burden on the Prover, researchers introduced recursive proofs and proposed the Halo proof system (as introduced by Vitalik’blog). The Halo proof system adopts the Polynomial IOP (Interactive Oracle Proof) technique from Sonic, describes a Recursive Proof Composition algorithm, and replaces the Polynomial Commitment Scheme in the algorithm with the Inner Product argument technique from Bulletproofs, eliminating the reliance on a Trusted Setup.\nHalo2 is a further optimization of Halo, mainly in the direction of Polynomial IOP. In recent years, researchers have discovered more efficient Polynomial IOP schemes than those used in Sonic, such as Marlin and Plonk. Among them, Plonk was chosen due to its support for more flexible circuit designs.\nAdvantages: No need for a trusted setup; introduces recursive proofs to optimize proof speed.\nDisadvantages: Less succinct proof size.\nCircom + Snarkjs\nCircom + Snarkjs is a major tool chain to build the zkSNARK proving system. Snarkjs is a JavaScript library for generating zk-SNARK proofs. This library includes all tools required to build a zkSNARK proof. To prove the validity of a given witness, an arithmetic circuit is first generated by Circom, then a proof is generated by Snarkjs. For the usage about this tool chain, we refer the reader to the guidance.\nAdvantages: This toolchain inherits the advantages of zkSNARKs such as Groth16, namely having smaller proof sizes and faster verification times.  In order to reduce overall costs, The computationally expensive operations (FFT and MSM) at the prover side can be done off-chain. If Groth16 is used for proving, the proof size on-chain consists of only three group elements, and the verifier can validate proof in a very short time, which leads to lower gas fee. In addition, this tool chain can handle large-scale computing, and the proof size as well as verification time is independent of the size of the computation task.\nDisadvantages:This language is not particularly ergonomic, making developers keenly aware that they are writing circuits.\nPerformance comparison\nWe use different programming languages (Circom, Noir, Halo2) to write the same circuit, and then test the proving and verifier time with different numbers of rounds. We run all experiments on the Intel Core i5 processor with 16GB RAM and MacOS 10.15.7. The experimental codes are shown in [link] .\n1699022015623989×465 17.4 KB\nThe above table is the experimental results. It shows that the circuit written in Circom outperforms in terms of the prover time, with only around 1s for 10 rounds, and Halo2 outperforms in terms of the verifier time when the circuit size is small.\nBoth the circuit written in Circom and Noir relies on KZG commitment. Therefore, the overall verifier time of Circom and Noir is independent of the circuit size (number of rounds in the table).  Meanwhile, Halo2 relies on the commitment scheme called inner product argument (IPA). The verifier time of IPA is logarithmic to the circuit size. With the increment of circuit size, the verification time will increase gently.\nGenerally speaking, circuit written in Circom enjoys the best prover time and verification time.\nEnclave: interactive shielding\nIn terms of  privacy protection, pure zk proving system can not construct a complete solution. For example, when considering the privacy of transaction information, we can use ZK technology to construct a dark pool. Orders within this dark pool are in a shielded state, which means they are obfuscated, and their contents cannot be discerned from the on-chain data by anyone. This condition is referred to as “fragmentation in shielded state.” However, when users need to maintain state variables, i.e., initiate transactions, they have to gain knowledge of these shielded pieces of information. So, how do we ascertain shielded information while ensuring its security? This is where enclaves come into play. An enclave is an off-chain secure storage area for confidential computations and data storage. How do we ensure the security of enclaves? For the shielding property in the short term, one may use secure hardware modules. In the long term, one should have systems in place for transitioning to trust-minimized Multi-Party Computation (MPC) networks.\nWith the introduction of enclaves, we will be able to implement interactive shielding. Namely, users can interact with fragmented shielded information while ensuring securiry. Implementing interactive shielding opens up unlimited possibilities while maintaining privacy.\nThe ability to have shielded state interact opens up a rich design space that’s largely unexplored. There’s an abundance of low hanging fruit:\nGamers can explore labryinths with hidden treasures from previous civilizations, raise fortresses that feign strength with facades of banners, or enter into bountiful trade agreements with underground merchants.\nTraders can fill orders through different dark pool variants, insure proprietary trading strategies through RFQ pools with bespoke triggers, or assume leveraged positions without risk of being stop hunted.\nCreators can generate pre-release content with distributed production studios, maintain exclusive feeds for core influencers, or spin up special purpose DAOs with internal proposals for competitive environments.\nCurrently available ZK enclaves:\nSeismic\nhttps://seismicsystems.substack.com/p/on-a-treasure-hunt-for-interactive?r=2zhkov&utm_campaign=post&utm_medium=web\nHow to integrate with Solidity?\nFor the Groth16 and Marlin proof systems, there are already high-level circuit languages and compilers that provide support for solidity, such as Circom  for Groth16 and Plonk, and zokrate for Groth16 and marlin proof systems.\nFor the halo2 proof system, Chiquito is the DSL of HALO2（Provided by Dr. CathieSo from the PSE team of the Ethereum Foundation）:\nGitHub - privacy-scaling-explorations/chiquito: DSL for Halo2 circuits\nDSL for Halo2 circuits. Contribute to privacy-scaling-explorations/chiquito development by creating an account on GitHub.\nPrivacy Enhencement\nAssuming we need to hide the price of a limit order from the chain and the order is represented as O=(t,s) , where t:= (\\phi, \\chi, d) , s:= (p, v, α)\n\\phi:  side of the order, 0 when it’s a bid, 1 when it’s an ask\n\\chi:  token address for the target project\nd: denomination, either the token address of USDC or ETH.  Set 0x0 represent USDC and 0x1 represent ETH.\np: price, denominated in d\nv: volume, the number of tokens to trade\n\\alpha: access key, a random element in bn128’s prime field , which mainly used as a blinding factor to prevent brute force attacks\nThe information we want to hide is the price, but in order to generate proof, we must expose the value related to the price, which is the balance b  required for this order. Specifically, if it’s a bid order, the bidder need to pay the required amount of denomination token for the order; If it is a ask order, the seller needs to pay the target token they want to sell. The balance b is a pair with the first element specifying an amount of the target project’s token and the second element specifying an amount of the denomination token.\nIn general, we use poseidon hash to mask out the price and volume of the order and only display the balance required for this order. We use zksnark to prove that the balance is indeed consistent with the price required for the order.\nHere is the example circom code:\nProver can use the above circuit to generate proof for specific orders using the Circom+Snarkjs toolchain. The Snarkjs tool can export Verifier.sol, with the following content [I am using the plonk proof system]. Run command:\nthen we get the verifier.sol\nAnd then, the project developer can use the verifier.sol to construct their project with solidity. The following code is just for demonstration.\nThe last thing I need to emphasize that privacy enhancement has actually brought MEV resistance to this project. So I will no longer do anti MEV demonstrations.\nComputational Optimization\nAssuming we want to execute the heavy AMM logic off-chain, we only need to perform computational verification operations on the chain.\nThe following is a simple AMM logic circuit (as an example only, the calculation logic of the actual protocol is much more complex)\nSimilarly, it is necessary to use the circom+snarkjs toolchain to generate proof and export it to verifier.sol.\nFinally, AMM project developers can develop their logic code with  verifier.sol in  solidity. The example solidity code for the on chain check contract is as follows:\nAnti-MEV attacks\nAfter privacy modification of protocol contracts, MEV attacks often fail to be achieved. Please refer to examples of privacy modification.\nFrom the example above, we can see that integrating ZK-SNARKs technology into Solidity can effectively address issues of privacy, computational optimization, and resistance to MEV (Maximum Extractable Value) in Ethereum applications. ZK-SNARKs offer privacy features, making it possible to implement private transfers, private transactions, private orders, and private voting within existing Ethereum projects. At the same time, ZK-SNARKs can optimize computational processes and address challenges related to MEV resistance in application-layer projects on Ethereum. By leveraging ZK-SNARKs technology, developers can enhance the privacy, performance, and security of their applications. This reaffirms the feasibility of writing ZK applications in Solidity on Ethereum and indicates that it is a trend for future development. This will bring improved privacy protection, computational optimization, and MEV resistance capabilities to the Ethereum ecosystem, propelling further development and innovation in Ethereum applications.\nEstablishing a Public ZK Verification Layer on the Ethereum Blockchain\nThis can provide various benefits such as privacy protection, scalability, flexibility, and extensibility. This can help drive the adoption of ZK technology in the Ethereum ecosystem and provide users and developers with more secure, efficient, and flexible verification solutions.\nZK Performance Optimization in Solidity\nConsidering methods such as batching techniques, optimizing computation and communication, parallel computing, caching and precomputation, and optimizing the verification process to improve the performance of ZK calls in Solidity. Enhancing the computational efficiency of ZK proofs, reducing communication overhead, and improving the overall performance of the ZK system.\nEstablishing Reusable Solidity ZK Components\nThis contributes to code maintainability and reusability, promotes collaboration and sharing, improves code scalability, and provides better code quality and security. These components can help developers efficiently develop Solidity applications and also contribute to the growth and development of the entire Solidity community\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": [
            "mev",
            "zk-roll-up"
        ]
    },
    {
        "title": "Plasma Cash: Plasma with much less per-user data checking",
        "link": "https://ethresear.ch/t/plasma-cash-plasma-with-much-less-per-user-data-checking/1298",
        "article": "[2018.03.10: updated with exit procedure]\nSpecial thanks to Karl Floersch for discussion and coming up with much of this, as well as @danrobinson’s earlier posts that expressed similar ideas.\nBasically, we can design a version of Plasma with the following modifications:\nEvery single deposit corresponds to a unique coin ID; tokens are indivisible and cannot be merged.\nInstead of storing transactions in a binary Merkle tree in order of txindex, we require them to be stored in either a sparse simple Merkle tree or a patricia tree, with the index being the ID of the coin that is spent.\nNote that this now allows a user to have a somewhat compact proof that their coin is valid: all the transactions since the time the coin was deposited that represent that coin’s history, plus a proof of non-inclusion for every block that does not contain a transaction spending the coin, to verify that the coin was not double spent. With n coins and t blocks, this proof has size t * log(n). If a user transfers a coin to another user, he could simply pass along the entire proof to that user.\nHence, a Plasma operator could simply maintain connections with each user, and every time they create a block they would publish to them only the proofs, not any data related to coins that they do not own. It’s clearly the case that any data that is not part of these proofs could not be used to fraudulently exit or double-spend the user’s coin, so the user is safe. Because coins are non-fungible, successfully defrauding other users cannot allow the Plasma contract to turn into a fractional reserve, as is possible in minimal viable plasma.\nThe Plasma chain operator could be sharded, so there is virtually no limit to the system’s scalability from the point of view of either the chain operator or the users, though the limitation that if Plasma-like systems (and channel systems) start processing a very high transaction load, mass challenge attacks may overflow the blockchain and prevent some users from exiting or responding to challenges still remain. This kind of setup seems ideal for very-high-throughput but low or medium-state applications, like micropayments and exchanges.\nAdditionally, we can remove the need for confirmations. We do this by having the following exit procedure:\nAnyone can exit their coin by providing the last two transactions in the coin’s ownership history (ie. the coin they are exiting C and its parent P( C )).\nAn exit can be challenged in three ways: (i) provide a proof of a transaction spending C, (ii) provide a proof of a transaction spending P( C ) that appears before C, (iii) provide a transaction C* in the coin’s history before P( C )\nA challenge of type (i) and (ii) blocks the exit immediately. A challenge of type (iii) can be responded to by providing the direct child of C*, which must be either equal to or before P( C )\nThis relies on honest users maintaining the key property that they never spend a coin until they have fully authenticated the entire history up to that coin. It could be the case that a Plasma chain starts including unavailable or invalid data while a transaction is in flight, in which case double spends or invalid spends could appear between P( C ) and C; the slightly more complicated exit mechanism takes this into account.\n",
        "category": [
            "Layer 2",
            "Plasma"
        ],
        "discourse": [
            "new-extension"
        ]
    },
    {
        "title": "Roll_up / roll_back snark side chain ~17000 tps",
        "link": "https://ethresear.ch/t/roll-up-roll-back-snark-side-chain-17000-tps/3675",
        "article": "Authors BarryWhitehat, Alex Gluchowski, HarryR, Yondon Fu, Philippe Castonguay\nOverview\nA snark based side chain is introduced. It requires constant gas per state transition independent of the number of transactions included in each transition. This limits scalability at the size of snark that can be economically proven, rather than the gasBlockLimit/gasPerTx as proposed previous.\nGiven a malicious operator (the worst case), the system degrades to an on-chain token. A malicious operator cannot steal funds and cannot deprive people of their funds for any meaningful amount of time.\nIf data become unavailable the operator can be replaced, we can roll back to a previously valid state (exiting users upon request) and continue from that state with a new operator.\nSystem roles\nWe have two roles\nthe users, create transactions to update the state\nthe operator, uses snarks to aggregate these transactions into single on-chain updates.\nThey use a smart contract to interact. We have a list of items in a merkle tree that relates a public key (the owner) to a non-fungible token.\nTokens can be withdrawn, but only once.\nIn snark transactions\nThe users create transactions to update the ownership of the tokens which are sent to the operator off-chain. The operator creates a proof that a\nprevious state\nset of transactions\nproduce the newState. We then verify the proof inside the EVM and update the merkle root if and only if the proof is valid.\nPriority queue\nThe users are also able to request a withdrawal at the smart contract level. If the operator fails to serve this queue inside a given time limit, we assume data is unavailable. We slash the operator and begin looking for a new opeartor.\nIt is impossible to withdraw the same leaf twice as on a withdrawl we store each leaf that has been exited and check this for all future exits.\nOperator auction\nIf a previous operator has been slashed we begin the search for a new operator. We have an auction where users bid for the right to be the operator giving a deposit and the state from which they wish to begin.\nAfter a certain time, the new operator will be selected based on the highest bid (above a certain minimum) with the most recent sidechain state.\nRoll back\nWhen the operator is changed, we allow users to exit. The only reason a user needs to do this is if they got their token in a state that will be rolled back.\nWe order these withdrawals by state and roll back the chain processing transactions in that state until we get to the state where the new operator will continue from.\nNote that since it is impossible to withdraw the same leaf twice, user cannot exit the same leaf from an older state.\nDiscussion\nThe operator is forced to process requests in the priority queue, otherwise they are slashed. If they refuse to operate the snark side of the system they are still forced to allow priority queue exits. Therefore, the system will degrade to an on-chain token if the operator becomes malicious.\nA new operator should only join from a state for which they possess all the data. If not, they could be slashed by a priority queue request they can’t process.\nA user should not accept a leaf as being transfered unless all the data of the chain is available so that they know in the worst case they can become the new operator if a roll back happens.\nIt is probably outside the regular users power to become an operator, but their coin will be safe as long as there is a single honest operator who wants to take over. Also bidding on a newer state will give them an advantage over all other bids.\nThis however allows the current operator to again become an operator because they will know the data of the most recent state and can bid on the latest state. However we can define a minimum stake that will be slashed again if they again refuse to serve the priority que. Therefor we can guarantee that someone will come forward to process the queue or else the chain will roll back to state 0 and users can exit as we roll back.\nUnlike Plasma constructions that cannot guarantee the validity of all states, this design avoids competitive withdrawals since snarks disallow invalid state transitions. As a result, we can recover from scenarios with malicious operators without forcing all users to exit (however those that wish to exit can still do so).\nAppendix 1 Calculations of tps\nCurrently it takes ~500k constraints per signature. With optimization we think we can reduce this to 2k constraints.\nCurrently our hash function (sha256) costs 50k transactions per second. We can replace this with pedersen commitments which cost 1k constraints.\nIf we make our merkle tree 29 layers we can hold 536,870,912 leaves.\nFor each transaction we must\nConfirm the signatures = 2k constraints\nConfirm the old leaf is in the tree = 1k * 29 = 29k constraints\nAdd the new leaf and recalculate the root = 1k * 29 = 29k constraints\nThat equals 60k constraints per transaction.\nwu et al report that they can prove a snark with 1 billion gates.\n1000000000 / 60,000 = 16666 transactions per snark confirmation\nValidating a snark takes 500k gas and we have 8 million gas per block. Which means we can include 16 such updates per block.\n16666 * 16 = 266656 transactions per block\n266656 / 15 = 17777 transactions per second.\nWe can probably reach bigger tps rates than this by building bigger clusters than they have.\nNote: running hardware to reach this rate may be quite expensive, but at the same time much less than the current block reward.\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": [
            "zk-roll-up"
        ]
    },
    {
        "title": "The return of Torus Based Cryptography: Whisk and Curdleproof in the target group",
        "link": "https://ethresear.ch/t/the-return-of-torus-based-cryptography-whisk-and-curdleproof-in-the-target-group/16678",
        "article": "In the last two years the Ethereum Foundation has been working hard to devise a practical shuffle-based Single Secret Leader Election (SSLE) protocol. This dedicated effort by the Ethereum Foundation’s research team has culminated in the creation of two remarkable protocols: Whisk and Curdleproof (a zero-knowledge shuffle argument inspired by BG12).\nA series of posts by Dapplion:\nWhisk: Induced missed initial slots\nWhisk: Expensive duty discovery\nWhisk: the bootstrapping problem\naccurately describes the status quo.\nThe problem that we are discussing and trying to solve in this post is the bootstrapping problem. To gain a clearer insight into this challenge, let’s take a moment to revisit the workings of Whisk.\nFor the rest of this post:\nG_1, G_2 are the BLS12-381 generators\ng_T = e(G_1,G_2)\nWhisk’s recap\nAs mentioned above Whisk is a proposal to fully implement SSLE from DDHand shuffles scheme (see also section 6 from Boneh et al paper).\nThe idea behind this solution is pretty straightforward and neat. Let’s list below the key ingredients of the commitment scheme in Whisk (at the net of the shuffles):\nAlice commits to a random long-term secret k using a tuple (rG,krG) (called tracker).\nBob randomizes Alice’s tracker with a random secret z by multiplying both elements of the tuple: (zrG,zkrG).\nAlice proves ownership of her randomized tracker (i.e. open it) by providing a proof of knowledge of a discrete log (DLOG NIZK) that proves knowledge of a k such that k(zrG)==zkrG .\nIdentity binding is achieved by having Alice provide a deterministic commitment com(k)=kG when she registers her tracker.\nWe also use it at registration and when opening the trackers to check that both the tracker and com(k) use the same k using a discrete log equivalence proof (DLEQ NIZK).\nWhisk can be implemented in any group where the Decisional Diffie Hellman problem (DDH) is hard. Currently Whisk is instantiated via a commitment scheme in BLS12-381.\nSo far, everything is going well. BUT, the question remains: which long-term secret ‘k’ should be used?\nBootstrapping problem and de-anonymization\nThis problem is discussed at length in Dapplion’s post, but just to give you a flavor of the issue, let’s assume we take the easy way and use the validator’s secret signing key k and its associated public key kG_1 for bootstrapping. The issue here is that if the validator signs at least one message where the message M is known (this is usually true for attestation), the validator’s tracker could be de-anonimized forever with a single pairing operation:\ne(rG_1, kH(m)) \\stackrel{?}{=} e(rk G_1, H(m))\nSo it is clear that the pairing operation is acting as a de-anonymization oracle.\nIn order to contrast the pairing oracle, Justin Ðrake proposed  instantiating Whisk using the target group instead.\nNote: As it is customary, we will switch to multiplicative notation.\nN.B.: In case there are concerns about exposing the validator key k, please note that g_T^{kr} is nothing else that e(rG_1.kG_2).\nThe downsize of this approach respect to the current status quo is that the coordinates of g_T lie in a large finite field \\mathbb{F}^*_{p^{12}} in this case. This makes the scheme a considerably slower.\nIn the next section we show an attempt to improve the situation  a bit.\nXTR and CEILIDH to the rescue\nXTR is a Public Key System that was designed by A. Lenstra and E. Verheul presented at  CRYPTO 2000. XTR stands for Compact Subgroup Trace Representation and it is based on a clever observation that elements in G_{p^2-p+1}, where a prime order N subgroup G_N \\subset G_{p^2-p+1} with N dividing \\Phi_6(p)=p^2-p+1, can be compactly represented by their trace over \\mathbb{F}^*_{p^2} which is defined by:\nTr: x \\rightarrow x + x^{p^2}+x^{p^4}\nLenstra and Verheul showed that if g \\in G_{p^2-p+1} and c = Tr(g) then it is possible to work eficiently and compactly using the compressed generator (with some limitations that we explore in the next subsection).\nHere you can find an example of the DH protocol implented applying XTR to the target group of the BLS6 curve defined in this paper.\nThe XTR cryptosystem works well in the cyclotomic subgroup of \\mathbb{F}^*_{p^6} for prime p, although it is possible to generalize it to extension fields \\mathbb{F}^*_{q^6} with q = p^m making it compatible with the target group of BLS12-381.\nLimitations of XTR\nOne drawback of using traces, as XTR does, is that the compression is not lossless, which can lead to the mapping of conjugates into identical compressed elements. However, this is not a significant issue since it is possible to employ a trace-plus-a-trit method to compress and decompress elements, much like what is done for elliptic curve compressed points.\nAnother, more stringent issue to consider is this: performing a single exponentiation (Tr(g^x)) in compressed form is easy, and a double exponentiation (Tr(g^xh^y)) is feasible. However, going beyond that and performing more complex operations in a compressed format is not really feasible (this limitation resembles what we encountered when attempting to translate the SSLE protocol into the isogeny/action group setting). Consequently, the XTR setting may not be suitable for porting all discrete-log cryptosystems. We believe that translating Whisk into the XTR setting is a simple exercise, while more caution needs to be applied to Curdleproof.\nCEILIDH\nA significantly cleaner approach to lossless compression can be found in algebraic tori, as introduced by Rubin and Silverberg in 2003. They presented two novel systems: \\mathbb{T}_2, designed as a substitute for LUC and utilizing quadratic extension fields, and CEILIDH, positioned as an alternative to XTR. CEILIDH, pronounced as “Cayley,” was introduced as an acronym representing “Compact, Efficient, Enhancements over LUC, Enhancements over Diffie–Hellman” and is primarily a factor-3 compression and decompression technique designed for “hard” subgroups of sextic extension fields, lacking an efficient exponentiation method. However, it is possible to enhance CEILIDH by combining it with efficient arithmetic over compressed element.\nConclusion\nIn an attempt to address the bootstrapping problem in Whisk, we explored the potential of using the target group and discussed the application of XTR and CEILIDH. While XTR does exhibit some limitations, both XTR and CEILIDH present a promising path for achieving lossless compression and improved efficiency in discrete-log cryptosystems.  To the best of our knowledge, this may be the first deployment of a real-life protocol employing XTR. We will dedicate time to investigate further and will report back on our findings. For an in-depth overview of XTR and Tori, refer to the excellent paper by Martijn Stam.\nAcknowledgments: We would like to thank Justin Drake, Robert Granger, Ignacio Hagopian, Gottfried Herold, Youssef El Housni, George Kadianakis, Mark Simkin, Dapplion, Michael Scott for fruitful discussions.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": [
            "single-secret-leader-election"
        ]
    },
    {
        "title": "Proving multi-client Beam chain",
        "link": "https://ethresear.ch/t/proving-multi-client-beam-chain/21027",
        "article": "Hello teams,\nI have a couple questions to make sure zkVM teams support the Beam chain proposal as best as possible.\nImplementation\nWhat languages will clients be built in. In our case at Lita/Valida, we can support C, Nim (through C), Rust. And we plan to add Go, WASM (for Javascript clients) and Zig support (seems to be a popular new language that Eth devs want to build in).\nAre Nethermind and Teku/Besu team planning to build a Beam Client, will it be in C# and Java? What about LambdaClass and Elixir?\nNetworking\nWhat proof sizes are we targeting?\nPerformance\nWhat proving speed do we need? We plan to add some benchmarks of the current state transition function of Grandine, Lighthouse and Nimbus to get a baseline.\nIf block times become 4s, should the proof be aggregated over a whole “epoch”\n",
        "category": [
            "Consensus"
        ],
        "discourse": []
    },
    {
        "title": "Dynamic Finalization Considering 51% Attacks",
        "link": "https://ethresear.ch/t/dynamic-finalization-considering-51-attacks/21112",
        "article": "from Titania Research\nThank you Ambition, terence, Artem, Titania Research Protocol Team for discussion and feedback\nTL;DR\nThis document classifies attack methods against PoS Ethereum and proposes countermeasures, particularly against the notably dangerous 51% attack. The main points are as follows:\nClassification of Attack Methods: Two indicators, attack stealthability and attack sustainability, are introduced to analyze known attack methods.\nRisks of a 51% Attack: It highlights the particular danger posed by attacks where the attacker controls more than 51% of the staking ratio and explains why this is the case.\nProposals for New Defenses: Two new mechanisms are suggested to counter the high likelihood of a 51% attack: Close Vote Detection, which detects the potential for such an attack, and Emergent Dynamic Finalization, which delays finalization when the risk is elevated.\nConcerns and Future Challenges: It addresses potential issues with the proposed mechanisms and discusses future research directions.\nThe aim of this proposal is to enhance the security of PoS Ethereum, specifically by strengthening defenses against the perilous 51% attack.\n1. Classification of Existing Attack Methods\nSeveral attack methods against PoS Ethereum are known, with potential outcomes that attackers might realistically target, including reorg, double finality, and finality delay. A crucial factor in this analysis is the staking ratio required for an attack, indicating the minimum stake necessary, which serves as a barrier to entry. However, nearly as critical is attack sustainability, which measures how continuously an attacker can maintain the attack. If an attack is sustainable, it could cause significant damage. Additionally, attack stealthability is also important, as it indicates how covertly an attacker can execute an attack. If a protocol cannot detect an attack, it becomes difficult to determine whether defensive measures are necessary. Higher values for both metrics indicate a more negative outlook from the protocol’s perspective. The representative attack methods analyzed include:\nFinality delay 33% attack\nDouble finality 34% attack\nShort-reorg & censoring 51% attack (control over future)\nShort-reorg & censoring 66% attack (control over past and future)\nA: Finality delay 33% attack\nThe finality delay is an attack that can be executed with a staking ratio of 33%. The attacker prevents finalization by failing to provide 33% of attestations. A defensive measure during this attack is the inactivity leak mechanism. This mechanism identifies validators who either fail to attest or attest against the majority, reducing the staked ETH of such inactive validators. During a 33% attack, the inactivity leak activates, causing the attacker’s ETH to decrease and fall below the amount needed to sustain the finality delay. Consequently, the attack’s sustainability is relatively low and temporary, making it easier to detect due to the inactivity leak.\nB: Double finality 34% attack\nDouble finality refers to an attack wherein the attacker submits attestations to finalize two branches simultaneously. To achieve double finality, the attacker requires a staking ratio of 34%. The attacker engages in double voting for the 34% of attestations, working to finalize both forks. Defensive measures during this attack include the slashing mechanism. Since double voting is prohibited, the attacker would lose their staked ETH, making the attack easily detectable (low undetectability). Furthermore, the substantial slashing penalty means that attacking will likely only happen once; if the attacker had the budget to attack multiple times, they would likely choose a 66% attack instead. Thus, attack sustainability for this method is also very low.\nC: Short-reorg & censoring 51% attack (control over future)\nWhen an attacker possesses a staking ratio of 51%, they can manipulate the fork choice algorithm. Attacks A and B were directed at the Casper FFG (finality gadget), whereas this attack targets the LMD GHOST (fork choice algorithm). In this scenario, the attacker can freely create the heaviest branch in LMD GHOST, causing honest validators to follow the attacker’s branch, resulting in finalization. This enables the attacker to censor specific transactions and perform short-term reorganization (reorg) to maximize their miner extractable value (MEV) without incurring slashing penalties.\nIn attacks A and B, mechanisms existed to reduce the attacker’s potential upon occurrence. In attack A, the inactivity leak decreases the attacker’s staking ratio below the 33% threshold, rendering the attack impossible. In attack B, one-third of their staking ratio is slashed during that epoch, making repeated attacks effectively unfeasible.\nHowever, there are currently no algorithmic defensive measures against attack C. Even if there is a slot with a 51% voting ratio, there is no way to distinguish whether that attestation is malicious or a legitimate disagreement among honest validators. This means that attack undetectability is significantly high. Once an attack succeeds, the attacker can persistently continue the attack until a hard fork decision is made through the social layer, resulting in very high attack sustainability.\nD: Short-reorg & censoring 66% attack (control over past and future)\nIn the short-reorg & censoring 66% attack, the attacker can freely manipulate finalization, rewriting past chains and finalizing new branches. The characteristics of attack D are similar to attack C, with both exhibiting high undetectability and high sustainability.\nA critical point to highlight is that after executing a 51% attack, the attacker can utilize the profits to aim for a 66% attack. The potential gains from a 51% attack are significantly higher compared to the 33% and 34% attacks, and because they incur no penalties such as inactivity leak or slashing, a successful attempt could exponentially increase their dominance.\nSummary of attack methods\nThe following table summarizes the characteristics of the representative attack methods analyzed:\nFrom this table, an interesting trend can be observed: attacks at the 33% and 34% levels (A and B) are easy to detect and exhibit low sustainability, while attacks of 51% and higher (C and D) are difficult to detect and show high sustainability, illustrating a clear dichotomy.\n2. The Potential Impact of a 51% Attack\nI would like to emphasize the importance of considering worst-case scenarios regarding the security of PoS Ethereum. Simply put, there is a real possibility that Ethereum could face a situation described as ‘game over.’ If such a scenario were to occur, all past activities and data within countless ecosystems would be rendered null and void.\nReferring to the earlier table, attacks A and B have low levels of both attack undetectability and attack sustainability. From the perspective of an attacker, there is a high likelihood that their actions will be exposed, and these attacks tend to be short-lived.\nIn contrast, attacks C and D exhibit high levels of both attack stealthiness and sustainability. For attackers, these actions are less likely to be detected, allowing them to sustain the attack over a longer period and potentially reap immense profits. When considering which of the two attacks, C or D, to focus on, we must first pay attention to the staking ratio as a barrier to attack. While both attacks could cause significant damage, attack C, which requires a smaller absolute amount to execute, is more realistically targeted (especially considering its potential to lead to attack D). In light of these considerations, this discussion will explore defensive measures against short-reorganization and censoring 51% attacks.\nThe key issue with short-reorganization and censoring 51% attacks, as mentioned above, is their high levels of attack undetectability and sustainability, which imply that the potential damage could be extensive.\nLet’s delve deeper into attack sustainability. The reason these attacks are sustainable is that the only defensive measure available is a hard fork through social consensus, which takes considerable time (as demonstrated by the DAO incident, which took a month from the discovery of the hack to the hard fork). During this interval, blocks and epochs finalized by the attacker will accumulate on the legitimate chain. Honest validators risk being penalized for attesting to blocks on an illegitimate chain that has become the minority despite being the canonical one. The crux of the matter lies in the fact that the number of epochs required for finalization is fixed; hence, even in emergencies, the finalization occurs over the same two epochs (approximately 13 minutes) as it does under normal circumstances.\n3. Proposals for Detecting and Defending Against 51% Attacks\nIn the event of a 51% attack, we anticipate that attestations will exhibit a tight margin, such as 50.5% vs. 49.5%, and such close contests are relatively rare during normal operations. We introduce a metric to indicate the likelihood of the current epoch being attacked based on the number of slots where the head votes are ‘close.’ Furthermore, as this metric increases, the number of epochs necessary for finalization will rise exponentially. This mechanism allows for the algorithmic postponement of finalization during emergencies, enabling the community to respond to attackers through social means without requiring a hard fork. Because normal finalization periods will remain unchanged, this implementation can be seamlessly integrated without compromising user experience. We propose the close vote detection mechanism for the former and emergent dynamic finalization for the latter as defenses against 51% attacks.\nClose Vote Detection\nWhen a 51% attack occurs, attackers will deliberately choose a head that appears canonical by being the heaviest. Honest validators can still propose blocks, but attackers can easily manipulate the canonical head through short-term reorganizations whenever they find the proposed blocks undesirable. The closer the attacker’s staking ratio is to 50%, the closer the amount of attestations will be to 50%. Such attestations that are very near to 50% of the head will be referred to as ‘close votes.’ Currently, the determination of whether to finalize an epoch is made at the last slot of that epoch, where we will add the counting of close votes.\nEmergent Dynamic Finalization\nIf the occurrence of close votes exceeds a certain threshold, the system will recognize a state of emergency and significantly increase the number of epochs required for finalization. As a result, the attacker will need to maintain a substantial majority of votes over a longer period to achieve finalization. During this time, the community will have the opportunity to implement countermeasures. Specifically, if the number of slots classified as close votes in the current epoch exceeds a certain threshold, the required number of epochs for finalization will be raised dramatically from the standard two. We refer to this as emergency mode. While there is plenty of room for debate on what this value should be, aiming for a significant improvement over the DAO incident’s month-long delay might suggest trying a value like 2^{15}. This would require the attacker to continue their assault for about nine days (32,768 * 12 seconds ≈ 4,551,168 seconds ≈ 9 days), providing the community ample time to implement countermeasures quickly. This defensive mechanism ensures that normal network operations are unaffected and activates only during emergencies, thereby allowing for smooth implementation without degrading user experience. Moreover, since it functions algorithmically, it can be executed immediately without waiting for human judgment, allowing for rapid responses.\nFormalization\nLet’s define the following symbols, where W, E, F are parameters:\ni: Slot index of the current epoch, ranging from 1 to 32\nC_i: Indicates whether the voting at slot index i is close (1) or not (0)\nV_i: The percentage of attestations at slot index i, expressed in %\nF: The number of epochs required for finalization\nIn its simplest initial form, we propose the following:\nHere are the parameters defined:\nW: The percentage point deviation from 50% that qualifies as a close vote\nE: The threshold number of close vote slots to trigger the emergency mode\nD: The number of epochs required for finalization when in emergency mode\nThe formulas provided define two indicators indicating the possibility of a 51% attack. First, C_i indicates whether a specific slot is considered a close vote, yielding 1 when |V_i - 0.5| falls within the threshold W. Second, F indicates the number of epochs required for finalization. Hence, if the number of close vote slots reaches the threshold E, the required number of epochs increases to D, thereby planning for sustained attacks and mitigating their potential impacts.\nLet’s consider specific values:\nThus, we have:\nWith these settings, if the attestation percentage V_i for any slot is within ±1% of 50%, that slot will be counted as a close vote. If, for instance, 4 out of the 32 slots are close votes, the total of C_i will be 4, requiring F to be set to 2^{15}. Consequently, the attacker will not be able to finalize the chain for approximately nine days, allowing the community enough time to implement a quick hard fork to restore the legitimate Ethereum blockchain.\nReducing the estimated maximum damage\nThe goal of this proposal is to reduce the estimated maximum damage during a 51% attack. It aims to mitigate the likelihood of a ‘game over’ scenario. While it’s challenging to discuss specific quantitative changes, it is feasible to set the parameter D to ensure that the duration does not extend to a month like in the DAO incident. It is essential to consider that the anticipated response time from the social layer should also be factored into this aspect.\nMoreover, various services that interact with Ethereum, such as other chains and centralized exchanges, can operate based on this D. By introducing algorithmic mechanisms, the surrounding ecosystems will also be able to respond algorithmically.\nConcerns about new finality delay mechanisms\nThere is a concern that this proposal may inadvertently create a new finality delay mechanism. For example, it is possible to randomly control 51% dominance over L occurrences among 32 slots, which can be easily calculated using a binomial distribution. While the economic incentive to delay finality is generally low, we cannot rule out potential incentives that may not have been considered. If such incentives arise, they could potentially be addressed by introducing a reputation system. Since attestations involve signatures, attempts to impersonate other validators would require significant time to execute.\nScrutinizing the procedure for implementing a hard fork via social layer\nTo determine optimal parameters, we need to carefully examine the specific procedures required to execute a hard fork through the social layer.\nDetermining parameters W, E, D and formula F through empirical evidence\nIt is necessary to empirically determine suitable values for parameters W (defining the range for close votes), E (defining the threshold for emergency mode activation), and D (defining how much to delay finalization). Additionally, D is a component of the formula F, but we could also consider a more dynamic design where the increase in the number of close votes \\sum_i C_i would result in a greater value for F.\nDetermining the specifications of attestation\nWe need to determine the specifications for attestations.\nHow to handle justifications during emergency mode\nThe behavior of inactivity leaks during emergency mode\nHow to specifically update the data types submitted through attestations.\n5. Conclusion\nIn this proposal, we focused on the particularly dangerous 51% attack as one of the attack methods against PoS Ethereum, discussing its risks and implications while proposing new defense strategies. Specifically, we aimed to enhance resistance to 51% attacks by introducing mechanisms like Close Vote Detection and Emergent Dynamic Finalization.\nFuture research should further explore the effectiveness of the proposed defense strategies and their applicability to other attack methods. There is also a need to continue investigating parameter optimization and specific implementation methods.\nAdditionally, analyzing attack methods against different consensus algorithms and formulating defense strategies based on social incentives are valuable directions for further discussion. I look forward to engaging with the Ethereum community about the value of these ideas and addressing any concerns.\nReference\nEthereum proof-of-stake attack and defense | ethereum.org\nHistory and Forks of Ethereum | ethereum.org\nUnderstanding The DAO Attack\nHard Fork Completed | Ethereum Foundation Blog\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": [
            "security"
        ]
    },
    {
        "title": "Ethereum Needs to Dream Bigger",
        "link": "https://ethresear.ch/t/ethereum-needs-to-dream-bigger/20963",
        "article": "Ethereum Needs to Dream Bigger\nEthereum was conceived in 2013 to change the world. Since then, the original vision of a blockchain that would serve as the backbone for a new world order—a permissionless financial substrate with endless possibilities—has been ground to dust by the harsh realities of developing bulletproof production software and navigating core developer politics. In other words, Ethereum is jaded and burnt out. Our most “ambitious” proposals are five-year megaprojects to ship marginal improvements, and progress is slowing incrementally with each hard fork.\nReversing these trends requires accepting two hard truths:\n\nWe are no longer the only game in town; Ethereum is now competing in a marketplace of blockchains. Other teams with different visions are credible threats to Ethereum’s dominance.\n\n\nMaking decisions on five-year timescales doesn’t make sense when cryptography, consensus, engineering, and mechanism design are advancing at an extreme pace. State-of-the-art technology today may not be state-of-the-art in five years.\n\nWe are no longer the only game in town; Ethereum is now competing in a marketplace of blockchains. Other teams with different visions are credible threats to Ethereum’s dominance.\nMaking decisions on five-year timescales doesn’t make sense when cryptography, consensus, engineering, and mechanism design are advancing at an extreme pace. State-of-the-art technology today may not be state-of-the-art in five years.\nBut enough with the diagnosis; how can we treat this ailment? First, we need to agree on and articulate a clear vision for what we are trying to build. In my opinion, this doesn’t require changing course; it requires doubling down on the core values that we started with in 2013:\n“Ethereum should be a global compute platform that provides developers all the tools they need to solve the world’s hardest coordination problems.”\nIn many ways, we have achieved, at least partially, this vision already, but to complete the project we have a long way to go. Smart contracts on Ethereum are Turing complete, but fees on the blockchain are too high for many applications, and block times are long enough that Ethereum isn’t the ideal place to host financial applications. Moreover, Turing completeness means we can write arbitrary smart contract programs that execute logic on their inputs, but if app developers can’t trust the quality of their inputs, then they can’t trust the outputs either.\nEthereum today is credibly neutral and censorship-resistant if given enough time, but when you zoom in to millisecond timescales, it looks a lot more centralized. Within each 12-second block, a single proposer must approve every transaction. This temporary proposer monopoly distorts markets and makes many mechanisms that app developers want to build completely intractable on Ethereum.\nWith that said, here are the goals for the next five years on Ethereum:\n1-second block times\nSingle-slot finality\nVastly increased throughput (>1000 TPS)\nMultiple concurrent proposers\n1-Second Block Times\nA round trip from NYC to Tokyo takes less than 200 ms. You can round-trip a message from NYC to Tokyo five times in one second. If anything, 1-second block times are not ambitious enough. Implementing them would be a major unlock for applications on the L1 and would improve the speed to L1 confirmation for L2 transactions.\nSingle-Slot Finality\nSingle-slot finality is important for rollup interoperability. In five years, all rollups will be ZK rollups. Bridging will happen in the span of two slots on the L1. But this requires single-slot finality because a ZK rollup cannot allow funds to be withdrawn before finality, even with a valid proof.\nVastly Increased Throughput\nFees on Ethereum L1 are too high. Even if we think rollups are going to be where Ethereum activity takes place, L1 fees will still be a multiplier in those fees. We also need to credibly signal to app developers that Ethereum is a place they can safely build their apps—that we will continue to scale as new technologies become available—so there is no possibility that their app, if it becomes successful, will outgrow the capacity of Ethereum.\nMultiple Concurrent Proposers\nOf all these proposed changes, Multiple Concurrent Proposers may be the highest lift but also the highest potential reward. While the other items on the list are parameter changes (which will require a lot of work to accomplish, no doubt), MCP unlocks capabilities that no blockchain has had ever before: real-time censorship resistance guarantees. The types of applications that can be built with this technology are purely theoretical today. MCP is the capstone that brings Ethereum’s market microstructure—which today is extremely centralized and extractive—in line with its broader macro vision of credible neutrality and decentralization.\n",
        "category": [
            "Proof-of-Stake"
        ],
        "discourse": [
            "mev"
        ]
    },
    {
        "title": "AI4Science Oracle for DeSci: Revolutionizing Scientific Discovery with Blockchain",
        "link": "https://ethresear.ch/t/ai4science-oracle-for-desci-revolutionizing-scientific-discovery-with-blockchain/21114",
        "article": "By KD.Conway and Chelsea\nAbstract\nThis proposal outlines a project to build an AI4Science Oracle, a decentralized and verifiable platform that incentivizes the development of AI for science. The platform enables individuals to contribute scientific formulas, findings, or conjectures, which are evaluated for correctness and novelty using formal verification and AI-based novelty assessment. Contributors who meet these criteria are rewarded with tokens. The project aims to accelerate AI’s capabilities in science by systematically integrating novel scientific findings into AI models, fostering a community-driven ecosystem for advancing human and AI collaboration in scientific discovery.\nBackground\n\nAI4Science (AI for Science): uses artificial intelligence to accelerate scientific discoveries, model complex phenomena, and solve problems across disciplines like biology, physics, and medicine.\n\n\nDeSci (Decentralized Science) leverages decentralized technologies, such as blockchain and DAOs, to make scientific research more transparent, accessible, and community-driven.\n\nAI4Science (AI for Science): uses artificial intelligence to accelerate scientific discoveries, model complex phenomena, and solve problems across disciplines like biology, physics, and medicine.\nDeSci (Decentralized Science) leverages decentralized technologies, such as blockchain and DAOs, to make scientific research more transparent, accessible, and community-driven.\nWorkflow\nThe AI4Science Oracle operates through a systematic and decentralized workflow, ensuring that contributions are validated, incentivized, and integrated into the broader AI4Science ecosystem. Let’s take AI4Math as an example.The steps are as follows:\nIndividuals submit their mathematical formulas, findings, or conjectures to the AI4Science Oracle via a decentralized platform.\nEach submission is recorded on the blockchain to ensure transparency and immutability.\nThe oracle conducts two core tests to evaluate the submissions:\n\nCorrectness Verification:\n\nThe oracle uses formal verification, random testing, and other validation techniques to ensure the correctness of the submitted results.\n\n\nThe oracle uses formal verification, random testing, and other validation techniques to ensure the correctness of the submitted results.\n\nNovelty Assessment:\n\n\nLeveraging the AI4Science model, the oracle evaluates the “novelty distance” between the proposed result and existing knowledge.\n\n\nSubmissions that can be easily extrapolated or derived from known formulas fail the novelty test, ensuring only significant contributions pass.\n\n\n\n\nLeveraging the AI4Science model, the oracle evaluates the “novelty distance” between the proposed result and existing knowledge.\n\n\nSubmissions that can be easily extrapolated or derived from known formulas fail the novelty test, ensuring only significant contributions pass.\n\nCorrectness Verification:\nThe oracle uses formal verification, random testing, and other validation techniques to ensure the correctness of the submitted results.\nNovelty Assessment:\n\nLeveraging the AI4Science model, the oracle evaluates the “novelty distance” between the proposed result and existing knowledge.\n\n\nSubmissions that can be easily extrapolated or derived from known formulas fail the novelty test, ensuring only significant contributions pass.\n\nLeveraging the AI4Science model, the oracle evaluates the “novelty distance” between the proposed result and existing knowledge.\nSubmissions that can be easily extrapolated or derived from known formulas fail the novelty test, ensuring only significant contributions pass.\nSubmissions that pass both tests earn the proposer AI4Science tokens as a reward for their contribution.\nTokens are minted exclusively for those who contribute validated results, ensuring value is tied to scientific advancement.\nOnce a proposal passes the initial validation, it enters a public challenge period.\nDuring this time, anyone can challenge the proposal by:\n\nProviding a Counterexample: Demonstrating the incorrectness of the submission.\n\n\nSubmitting a Formal Proof: Proving the correctness or invalidity of the proposal.\n\nProviding a Counterexample: Demonstrating the incorrectness of the submission.\nSubmitting a Formal Proof: Proving the correctness or invalidity of the proposal.\nSuccessful challengers receive AI4Science tokens as a reward for their contributions to the validation process.\nProposals deemed correct and novel after the challenge period are integrated into the AI4Science model.\nThese validated findings are used to improve the AI model’s reasoning, capabilities, and knowledge base, accelerating its ability to solve more complex problems.\nAll operations, including validation, challenges, and token distribution, are conducted in a decentralized and verifiable manner.\nTechnologies such as opML (Optimistic Machine Learning) are employed to ensure trustless and verifiable execution of AI models and validation algorithms.\nBy continuously validating and integrating novel findings, the AI4Science Oracle incentivizes contributions and fosters a thriving community of researchers and validators.\nThe resulting improvements in AI capabilities further accelerate scientific discovery, creating a positive feedback loop of innovation.\nThis proposal ensures a robust, transparent, and decentralized system for advancing both human and AI contributions to science.\nConclusion\nThe AI4Science Oracle bridges the gap between human ingenuity and AI capabilities, offering a decentralized, transparent, and incentivized approach to advancing scientific discovery. By rewarding contributors and challengers and continuously integrating novel findings into AI models, this project not only accelerates the development of AI4Math but also lays the foundation for broader applications across scientific fields.\n",
        "category": [
            "Applications"
        ],
        "discourse": []
    },
    {
        "title": "Deterministic Consensus using Overpass Channels [𝗏.2] in Distributed Ledger Technology",
        "link": "https://ethresear.ch/t/deterministic-consensus-using-overpass-channels-2-in-distributed-ledger-technology/21046",
        "article": "Brandon “Cryptskii” Ramsay\n[email protected]\nNovember 26th 2024\nAbstract\nThis paper presents a formal analysis of the Overpass protocol, a novel layer-2 scaling solution that achieves deterministic consensus through self-proving unilateral state channels. The protocol achieves unprecedented guarantees through zero-knowledge proofs:\nInstant finality through self-proving state transitions\nUnilateral updates without counterparty presence\nProvable security bounds of 2^{-\\lambda} for all attack vectors\nThroughput scaling of O(n \\cdot m) for n wallets and m channels\nSub-logarithmic cost scaling (O(\\log d) for tree depth d)\nThrough rigorous mathematical proofs, we demonstrate how the protocol’s novel combination of zero-knowledge proofs and Sparse Merkle Trees (SMTs) enables individual participants to make instant, provably valid state transitions without consensus, watchtowers, or challenge periods. Each state update generates its own mathematical proof of correctness, enabling true unilateral operation while maintaining global consistency. We establish comprehensive security properties, performance characteristics, and economic guarantees through both theoretical analysis and practical examples.\nProblem Statement and Motivation\nLayer-2 blockchain scaling solutions face a fundamental trilemma:\nFinality vs. Liveness: Traditional channels require both parties online or complex challenge periods\nSecurity vs. Speed: Existing solutions trade instant finality for probabilistic security\nScalability vs. Cost: Higher throughput typically requires expensive validation\nExisting approaches face fundamental limitations:\nConsider Alice’s high-volume marketplace requirements:\nTransaction volume: >10^4 microtransactions daily\nFinality requirement: Instant (no waiting periods)\nSecurity threshold: Pr[\\text{double-spend}] \\leq 2^{-128}\nMaximum per-transaction cost: \\leq \\$0.01\nOperational constraint: Must work when counterparties offline\nNo existing layer-2 solution satisfies these constraints because they all require some form of interactive protocol, challenge period, or external validation.\nThe fundamental challenge Overpass solves: enabling thousands of fast, cheap transactions while maintaining security. Think of traditional blockchain systems like a busy highway with a single toll booth—everyone must wait in line and pay a high fee. Overpass, instead, creates multiple parallel roads (channels) with automated toll systems (cryptographic proofs), allowing many people to travel simultaneously while still ensuring no one can cheat the system.\nContribution\nThe Overpass protocol resolves these challenges through a fundamental innovation: self-proving unilateral state channels. Each state transition generates its own zero-knowledge proof of validity, enabling:\n\nUnilateral Operation:\n\nValid(State_{new}) \\iff VerifyProof(\\pi_{validity})  \n\nNo other validation required.\n\n\nHierarchical State Management:\n\n\\mathcal{H} = \\{Root \\rightarrow Wallet \\rightarrow Channel\\}\n\nEach level maintains independent proof verification.\n\n\nSecurity Guarantees:\n\n\\begin{split}\n   Security_{total} &= Pr[\\text{Break proof system}] \\\\\n   &\\leq 2^{-\\lambda}\n   \\end{split}\n\n\n\nPerformance Characteristics:\n\n\\begin{split}\n   TPS_{Overpass} &= O(n \\cdot m) \\text{ (throughput)} \\\\\n   Time_{finality} &= O(1) \\text{ (instant)} \\\\\n   Cost_{tx} &= O(\\log d) \\text{ (proof size)}\n   \\end{split} \n\n\nUnilateral Operation:\nNo other validation required.\nHierarchical State Management:\nEach level maintains independent proof verification.\nSecurity Guarantees:\nPerformance Characteristics:\nWhere:\nn: Number of parallel channels (\\approx 2^{20} practical maximum)\nm: Transactions per channel (\\approx 2^{16} practical maximum)\nd: Tree depth (= \\log_2(n \\cdot m))\n\\lambda: Security parameter (\\geq 128 bits)\nWhat This Means in Practice\nThe key innovation in Overpass is that each participant can independently update their state by generating mathematical proofs, without requiring counterparty presence, consensus, watchtowers, or challenge periods. This achieves:\nTrue Unilateral Operation: Updates possible while counterparty offline\nInstant Finality: No waiting for confirmations or challenges\nMathematical Security: Proofs guarantee correctness\nMinimal Trust: No reliance on external validators\nPractical Performance: >10^6 TPS at <\\$0.01 per transaction\nThe following sections provide rigorous mathematical proofs of these claims, along with practical implementation guidance.\nNetwork Model\nDefinition (Network Model)\nThe Overpass network consists of:\nKey Property: No synchronous communication required between participants.\nImagine the Overpass network as a bustling financial district in a major city. The Participants are like the businesses and individuals operating within this district, each managing their own accounts (channels). The Channels are the various financial roads connecting these accounts, facilitating seamless transactions. The L1 Settlement Layer acts as the central bank, ensuring that all transactions are officially recorded and secured. Just as no physical roads require all drivers to communicate with each other to navigate, Overpass channels operate independently without needing constant communication between participants.\nCryptographic Primitives\nDefinition (Core Components)\nLet \\mathcal{S} = (\\mathcal{H}, \\mathcal{P}, \\mathcal{Z}) be a tuple where:\nWith properties:\n\\mathcal{H}: Collision-resistant hash function\n\\mathcal{P}: Creates validity proofs for state transitions\n\\mathcal{Z}: Verifies proofs with perfect completeness\nThink of cryptographic primitives as the building blocks of a secure vault system. The Hash Function is like a unique lock that ensures each vault (transaction) can only be accessed with the correct key, preventing unauthorized access. The zk-SNARK Prover is akin to a master key generator that creates proofs (keys) verifying the validity of each transaction without revealing its contents. The Verifier System acts as the security guard, quickly checking these proofs to confirm their authenticity before allowing access. This ensures that every transaction is both secure and efficiently verifiable.\nChannel Construction\nDefinition (Unilateral Channel)\nA channel c is defined as:\nWith properties:\nUnilateral updates: No counterparty needed\nSelf-proving: Validity proven by \\pi_c\nInstant finality: No challenge period\nConstructing a channel in Overpass is similar to setting up a private payment lane between two stores in a shopping mall. This lane allows for quick and direct transactions without the need for shoppers to queue at the main checkout counters. Each transaction within this lane is automatically verified by built-in sensors (proofs), ensuring that every payment is legitimate and instantly recorded, eliminating the need for external supervision or manual checks.\nHierarchical Structure\nDefinition (Protocol Hierarchy)\nTwo-level hierarchy \\mathcal{H}:\nEach level uses a Sparse Merkle Tree:\nWhere:\nV: Channel states\nE: State transitions\nroot: Merkle root\n\\Pi: Validity proofs\n1438×904 31.4 KB\nThe hierarchical structure of Overpass can be compared to a well-organized corporate hierarchy. At the top level, the Root serves as the CEO, overseeing the entire organization. Each Channel acts like a department manager, responsible for their specific teams (transactions). This clear structure ensures accountability and efficient management, where each department independently verifies its activities while contributing to the overall integrity of the company.\nState Representation\nDefinition (Channel State)\nFor a channel c:\nWith invariants proven by \\pi_c:\n\\sum_i balances_i = Total_c (conservation)\nnonce_{t+1} > nonce_t (monotonicity)\nRepresenting the channel state in Overpass is akin to maintaining an up-to-date ledger in a financial office. Each State_c includes detailed records of account balances (like individual bank accounts), a nonce to track the sequence of transactions (similar to transaction numbers), and metadata for additional context (like transaction descriptions). Just as a ledger ensures that all financial transactions are accurately recorded and sequentially ordered, Overpass ensures that all state changes are transparent and tamper-proof.\nState Transitions\nDefinition (Valid State Transition)\nA transition \\Delta is valid if:\nTheorem (State Transition Security)\nFor any adversary \\mathcal{A}:\nProof: State Transition Security\nIntuitive Explanation:\nThis theorem asserts that any adversary attempting to create a valid proof \\pi for an invalid state transition \\Delta has a negligible probability of success, bounded by 2^{-\\lambda}. This security guarantee is foundational to ensuring the integrity of state transitions within the Overpass protocol.\nFormal Proof:\nBy the soundness property of ZK-SNARKs, any attempt to generate a proof for an invalid statement (in this case, an invalid state transition) will fail with high probability. Specifically:\nThe soundness error of the ZK-SNARK is at most 2^{-\\lambda}.\nTherefore, the probability that an adversary can generate a valid proof for an invalid transition is bounded by 2^{-\\lambda}.\nNo additional verification steps can reduce this probability further.\nThus, the theorem holds.\nValid state transitions in Overpass are like approved changes in a company’s financial records. Each time a transaction occurs, it must pass a rigorous approval process (proof verification) to ensure accuracy and legitimacy. This is similar to how a company ensures that every financial entry is reviewed and approved before being recorded, maintaining the integrity of the entire financial system.\nKey Properties\nThis model provides:\nUnilateral Operation: Participants update independently.\nInstant Finality: Valid proof means valid state.\nNo Trust Required: Pure cryptographic security.\nSimple Hierarchy: Two-level structure sufficient.\nThese properties emerge from the self-proving nature of the ZK proofs, requiring no external validation or consensus.\nState Updates\nDefinition (State Update):\nA state update is self-contained:\n\n\\begin{split}\nUpdate = (&State_{new}, \\pi_{validity}) \\text{ where:} \\\\\nState_{new} = \\{&balances', nonce', metadata'\\} \\\\\n\\pi_{validity} = &\\text{Proof of correct transition}\n\\end{split}\n\nExample (Payment Transaction):\nConsider a payment:\n\n\\begin{split}\nState_{t} = \\{&balance_A = 100, \\\\\n&balance_B = 50, \\\\\n&nonce = 15\\}\n\\end{split}\n\nUpdate to:\n\n\\begin{split}\nState_{t+1} = \\{&balance_A = 97, \\\\\n&balance_B = 53, \\\\\n&nonce = 16\\}\n\\end{split}\n\nWith proof \\pi_{validity} showing:\nConservation: 100 + 50 = 97 + 53\nMonotonicity: 16 > 15\nValid ownership: A controls funds\nThe core protocol mechanisms of Overpass can be likened to the operations of an automated trading system in a stock exchange. Each State Update is like executing a trade—it’s processed autonomously, validated through complex algorithms (proof generation and verification), and instantly reflected in the system without manual intervention. This automation ensures high-speed, reliable transactions that scale effortlessly with increased trading volume.\nSecurity Guarantees\nTheorem (Update Security):\nFor any update:\nProof:\nIntuitive Explanation:\nThis theorem ensures that if a proof \\pi_{validity} verifies successfully, the state transition from State_{old} to State_{new} is indeed valid. This guarantees that only legitimate state updates are accepted by the protocol.\nFormal Proof:\nGiven the properties of ZK-SNARKs:\nPerfect Completeness: If the state transition is valid, then a valid proof exists and will always verify.\nSoundness: If the state transition is invalid, no valid proof can be generated that verifies successfully.\nTherefore, if VerifyProof(\\pi_{validity}) = 1, it must be that the state transition is valid.\nTheorem (Atomic Updates):\nUpdates are atomic:\nProof:\nIntuitive Explanation:\nAtomicity ensures that a state update either fully succeeds or fails without partial changes. This prevents inconsistent states from arising due to failed updates.\nFormal Proof:\nThe protocol’s update mechanism includes an assertion that the proof verifies successfully before committing the new state. If VerifyProof(\\pi) = 1, the new state is accepted. If the proof fails to verify, the state remains unchanged. This binary outcome guarantees atomicity.\nPerformance Characteristics\nConcrete performance metrics:\n\nTime Complexity:\n\n\\begin{split}\n   Time_{prove} &= O(\\log n) \\text{ (proof generation)} \\\\\n   Time_{verify} &= O(1) \\text{ (verification)} \\\\\n   Time_{update} &= O(1) \\text{ (state update)}\n   \\end{split}\n\n\n\nSpace Complexity:\n\n\\begin{split}\n   Size_{proof} &= O(\\log n) \\text{ (proof size)} \\\\\n   Size_{state} &= O(1) \\text{ (state size)}\n   \\end{split}\n\n\nTime Complexity:\nSpace Complexity:\nPractical Benefits\nThe unilateral ZKP design provides:\n\nSimplicity:\n\nSingle proof per update\nNo multi-phase protocol\nNo coordination needed\nSelf-contained verification\n\n\nSingle proof per update\nNo multi-phase protocol\nNo coordination needed\nSelf-contained verification\n\nSecurity:\n\nMathematical proof of correctness\nNo trust assumptions\nNo external validation\nInstant finality\n\n\nMathematical proof of correctness\nNo trust assumptions\nNo external validation\nInstant finality\n\nEfficiency:\n\nMinimal communication\nFast verification\nLow overhead\nScalable design\n\n\nMinimal communication\nFast verification\nLow overhead\nScalable design\nSimplicity:\nSingle proof per update\nNo multi-phase protocol\nNo coordination needed\nSelf-contained verification\nSecurity:\nMathematical proof of correctness\nNo trust assumptions\nNo external validation\nInstant finality\nEfficiency:\nMinimal communication\nFast verification\nLow overhead\nScalable design\nSecurity Model\nThe security of Overpass reduces to the security of its cryptographic primitives:\nDefinition (Security Model):\nAgainst any PPT adversary with:\nStandard computational bounds\nAccess to public parameters\nAbility to generate proofs\nThe fundamental security theorems of Overpass are comparable to the rigorous safety standards in aviation. Just as aircraft undergo extensive testing to ensure they can withstand extreme conditions, Overpass’s mathematical proofs ensure that its system is impervious to common attack vectors. This means businesses and users can trust that their transactions are secure, much like passengers trust that their flights are safe.\nCore Security Properties\nTheorem (Proof Security):\nFor any adversary \\mathcal{A}:\nProof:\nIntuitive Explanation:\nThis theorem ensures that the probability of an adversary successfully forging a proof for an invalid state is negligible, bounded by 2^{-\\lambda}. This is crucial for maintaining the integrity and trustworthiness of state transitions within the protocol.\nFormal Proof:\nBy the soundness property of ZK-SNARKs:\nThe probability that an adversary can generate a valid proof for an invalid state is at most the soundness error of the ZK-SNARK, which is 2^{-\\lambda}.\nThis bound holds under the assumption that the underlying cryptographic primitives are secure and the adversary is computationally bounded.\nTherefore, the probability Pr[\\mathcal{A} \\text{ creates valid proof for invalid state}] \\leq 2^{-\\lambda}.\nDouble-Spend Prevention\nTheorem (Double-Spend Prevention):\nThe probability of creating conflicting valid states is:\nProof:\nIntuitive Explanation:\nThis theorem guarantees that the likelihood of an adversary successfully creating two conflicting valid states (i.e., double-spending) is extremely low, bounded by 2^{-\\lambda}. This is essential for preventing double-spending attacks in the protocol.\nFormal Proof:\nLeveraging the soundness of ZK-SNARKs:\nEach valid state transition is accompanied by a proof that must verify successfully.\nNonces ensure unique ordering of state transitions, preventing replay or conflicting updates.\nFor two conflicting states to both be valid, the adversary must forge proofs for at least one invalid state transition.\nThe probability of successfully forging such a proof is bounded by 2^{-\\lambda}.\nThus, Pr[Valid(State_1) \\land Valid(State_2) \\land Conflict(State_1, State_2)] \\leq 2^{-\\lambda}.\nState Integrity\nTheorem (State Integrity):\nFor any state transition sequence:\nProof:\nIntuitive Explanation:\nThis theorem ensures that if all proofs in a sequence of state transitions verify successfully, then all resultant states are valid. This maintains the integrity of the entire state transition history.\nFormal Proof:\nBy induction:\nBase Case: VerifyProof(\\pi_1) = 1 \\implies State_1 is valid.\nInductive Step: Assuming State_i is valid, if VerifyProof(\\pi_{i+1}) = 1, then State_{i+1} is also valid.\nConclusion: Therefore, if all proofs verify, all states in the sequence are valid.\nSecurity Composition\nTheorem (Overall Security):\nSystem security reduces to primitive security:\nProof:\nIntuitive Explanation:\nThe overall security of the Overpass protocol is directly inherited from the security of its underlying cryptographic primitives, particularly the ZK-SNARKs.\nFormal Proof:\nSince all security guarantees (such as state transition validity and double-spend prevention) are based on the soundness of ZK-SNARKs, the system’s security level is equivalent to that of the ZK-SNARKs used. Given that the ZK-SNARKs have a soundness error of 2^{-\\lambda}, the entire system inherits this security level.\nPractical Implications\nThese guarantees provide:\n\nInstant Finality\n\nNo waiting periods needed\nNo probabilistic confirmation\nNo external validation\nPure mathematical certainty\n\n\nNo waiting periods needed\nNo probabilistic confirmation\nNo external validation\nPure mathematical certainty\n\nUnconditional Security\n\nNo network assumptions\nNo trust requirements\nNo timing dependencies\nPure cryptographic guarantees\n\n\nNo network assumptions\nNo trust requirements\nNo timing dependencies\nPure cryptographic guarantees\n\nPractical Performance\n\nFast verification\nCompact proofs\nMinimal computation\nEfficient storage\n\n\nFast verification\nCompact proofs\nMinimal computation\nEfficient storage\nInstant Finality\nNo waiting periods needed\nNo probabilistic confirmation\nNo external validation\nPure mathematical certainty\nUnconditional Security\nNo network assumptions\nNo trust requirements\nNo timing dependencies\nPure cryptographic guarantees\nPractical Performance\nFast verification\nCompact proofs\nMinimal computation\nEfficient storage\nMulti-State Updates\nDefinition (State Update Group):\nA group of related updates G = \\{u_1, ..., u_k\\} where each u_i produces a new state:\nTheorem (Group Update Security):\nFor any update group G:\nProof:\nIntuitive Explanation:\nThis theorem ensures that when a group proof \\pi_G verifies successfully, it implies that every individual update within the group is valid, all state changes are correctly applied, and all protocol invariants are maintained.\nFormal Proof:\nThe group proof \\pi_G encompasses all updates u_1, ..., u_k. For \\pi_G to verify:\nEach update u_i must individually satisfy the state transition rules.\nThe collective state changes must adhere to global invariants such as total balance conservation and nonce monotonicity.\nNo conflicting updates are present within the group.\nTherefore, if VerifyProof(\\pi_G) = 1, all updates are valid, state changes are correct, and invariants are preserved.\nCross-Channel Operations\nTheorem (Cross-Channel Atomicity):\nFor updates across channels c_1,...,c_n:\nProof:\nIntuitive Explanation:\nThis theorem guarantees that when a cross-channel proof \\pi_{cross} verifies successfully, all involved channels c_1,...,c_n have been updated correctly and consistently.\nFormal Proof:\nThe ZK proof circuit for cross-channel updates enforces:\nConservation of total value across all channels.\nValid state transitions for each individual channel.\nAtomicity, ensuring that either all channel updates succeed or none do.\nThus, if VerifyProof(\\pi_{cross}) = 1, it ensures that all channels have been updated validly and atomically.\nState Composition\nTheorem (Compositional Security):\nMultiple valid proofs compose securely:\nProof:\nIntuitive Explanation:\nThis theorem states that if a series of proofs \\pi_1,...,\\pi_k all verify successfully, then the final state after all transitions is valid. This ensures that composing multiple valid updates maintains overall system integrity.\nFormal Proof:\nEach proof \\pi_i ensures that the corresponding state transition is valid. By the integrity of individual proofs:\nEach transition preserves the required invariants.\nSequential application of valid transitions maintains state consistency.\nTherefore, the final state State_{final} is valid.\nPractical Implications\nThis simplified design enables:\n\nComplex Operations\n\nMulti-party transactions\nCross-channel transfers\nAtomic swaps\nState composition\n\n\nMulti-party transactions\nCross-channel transfers\nAtomic swaps\nState composition\n\nSecurity Guarantees\n\nSingle-proof verification\nDeterministic outcomes\nNo coordination needed\nInstant finality\n\n\nSingle-proof verification\nDeterministic outcomes\nNo coordination needed\nInstant finality\n\nImplementation\n\nComplex Operations\nMulti-party transactions\nCross-channel transfers\nAtomic swaps\nState composition\nSecurity Guarantees\nSingle-proof verification\nDeterministic outcomes\nNo coordination needed\nInstant finality\nImplementation\nAdvanced security properties in Overpass function like the multi-layered security protocols of a high-security facility. Group Update Security ensures that batches of transactions are processed securely, similar to how a secure facility handles groups of visitors with coordinated access protocols. Cross-Channel Atomicity guarantees that interconnected transactions are executed flawlessly, akin to synchronized operations in a complex manufacturing process where each step must align perfectly to ensure product quality.\nUnilateral Progress\nTheorem (Unilateral Progress):\nAny participant can always progress their state:\nIndependent of:\nOther participants’ availability\nNetwork conditions\nSystem load\nExternal factors\nProof:\nIntuitive Explanation:\nThis theorem ensures that any participant can independently update their state without relying on the availability or participation of others, and regardless of external conditions.\nFormal Proof:\nThe protocol allows unilateral updates by:\nAllowing participants to generate and verify proofs independently.\nNot requiring any interaction or synchronization with other participants.\nTherefore, as long as a participant can generate a valid proof, they can progress their state irrespective of other factors.\nSettlement Guarantee\nTheorem (Settlement Finality):\nFor any valid state update:\nProof:\nIntuitive Explanation:\nSettlement finality refers to the time it takes for a state update to be finalized and irrevocable on the settlement layer. This theorem states that the total time is effectively constant due to the logarithmic time for proof generation and constant time for verification.\nFormal Proof:\nTime_{prove} = O(\\log n): Proof generation scales logarithmically with the number of channels.\nTime_{verify} = O(1): Proof verification time remains constant regardless of the number of channels.\nSince O(\\log n) + O(1) = O(\\log n) and for practical purposes with large n, O(\\log n) is considered effectively constant, especially when optimized with hardware acceleration.\nL1 Settlement\nTheorem (L1 Settlement):\nAny valid state can settle to L1:\nProof:\nIntuitive Explanation:\nThis theorem ensures that any state verified as valid can be settled on the Layer 1 blockchain within a constant number of blocks, ensuring quick and reliable settlement.\nFormal Proof:\nUpon successful verification of the proof \\pi, the state is deemed valid.\nThe settlement transaction is prepared and submitted to L1.\nDue to L1’s consensus mechanism, the transaction will be included in the next available block.\nAssuming L1 has a constant block time, settlement occurs in O(1) blocks.\nThus, any valid state can be settled to L1 within a constant number of blocks.\nPractical Guarantees\nThe system provides:\n\nInstant Progress\n\nNo coordination needed\nNo waiting periods\nNo external dependencies\nNo failure modes\n\n\nNo coordination needed\nNo waiting periods\nNo external dependencies\nNo failure modes\n\nSettlement Assurance\n\nGuaranteed L1 settlement\nFixed settlement time\nNo challenge periods\nNo reversion possible\n\n\nGuaranteed L1 settlement\nFixed settlement time\nNo challenge periods\nNo reversion possible\n\nOperational Properties\n\nDeterministic operation\nLoad-independent\nNetwork-independent\nAlways available\n\n\nDeterministic operation\nLoad-independent\nNetwork-independent\nAlways available\nInstant Progress\nNo coordination needed\nNo waiting periods\nNo external dependencies\nNo failure modes\nSettlement Assurance\nGuaranteed L1 settlement\nFixed settlement time\nNo challenge periods\nNo reversion possible\nOperational Properties\nDeterministic operation\nLoad-independent\nNetwork-independent\nAlways available\nKey properties:\nSelf-contained execution\nNo external dependencies\nImmediate completion\nGuaranteed finality\nLiveness guarantees in Overpass are like the reliability of a 24/7 customer support center. No matter the time or external conditions, participants can always initiate and complete transactions, just as customers can always reach support services when needed. This ensures that the system remains operational and responsive, providing businesses with the confidence that their transactions will always be processed without unnecessary delays.\nCost Model\nDefinition (Operational Costs):\nFor any state update u:\nWith components:\nWhere:\nc_p: Cost per circuit constraint\nc_s: Cost per byte of storage\nc_l: L1 gas cost\nCircuit Size Analysis\nTheorem (Circuit Complexity):\nFor standard operations:\nWhere:\nProof:\nIntuitive Explanation:\nThe size of the circuit required for generating and verifying proofs scales logarithmically with the number of channels due to the hierarchical structure, while other components remain constant.\nFormal Proof:\nSize_{base} accounts for fixed operations unrelated to the number of channels.\nSize_{transition} involves operations proportional to \\log n, stemming from the use of Sparse Merkle Trees which have logarithmic depth.\nSize_{verification} is constant as proof verification does not depend on the number of channels.\nThus, the total circuit size is O(1) + O(\\log n) + O(1) = O(\\log n).\n1439×735 53 KB\nStorage Requirements\nTheorem (Storage Costs):\nState storage requirements:\nWith bounds:\nProof:\nIntuitive Explanation:\nThe storage required for each state update comprises fixed-size components (balances and metadata) and a variable-size component (proof), which scales logarithmically with the number of channels.\nFormal Proof:\nSize_{balance} is constant per account as it only needs to store the numerical balance.\nSize_{proof} scales with \\log n due to the Sparse Merkle Tree structure.\nSize_{metadata} remains constant as it includes fixed attributes like channel ID and configuration.\nTherefore, Size_{state}(u) = O(1) + O(\\log n) + O(1) = O(\\log n).\nL1 Settlement Costs\nTheorem (Settlement Economics):\nL1 settlement costs for state s:\nWhere:\nGas_{base}: Base transaction cost\nGas_{proof}: Cost per proof byte\nGas_{data}: Cost per state byte\nProof:\nIntuitive Explanation:\nThe total gas cost for settling a state on Layer 1 includes a fixed base cost, a variable cost based on the size of the proof, and another variable cost based on the size of the state data.\nFormal Proof:\nGas_{base} accounts for the fixed overhead of initiating a transaction on L1.\nGas_{proof} \\cdot Size_{proof} represents the variable cost associated with transmitting the proof data.\nGas_{data} \\cdot Size_{state}(s) accounts for the storage of state data on L1.\nThus, the total settlement cost is the sum of these components.\nTotal Cost Analysis\nTheorem (Total Cost Bounds):\nFor any update u:\nWhere:\n\\alpha: Circuit computation coefficient\n\\beta: Storage coefficient\n\\gamma: L1 settlement coefficient\n\\mathbb{1}_{settlement}: Settlement indicator\nProof:\nIntuitive Explanation:\nThe total cost of a state update is bounded by the sum of costs associated with proof computation, storage, and optional settlement. Each component scales differently with system parameters.\nFormal Proof:\n\\alpha \\cdot \\log(n) represents the cost associated with the logarithmic scaling of proof computation.\n\\beta \\cdot Size_{state}(u) accounts for the storage cost, which also scales logarithmically.\n\\gamma \\cdot \\mathbb{1}_{settlement} includes the fixed or variable cost of settling to L1, depending on whether settlement occurs.\nThus, Cost_{total}(u) is bounded by the sum of these three components.\nSimple Transfer\nFor basic value transfer:\nComplex Update\nFor multi-party transfer:\nL1 Settlement\nWith L1 settlement:\n1309×1288 63.5 KB\nEconomic Benefits\nThis cost model provides:\n\nPredictable Costs\n\nFixed computation costs\nLinear storage scaling\nOptional L1 settlement\nNo congestion pricing\n\n\nFixed computation costs\nLinear storage scaling\nOptional L1 settlement\nNo congestion pricing\n\nEconomic Efficiency\n\nMinimal overhead\nBatching benefits\nProof amortization\nStorage optimization\n\n\nMinimal overhead\nBatching benefits\nProof amortization\nStorage optimization\n\nUser Advantages\n\nLow base fees\nPredictable costs\nSettlement flexibility\nCost optimization options\n\n\nLow base fees\nPredictable costs\nSettlement flexibility\nCost optimization options\nPredictable Costs\nFixed computation costs\nLinear storage scaling\nOptional L1 settlement\nNo congestion pricing\nEconomic Efficiency\nMinimal overhead\nBatching benefits\nProof amortization\nStorage optimization\nUser Advantages\nLow base fees\nPredictable costs\nSettlement flexibility\nCost optimization options\nThis economic model enables efficient operation at any scale.\nThe economic model of Overpass can be compared to a highly efficient utility service. Just as electricity costs are predictable and scale with usage, Overpass ensures that transaction costs remain low and scale logarithmically with the number of users and transactions. This predictability allows businesses to budget effectively, knowing that their operational costs will remain manageable even as their transaction volume grows exponentially.\nCore Components\nDefinition (System Architecture):\nThe Overpass implementation consists of:\nWith key parameters:\n1429×722 41.1 KB\nProof Circuit\nDefinition (Proof Circuit):\nThe Proof Circuit is a critical component of the Overpass protocol, responsible for generating and verifying zero-knowledge proofs for each state transition. The process begins with the Prover generating a proof \\pi that encapsulates the validity of the state transition from State_{old} to State_{new}. The Verifier then checks the integrity of \\pi. If the proof is valid, the new state is accepted; otherwise, the update is rejected.\nThe StoreState function handles the storage of both the state and its corresponding proof. By hashing the state and proof, unique keys are generated for efficient retrieval. Updating the Merkle tree ensures that the global state remains consistent and verifiable.\nOptimization Techniques\nKey optimizations:\n\nProof Generation\n\nGPU acceleration: Utilize parallel processing capabilities to speed up proof generation.\nCircuit optimization: Design efficient circuits to reduce the computational overhead.\nParallel computation: Generate multiple proofs simultaneously to increase throughput.\nProof caching: Store frequently used proofs to avoid redundant computations.\n\n\nGPU acceleration: Utilize parallel processing capabilities to speed up proof generation.\nCircuit optimization: Design efficient circuits to reduce the computational overhead.\nParallel computation: Generate multiple proofs simultaneously to increase throughput.\nProof caching: Store frequently used proofs to avoid redundant computations.\n\nState Management\n\nEfficient Merkle trees: Implement optimized data structures for faster state verification.\nState compression: Reduce the size of state data to minimize storage and transmission costs.\nLazy evaluation: Defer computation of certain state aspects until necessary.\nBatch updates: Process multiple state updates in a single operation to enhance efficiency.\n\n\nEfficient Merkle trees: Implement optimized data structures for faster state verification.\nState compression: Reduce the size of state data to minimize storage and transmission costs.\nLazy evaluation: Defer computation of certain state aspects until necessary.\nBatch updates: Process multiple state updates in a single operation to enhance efficiency.\n\nL1 Integration\n\nBatched settlements: Aggregate multiple settlements into a single transaction to save gas.\nGas optimization: Optimize smart contract code to reduce gas consumption.\nProof aggregation: Combine multiple proofs into a single aggregated proof for efficiency.\nSmart contract efficiency: Design lean smart contracts to handle settlements effectively.\n\n\nBatched settlements: Aggregate multiple settlements into a single transaction to save gas.\nGas optimization: Optimize smart contract code to reduce gas consumption.\nProof aggregation: Combine multiple proofs into a single aggregated proof for efficiency.\nSmart contract efficiency: Design lean smart contracts to handle settlements effectively.\nProof Generation\nGPU acceleration: Utilize parallel processing capabilities to speed up proof generation.\nCircuit optimization: Design efficient circuits to reduce the computational overhead.\nParallel computation: Generate multiple proofs simultaneously to increase throughput.\nProof caching: Store frequently used proofs to avoid redundant computations.\nState Management\nEfficient Merkle trees: Implement optimized data structures for faster state verification.\nState compression: Reduce the size of state data to minimize storage and transmission costs.\nLazy evaluation: Defer computation of certain state aspects until necessary.\nBatch updates: Process multiple state updates in a single operation to enhance efficiency.\nL1 Integration\nBatched settlements: Aggregate multiple settlements into a single transaction to save gas.\nGas optimization: Optimize smart contract code to reduce gas consumption.\nProof aggregation: Combine multiple proofs into a single aggregated proof for efficiency.\nSmart contract efficiency: Design lean smart contracts to handle settlements effectively.\nDeployment Requirements\nProduction specifications:\n\nComputation Node\n\n\\begin{split}\n   Hardware_{min} = \\{&CPU: 32\\text{ cores}, \\\\\n   &GPU: \\text{CUDA-capable}, \\\\\n   &RAM: 128\\text{ GB}, \\\\\n   &SSD: 2\\text{ TB NVMe}\\}\n   \\end{split}\n\n\n\nSoftware Stack\n\n\\begin{split}\n   Software = \\{&OS: \\text{Ubuntu 22.04}, \\\\\n   &Language: \\text{Rust 1.70+}, \\\\\n   &Storage: \\text{RocksDB}, \\\\\n   &ZK: \\text{PLONKY2}\\}\n   \\end{split}\n\n\nComputation Node\nSoftware Stack\nIntuitive Explanation:\nThe specified hardware ensures that the system can handle high-throughput proof generation and verification efficiently. The software stack is chosen for performance and security, with Rust providing memory safety and concurrency, RocksDB offering fast storage, and PLONKY2 facilitating efficient ZK-SNARKs.\nSystem Monitoring\nKey metrics:\n\nPerformance\n\n\\begin{split}\n   Metrics = \\{&Time_{proof}, \\\\\n   &Time_{verify}, \\\\\n   &Size_{state}, \\\\\n   &Size_{proof}\\}\n   \\end{split}\n\n\n\nResources\n\n\\begin{split}\n   Resources = \\{&CPU_{usage}, \\\\\n   &GPU_{usage}, \\\\\n   &Memory_{usage}, \\\\\n   &Disk_{io}\\}\n   \\end{split}\n\n\nPerformance\nResources\nIntuitive Explanation:\nMonitoring these metrics ensures that the system operates within optimal parameters. Tracking proof times and resource usage helps in identifying bottlenecks and optimizing performance. Resource metrics are crucial for maintaining system stability and scalability.\nImplementing Overpass is similar to deploying a robust IT infrastructure in a large enterprise. The Prover, Verifier, Storage, and L1 Interface components are like the servers, security systems, databases, and network interfaces that keep a business running smoothly. The recommended hardware and software stack ensures that the system is both powerful and reliable, capable of handling high transaction volumes with ease, much like a well-designed IT system supports a growing company’s needs.\nCross-Chain Integration\nCross-chain transfers can leverage ZK proofs directly:\nDefinition (Cross-Chain Proof):\nA cross-chain transfer requires:\nTheorem (Cross-Chain Security):\nSecurity reduces to individual proof verification:\nProof:\nIntuitive Explanation:\nCross-chain transfers rely on multiple proofs to ensure the validity of each step in the transfer process. This theorem states that the overall security of cross-chain operations is equivalent to the security of the underlying ZK proofs.\nFormal Proof:\nEach component proof (\\pi_{source}, \\pi_{lock}, \\pi_{destination}) must individually verify successfully.\nSince each proof has a security bound of 2^{-\\lambda}, the combined security of verifying all proofs remains at 2^{-\\lambda}, assuming independent security guarantees.\nPrivacy Enhancements\nPrivacy improvements through nested proofs:\nDefinition (Private State):\nA private state update:\nTheorem (Privacy Guarantees):\nFor any adversary \\mathcal{A}:\nProof:\nIntuitive Explanation:\nThis theorem ensures that the actual state cannot be inferred from the hidden state with any significant probability, preserving the privacy of participants.\nFormal Proof:\nState_{hidden} is a cryptographic commitment to State_{real}, ensuring that the real state is concealed.\nThe zero-knowledge property ensures that no information about State_{real} is leaked through \\pi_{private}.\nTherefore, the probability that an adversary can deduce State_{real} from State_{hidden} is bounded by the soundness error of the ZK-SNARK, which is 2^{-\\lambda}.\nThus, Pr[\\mathcal{A}(State_{hidden}) \\rightarrow State_{real}] \\leq 2^{-\\lambda}.\nRecursive Proofs\nDefinition (Recursive Proof Chain):\nRecursive composition of proofs:\nTheorem (Recursive Scalability):\nWith recursive proofs:\nProof:\nIntuitive Explanation:\nRecursive proofs allow multiple proofs to be combined into a single proof without increasing the size or verification time, enabling scalable verification regardless of the number of underlying proofs.\nFormal Proof:\nThe recursive proof \\pi_{recursive} encapsulates multiple individual proofs \\pi_1, ..., \\pi_n.\nDue to recursion, the verification of \\pi_{recursive} remains constant in time and size, regardless of n.\nThe security of the recursive proof is maintained as each individual proof contributes to the overall security without introducing additional vulnerabilities.\nTherefore, Verification_{time} = O(1), Proof_{size} = O(1), and Security = 2^{-\\lambda}.\nProof System Improvements\nFaster proof generation\nMore efficient circuits\nHardware acceleration\nProof compression\nPrivacy Enhancements\nHidden state transitions\nAnonymous ownership\nConfidential amounts\nMetadata protection\nRecursive Composition\nEfficient recursion\nProof aggregation\nBatch verification\nConstant-size proofs\nPhase 1: Core Optimization\nPhase 2: Advanced Features\nPhase 3: Ecosystem\nResearch Challenges\nKey open problems:\n\nTheoretical\n\n\\begin{split}\n   Challenges = \\{&Proof_{efficiency}, \\\\\n   &Circuit_{optimization}, \\\\\n   &Privacy_{techniques}, \\\\\n   &Recursion_{methods}\\}\n   \\end{split}\n\n\n\nTechnical\n\n\\begin{split}\n   Engineering = \\{&Hardware_{speedup}, \\\\\n   &Storage_{scaling}, \\\\\n   &Proof_{compression}, \\\\\n   &Implementation_{tools}\\}\n   \\end{split}\n\n\nTheoretical\nTechnical\nThe potential extensions of Overpass are comparable to the future expansions of a tech company’s product line. Cross-Chain Integration is like integrating new software platforms, allowing Overpass to connect seamlessly with other blockchain systems. Privacy Enhancements are akin to adding new security features to protect user data. Recursive Proofs enable Overpass to scale effortlessly, much like a company can expand its operations without compromising on quality or efficiency.\nCore Protocol Achievements\nThe Overpass protocol demonstrates:\n\nSecurity Guarantee:\n\n\\begin{split}\n   Security_{system} = &Security_{ZK\\text{-}SNARK} \\\\\n   = &2^{-\\lambda}\n   \\end{split}\n\n\n\nPerformance:\n\n\\begin{split}\n   Performance = \\{&Time_{prove} = O(\\log n), \\\\\n   &Time_{verify} = O(1), \\\\\n   &Time_{settlement} = O(1)\\}\n   \\end{split}\n\n\n\nCosts:\n\n\\begin{split}\n   Cost_{total} = &Cost_{proof} + \\\\\n   &Cost_{storage} + \\\\\n   &(Optional)Cost_{L1}\n   \\end{split}\n\n\nSecurity Guarantee:\nPerformance:\nCosts:\nComparative Analysis\nTheorem (System Comparison):\nTraditional L2s vs Overpass:\n\nSecurity Model\n\n\\begin{split}\n   Security_{L2} &= \\begin{cases}\n   Consensus_{security} & \\text{(Rollups)} \\\\\n   Challenge_{period} & \\text{(Channels)} \\\\\n   Watchtower_{reliability} & \\text{(Plasma)}\n   \\end{cases} \\\\\n   Security_{Overpass} &= 2^{-\\lambda} \\text{ (ZK proof)}\n   \\end{split}\n\n\n\nFinality Time\n\n\\begin{split}\n   Time_{L2} &= \\begin{cases}\n   O(\\text{blocks}) & \\text{(Rollups)} \\\\\n   O(\\text{days}) & \\text{(Channels)} \\\\\n   O(\\text{hours}) & \\text{(Plasma)}\n   \\end{cases} \\\\\n   Time_{Overpass} &= O(1) \\text{ (instant)}\n   \\end{split}\n\n\n\nDependencies\n\n\\begin{split}\n   Requires_{L2} &= \\begin{cases}\n   Consensus & \\text{(Rollups)} \\\\\n   Counterparty & \\text{(Channels)} \\\\\n   Watchtowers & \\text{(Plasma)}\n   \\end{cases} \\\\\n   Requires_{Overpass} &= \\text{None (self-proving)}\n   \\end{split}\n\n\nSecurity Model\nFinality Time\nDependencies\nIntuitive Explanation:\nThis theorem compares the security models, finality times, and dependencies of traditional Layer-2 solutions (Rollups, Channels, Plasma) with Overpass. It highlights how Overpass achieves a higher security guarantee with instant finality and minimal dependencies.\nFormal Proof:\nSecurity Model: Traditional L2s rely on consensus mechanisms, challenge periods, or watchtowers, each introducing potential vulnerabilities. Overpass relies solely on the soundness of ZK-SNARKs, providing a direct security guarantee of 2^{-\\lambda}.\nFinality Time: Rollups depend on block confirmations, Channels require waiting periods for challenges, and Plasma relies on watchtower operations. Overpass, through instant proof verification, achieves finality in constant time.\nDependencies: Traditional L2s require consensus participation, active counterparty engagement, or reliable watchtowers. Overpass eliminates these dependencies by enabling self-proving state transitions.\nTherefore, Overpass offers superior security and efficiency compared to traditional Layer-2 solutions.\n\\end{proof}\nKey Innovations\nTheorem (Core Advantages):\nOverpass provides fundamental benefits:\n\nUnilateral Operation\n\n\\begin{split}\n   Independent_{operation} = &No_{consensus} \\land \\\\\n   &No_{counterparty} \\land \\\\\n   &No_{watchtowers}\n   \\end{split}\n\n\n\nPure Cryptographic Security\n\n\\begin{split}\n   Security_{basis} = &ZK\\text{-}SNARK_{soundness} \\land \\\\\n   &Hash_{collision} \\land \\\\\n   &Merkle_{binding}\n   \\end{split}\n\n\n\nPractical Efficiency\n\n\\begin{split}\n   Cost_{update} &= O(\\log n) \\text{ computation} \\\\\n   Time_{update} &= O(1) \\text{ latency} \\\\\n   Storage_{update} &= O(1) \\text{ space}\n   \\end{split}\n\n\nUnilateral Operation\nPure Cryptographic Security\nPractical Efficiency\nProof:\nIntuitive Explanation:\nThis theorem summarizes the primary advantages of Overpass, emphasizing its ability to operate unilaterally without reliance on consensus mechanisms or counterparties, its robust cryptographic security, and its efficient computational and storage requirements.\nFormal Proof:\nUnilateral Operation: Overpass allows participants to independently update their state without needing consensus, counterparty involvement, or watchtowers, as demonstrated in previous theorems.\nPure Cryptographic Security: The security of Overpass is based solely on the soundness of ZK-SNARKs, collision resistance of hash functions, and binding properties of Merkle trees.\nPractical Efficiency:\n\nComputational costs scale logarithmically with the number of channels (O(\\log n)).\nProof verification and state updates occur in constant time (O(1)).\nStorage requirements per update remain constant (O(1)).\n\n\nComputational costs scale logarithmically with the number of channels (O(\\log n)).\nProof verification and state updates occur in constant time (O(1)).\nStorage requirements per update remain constant (O(1)).\nThus, Overpass achieves its core advantages through its innovative design and efficient implementation.\nIn summary, Overpass stands out in the blockchain landscape much like a high-performance sports car in the automotive industry. While traditional Layer-2 solutions are like standard vehicles—reliable but limited in speed and efficiency—Overpass offers unparalleled performance with instant finality, minimal costs, and robust security. This makes it an attractive choice for businesses seeking both speed and reliability without the complexities and limitations of existing solutions.\nTheoretical\nNovel unilateral ZKP channel design\nPure cryptographic security model\nSelf-proving state transitions\nInstant mathematical finality\nTechnical\nEfficient proof circuits\nMinimal dependencies\nSimple state model\nPractical implementation\nPractical\nProduction-ready design\nClear scaling path\nLow operating costs\nEasy deployment\nImpact\nThe Overpass protocol represents a fundamental rethinking of blockchain scaling:\n\nNovel Paradigm\n\nProofs replace consensus\nUnilateral replaces bilateral\nMathematics replaces game theory\nSimplicity replaces complexity\n\n\nProofs replace consensus\nUnilateral replaces bilateral\nMathematics replaces game theory\nSimplicity replaces complexity\n\nReal-World Benefits\n\nInstant finality\nIndependent operation\nMathematical security\nPractical efficiency\n\n\nInstant finality\nIndependent operation\nMathematical security\nPractical efficiency\nNovel Paradigm\nProofs replace consensus\nUnilateral replaces bilateral\nMathematics replaces game theory\nSimplicity replaces complexity\nReal-World Benefits\nInstant finality\nIndependent operation\nMathematical security\nPractical efficiency\nThe final conclusions highlight Overpass as a transformative innovation in blockchain technology, comparable to the advent of the internet in the late 20th century. Just as the internet revolutionized communication and commerce by enabling instant, scalable interactions, Overpass redefines blockchain transactions by offering deterministic consensus, instant finality, and unlimited scalability. These mathematical guarantees lay the foundation for building real-world financial infrastructure that is as reliable and efficient as critical systems in aviation or space exploration.\nPractical Applications and Use Cases\nPractical Insert:\nHigh-Frequency Trading Platforms:\nImagine a stock exchange where trades are executed and settled within milliseconds, ensuring traders can capitalize on fleeting market opportunities without delay. Overpass provides the mathematical certainty and speed required for such high-stakes environments, eliminating risks like double-spending and ensuring fair execution orders.\nRetail Payment Networks:\nPicture a global retail chain where customers can make purchases instantly without worrying about transaction delays or high fees. Overpass enables point-of-sale transactions to finalize instantly with minimal costs, enhancing customer experience and reducing operational expenses for retailers.\nCross-Border Payments:\nConsider international businesses that require swift and reliable cross-border transactions without the unpredictability of traditional banking systems. Overpass offers mathematically guaranteed settlement and predictable execution times, streamlining global commerce with transparent and efficient fee structures.\nFinancial Services:\nEnvision financial institutions that can automate compliance checks and instantaneously reconcile transactions, all while maintaining tamper-proof audit trails. Overpass empowers these services with the necessary mathematical proofs to ensure integrity and efficiency, revolutionizing how financial operations are conducted.\nRamsay, B. “Cryptskii” (2024). Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof, Payments. Cryptology ePrint Archive, Paper 2024/1526. Overpass v1\nHioki, L., Dompeldorius, A., & Hashimoto, Y. (2024). Plasma Next: Plasma without Online Requirements. Ethereum Research. Retrieved from Plasma Next\nNakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. Bitcoin\nMerkle, R. C. (1987). A Digital Signature Based on a Conventional Encryption Function. In Advances in Cryptology — CRYPTO ’87 (pp. 369–378). Springer Berlin Heidelberg.\nGroth, J. (2016). On the Size of Pairing-Based Non-interactive Arguments. In Annual International Conference on the Theory and Applications of Cryptographic Techniques (pp. 305–326). Springer.\nBen-Sasson, E., Bentov, I., Horesh, Y., & Riabzev, M. (2019). Scalable Zero Knowledge with No Trusted Setup. In Advances in Cryptology – CRYPTO 2019 (pp. 701–732). Springer.\nButerin, V. (2016). Chain Interoperability. R3 Research Paper.\nPoon, J., & Dryja, T. (2016). The Bitcoin Lightning Network: Scalable Off-Chain Instant Payments. Technical Report.\nKhalil, R., & Gervais, A. (2018). NOCUST – A Non-Custodial 2nd-Layer Financial Intermediary. Cryptology ePrint Archive, Report 2018/642.\nGudgeon, L., Moreno-Sanchez, P., Roos, S., McCorry, P., & Gervais, A. (2020). SoK: Layer-Two Blockchain Protocols. In Financial Cryptography and Data Security. Springer.\nZamani, M., Movahedi, M., & Raykova, M. (2018). RapidChain: Scaling Blockchain via Full Sharding. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (pp. 931–948).\nKokoris-Kogias, E., Jovanovic, P., Gasser, L., Gailly, N., Syta, E., & Ford, B. (2018). OmniLedger: A Secure, Scale-Out, Decentralized Ledger via Sharding. In 2018 IEEE Symposium on Security and Privacy (SP) (pp. 583–598).\nYu, M., Sahraei, S., Li, S., Avestimehr, S., Kannan, S., & Viswanath, P. (2020). Ohie: Blockchain Scaling Made Simple. In IEEE Symposium on Security and Privacy (SP).\nGoldberg, S., Naor, M., Papadopoulos, D., & Reyzin, L. (2020). SPHINX: A Password Store that Perfectly Hides Passwords from Itself. In 2020 IEEE Symposium on Security and Privacy (SP) (pp. 1051–1069).\nBoneh, D., Bünz, B., & Fisch, B. (2019). Batching Techniques for Accumulators with Applications to IOPs and Stateless Blockchains. In Annual International Cryptology Conference (pp. 561–586). Springer.\nGabizon, A., Williamson, Z., & Ciobotaru, O. (2019). PLONK: Permutations over Lagrange-bases for Oecumenical Noninteractive Arguments of Knowledge. Cryptology ePrint Archive, Report 2019/953.\nMaller, M., Bowe, S., Kohlweiss, M., & Meiklejohn, S. (2019). Sonic: Zero-Knowledge SNARKs from Linear-Size Universal and Updateable Structured Reference Strings. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (pp. 2111–2128).\nBen-Sasson, E., Bentov, I., Horesh, Y., & Riabzev, M. (2018). Scalable, Transparent, and Post-Quantum Secure Computational Integrity. Cryptology ePrint Archive, Report 2018/046.\nChiesa, A., Hu, Y., Maller, M., Mishra, P., Vesely, N., & Ward, N. (2019). Marlin: Preprocessing zkSNARKs with Universal and Updatable SRS. Cryptology ePrint Archive, Report 2019/1047.\nBünz, B., Bootle, J., Boneh, D., Poelstra, A., Wuille, P., & Maxwell, G. (2018). Bulletproofs: Short Proofs for Confidential Transactions and More. In 2018 IEEE Symposium on Security and Privacy (SP) (pp. 315–334).\nWang, J., & Wang, H. (2020). Fractal: Post-Quantum and Transparent Recursive Proofs from Holography. Cryptology ePrint Archive, Report 2020/1280.\nTomescu, A., Abraham, I., Buterin, V., Drake, J., Feist, D., & Khovratovich, D. (2020). Aggregatable Subvector Commitments for Stateless Cryptocurrencies. In Security and Cryptography for Networks (pp. 45–64). Springer.\n",
        "category": [
            "Layer 2",
            "State channels"
        ],
        "discourse": [
            "zk-roll-up"
        ]
    },
    {
        "title": "On Distributed FRI Computation",
        "link": "https://ethresear.ch/t/on-distributed-fri-computation/20697",
        "article": "In this note we discuss the distributed computation of the FRI protocol. In practice, we often need to distribute the prover’s work across many servers. In the case of using a FRI-based proof system, this leads to the expensive recursive aggregation of the obtained proofs or exchanging data, the size of which is comparable to the size of the circuit. Below, we describe a technical trick that allows us to optimize obtaining a single final proof.\nBatched FRI\nThe batched version of the \\mathtt{FRI} protocol allows one to estimate the closeness of each of the functions f_1, \\dots, f_L to the \\mathsf{RS} code. To do this, the \\mathtt{Verifier} samples and sends a random \\theta \\in \\mathbb{F}_p to the \\mathtt{Prover}. The latter calculates a linear combination\nThen the \\mathtt{Prover} and \\mathtt{Verifier} execute the regular version of the \\mathtt{FRI} protocol for testing F. The only difference is that each time F is queried at point x, the \\mathtt{Verifier} also performs a consistency check:\nIf the \\mathtt{Verifier} accepted in the end of the protocol, then all f_i are close to \\mathsf{RS}.\nDistributed FRI\nLet us now consider a distributed setting in which n=L \\cdot M polynomials of degree at most d are divided among M \\mathtt{Provers}. The output of the protocol should be a proof that all the polynomials f_1, \\dots, f_n are close enough to the \\mathsf{RS} code. A naive approach would be to send all polynomials in plaintext to one of the provers, who in turn would execute the batched \\mathtt{FRI} protocol. Let us consider how this problem can be solved more efficiently.\n\\mathtt{Provers} generate \\mathsf{Merkle~ Tree} commitments to their polynomials and send them to the \\mathtt{Master~Prover} (this function can be performed by one of the provers, for simplicity we will assume that this is a separate entity). The \\mathtt{Master~Prover} gets a random challenge \\theta from the \\mathtt{Verifier} and broadcasts it among all \\mathtt{Provers}. Now each \\mathtt{Prover} P_i, knowing its number i, can generate its “part of the linear combination” and send it to the \\mathtt{Master~Prover}.\n\\mathtt{Master~Prover} runs a regular version of \\mathtt{FRI} for the polynomial \\sum_{i=1}^{M}F_i. However, it cannot provide polynomial evaluations and Merkle auth paths for  consistency checks in the query phase of the protocol for individual polynomials, so it asks the corresponding \\mathtt{Prover} for each of them.\n\\mathtt{Master~prover} can easily detect malicious behavior of individual \\mathtt{Provers}. This is achieved due to the fact that the partial linear combinations F_i belong to \\mathsf{RS} code. This property is especially useful in a distributed SNARK generation process, as it allows for the implementation of economic measures to penalize participants for misbehaving.\nIt is easy to see that the time complexity of the \\mathtt{Provers} is O(d\\log d). The communication cost (this is communication between provers and master prover) is dominated by sending a partial linear combination, whose size is O(d) elements from \\mathbb{F}_p. Moreover, the number of hash invocations required to verify the final proof is significantly less than that needed to verify M independent proofs.\nYou can find a more detailed description here. Feel free to share your comments!\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "QUIC Support Among Ethereum Consensus Layer Clients",
        "link": "https://ethresear.ch/t/quic-support-among-ethereum-consensus-layer-clients/21102",
        "article": "TL;DR: We (ProbeLab) analyzed the adoption of QUIC among Ethereum Consensus Layer (CL) clients using the Nebula Crawler. Approximately 42% of CL nodes support QUIC, primarily because Lighthouse has it enabled by default. While QUIC support is growing, mainly over IPv4, there’s minimal adoption over IPv6. This post details our findings and encourages monitoring future trends on probelab.io.\nThe Ethereum network is continually evolving to improve performance, scalability, and security. One of the recent advancements is the integration of QUIC support in Ethereum Consensus Layer clients. This post examines the current state of QUIC adoption among these clients, based on data collected and analyzed using the Nebula Crawler on probelab.io.\nIntroduction to QUIC and libp2p\nQUIC is a transport protocol built on top of UDP, designed to enhance the performance of connection-oriented applications. It offers several benefits:\nReduced Latency: QUIC integrates the handshake processes, reducing connection establishment times.\nImproved Congestion Control: It adapts effectively to network conditions, enhancing throughput.\nStream Multiplexing: Allows multiple streams within a single connection without head-of-line blocking.\nIn the Ethereum ecosystem, libp2p provides QUIC support, enabling Consensus Layer clients to leverage these advantages for peer-to-peer communication.\nIncorporating QUIC into Ethereum’s ENR\nAn open pull request recommends including QUIC entries in Ethereum Node Records (ENRs): ethereum/consensus-specs#3644. Sigma Prime has been driving this initiative by adding QUIC addresses to ENRs proactively.\nTo facilitate this integration, we implemented ENR QUIC entry parsing in go-ethereum: ethereum/go-ethereum#30283. This enhancement allows clients like Prysm and tools like the Nebula Crawler to recognize and utilize QUIC addresses effectively.\nData Collection Methodology\nAt ProbeLab, we conduct two crawls per hour of the Ethereum CL network using the Nebula Crawler. We are able to identify which clients support QUIC and quantify the number of nodes for each client. The Nebula Crawler achieves this by:\nParsing ENRs: Extracting QUIC addresses present in the node records.\nUsing libp2p’s Identify Protocol: Discovering QUIC addresses advertised by the node after opening a libp2p connection.\nFindings on QUIC Support\nOur analysis reveals insightful trends in QUIC adoption among Ethereum CL clients. Below are the key observations, with full results available at probelab.io.\nOverall QUIC Support\nTotal QUIC-enabled Nodes: Approximately 42% (~3,700 nodes) of the CL nodes support QUIC.\nClient Distribution: Out of these QUIC-supporting nodes, the vast majority are running Lighthouse.\nall-nodes1568×1200 73.1 KB\nlighthouse1568×1200 72.5 KB\nPrysm\nQUIC Support: Around 2% (~75 nodes) of Prysm nodes support QUIC.\nNote: QUIC support isn’t enabled by default in Prysm, but the presence of these nodes is encouraging for future adoption.\nprysm1568×1200 67.7 KB\nGrandine\nQUIC Support: All nodes are advertising QUIC addresses.\nComment: Grandine has QUIC enabled by default since it uses Lighthouse network stack, but doesn’t constitute a significant portion of the network yet.\ngrandine1568×1200 46 KB\nOther Clients\nClients: Teku, Nimbus, Lodestar, Erigon.\nQUIC Support: Currently, none of these clients support QUIC.\nExpectation: With recent spec changes, we expect that these clients will incorporate QUIC support in upcoming releases.\nQUIC over IPv6 vs. IPv4\nDespite the growing support for QUIC, primarily due to Lighthouse, there are only a few nodes supporting QUIC over IPv6. The overwhelming majority of QUIC-enabled nodes operate over IPv4. This indicates an area for potential growth in IPv6 adoption.\nConclusion and Future Outlook\nThe integration of QUIC into Ethereum’s networking stack represents a significant step toward enhanced network performance. While Lighthouse leads in adoption, we are encouraged by the initial support seen in Prysm and Grandine. As the specification matures, we anticipate broader adoption across other clients.\nAt ProbeLab, we will continue to monitor and report on QUIC adoption trends. Our discv5 CL Weekly Reports will provide ongoing insights, so we encourage stakeholders to stay tuned for further updates.\nFor detailed data and interactive charts, please visit probelab.io.\n",
        "category": [
            "Networking"
        ],
        "discourse": [
            "p2p"
        ]
    },
    {
        "title": "Privacy preserving nullifiers for proof of identity applications",
        "link": "https://ethresear.ch/t/privacy-preserving-nullifiers-for-proof-of-identity-applications/18551",
        "article": "Thanks Jan, Aayush, Yanis, Michael and Maxence for feedback and discussions.\nA growing number of applications like Proof of passport, anon-aadhaar and zk-email use existing infrastructure from institutions to verify identities, respectively electronic passports, India’s Aadhaar system and DKIM registries. As they are based on already existing Public Key Infrastructure, building privacy preserving nullifiers can be quite tricky.\nWe’ll take the example of Proof of Passport, but what is discussed here applies to many other projects. Proof of Passport allows users to scan the NFC chip in their government-issued passport and generate zk proofs showing that they have a valid identity. It can be used for proof of humanity, sybil resistance and selective disclosure of private data like nationality or age.\nWe want to build a proof of passport SBT that displays information the user wants to disclose. It shouldn’t let someone mint two SBTs with a single passport.\nNaively, one can think of using the hash of the government’s signature as a nullifier and storing it publicly. It does prevent reusage of a proof, but if the government keeps a database with all signature it has issued, it can link each nullifier with each user’s address. As the ICAO specifications do not describe whether governments should do it or not, we can assume some governments keep those signatures. Additionally, anyone that gets access to the passport data before or after a mint can store it and deanonymize users. Same problem with zk-email: nullifying an email publicly deanonymizes the user to the mail server that signed the email.\nNote that in the following constructions, we are not trying to hide the fact that a user minted a SBT from the government. This would be hard to do as the government has the same information as the user, and can just request a new SBT to see if the request is rejected or not. Instead, we’re trying to at least hide the user’s address from the government.\nUsing a trusted server\nOne approach would be to let applications manage user privacy themselves. Each application would have a server with its own key pair that can sign government attestations so that:\nThe flow would be the following:\nThe user extracts their government attestation from their passport and sends it to the application’s trusted server\nThe server signs the attestation and hashes the result to get the nullifier. If it has already been included on chain, it refuses the request and responds \\{false, \\emptyset\\}. If it’s not already included, it responds with the \\{true, signature\\}.\nWhen the user submits the proof on chain, they shows that:\nThe disclosed attributes (e.g. nationality) are valid given the government attestation\nThe nullifier, a public output of the proof, is correctly computed as a hash of the server’s signature of the attestation\nThe nullifier is then stored on chain.\nThe server has to block duplicate requests because if it doesn’t, a government could pose as a user, send an attestation, get back a nullifier and identify the user’s address. Note that the government can still know if a user has minted an SBT, but not their address.\nThe main drawback here is obvious: the server sees every government attestation, so it can link every user’s attestation to its address. Also, there is a liveliness assumption on the server. This might be sufficient for some applications like a vote that happens only once. It can also be improved by keeping the secret in an SGX. But for long-lasting general purpose applications like the proof of passport SBT, it’s insufficient.\nUsing two-party MPC\nA better way to do it is using two-party MPC. This way, we can have the user and the server compute a circuit together so that the server does not learn the government attestation.\nThe circuit takes as inputs:\nThe disclosed attributes, e.g. nationality (public)\nThe government pubkey (public)\nThe list of nullifiers already on chain (public)\nThe server’s public key (public)\nThe server’s private key (known only to the server)\nThe government attestation (known only to the user)\nOver MPC, the user and the server compute the signature of the attestation, hashe it and compare it with the nullifiers already posted. Just like before, if it’s included the circuit returns \\{false, \\emptyset\\} and if it’s not included it returns \\{true, nullifier\\}. The user and the server then generate SNARKs proving they performed the MPC correctly and they can be verified on chain.\nNote that we have to do the whole computation from disclosed attributes to nullifier over MPC and in one proof. If we tried to split it so that only the nullifier generation is done over MPC, the user’s proof would have to disclose the government attestation or something deterministically derived from it, which would allow the government to identify them.\nNow, the server can’t link user’s attestations to their addresses. However:\nIf the server colludes with the government and shares its private key, they can deanonymize users.\nThere is still a liveliness assumption on the server\nUsing n-way MPC\nIf we had something like iO, we could do an obfuscated inclusion check on chain. This way, if a government tried to provide the same proof as a user, it would be able to know if the SBT was already minted, but not to which address. We can try to emulate this by extending the MPC setup. It also relaxes the liveliness assumption on a single server.\nA rough flow would be:\nNodes join a network. This can be done on top of EigenLayer so as to slash nodes that misbehave.\nThe circuit is computed by the user together with the nodes. This time, the server secret is a threshold secret reconstructed from t/N nodes.\nAgain, each participant can prove correct computation using a snark. In particular, nodes show their secret is consistent over time by showing the hash of their secret is always the same. This guarantees the reconstructed secret is always the same, which makes the nullifier generation deterministic.\nSigning the attestation with a threshold private key can be replaced with hashing it with the threshold secret for convenience.\nTo avoid putting a large burden on the nodes, it might be possible to adapt the design to something closer to Octopus.\nSomething very similar can be done with threshold FHE. The main difference is that instead of being part of the setup, the user encrypts the attestation with their FHE private key, makes their evaluation key public, and at the end of the process decrypts the encrypted nullifier before posting it on chain. Depending on the overhead of threshold FHE, this could be lighter for the user, but probably heavier for the nodes.\nBut this setup raise new issues:\nLeaking secrets is undetectable. A government corrupting t nodes privately could deanonymize every user. It would be possible to slash nodes only if they make their secret public to everybody. We can still have some cryptoeconomic security, but it’s insufficient if we assume governments are willing to forgo tens of millions in slashing rewards.\nThere must be a way for nodes to rotate, so secrets need some way to be shared or at least added and removed.\nAfter some turnover, there will be a point at which nodes that stopped restaking or taking part in the computation can collude at no cost and deanonymize everyone.\nUsing embedded private keys\nA growing number of official documents like new passports, most of EU’s ID cards and Japanese Myna Cards not only contain a signed attestation from the issuing authority, but also their own public-private key pair. Authentication is done dynamically by asking the chip to sign a message. This is called active authentication, as opposed to passive authentication. It prevents cloning by simply copying the attestation on a forged document.\nMost smart cards are designed to generate their own private key so that it can never leave the chip. If a passport is made this way, the government attests to the public key without ever knowing the private key. In this case, we can construct a privacy preserving nullifier as the hash of a signature of a fixed message.\nWe can prove in a SNARK that the signing private key is correctly derived from the attestation, but it’s impossible for the government to deanonymize the user without knowing their private key.\nSome documents do not support Active Authentication but only Chip Authentication. Chip authentication does not allow for signing messages but performs semi-static ECDH. It involves a scalar multiplication on a secure elliptic curve so it can be used for nullifier generation in the same way.\nThis design is really cool because it doesn’t only hide the address of the user, it can also make it impossible for the government to even know if a user has minted a SBT, as it can’t produce the right signature.\nUnfortunately:\nWe do not currently know how much authorities generate the private keys on the chip vs embed them and store them in a registry. Please reach out if you know more about this.\nThis doesn’t work with passive attestations like anon-aadhaar, email DKIM signatures and older passports.\nThis only works if the signature is deterministic. The ICAO docs allow for RSA or ECDSA to be used. If there is no way of choosing the random k used in ECDSA, then it only works with RSA.\nIt does not prevent someone accessing the document before or after the mint from recording the same signature and deanonymizing the user.\nOther approaches\nIf some application needed to always combine multiple sources, we could nullify the combination using a hash of multiple attestations. This way, all institutions involved would have to collude to deanonymize users.\nAnother approach would be storing hard hashes (Argon2, PBKDF2 or scrypt) of nullifiers to make them harder to check. It would make checking a large set of people impractical, but an attacker looking for one person could easily find it.\nAnother approach would involve biometrics. The photo signed in the passport chip looks too low-resolution to do any significant zkml, but using FaceID or TouchID to sign deterministic messages from a smartphone’s secure enclave might be promising while adding a relatively low trust assumption.\nWe are actively looking for better ideas. Please reach out if you have one!\n",
        "category": [
            "zk-s[nt]arks"
        ],
        "discourse": []
    },
    {
        "title": "LVR-minimization in Uniswap V4",
        "link": "https://ethresear.ch/t/lvr-minimization-in-uniswap-v4/15900",
        "article": "This research has received funding from the Uniswap Foundation Grants Program. Any opinions in this post are my own.\nThe long-anticipated release of Uniswap V4 is upon us. This blog post sketches a straightforward combination of a singleton pool and hooks within the new V4 framework to tackles cross-domain MEV at the source: the block producer, or searchers paying for that privilege.\nRecently, I’ve been doing research on cross-domain MEV sources within the DEX ecosystem, and it almost always came back to the same term: Loss-versus-rebalancing or LVR. LVR put a name to the face of one of the primary costs incurred by DEX liquidity providers. Block builders on any chain are being goose-stepped into profit-maximizing machines. To do this requires a knowledge of the most recent state of the primary exchanges and market-places. Typically, these take the form of centralized exchanges and other large-volume DEXs, not to mention the swaps in the mempool (discussion for another post).\nThe first, guaranteed cost that a DEX must pay each block is that of arbitraging the DEX’s stale reserves to line up with the block builders best guess of where the underlying price is. From here, the builder then sequences the DEX swaps in a way so as to maximize the builders back-running profits. This proceeding sequence of back-runs pays fees to the pool, but are not guaranteed to take place.\nIt is only the arbitrage of the pool reserves that is guaranteed. Luckily, this can be addressed with hooks.\nThe exact implementation of hooks hasn’t been nailed down yet, but in this article we assume, as in the whitepaper, that a hook implements custom logic before or after 4 key phases in the pool contract:\n\nInitialize: When the pool is deployed\n\nModify Position: Add or remove liquidity.\n\nSwap: Execute a swap from one token to another in the V4 ecosystem.\n\nDonate: Donate liquidity to a V4 pool.\nThe Solution: Low-Impact re-addition of retained LVR into the liquidity pool\nOur proposed solution is based on techniques formalized as the Diamond protocol, with similarities to another ethresearch post. We are only interested in hooks before and after swaps. For a particular pool, we need to make a distinction between the first swap interacting with the pool in a block and all other swaps.\nFor our solutions we introduce an LVR rebate function \\beta: \\{1,...,Z \\} \\rightarrow [0,1]. It suffices to consider \\beta() as a strictly decreasing function with \\beta(Z)=0, for some Z \\in \\mathbb{N}. Whenever we call a hook, let B_\\text{current} be the current block number when the hook is called, and B_\\text{previous} be the number of the block in which the most recent swap occurred. We also need to introduce the idea of a vault contract \\texttt{vault}, and a hedging contract \\texttt{hedger}.\nDepositing x_A token $A$s to \\texttt{hedger} increases a contract variable \\texttt{hedgeAvailableA} by x_A (likewise \\texttt{hedgeAvailableB} for token B deposits).  At all times, the block builder can submit a transaction removing some amount of tokens x_A from \\texttt{hedger} if at the end of the transaction \\texttt{hedgeAvailableA}>x_A. If the builder withdraws x_A tokens, reduce  \\texttt{hedgeAvailableA}  by  x_A.\nSolution Description\nIn this solution, consider the following logic(described algorithmically as \\texttt{beforeSwap()} and \\texttt{afterSwap()} hooks in Algorithms 1, 2, and 3):\n\n\\textbf{If}  \\ B_\\text{current}-B_\\text{previous}>0, the swap is the first swap in this pool this block. Send any remaining tokens in \\texttt{hedger} to pool. After this, add some percentage of the tokens in \\texttt{vault} back into the pool. The correct percentage is the subject of further research, although we justify approximately 1% later in this post. Set the \\texttt{hedgeAvailable} variables to 0.\nExecute 1-\\beta(B_\\text{current}-B_\\text{previous}) of \\texttt{swap}_1, and remove the required amount of token A from the pool so the implied price of the pool is equal to the implied price given \\texttt{swap}_1 was executed. This is necessary because if only 1-\\beta(B_\\text{current}-B_\\text{previous}) of \\texttt{swap}_1 is executed the price of the pool will not be adjust to reflect the information of \\texttt{swap}_1. Add the removed tokens to \\texttt{vault}.\n\n\n\\textbf{Else}, it must be that B_\\text{current}-B_\\text{previous}==0, which implies the swap is a \\texttt{swap}_2. Let \\texttt{swap}_2 be buying some quantity x_A of token A. One of the following three conditions must hold:\n\n\n\\textbf{If }\\texttt{hedgeAvailableA}\\geq x_A  AND x_A>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableA} by x_A, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableB} by x_B.\n\n\n\\textbf{Else if } \\texttt{hedgeAvailableB}\\geq x_B  AND x_B>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableB} by x_B, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableA} by x_A.\n\n\n\\textbf{Else} there is not enough tokens deposited to perform the swap, then revert.\n\n\n\n\n\\textbf{If }\\texttt{hedgeAvailableA}\\geq x_A  AND x_A>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableA} by x_A, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableB} by x_B.\n\n\n\\textbf{Else if } \\texttt{hedgeAvailableB}\\geq x_B  AND x_B>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableB} by x_B, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableA} by x_A.\n\n\n\\textbf{Else} there is not enough tokens deposited to perform the swap, then revert.\n\n\\textbf{If}  \\ B_\\text{current}-B_\\text{previous}>0, the swap is the first swap in this pool this block. Send any remaining tokens in \\texttt{hedger} to pool. After this, add some percentage of the tokens in \\texttt{vault} back into the pool. The correct percentage is the subject of further research, although we justify approximately 1% later in this post. Set the \\texttt{hedgeAvailable} variables to 0.\nExecute 1-\\beta(B_\\text{current}-B_\\text{previous}) of \\texttt{swap}_1, and remove the required amount of token A from the pool so the implied price of the pool is equal to the implied price given \\texttt{swap}_1 was executed. This is necessary because if only 1-\\beta(B_\\text{current}-B_\\text{previous}) of \\texttt{swap}_1 is executed the price of the pool will not be adjust to reflect the information of \\texttt{swap}_1. Add the removed tokens to \\texttt{vault}.\n\\textbf{Else}, it must be that B_\\text{current}-B_\\text{previous}==0, which implies the swap is a \\texttt{swap}_2. Let \\texttt{swap}_2 be buying some quantity x_A of token A. One of the following three conditions must hold:\n\n\\textbf{If }\\texttt{hedgeAvailableA}\\geq x_A  AND x_A>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableA} by x_A, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableB} by x_B.\n\n\n\\textbf{Else if } \\texttt{hedgeAvailableB}\\geq x_B  AND x_B>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableB} by x_B, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableA} by x_A.\n\n\n\\textbf{Else} there is not enough tokens deposited to perform the swap, then revert.\n\n\\textbf{If }\\texttt{hedgeAvailableA}\\geq x_A  AND x_A>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableA} by x_A, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableB} by x_B.\n\\textbf{Else if } \\texttt{hedgeAvailableB}\\geq x_B  AND x_B>0, then execute \\texttt{swap}_2 and decrease \\texttt{hedgeAvailableB} by x_B, but do not remove any tokens from \\texttt{hedger}. Increase \\texttt{hedgeAvailableA} by x_A.\n\\textbf{Else} there is not enough tokens deposited to perform the swap, then revert.\nWhat does this solution solve?\nThis solution allows the producer to move the price of the block to any level with \\text{swap}_1, although only executing 1-\\beta(B_\\text{current}-B_\\text{previous}) of \\text{swap}_1. This \\text{swap}_1 can be thought of as the LVR swap, and is such it is discounted. From there, the producer is forced to match buy orders with sell orders. Orders are only executed against the pool if they can also be executed against the tokens in the hedge contract \\texttt{hedger}. If the price does not return to the price set after \\text{swap}_1 (the tokens in \\texttt{hedger} don’t match the \\texttt{hedgeAvailable} variables,) there are sufficient tokens in \\texttt{hedger} to rebalance the pool, and these tokens in the hedging contract are used to do so in the next block the pool is accessed.\nAn ideal solution would allow the producer to execute arbitrary transactions, and then repay \\beta of the implied swap between the start and end of the block, as this is seen as the true LVR (the end of block price is the no-arbitrage price vs. external markets, otherwise the producer has ignored a profitable arbitrage opportunity). Our solution does this in a roundabout way, although using hooks. The producer moves the price of the block to the no-arbitrage price in the first swap, and is then forced to return the price here at the end of the block, all through hooks.\nDoes the solution work?\n\\texttt{yes}\nHow? This solution differs from the theoretical proposal of that of Diamond in 2 important functional ways. Firstly, Diamond depends on the existence of a censorship-resistant auction to convert some % of the vault tokens so the vault tokens can be re-added into the liquidity pool. The solution provided above directly readds the vault tokens into the pool. Through simulation, we have identified the solution provided in this post approximates the returns of Diamond and its perfect auction when the amount being re-added to the pool is less than 5% per block.\nDon´t just take our word for it! The simplest solution in Diamond periodically re-adds the retained LVR proceeds from the vault into the pool at the pool price. We include the graph of the payoff of this protection applied to Uniswap V2 pool vs. a Uniswap V2 pool without this protection. The payoff of this protocol is presented below. \n3200×2400 269 KB\n.\n(The core code used to generate these simulations is available here.)\nWe ran some simulations to compare our solution to the theoretical optimal of Diamond. We chose a $300M TVL ETH/USDC pool at a starting price of $1844, and a daily volatility of 5%. These simulations were run The expected returns of the Diamond-protected pool relative to the unprotected pool over 180 days (simulated 1,000 times) is 1.057961. This is almost exactly equal to the derived cost from the original LVR paper of 3.125bps per day, computed as  \\frac{1}{(1-0.0003125)^{180}} \\approx 1.05787. That’s a saving of $17M over half a year!\nUnfortunately, 100% LVR retention is unrealistic. Let’s instead assume an average LVR retention of 75% (LVR rebate value \\beta of 0.75). Through simulation, the Diamond-protected UniV2 pool gives a relative return vs the unprotected UniV2 pool of 1.0431.\nCompare this to the relative returns of the protocol described in this post applied to a UniV2 pool vs the unprotected UniV2 pool. These simulations are plotted below for several re-add percentages (the amount of the vault tokens (retained LVR) to be re-added to the pool each block). The average relative returns are 1.0456, 1.0436, and 1.0335, for re-add percentages of 1%, 5%, and 12.5% respectively. \n3200×2400 256 KB\n\nFrom this, we can see that the 1% re-add strategy each block actually outperforms the optimal conversion strategy (1.0456 for % re-adding vs. 1.0431 in Diamond). This is because for simulations with large moves, converting the pool is less profitable. Without fees, HODLing is optimal, and re-adding less tokens approaches some form of HODLing. If we introduce fees related to pool size, we can counteract this out-performance.\nIn the following graph, we include a representation of the performance of the theoretical conversion protocol of Diamond (pink), the low-impact re-adding protocol of this post with re-add % of 1% (blue), and HODLing (orangey). All of these returns are vs. the corresponding unprotected UniV2 pool.\n\n3200×2400 236 KB\n\nConsiderations and Limitations\nRe-adding tokens from the pool to the vault creates an expected arbitrage opportunity in the next block, so re-adding less tokens intuitively reduces the losses (increases the profitability of the pool).\nTo access the pool, someone must submit the initial pool swap, and then deposit tokens to the \\texttt{hedger} contract. However, as we typically expect searchers to be profit maximizing, we should then expect these same searchers to back run any set of buy or sell orders to the no-arbitrage price.\nIt is possible that the tokens in \\texttt{hedger} can be used for this back-running. The balance of \\texttt{hedger} can also be updated mid-block before all swaps are executed. This may be necessary if tokens are required mid-block, with benefits outweighing the gas cost to exit and re-enter tokens to \\texttt{hedger}.\nThere are lots of other quirky potential possibilities with hooks on top of this core LVR-retention framework. This post is intended to demonstrate one of the many possibilities that hooks give us.\nAlgorithm Pseudocode\n\nimage970×747 167 KB\n\n\nimage1190×584 88.2 KB\n\n\nimage957×421 59 KB\n\n",
        "category": [
            "Decentralized exchanges"
        ],
        "discourse": [
            "mev"
        ]
    },
    {
        "title": "[RFC] [DRAFT] Anoma as the universal intent machine for Ethereum",
        "link": "https://ethresear.ch/t/rfc-draft-anoma-as-the-universal-intent-machine-for-ethereum/19109",
        "article": "Ethereum intent network1024×1024 366 KB\nThanks to @0xapriori, @adrianbrink, @awasunyin, and @0xemperor for reviewing this post. This is an active draft and further feedback (both solicited and unsolicited) is very welcome. All opinions and errors within are my own.\nOne-line summary\nAnoma brings a universal intent machine to Ethereum, allowing developers to write applications in terms of intents, which can be ordered, solved, and settled anywhere in the Ethereum ecosystem.\ntl; dr summary\nAn intent is a commitment to user preferences and constraints over the space of possible state transitions.\nThe Anoma protocol brings a universal intent machine to Ethereum, allowing developers to write applications in terms of intents instead of transactions.\nAnoma is an interface, not an intermediary - it’s not another MEV redirection device.\nAnoma provides a universal intent standard which does not constrain what kinds of intents can be expressed, but allows for built-in state, network, and application interoperability.\nIntents and applications written with Anoma can be ordered, solved, and settled anywhere - on the Ethereum main chain, on EVM and non-EVM rollups, on Eigenlayer AVSs, on Cosmos chains, Solana, or any sufficiently programmable state machine.\nAnoma provides four key affordances: permissionless intent infrastructure, intent-level composability, information flow control, and heterogeneous trust - using three mechanisms: the resource machine, the heterogeneous trust node architecture, and languages for explicit service commitments.\nAnoma is compatible with any topology the Ethereum network chooses.\nIntroduction\nIn this post, I will describe how Anoma can provide a universal intent machine for Ethereum - both the Ethereum base chain and the Ethereum ecosystem of rollups, searchers/solvers, and other network participants. What is a universal intent machine, you might ask? In what follows I shall attempt to clearly define this concept and what it means for the Ethereum ecosystem - but first I want to explain my motivations, tell you a bit about my personal history with Ethereum, and clarify some essential background context.\nMotivations\nI have three key motivations in writing this piece. First, I want to establish a conceptual and discursive bridge to the Ethereum research community, which Anoma has historically operated somewhat separately from. I think that this separation was necessary in order for us to independently develop useful concepts, but the time has now come to reunite. Second, I want to clearly articulate to potential developers and users of applications built with Anoma what affordances Anoma can provide and why. I do this in order to set accurate expectations for what I think the technology - and often, any intent technology - can and cannot do, so that developers and users can make informed choices. Third, I want to reach out to Ethereum application developers in order to better understand their needs, see which of our ideas might be most helpful, and map out who specifically might be interested in collaborating on what.\nPersonal interlude\nForgive me a bit of self-indulgence here, but I think it will help contextualize the history.\nMy blockchain development journey actually started with Ethereum. In the summer of 2017, I started playing around with the system, and I was looking for a good starter Solidity development project to learn how smart contracts worked. These were the early days of decentralized exchanges (pre-Uniswap). At the time, I briefly used a since-defunct exchange called EtherDelta, and I thought it was a shame that someone had gone to all the trouble to build a complicated smart contract, orderbook server, and web frontend to support only ERC20 fungible tokens, when - to me - a key benefit of Ethereum’s fully-featured VM was the ability to generalize and support any asset representation a user wanted. I thought the best way to test this hypothesis was to try to implement it myself, so I did - and I decided to call it “Wyvern”.\nWyvern was really a proto-intent system - and the whitepaper I wrote at the time uses the word - but I didn’t have a clear mathematical concept in mind then, nor an understanding of what a generalized intent system would really entail. Nevertheless, working on Wyvern taught me quite a bit about how Ethereum and the EVM worked, and also that they might not be the best fit for intents - or, at least, didn’t seem to have been designed for them. Around the time I published the first version of Wyvern, a startup company was founded by Devin Finzer and Alex Atallah called OpenSea. OpenSea - aiming to create a marketplace for non-fungible collectibles, and thus in need of a more generalized exchange protocol - became the first (and more-or-less only) user of Wyvern, which they used until 2022. I didn’t do much work on Wyvern after 2018 - but luckily, since it was already generalized, OpenSea could add features such as batch sales and multi-asset payments without needing any changes to the core protocol.\nAside: Believe it or not, the Wyvern contracts are apparently still #1 in Ethereum DEX state consumption. In retrospect, I really wish I had optimized my Solidity code. (I also looked for historical gas consumption data, but didn’t easily find it)\nAfter building Wyvern, I wanted to explore other aspects of blockchain systems design - particularly interoperability - and I was lucky enough to land a position at the (now defunct) Tendermint company, where I worked on the Cosmos project, specifically IBC, until the launch of IBC in 2021. After Wyvern and IBC, I felt like I had a few key pieces of the protocol puzzle, but not the whole picture - so I decided to co-found Anoma with @awasunyin and @adrianbrink to figure out if I could piece together the rest. Anoma started with a vision, but little idea of how exactly to implement it. This is the primary reason why we didn’t engage with the Ethereum ecosystem earlier on - we didn’t have a clear idea of what we wanted to do, and we didn’t know what Anoma could offer. Through years of research and design - most of it not by myself but rather by the brilliant and compassionate folks in the Anoma research ecosystem - we’ve come to a much better understanding of what Anoma is, and we now have a much better idea of what Anoma can bring to the Ethereum ecosystem. I’m happy to be coming back.\nSome definitions\nBefore discussing Anoma and Ethereum, I want to clarify what I mean by those words. In common language, words for blockchain networks, such as “Ethereum”, “Cosmos”, and “Anoma”, are commonly used to bundle together six distinct components:\nA protocol, defining what transactions do - in Ethereum’s case, the EVM.\nA specific security model, defining how blocks are produced - in Ethereum’s case, Gasper.\nA network or ecosystem of connected machines (physical and virtual) - in Ethereum’s case, the Ethereum ecosystem, including the base chain validators, rollups/L2s, bridges, off-chain services, etc.\nA history (all blocks since genesis)\nA nominal asset - in Ethereum’s case, ETH.\nA community of people who self-identify as members - in Ethereum’s case, the Ethereum community.\nIn this post - which is addressed to the Ethereum community - I shall be talking only about possible relationships between the Ethereum and Anoma protocols and networks - so when I use the word “Ethereum” or “Anoma”, I mean only these components. Possible relationships between histories, assets, and security models are a fascinating topic, but one which I think had better be covered in a separate post.\nInterfaces not intermediaries\nI also want to clarify that Anoma aims to provide a universal intent machine interface for applications - not an intent intermediary. What’s the difference? An interface, as I use the word here, is simply a protocol which translates, or represents, one semantics - in this case, declarative intent semantics - in terms of another - in this case, imperative ordering, compute, and storage semantics of underlying machines. TCP/IP, for example, is an interface - TCP translates the semantics of declarative ordered packet delivery into imperative send, retry, and window management semantics of underlying network hardware. An intermediary, on the other hand, is a particular network participant (possibly a chain or network) through which data (such as intents) flow. Many bridges in the Ethereum ecosystem - multisignature and chain alike - are intermediaries, as are, for example, banks in the US banking system which simply reissue dollars. Interfaces are simply code - freely copied, implemented anywhere, and usable by everybody. Intermediaries, however, are actors in a network - sometimes valuable ones - but their privileged positions often allow to extract rents, introduce additional security assumptions, or add unnecessary latency and complexity. The Anoma protocol is an interface - not an intermediary. There is no “Anoma chain” to which you must send intents.\nWhere are we at?\nThe final (I promise!) clarifying note: Anoma is in active development and not yet finished. Ongoing research will continue to evolve the live system. We’ve finished enough research (indexed here) that the overall design is pretty clear, but some details and priorities in implementation and deployment are not yet fixed. We think the design is at a stage which is possible to articulate and communicate, and we want to get feedback on it from Ethereum’s perspective (hence this post). The first full implementation of Anoma (in Elixir!) is also in the works, and will be open source soon.\nWhat does a “universal intent machine” mean for Ethereum?\nAt a high level, three things:\nUsing Anoma, developers can write applications in terms of intents and distributed intent machines, instead of transactions and specific state machines. These intents can request particular network actors to perform roles such as computational search, data storage, intent matching, and transaction ordering, and they can disclose specific information to selected parties chosen either initially by the user authoring the intent, or programmatically during the intent matching and settlement process.\nAnoma provides a universal standard for intent and application formats which does not constrain what kinds of intents can be expressed (beyond the fundamental constraints - e.g. intents must be computable functions), but allows for state, network, and application interoperability. Broadly, Anoma standardizes what it takes to verify that an intent has been satisfied - but not how the solution is computed.\nThese intents and applications written with Anoma can be ordered, solved, and settled anywhere - on the Ethereum main chain, on EVM and non-EVM rollups, on Eigenlayer AVSs, on Cosmos chains, Solana, etc. - anywhere an Anoma protocol adapter (which the post will cover in more detail later) is deployed.\nThe rest of this post will expand on the details here - but first, I want to motivate why we think this kind of universal intent machine standard may be compelling.\nWhy use Anoma?\nIn my view, a universal intent machine standard has three key benefits: intent-level and intent machine composability, application portability, and the availability of permissionless intent infrastructure. I’ll expect upon these aspects further in what follows, but in brief:\nIntent-level composability allows applications to compose interactions at the intent level, not just the transaction level. This unifies liquidity (as much as possible given heterogeneous preferences) and allows users to precisely select which decisions they would like to make themselves, which decisions they would like to delegate to specific network operators, and what constraints they would like to be enforced on those delegated decisions.\nIntent machine composability allows users to treat a distributed system of many computers, chains, and networks, as if it were a single intent machine, while that intent machine is internally composed of many smaller or special-purpose intent machines networked together.\nApplication portability allows applications to move freely across concurrency and security domains without additional development work or protocol incompatibility barriers. Applications written for Anoma can treat the entire Anoma network as a virtualized state space - they do not need to be deployed separately to different chains, integrate different bridges, or pick any specific security model.\nPermissionless intent infrastructure allows applications to make use of existing nodes and chains running the Anoma protocol. Of course, what nodes choose to do is up to them - but a standardized protocol allows application developers to think only about their application, make use of services provided by existing node operators, and not spend any time building complicated, custom off-chain infrastructure - and it allows node operators to focus purely on the services they want to provide and the conditions under which they want to provide them, without needing to care (or even know) which applications are using those services.\nNow, I’d like to hope that maybe you’re interested (or, if not, you probably wouldn’t be reading this line of text anyways). What do intent-centric applications actually look like?\nFor more information on what Anoma means by the word “intent”, see this blog post.\nI’m going to split this section into two parts: architecture and topology. Let me first define what I mean by those concepts. The architecture of a protocol is a mathematical specification of what the protocol is and does - typically, what (complex) pattern of messages will be sent in response to (complex) patterns of messages received. The topology of a network is the specific structure of connections - in the case of intent-based systems, connections induced by user choices of what intents to craft and where to send them. I find drawing a clear line between architecture and topology very helpful in designing and analyzing distributed, networked systems, for three reasons:\nThe architecture and topology are always cleanly separable. Particular identities or connections can only be configuration parameters of a particular protocol - one could always copy the protocol (in design or in code), and change the particular parameters of who to connect to - thus keeping the same architecture but picking an arbitrarily different topology. More limited architectures may support only a certain subset of topologies, or may provide certain guarantees only for a certain subset of topologies - but the particular topology is always a matter of runtime configuration.\nThe architecture and topology are chosen by different parties - the architecture of a particular protocol is chosen by whoever designed the protocol, while the topology of a network using that protocol is chosen (in a distributed fashion) by the users of that network making choices. Wearing my protocol designer hat, I consider it my responsibility to clearly define the architecture - but I have neither desire nor ability to influence the topology, which is a function of decisions made by network participants, themselves often influenced by cryptoeconomic parameters.\nAlthough the protocol designers may be subject to incentives of their own, the architecture of a particular protocol is fixed - in a sense, it is what defines the protocol - so one can easily analyze a particular architecture as a discrete, static mathematical object. The topology of a live network, however, changes all the time, and is subject to incentives both inside and outside the domain of what is legible to the protocol. In other words, architectural definition precedes topological analysis (using game theory, mechanism design, or similar). In order to understand how participants might use a language, one must first define the semantics which that language can express.\nArchitecture\nLet’s start with Anoma’s architecture. What kind of architecture does Anoma have, and what can this architecture provide? In this section, I will define the architecture in two levels: the affordances which Anoma’s architecture offers to applications, and the mechanisms with which Anoma provides these affordances. I will then detail a few example applications which I’m particularly excited about.\nNote: I have tried to select a level of abstraction which will provide a solid intuition for how Anoma works and why without getting lost in extraneous implementation details - but I certainly will not have selected optimally for all audiences (or even any), so please let me know where you would like more detail and which parts don’t make sense.\nI’ve borrowed the word “affordance” from cognitive psychology - in this context it means, simply, what application developers can do with Anoma - what capabilities of application design and instantiation Anoma offers that they didn’t have before. In contrast to, for example, the perspective of the protocol, affordances describe the perspective of the application developer. To applications, Anoma provides four key affordances: permissionless intent infrastructure, intent-level composability, information flow control, and heterogeneous trust. I will detail each of these in turn.\nPermissionless intent infrastructure means that - once Anoma is live - developers will be able to write complex intent-centric applications, which might need solvers, various intent pools, and multiple consensi, and deploy them to the Anoma network directly without needing to develop or operate any bespoke off-chain infrastructure of their own. The word “permissionless” can be a bit misleading - in any multi-user interaction, you’re interacting with someone, and they could always choose not to interact with you - but here I mean simply that Anoma provides intent infrastructure with no specific party (or parties) whose permission you must seek in order to use it.\nThis is possible because Anoma nodes are topology-agnostic. Depending on the operator’s configuration and the intents received, they can act as consensus nodes, storage providers, solvers, gossip forwarders, or any other role - not just for one chain but for any number. For Anoma, topology is a matter of runtime configuration - often even negotiated over the network. What consensi users want shifts as demand shifts for particular applications and atomicity between particular partitions of state, and consensus providers must themselves shift to meet this demand.\nOne concern with intent systems raised by, among others, Paradigm and Flashbots, is that intent systems could lead to centralization if specific parts of the intent lifecycle are performed by designated trusted parties (who would then have an undue influence). On the other hand, Chitra, Kulkarni, Pai, and Diamandis recently showed that competitive solver markets could drive most solvers out and lead to oligopoly. Permissionless intent infrastructure does not change the mechanism design landscape, but it does remove all protocol barriers to decentralization and mechanism experimentation. The likelihood of designated trusted infrastructure or oligopoly is much lower when intents are built into the core protocol and nodes can be spun up on demand, and tracking intent satisfaction in the protocol allows for users to much more easily automatically switch between providers (or credibly threaten to) when their interests aren’t being sufficiently optimized for.\nIntent-level composability means that application interactions can be composed at the intent level instead of at the transaction level. If applications use intents, but can be composed only with transactions, liquidity is fragmented between applications, and users cannot access the entire intent pool without writing an application intent aggregation interface on top of all the applications which provide liquidity of a relevant nature. Intent-level composability unifies this intent liquidity pool, such that users’ intents can be composed even with intents from another application the user has never heard of and never needs to think about - arbitrary composition is possible as long as the criteria in the user’s intent are satisfied.\nIntent-level composability also opens up efficiency improvements possible only when users can articulate precisely the nature and granularity of their preferences, such that their intents can be more flexibly composed - and more surplus returned back - than if the user had articulated more specific preferences than they actually have. Suppose that I want to swap USDC for ETH, and I’m happy to receive ETH on either the main chain, Optimism, or zkSync. With intent-level composability, my intent can be matched with the best offer available to settle on any of those chains - or perhaps even partially settled in multiple places, if I’m willing to accept the promise of a staked solver with liquidity in multiple places.\nInformation flow control means that developers of Anoma applications - and users of these applications - can reason precisely about what information actions taken in their application disclose to whom. Anoma provides information flow control at three distinct levels of the system:\nState-level information flow control describes what can be seen by whom after transaction creation and execution. For example, a shielded transaction (as in Zcash) only reveals specific state changes and a proof that they satisfied required invariants, while a transparent transaction reveals all involved data to all observers.\nIntent-level information flow control describes what can be seen by whom during intent solving, intent composition, and transaction creation. For example, a user may elect to disclose certain information to well-known solvers in order to help those solvers find valid matches quickly, but elect not to disclose that information to other solvers which they do not know.\nNetwork-level information flow control describes what metadata going around the network can be seen by whom. For example, a user may elect to disclose certain physical network addresses and transport options - the Bluetooth ID of a phone, for example - only to a few well-known friends, lest the information revealed by the address render the user vulnerable to denial-of-service or deanonymization.\nInformation flow control is a declarative specification of what should be disclosed to whom under which conditions. Desired information flow control properties constrain, but do not fix, the choice of specific cryptographic primitives such as regular encryption, succinct zero-knowledge proofs, or partially or fully homomorphic encryption - as long as the primitives chosen preserve the desired properties, the choice can be made on the basis of implementation availability, computational efficiency, and cryptographic interoperability. This is not a new concept - it is pretty well-covered in existing computer science research literature. In particular, the framework used by Anoma has been inspired and informed by the Viaduct paper, although Anoma differs in providing a dynamic runtime instead of a one-shot compiler.\nHeterogeneous trust means that Anoma application developers and users can make their own assumptions about the behavior of other participants on the network and their own choices about who to entrust with specific service-provisioning roles in the operation of their application, while reasoning about these assumptions and choices explicitly and detecting when their assumptions do not match their observations. In particular, most applications need three basic services: reliable ordering of intents and transactions related to the application, reliable storage of application data, and efficient compute of new application states and temporal statistics. Anoma makes all three of these services programmable:\nUsing programmable ordering, users and developers can choose who will order their intents and transactions. These choices may be made independently for each intent, and conditionally delegated to third parties. For example, a user may request their intent to be ordered by A or B, and delegate the choice of whether it is in fact ordered by A or B to a solver, to be made on the basis of relative price.\nUsing programmable storage, users and developers can choose who will store the data needed by their application. These choices may be made independently for each intent and each piece of data, conditionally delegated to third parties, and changed over time. For example, an application may choose to store recently-used data with the same nodes who will order its transactions, but move stale data to a cheaper long-term storage provider.\nUsing programmable compute, users and developers can choose who will perform the computation required for their intents, transactions, and ongoing application usage. For example, an application may choose to pay low-latency solver nodes for the compute required to match intents, but pay higher-latency but cheaper-per-FLOP server farms for long-term statistical indexing.\nHeterogeneous trust is also a pre-existing concept in the academic research literature, from which we’ve been lucky to draw many ideas, such as Heterogeneous Paxos. A lot of study still remains here, however - and I’m hopeful that blockchain projects (who desperately need this research) can step up a bit more to help organize and fund it.\nHow is all of this implemented? This post is already long enough without fully specifying exactly how Anoma works, but I will detail three key mechanisms here: the resource machine, which implements intents, the heterogeneous trust node architecture, which supports many networks with one piece of node software, and languages for explicit service commitments, which match user requests and operator offers for ordering, compute, and storage services.\nThe Anoma resource machine implements intents without loss of generality. In relation to the affordances above, the resource machine provides state-level and intent-level information flow control, intent-level composability, and the programming framework for ordering, storage, and compute services required for heterogeneous trust. Heterogeneous trust also requires cross-domain state synchronization and verification, which the resource machine’s state architecture is designed to make simple and efficient.\nNote: I’ve chosen to simplify some of the definitions here for conceptual legibility of the key intuitions. For full details, please see the resource machine ART report.\nThe basic concept of the resource machine is to organize state around resources. Resources are immutable: they can only be created and consumed exactly once. The current state of the system can be defined as the set of resources which have been created but not yet consumed. Each resource has an associated predicate called a resource logic that specifies the conditions under which the resource can be created and consumed. These conditions could include, for example, the creation or consumption of other resources, data such as a signature over a specific payload, or a proof that state elsewhere in the system satisfies a certain property. A transaction is considered valid only if the logics associated with all resources created and consumed in the transaction are satisfied, the transaction balances (enforcing e.g. linearity of currency), and no resources consumed in the transaction have been consumed before (no double-spends).\nResource logic examples2001×1072 254 KB\nDiagram credit Yulia Khalniyazova\nA resource is a tuple R = (l, label, q, v, nonce) where:\nResource = \\mathbb{F}_{l} \\times \\mathbb{F}_{label} \\times \\mathbb{F}_Q \\times \\mathbb{F}_{v} \\times \\mathbb{F}_{nonce}\nl: \\mathbb{F}_{l} is a succinct representation of the predicate associated with the resource (resource logic)\nlabel: \\mathbb{F}_{label} specifies the fungibility domain for the resource\nq: \\mathbb{F}_Q is an number representing the quantity of the resource\nv: \\mathbb{F}_{v} is the fungible data of the resource (data which does not affect fungibility)\nnonce: \\mathbb{F}_{nonce} guarantees the uniqueness of the resource computable components\nFrom a resource r may be computed:\nr.kind = h_{kind}(r.l, r.label)\nr.\\Delta = h_{\\Delta}(r.kind, r.q)\nA transaction is a composite structure TX = (rts, cms, nfs, \\Pi, \\Delta, extra), where:\nrts \\subseteq \\mathbb{F}_{rt} is a set of roots of the commitment tree\ncms \\subseteq  \\mathbb{F}_{cm} is a set of created resources’ commitments.\nnfs \\subseteq \\mathbb{F}_{nf} is a set of consumed resources’ nullifiers.\n\\Pi: \\{ \\pi: ProofRecord\\} is a set of proof records.\n\\Delta_{tx}: \\mathbb{F}_{\\Delta} is computed from \\Delta parameters of created and consumed resources. It represents the total delta change induced by the transaction.\nextra: \\{(k, d): k \\in \\mathbb{F}_{key}, d \\subseteq \\mathbb{F}_{d}\\} contains extra information requested by the logics of created and consumed resources\nA transaction is considered valid with respect to a previous state if and only if:\nrts contains valid commitment tree roots that are correct inputs for the membership proofs\nnfs contains valid nullifiers that correspond to consumed resources, and none of these nullifiers have been previously revealed\ninput resources have valid resource logic proofs and the compliance proofs associated with them\noutput resources have valid resource logic proofs and the compliance proofs associated with them\n\\Delta is computed correctly\nA transaction is considered balanced if and only if \\Delta = 0.\nApplying a transaction to the state simply entails adding new commitments and nullifiers to their respective Merkle tree and set. Transaction candidates - functions which generate transactions - are provided access to the current state and can look up resources by kind, which allows for post-ordering state-dependent updates without conflicts.\nCompared to the EVM (and other blockchain VMs such as the SVM and Move), the resource machine unbundles three aspects which these VMs intertwine: the state architecture, the instruction set, and the message-passing model. The EVM, for example, specifies:\nA state architecture where state is read and written in 256-bit blocks, partitioned by smart contract addresses, and encoded into a Merkle-Patricia Trie.\nAn instruction set for stack-based computations with a 256-bit native word type and an assortment of specially optimized Ethereum-related instructions (e.g. ecrecover).\nA message-passing model where one smart contract owns the execution frame at a time, and sending a message to another contract switches control of the execution frame to the message recipient.\nThe resource machine, by contrast, specifies only the state architecture. Different instruction sets can be chosen by different operators as long as valid state updates are ultimately produced, and messages can be passed in whatever fashion is most efficient in each particular case. This unbundling is necessary for native intent support, as with intents, much of the computation (expressed in an instruction set) and communication (expressed by a message-passing model) happens before final transaction execution and state change verification. With the resource machine, the instruction set can be chosen by whomever is performing the computation in question, and the message-passing model can simply reflect the actual intent-matching topology.\nP2P overlay diagram1215×735 49 KB\nDiagram credit Naqib Zarin\nAnoma’s heterogeneous trust node architecture weaves together the key ordering, storage, and compute functionalities needed to provide permissionless intent infrastructure into a single piece of node software. The base of the node stack is a generalized heterogeneous P2P network stack with support for network-level information flow control. The node architecture can be split into two components: the networking machine, which implements the heterogeneous P2P network stack and provides interfaces for storage and compute resource provisioning, and the ordering machine, which implements heterogeneous consensus (total ordering), heterogeneous mempools (partial ordering), and parallel transaction execution.\nThe networking machine consists of a set of communicating sub-processes which are responsible for message-passing between nodes. Anoma’s network architecture, based on P2P Overlay Domains with Sovereignty, assumes heterogeneous P2P preferences: different nodes want to broadcast and subscribe to different types of intents, participate in different domains of consensus, storage, and compute, and choose different bandwidth, latency, and resource usage tradeoffs in their network connection choices. The networking machine is designed to abstract this complexity and the ever-changing physical and overlay network topologies behind an interface which simply allows sending a message to any other node or topic in the network - the internal processes within the networking machine are responsible for using network metadata to figure out where it should go and how to route it there efficiently. Generic interfaces are also provided for provisioning and requesting storage and compute.\nThe ordering machine consists of a set of communicating sub-processes which are responsible for receiving transactions, partially ordering those transactions in the mempool, coming to consensus over a total order for each logical consensus clock, executing the transactions, updating the state accordingly, and sending updated state to the appropriate recipients. Both the mempool and consensus are heterogeneous - implementing Heterogeneous Narwhal and Heterogeneous Paxos, respectively - and the execution engine based on CalvinDB is capable of n-processor parallel scale-out.\nHeterogeneous chains are operationally expensive if you have to run a separate node for each one - so Anoma’s node architecture supports many networks with a single node. The operator simply configures which networks they’d like to participate in and the node software takes care of all the bookkeeping. Processes within the ordering machine, for example, can be spun up or spun down automatically based on which consensi the operator is participating in and how much throughput each needs at the moment.\nA full paper on the heterogeneous trust node architecture is still in the works, but the curious reader may be interested in more about multi-chain atomic commits or a brief introduction to Anoma’s P2P layer.\nLanguages for explicit service commitments provide a way for clients (requesting a service) and servers (providing a service) to automatically negotiate the terms of that service in a way which is legible to the network, so that records can be kept of services performed as promised - and promises broken. For example, service commitments could include\n“I will vote in consensus XYZ”\n“I will store your data D for T weeks”\n“I will spend N units of CPU time searching for an answer to problem P”\nGenerally, these commitments will be comprised of aspects classifiable into one of two categories: safety - roughly, promising not to send any messages which violate a certain invariant, and liveness - roughly, promising to send responses promptly when queried. Initially, I think three kinds of services are particularly important in the context of distributed systems: ordering services, storage services, and compute services. I will expand a bit upon each of these in turn.\nOrdering services are perhaps the most basic historical function of blockchains. Satoshi actually uses the term “timestamping server”. In order for multiple observers to agree on the history of a particular piece of state, they must agree on a particular party whose local order of observations will determine the order in which events (transactions) are applied to that state. This party must attest to the order in which it has received events, from which we can derive the desired liveness property: signing updates to the event graph (blocks of transactions). Consistency for other observers requires consistency of ordering attestations, from which we can derive the desired safety property: never signing two conflicting updates. Other properties of interest here could include censorship resistance - perhaps promising to construct blocks in a certain fashion (connecting directly to PEPC).\nStorage services are recently in the spotlight thanks to the concept of “data availability”, which illuminates an important nuance: it is not enough for observers that data be stored, it must be available - they must be able to retrieve it. In order to make data available, though, one must store it. I think storage services need only a liveness property: responding with the requested data, perhaps only within a certain span of time from the initial request. Thanks to content-addressing, the recipients of that data can quickly check that the correct bits were provided. Storage markets are very heterogeneous - different data needs to be retrieved by different parties, at different frequencies, and stored with different level of redundancy and distribution.\nCompute services are currently split across a few different areas: searching performed in the Ethereum MEV supply chain, indexing still typically performed by web2-style services, and historical statistics calculated on the backend by explorers or specialized providers. I think that these various specific services are variants of one basic unit: the provision of computational infrastructure to search for a solution satisfying a particular relation, which may include network history. Safety for compute means providing the correct result, and liveness means getting a result quickly and with high likelihood.\nWe’re still in the design process for these service commitment languages, and I see a lot of potential for collaboration. In particular, several folks in the Ethereum research community have been exploring relevant concepts: protocol-enforced proposer commitments, whereby Ethereum block proposers can make commitments which are enforced by the core protocol, and rainbow staking, where staking services are split into “heavy” and “light” categories (the names follow from expected node operator resource requirements). I expect that clear specification of various flavors of heavy and light services as described in the post, and negotiation of who is to perform those services, how they are to be compensated, and how other network actors can tell whether or not they are performing as promised will require some form of service commitment language, and PEPC would allow these commitments to be legible to and enforceable by the core protocol. Promise Theory may also provide helpful theoretical ammunition. I hope to develop a better understanding of the Ethereum ecosystem’s requirements here and see whether it is possible to develop a common standard.\nWhat kinds of applications can be built with these affordances and mechanisms? I’ll save a comprehensive survey for a separate discussion, but I want to list a few here which I’m particularly excited about.\nMultichat is a distributed chat network without any specially designated server operators. Like Slack or Discord, multichat supports permissioned channels in complex topologies. Like Signal, multichat encrypts messages and metadata. Unlike Signal, Slack, or Discord, multichat can run offline, or in an isolated physical subnetwork, using direct connections between chat participants. Unlike Slack or Discord, multichat cleanly separates protocol, operators, and interfaces, allowing different interfaces to be designed for different user needs while retaining protocol compatibility, and allowing users to choose which operators they want to trust with what roles.\nFor more details on this application concept, see this forum thread.\nPublic Signal is a double-sided version of Kickstarter implemented with intents. Kickstarter is a supply-side driven platform: a group or individual wishing to produce a particular good or service and in need of funding posts a description of their product or service and organizes a campaign to raise small donations/payments in order to cover their capital expenditure requirements and labor costs. Public Signal, by contrast, is demand-side driven: potential purchasers of a good or beneficiaries of a service describe their preferences and are gradually matched together, generating a demand signal which potential suppliers can use to decide what to produce. The demand-side approach is particularly appealing because it can generate an otherwise difficult-to-observe signal for public or hybrid goods.\nFor more details on this application concept, see the Public Signal blog post.\nScale-free money is a hypothetical monetary system design which allows anyone to create arbitrary denominations of money at any time for any purpose and provides the necessary infrastructure to unbundle the three key functions of money (store of value, means of exchange, and unit of account) and allow for the choice of what money to use to be made differently by different network participants without compromising their ability to enact voluntary economic interchange. To me, scale-free money is the natural synthesis of the experimental practice of decentralized cryptocurrencies, the anthropological record of heterogeneous credit, and the cybernetic approach to coordination systems design.\nFor more details on this application concept, see this blog post.\nPromise Graph, inspired in part by the Collaborative Web model of Galois, and Informal Systems’ Workflow, is a language and structured accounting logic for making and managing promises in order to effectively synchronize causally-interwoven workstreams across a distributed organization and demonstrate to customers that the organization can keep its promises. Heliax has been dogfooding the promise graph system internally for awhile now, but it is designed in particular to facilitate coordination irrespective of, and even without knowledge of, organizational boundaries. Similar to how Anoma splits the architecture of an intent-centric system from the particular topology users choose, Promise Graph splits the architecture of promise-based coordination from the particular topology of promises necessary to achieve a particular aim.\nFor more details on this application concept, see this description of the system.\nTopology\nNow, let’s talk topology. How does Anoma relate to the topology of the Ethereum network today? As a refresher, by topology, I mean specific network connections, specific security relations, and specific service providers performing specific network roles. My understanding is that the Ethereum network is still figuring out what topology it wants to have, as evidenced by the fervent discussions around shared sequencing for rollups/L2s, EigenLayer, and rainbow staking. I think the existence of such discussions is evidence of a healthy network - a network with a static topology is a network which can no longer adapt to changes in the world outside.\nThere are infinitely many possible topologies, and I don’t know of any finite framework to classify them - so here I will simply list a few topologies under discussion in the Ethereum ecosystem and explain at a high level how Anoma could be used with them. I will also discuss how Anoma could interface with specific specialized ordering, compute, and storage service providers which already exist, and how service commitments could be made using existing assets. Finally, I will detail two components which I believe sufficient to connect Anoma to any and all of these topologies.\nFor several years now, the Ethereum ecosystem has been following the rollup-centric roadmap, where different sequencers order transactions for different rollup chains, which then post proofs and some data to Ethereum. Most rollups use the EVM, although a few have developed distinct VMs with different properties designed for particular kinds of applications. Recent proposals have brought forth the idea that many rollups may wish to share a sequencer, which is subsequently referred to as a “shared sequencer”. The Ethereum main chain could in fact play this role of the shared sequencer.\nAnoma could be useful for rollups in two ways:\nThe resource machine could be used as a rollup execution environment instead of or in addition to the EVM, providing the rollup’s users with intent-level and transaction-level information flow control, intent-level composability, and parallel transaction execution.\nThe heterogeneous node architecture could be used to simply run the EVM, which would allow rollup operators to participate in a shared heterogeneous P2P network on which they could connect to other rollups, Ethereum main chain validators, searchers, and other intent infrastructure operators. Heterogeneous Paxos, in particular, may be interesting for rollups with frequent cross-traffic, because it generalizes shared sequencers.\nPlasma is the name for a family of Ethereum scaling solutions which require only posting deposit transactions, withdrawal transactions, and updated state Merkle roots to the main chain, while keeping all data and compute happening “within Plasma” off-chain. The basic concept assumes a block-producing operator who publishes state roots for each new block to the main chain and sends to individual users any state updates affecting them (e.g. tokens they’ve received). Should the operator misbehave, users can themselves publish proof of state they own to the main chain in order to withdraw it. In the original Plasma design, this forced withdrawal requires a 7-day challenge period, but the addition of validity proofs (where the operator proves that each new state root is the valid result of block execution) allows for withdrawals referring to the latest state root to be processed instantly. For a comprehensive summary of Plasma research and recent work, I recommend Vitalik’s blog post on the topic.\nAs discussed in that blog post, even with validity proofs, several challenges remain for Plasma:\nDependency tracking: The EVM does not attempt to limit or explicitly track state change dependencies, so (for example) ETH held in an account in block n + 1 could have come from anywhere block n. Exiting may then require publication of the entire history, which would incur prohibitive gas costs - and this worst-case exit game would likely entail limitations on the compute and storage bandwidth of Plasma, since it must be possible to replay everything on the main chain.\nIncentive-compatible generalization: Plasma works well with state objects which have a clear economic owner, such as fungible or non-fungible tokens, but it’s not clear how to adapt the incentive model to objects without a clear economic owner such as a CDP, which a user might want to pretend to have forgotten the data for if the price of ETH goes below the DAI they’ve withdrawn.\nDeveloper-facing complexity: Plasma application developers must reason about state ownership graphs and data storage/publication incentives, adding mental overhead and introducing whole new classes of potential bugs.\nWhat happens if we try to build a Plasma-like construction on top of the Resource Machine, instead of the EVM? Let’s call it Resource Plasma. In Resource Plasma, the operator executes transactions and publishes only the Merkle root of the commitment tree, the Merkle root of the nullifier set, and a proof of correct transaction execution to the main chain. Withdrawals function as follows:\nWithdrawals with a proof made against the latest nullifier set Merkle root can be processed immediately. They also reveal the nullifier of the resource being withdrawn (so that it can no longer be withdrawn again).\nWithdrawals against older Merkle roots require a delay period. When the withdrawal is requested, the Plasma contract starts a “double spend” challenge game, where anyone can submit a proof that the nullifier for the resource being withdrawn was included in a later nullifier set than the Merkle root used for the withdrawal request. After the challenge period, if no conflicting nullifier has been found, the withdrawal is processed.\nThanks to the dual commitment-nullifier system used by the resource machine, client data storage requirements are minimal - the client need only store their resources, proofs that the commitments corresponding to those resources were included in the commitment Merkle tree root, and proofs that the nullifiers corresponding to those resources were not revealed at whatever height the client last synced at. Clients wanting to withdraw instantly must update these latter proofs, but have the option to exit if the operator withholds data (in this case, the latest nullifier set). Resource Plasma thus removes the need for separate dependency tracking - clients need not track dependencies, only the latest state relevant to them and proofs associated with it. Developer-facing complexity is also reduced because this state architecture, including commitments and nullifiers, is built into the resource machine and requires no additional effort on the part of developers.\nIncentive-compatible generalization is a trickier beast. One option is to have each resource designate a particular highly-available party whose signature over an attestation that they have stored the resource data is required in order to construct a valid transaction. For example, in the CDP example, perhaps it would be suitable for a quorum of operators elected by MKR holders to store data associated with all CDPs. If the CDP is to be liquidated and the owner pretends not to have the data, this quorum could then reveal the data, allowing for the liquidation of the CDP. MKR holders have an incentive to store and publish this data in order to keep the system on which their value flows depend credible. In a version of MakerDAO without governance, it is less clear what to do in this scenario. Perhaps the data could also be sent to a party who might want to liquidate the CDP, or more generally one with the opposite economic incentive. Perhaps systems such as Maker really do need a designated party (which could be a dynamic, distributed operator set) whose job it is to store the data. The resource machine makes it easy to designate who should store what state, but it doesn’t change the game design problem.\nResource Plasma is no panacea. Application developers must still reason about storage handoff, and users who want to be able to receive payments while offline must pick a highly available operator who they trust to temporarily store data on their behalf (or pay the main chain itself to do so). These limitations, however, are fundamental - they could not be alleviated by a different virtual machine - and other scaling solutions such as L2s or rollups will be faced with the same constraints. This is just a sketch of stirring the Plasma and Resource Machine models together, and seeing what emerges from the resulting conceptual melange. I think there may be some compelling options here, and I plan to explore this design space further in a separate post.\nMany blockchains today - including, typically, those who self-identify as data availability layers, in the modular conceptual framework - provide both ordering and storage services. Ordering services are mostly distinguished by the validator set and consensus mechanism, the combination of which determine latency, costs, and security. Storage services are often distinguished by how long the storage is to be provided for - blob storage on Ethereum, Celestia, and EigenDA, for example, is short-term, while file storage on Filecoin or Arweave is (at least nominally) permanent. Storage services are also distinguished by operators, redundancy, and optimization for specific patterns of retrieval. Many applications will likely want to use multiple storage providers with different specialties with different purposes. Compute services today are mostly provided by off-chain actors, as it is not typically necessary to replicate compute (since the results can be verified). I understand Ethereum ecosystem searchers/builders, indexers, and statistical data providers to all be providing compute services of various specialized flavors.\nEigenLayer provides a set of smart contracts and coordination infrastructure with which Ethereum validators can make commitments to operate additional services (AVSs), and “restake” their existing ETH staking bonds to provide security for those services (in the sense that these bonds can be slashed if the services are not performed as promised). The services performed by these validators could include ordering, storage, and compute as described here.\nAnoma’s service commitment languages can help both users and operators - the demand and supply sides of this service provisioning market - detail what services they are willing to provide, navigate through the services on offer, bargain for a mutually acceptable price, detect if service commitment properties have been violated, and publish evidence of defection in a standard way that can be embedded into network history and preserved as a reputation signal for future participants.\nWhat would be required to support these varied possible topologies? Just two main ingredients, I think: an Anoma protocol adapter and an Anoma node sidecar. I will describe each of these in turn.\nThe Anoma protocol adapter is a smart contract or set of smart contracts - primarily written for the EVM, but variants could be written for other VMs or execution environments - which emulate the resource machine and thus allow for execution of Anoma-formatted transactions, synchronization of Anoma-formatted state, and verification of state changes executed elsewhere on the Anoma network.\nThere may be some efficiency loss in execution emulation, but there shouldn’t be too much: since the resource machine does not fix a particular instruction set or message-passing architecture, these transactions can simply use EVM bytecode to compute new resource values and the EVM’s message-passing architecture to pass messages between different contracts required for the computation. Verification with succinct ZKPs is constant-cost anyways, so no problems there. Should the Anoma application model prove safe and popular, the EVM could easily enshrine it with a few precompiles for native performance.\nSlight aside: solvers will get to have some fun in this model in a pro-public-good way: now they get to compete on optimizing JIT EVM compilers, the software created for which could probably aid other languages and systems trying to target the EVM.\nThe Anoma node sidecar is a process which would run alongside and communicate bidirectionally with Ethereum execution & consensus clients, rollup sequencer processes, solver/searcher algorithms, etc. The node sidecar process runs all the Anoma network protocols, allowing nodes running the sidecar to connect to other Anoma nodes, subscribe to intents which are of interest to them, and solve, order, and execute as desired. This sidecar would be fully opt-in and configurable per the node operator’s preferences in terms of who they want to connect to (or not), what services they want to offer, what intents they want to receive (or even send), and how they want to process or forward them. If the node operator solves two intents to create an EVM transaction with the protocol adapter, they can submit it directly to their local web3 HTTP API - minimum latency, maximum throughput!\nWhat would this intent dataflow look like visually? Something like this:\nIntent dataflow1290×907 147 KB\nDiagram credit @0xapriori\nChallenges and trade-offs\nNothing new is without challenges or trade-offs. I don’t want to paint with too rosy a brush here - plenty remains to be figured out, and plenty could go wrong. I see three main challenges (and I’m sure there are many which I miss):\nImplementation risk: Anoma, although mathematically characterized at a high-level, is not yet fully implemented. We’re working to reduce uncertainty and risk here by conducting and publishing more research, writing a formal model of the protocol stack in Isabelle/HOL, and developing an initial node implementation in Elixir (to be released soon) - but there could still be mistakes in our current understanding, and there are plenty of details we need to get right (which will always be the case with any new architecture)\nHardware requirements: compared to just running a vanilla Ethereum full node, running an Ethereum full node plus the Anoma sidecar will increase resource usage somewhat. Different node operators can pick and choose which kinds of extra messages they want to receive and process, which should help, but perhaps full node operators and solo stakers are already strained to the limit. An interesting option could be to enmesh the two protocols more closely together in order to support more points in the spectrum in between light nodes and full nodes (related to the rainbow staking concept).\nDeveloper education: writing applications for Anoma is very different not only from writing applications for the EVM or other blockchains, but even from writing applications in imperative, computation-ordering-oriented languages at all - in a certain sense, it’s more akin to a kind of declarative object-oriented programming, where you define which kinds of objects exist (resource kinds), how they can change (resource logics), and what actions users can take with them (application actions). Personally, I find this model much easier to work with after getting used to it - it provides a lot of expressive power - but it will take developers some time to learn.\nRequest for comment\nThank you for reading this far! These are my initial thoughts, and I’d love to know what you think. I have four specific questions - or respond with anything you like.\nOf what I describe here, what makes sense to you? What doesn’t make sense?\nWhat challenges, risks, or trade-offs do you see that I didn’t cover?\nWhat do you think would be most valuable to the Ethereum community for us to focus on?\nWhat would you - or a team or project you know - like to collaborate on? Who should we talk to?\nCheers!\n",
        "category": [
            "Architecture"
        ],
        "discourse": []
    },
    {
        "title": "What happened to our decentralized private new internet?",
        "link": "https://ethresear.ch/t/what-happened-to-our-decentralized-private-new-internet/20657",
        "article": "The Selective Disclosure/Compliance Challenge in Web3\n*By Peyman Momeni, Fairblock; Amit Chaudhary, Labyrinth; Muhammad Yusuf, Delphi Digital\nWhy?\nToday, after 16 years, blockchains’ utility is mostly limited to meme coins and circular infrastructure projects. Yes it’s fun, yes a lot of people made millions, but are we still building the new internet? What happened to the idea of our decentralized private new internet?\nThe most obvious application – transfers– hasn’t yet been fully realised even for basic scenarios. Most businesses can’t use it. They can’t share their payroll with everyone. Vitalik is getting bullied for his onchain donations to science. Businesses don’t want to share their confidential strategies and data publicly, users don’t like the choice between a centralized service or getting sandwiched maximally and begging for a change from middlemen mercenaries.\nWhile the machine is running, we, as a collective, cannot continue this path for another 16 years.\nWith privacy should come confidentiality to secure your onchain actions, allowing for more useful and impactful applications to be built and for users to reap the benefits of a more expressive blockchain experience. Confidentiality is a standard across Web2, it’s imperative that it become a standard across Web3.\n1078×386 50.8 KB1176×536 55 KB1192×678 84.1 KB\nThe lack of onchain confidentiality has hindered the growth and adoption of even the most obvious applications. Confidentiality is one of the most misunderstood terms in crypto. On one end of the spectrum it is associated with money laundering and illegal financial activities and on the other, it unlocks high utility use cases such as normal private transfers, dark pools, frontrunning protection, confidential AI, zkTLS, gaming, healthcare and private governance.\nEconomies function efficiently when there is a balance between confidentiality and transparency. Take financial markets as an example - confidentiality makes information valuable and tradable, but selective disclosure of that information would be helpful in preventing [market abuse]. Many financial activities such as portfolio management, asset trading, payments, and banking require confidentiality with a need to balance data disclosure for compliance and regulation purposes. However, the phenomenon of selective disclosure is not limited to finance and compliance. A recent example is the confidential social media theme, which is currently battling misinformation and hate content, necessitating self-regulation by social media giants through disclosures).\nRecent challenges in fake content generation through AI models have raised questions about the value of sharing secrets and the on-demand disclosure of secrets. During the COVID pandemic, vaccine research raised controversy because important stakeholders were kept in the dark about the detailed results. Balancing confidentiality and transparency takes on many shapes, and sometimes regulation and compliance make this the most important problem to tackle. Technology comes to the rescue in finding the balance - the key question being asked is whether we can remove the centralized party in selective disclosure or compliance use cases.\nWe have confidentiality inside our walls, 95% of the internet, iPhones, bank accounts, elections and even a friendly poker game. Just the other day US slapped TD - the largest bank in Canada- with $3B fine over cartel money laundering. But no one is going to avoid banks, no one is scared of privacy in other banks or industries, and even TD itself is not going down. An impactful system shouldn’t be vaporized and banned because of a few bad actors. In web3, we shouldn’t overreact to a few bad examples and myths that we’ve seen. We shouldn’t shy away from building the new internet and turn it into short-term distractions. In most cases, we don’t even have a compliance problem. For private transfers, we can build systems that are as compliant as real-world banks, but still more transparent, private and decentralized. More impactful problems are harder to solve, that’s the way it is.\nEven if we only care about money, we can’t extract value from memecoins for another 16 years. Now that we have the scalable infrastructure, opportunities are going to be orders of magnitude greater if we have onchain confidentiality and real applications.\nThe Elephant in the room\nOne of the most pressing challenges of DeFi is the balance between confidentiality and compliance. Maintaining user privacy while ensuring regulatory oversight without centralization requires a delicate approach. This article explores the solution through selective disclosure, enabling privacy and accountability without compromising security or compliance.\nSo far in web3, we’ve figured out private transfers and know how to transfer and trade assets privately by proving the validity of transactions without leaking our private identities. The open-debated technical and philosophical challenge is how we can make sure that the technology is not used by minority of bad actors at the expense of the majority of active users, how can we have at least the same level of privacy as our current banks?\nWhile the private transfers themselves are enabled by ZKPs, different centralized or decentralized techniques and cryptographic schemes such as MPC can be used for compliance. Some of the current efforts for making compliant private transfers are:\n\nPre-transfer proof of legitimate funds (0xbow/Privacy Pools/Railgun): Users can prove non-association with lists of illicit activities or sanctioned addresses before execution of their transfers.\n\n\nPost-transfer selective de-anonymization: Balancing blockchain privacy and regulatory compliance by providing accountability using zk and threshold cryptography\n\n\nDID and regulatory smart contracts: Programming real-world rules such as the 10K limit, and other conditions by privately sharing information using decentralized identifiers and MPC/FHE.\n\n\nID verification: Users should engage with non-private and centralized long and haphazard processes of KYC for each of the services they are using.\n\n\nGeography-specific private compliance: It allows Virtual Asset Service Providers (VASPs) to set up their “zones” with custom KYC/B, allow lists, and transaction limits in accordance with their local laws. Additionally, MPC can be used to add multiple VASPs to govern a zone instead of it being managed by a single entity.\n\nPre-transfer proof of legitimate funds (0xbow/Privacy Pools/Railgun): Users can prove non-association with lists of illicit activities or sanctioned addresses before execution of their transfers.\nPost-transfer selective de-anonymization: Balancing blockchain privacy and regulatory compliance by providing accountability using zk and threshold cryptography\nDID and regulatory smart contracts: Programming real-world rules such as the 10K limit, and other conditions by privately sharing information using decentralized identifiers and MPC/FHE.\nID verification: Users should engage with non-private and centralized long and haphazard processes of KYC for each of the services they are using.\nGeography-specific private compliance: It allows Virtual Asset Service Providers (VASPs) to set up their “zones” with custom KYC/B, allow lists, and transaction limits in accordance with their local laws. Additionally, MPC can be used to add multiple VASPs to govern a zone instead of it being managed by a single entity.\nHowever, none of these approaches are complete by themselves as they fail to address the balance between privacy and regulations. Deposit limits aim to block illicit funds but often result in inconvenience to legitimate users. Sanction lists are slow to update, allowing bad actors to operate before detection, and there’s no recovery for wrongly flagged addresses. Blockchain analysis tools such as Chainalysis, miss illicit activities due to false negatives. “View-only” access relies on user cooperation, failing against malicious actors. The association sets in privacy pools delay the detection of illicit transactions and rely on untrusted set providers. KYC compromises privacy by forcing users to disclose sensitive information on the first step of using privacy applications, without solving the problem of users turning malicious later. Ultimately, these approaches rely on centralized controls, undermining the decentralized nature of Web3.\n\nCo-existence of Privacy and Compliance through Decentralized Approaches\nThe answer to balancing privacy and compliance lies in a decentralized compliance framework. This approach allows compliance to coexist with privacy by creating systems where compliance measures can be enforced without compromising user anonymity.\nThere needs to be different levels of decentralized pre-transfer compliance and case-by-case post-execution audibility through selective disclosure. This way, we still achieve common sense privacy for DeFi while allowing authorities to request more information on a rare case-by-case basis. At the very least, this offers an equivalent level of web2 and tradfi privacy with more decentralization and transparency properties.\nDark pools in traditional finance enable trader anonymity while ensuring regulatory post-trade transparency. Recently dark pools and privacy-focused blockchain protocols such as Railgun, Penumbra, and Renegade are gaining attention. However, they’re either non-compliant or only partially compliant. Selective disclosures can address these issues by ensuring that users’ actions are legitimate while preserving anonymity where appropriate. While users can use a mix of methods to prove their legitimate source of funds and identities, threshold networks can ensure post-transaction accountability.\nPost-transaction accountability through MPC/Threshold decryption\nIn a threshold network, compliance and accountability are enforced without relying on central authorities. The system is based on independent entities such as Revokers and Guardians:\nAccountable Privacy means that users must engage in legitimate activities. Malicious behavior can lead to selective de-anonymization, but only under lawful conditions, ensuring integrity without compromising user privacy unjustly.\nAccountable De-Anonymization ensures that de-anonymization requests are public and traceable, requiring cooperation between Revokers and Guardians, thus preventing unauthorized disclosure.\nNon-fabrication guarantees that honest users cannot be falsely accused, even if there is collusion. The cryptographic commitments ensure all participants are bound to act transparently, safeguarding user rights.\nHere’s a detailed end-to-end flow explaining how transactions are managed, de-anonymization is requested, and the process is carried out in an accountable and publicly verifiable way by users, revokers, and guardians:\nUser Transaction (Onchain)\nUsers are accountable for doing compliant transactions. Users face de-anonymization if they act maliciously. Misbehavior leads to loss of privacy, but only under lawful conditions. A user initiates an onchain private transaction on the protocol, and encrypted data is included in the transaction payload. This ensures that all transaction details remain private and secure on-chain, preventing unauthorized access to the user’s data.\nWhen creating a valid transaction, a user is constrained to encrypt transaction details (e.g. asset id, value, owner) using a specific encryption key and needs to provide a ZK proof for the same in the transaction payload. Otherwise, onchain ZKP verifier rejects the proof & transaction reverts as a result.\nSuspicious Activity Detection (Off-chain)\nA Revoker such as a DAO, trusted entity, or neutral gatekeeper, monitors the transaction off-chain, which uses monitoring tools to detect any potential illicit activity or suspicious behavior. The Revoker flags the user’s transaction if it appears to violate compliance rules or triggers suspicious activity alerts.\nDe-Anonymization Request Submission (Onchain)\nOnce the Revoker identifies a suspicious activity, they submit an onchain de-anonymization request on the governance dashboard. This request initiates the de-anonymization process and makes the request publicly verifiable and transparent to all the network participants. The Revoker does not have de-anonymization rights at this stage but is merely flagging the transaction for further review.\nGuardian Review and Voting (Off-chain)\nThe request is picked up by a decentralized network of Guardians (trusted entities picked up through the governance process). These Guardians act as decision-makers and are responsible for validating the Revoker’s de-anonymization request. They assess the flagged transaction according to governance policies and determine whether de-anonymization should be allowed. This review process occurs off-chain to ensure the privacy of decision-making and governance.\nThreshold Mechanism (Onchain)\nFor the de-anonymization request to proceed, a certain threshold of Guardian approvals must be met (e.g., 6 out of 10 Guardians need to approve). Each Guardian that votes in favor of de-anonymization submits their cryptographic permission onchain, which is aggregated to reach the required threshold. This on-chain submission guarantees transparency and prevents any foul play or unauthorized actions.\nDe-Anonymization Execution (Off-chain)\nOnce the necessary cryptographic permissions have been granted, the Revoker can decrypt the flagged transaction. This process happens off-chain, and only the specific transaction under investigation is revealed to the Revoker—no other data or transactions are affected or exposed. Importantly, even the Guardians who approved the request do not gain access to the transaction details; only the Revoker can view the decrypted transaction information.\nPost-De-Anonymization (Onchain)\nIf further suspicious activity is linked to the decrypted transaction, it will be flagged separately, requiring a new de-anonymization request to be submitted by the revoker and approved by the Guardians. The rest of the user’s transaction history and data remain encrypted and private. This ensures that privacy is maintained for non-flagged transactions while enabling compliant de-anonymization for suspicious activities.\n1600×831 119 KB\nSecurity\nThere’s a trust assumption in the threshold network. A dishonest majority of malicious validators can work together to decrypt transactions - with or without detection depending on the scheme.\nIt is worth mentioning that the consequence of such an attack is losing confidentiality, and neither the safety of the network, loss of funds nor private information regarding the identities of users. In this case, the system’s confidentiality will downgrade to the current state of public blockchains. The consequence is more limited in the cases where only ephemeral confidentiality is required or confidentiality is leveraged for better execution quality, not the privacy of users e.g. frontrunning protection, and sealed-bid auctions.\nHowever, validators and operators should be incentivized to protect user privacy with respect to the stakes in the game. The solution lies in building robust networks where compliance can be enforced without compromising decentralization. The transition will involve integrating permissionless compliance mechanisms, where incentives are aligned to encourage honest validator behavior. Approaches like Proof of Stake (PoS) and AVS ensure network security, while cryptographic traitor tracing and slashing mechanisms deter malicious actors. There are many promising recent works such as Multimodal Cryptography - Accountable MPC + TEE - HackMD\nThreshold MPC and confidentiality beyond compliant transfers\nThe use of Threshold MPC extends beyond compliance, finding applications across multiple confidentiality sectors:\nFrontrunning protection and MEV Protection: Preventing manipulative trading practices by hiding transaction data until completion. Replacing centralized relayers in Ethereum’s MEV supply chain. Leaderless and incentive-aligned MEV or preconfirmation auctions.\nPvP GameFi or prediction markets: Ensuring fairness and excitement by concealing actions until necessary. Adding the element of onchain surprise by decrypting values. Decentralize decryption of oracle updates in prediction markets (instead of naive hashes as in UMA)\nPrivate Governance/Voting: Prevention of manipulation during decision-making processes, coercion resistance, and privacy of voters.\nAccess Control and SocialFi: Enhancing privacy in decentralized applications while retaining usability and accountability as well monetization of contents in creator economies.\nConfidential AI: Decentralized and privacy-preserving systems for training, inference and data sharing to unlock access to more players and data and not being limited because of privacy concerns.\nHealthcare: Access to more and better personalized healthcare services by analysis of biological or health data without loss of privacy or trusting centralized parties.\nPath Forward: Seamless Web3 Confidentiality\nThe ultimate goal for privacy in Web3 is to make it as seamless as it is in Web2. Encryption in traditional internet applications (e.g., HTTP transitioning to HTTPS) has become so common that users hardly notice it. A similar evolution is required for Web3—Confidentiality should be invisible to the user, seamlessly integrated into their experience. While most confidentiality schemes don’t have compliance challenges in the first place, private transfers can be compliant through multimodal cryptography techniques such as MPC and ZKPs.\n",
        "category": [
            "Privacy"
        ],
        "discourse": [
            "mev"
        ]
    },
    {
        "title": "Concerning Ethereum's Fee Issues",
        "link": "https://ethresear.ch/t/concerning-ethereums-fee-issues/21070",
        "article": "I have been looking at the Ethereum prices for gas costs (transaction fees?).\nGas tracking resources  — https://etherscan.io/gastracker\nI noticed that simply sending a transaction is actually decently cheap, about under $1.\nWhat’s seems to be costly for making transactions is smart contract calls.\nMy assumption is that, the more operations a smart contract runs to execute its task the costlier it’s is in terms of fees.\nIt is said that swapping, costs about $16 per contract call.\nI am assuming that this is expensive because the swap is probably made via an automated market maker like uniswap. (i.e lots of code to run)\nHere is my idea.\nWhat if I am to strip down the concept of swapping a coin in to a very simple smart contract.\nBy simple, I mean;\nNo order books\nNo market making\nHigher dimensional swaps (ability to swap any combination of assets)\nNo Deposits\nNo Withdraws\nHow the basic swap application would work. (Let’s say a PEPE/USDC swap application)\nCreate a contract that only holds PEPE & USDC\nPeriodically push exchange rates between PEPE/USD to contract from centralised exchange\nWhen users send USDC to swap contact it simply uses the stored exchange rate to send an equivalent amount of PEPE\nWhen users send an an amount of PEPE to the swap contract it also uses the stored exchange rate to send back a proportional amount of USDC to the sender.\nHas basic error handling (when not enough USDC or PEPE to swap, unsupported coins etc)\nI am assuming that these stripped down smart contracts would cost way less since there would be less compute.\nWhy may this be useful.\n\nCheaper transaction costs for fee sensitive users.\n\n\nEasier to implement\n\n\nLower surface of attack and hence, reduced contract risk (hacking risk)\n\nCheaper transaction costs for fee sensitive users.\nEasier to implement\nLower surface of attack and hence, reduced contract risk (hacking risk)\nMy questions for this forum.\na) Has this minimalist approach to smart contracts been explored?\nb) Can it help in solving fee issues?\nc) If it can help, by how much can fees be reduced?\n",
        "category": [
            "Applications"
        ],
        "discourse": []
    },
    {
        "title": "In-Protocol Transaction Ordering",
        "link": "https://ethresear.ch/t/in-protocol-transaction-ordering/21084",
        "article": "Preface\nThis idea is an extension to the FOCIL design proposal. This was initially created concurrently/independently to FOCIL, and started as a way to funnel and order transactions with intention to fully remove the builder workflow from the protocol design. In speaking with several people it was brought to light that FOCIL was progressing, and after going through it, the timing put forth in that design was much better (required 1 less slot and used a larger committee) for transaction selection than this idea proposed.\nThis idea was whittled down to build on top of FOCIL instead of being a competitor as they achieved the same goal.  It’s possible that some of the idea contained here can be integrated with FOCIL but it is provided separately to facilitate targeted discussion and to not muddy the waters of that ongoing design.\nSpecial thanks to Phil Ngo, Nico Flaig, Cayman Nava, Guillaume Ballet, Greg Markou, Gajinder, NC and many many others for taking the time to help hone this idea.\nAbstract\nWhere this proposal focusses is the ordering of transactions.  It also addresses rewards and penalties to coincide with the new inclusion committees and updated proposer duties.\nThere are two main duties, with regards to builders, of the proposal process that need to be addressed to facilitate exclusion of centralized forces through crypto economic means.\nSelection of transactions for inclusion\nOrdering of transactions\nFOCIL is an excellent solution to address the heart of the first of those two topics, but it can be further refined by restricting transactions included in blocks to ONLY those on the inclusion list.  This removes economic incentives, through loss of agency, to add transactions to a block that would negatively affect ordering to capture MEV.\nOrdering will become a deterministic process using the Aggregated Inclusion List, put forth via FOCIL, and an Inclusion Seed.  The Inclusion Seed is entropy generated on a slot-wise basis, specifically to prevent collusive and extractive behaviors during transaction ordering.\nBecause the bulk of the work during block building (inclusion and ordering of transactions in a block that is proposed on the current head) is removed from the proposer the rewards mechanisms should be updated proportionally to compensate the parties providing the value to the protocol.\nDesign\nThere are three things that need to be accounted for to provision deterministic, non-gameable ordering. In particular ordering that is probabilistically infeasible to predict such that sandwich attacks and multi-block mev are economically disincentivized.\nThe cutoff time in which transactions can no longer be included in a block for slot N\nThe time in which the Inclusion Seed is selected for slot N\nThe verifiability of the randomization of transactions within slot N\nThe key to the ordering heuristic is such that the first two items happen in that order. If the seed is not known until after the window for inclusion closes, the heuristic can be built such that “mining an ordered transaction”, to be executed at a certain position in the transaction list, is infeasible.\nThe third item ensures that once the seed is known, any node on the network can calculate the same order of the transactions to prove compliance with the protocol. It also allows rewards and penalties to be assessed for (non)compliance.\nTiming Considerations\nSlot N-1\nt0: Proposer of slot N selects a random nonce, prepares and gossips it for use by the Inclusion Committee\nt9: Cutoff for FOCIL IL committee to select IL from local mempool and gossip individual IL based on consensus head of slot N-1, IL includes the hashed nonce produced by the producer in slot N\nSlot N\nt0: Inclusion nonce, gossiped in N-1 is un-blinded and used to create Inclusion Seed. Inclusion Seed is sent to EL for use as an argument to an idempotent ordering function. An ordered list is produced that represents a full blocks worth of gas, and that list of transactions is executed. Block is produced jointly by EL/CL and CL adds un-blinded nonce to block before releasing to the network.\nt4: Attesters must verify blinded/revealed nonce, inclusion seed and transaction ordering during block validity checks. Attesters vote on valid block at slot N.\nInclusion Seed\nThe purpose of the Inclusion Seed is to provide the entropy to the transaction order.  It should be similar to RANDAO such that it uses on-chain data that is provable and knowable by everyone that follows the chain. However, RANDAO is too infrequently refreshed as it is epoch based and not slot based. For slot-wise ordering the entropy would also need to be slot-wise. The key to minimizing extractive behavior is that the seed needs to be selected after the window for transaction inclusion has closed in slot N-1, but prior to the end of slot N-1, so that it is available to the proposer of slot N.\nWhen using the assumption that this proposal builds on top of FOCIL then the ideal source for entropy selection would be the aggregated Inclusion Lists which would be hard to mine against.\nA likely attack vector to this scheme though would be waiting to be the last committee member to submit an IL which will open the opportunity to include content that affects the seed, and thus the final ordering. It would also incentivize timing games with the knock-on effect of detracting from network propagation of the un-aggregated lists.\nA workaround would be to have the IL aggregation process include some additional entropy in the final list, such as the signature or a nonce.\nSigning the list or using a simple nonce is insufficient though, as it would open attack surface for the final committee member to collude with the next proposer. For the collusion to work the collusive committee member could either have access to the next proposers key or simply coordinate with the next proposer to mine a transaction (and IL with its inclusion) for slot N.\nTo prevent against this attack a reveal process should be used. The first duty a proposer would do is gossip a signed hash in the slot prior to proposal.  The gossiped message would need to be announced very early, within 0-2 sec into the slot, ideally before the block is published (realistically just needs to be before the un-aggregated inclusion lists are gossiped as they need to include the nonce in the IL for validation purposes).\nThe announcement of the blinded nonce before gossip of the un-aggregated lists, with revealed afterwards, makes it impossible to collude in mining a transaction unless the entire inclusion committee participates in the scheme.\nTo ensure adequate entropy the proposer would select some random bytes, perhaps in the range of 8-32 bytes to prevent brute force guessing within the alloted time, and then mix in some stateful randomness from RANDAO.  The randomness would then be hashed to blind the true value and the hash would be signed by the proposer of slot and, but critically it would be gossiped in slot N-1 so it could be appended to the IL’s to prove that it was received prior to IL creation.\nIn slot N the proposer, the only participant that should know the true un-blinded value thus far, would pass the nonce to the EL to the EL can order the transactions to protocol specifications.  The un-blinded value would be appended to the block so it can be revealed to the attestors for block validation and voting.\nThe Inclusion Seed used to shuffle the transactions could then be verified by attestors by checking the hashed nonce in the Inclusion Lists matches the hash of the un-blinded nonce value included in the block once the stateful randomness (from RANDAO) was mixed in.\nDeterministic and Verifiable Ordering\nThe only two pieces that are needed for ordering to be both deterministic and verifiable are known inputs and and a well-known, idempotent ordering function such that:\nTorderd, Tremaining = f(ILagg, IS)\nwhere:\nILagg - Aggregated Inclusion List\nIS - Inclusion Seed\nTordered - Ordered list of transactions\nTremaining - Left-Over transactions that roll over to next block\nOrdering Algorithm\nOrdering could be easily achieved by multiplying the transaction hash by the Inclusion Seed, letting values that overflow wrap around, and then putting the transactions in numeric order. This is overly simplistic and may not provide resistance to mining transactions that could game the system.\nAnother option is ordering the transactions in the Aggregated Inclusion List numerically, by hash, and then running an algorithm similar to swap-or-not over the set utilizing the Inclusion Seed instead of RANDAO. This may be more cpu intensive, but for a small set (1000± transactions) it should be relatively performant.\nThe sender address can alternately be used instead of the transaction hash as the root value that gets ordered but preference currently is to the transaction hash.\nOrdering Heuristic\nThere are some user considerations that need to be addressed when implementing deterministic, non-gameable ordering. There is a balance that needs to be achieved between the crypto economic incentives of priority fees and rigor for prevention of sandwich attacks and monitoring the mempool for value extraction through transaction copies.\nEveryone will have the same fair shot at the order within a block.  While the most egalitarian solution this ignores the crypto economics of paying a higher priority fee.  The most important instance where ignoring this makes sense is for reducing the potential for someone to monitor the transaction pool, for transactions that are profitable to front-run.  The illegitimate actor can simply submit a transaction that with a higher priority fee.\nRandomized ordering takes away much of the economic incentive to copy transactions but does not address undesirable spamming of several conditional-execution transactions to increase probability of being executed ahead of the legitimate actor.\nTo address the spamming of conditional-execution transactions to try to ensure one randomly executes first, a heuristic that crafts tranches of transactions, bundled by fees, such that higher fees go towards the beginning of the block could be used. For transactions with relatively high priority fee (likely profitable arbitrage) it would be very expensive to match the fee for a large number of transactions. Keeping pseudo-randomness within the tranches will make that gamble, or single transaction out bidding expensive and without guarantee of success. It would also be possible to craft the tranches dynamically so that if there are transactions, with fees that are outliers, the bounds of the tranches can be structured to promote fairer execution.\nOnce encrypted mem-pools are fully implemented it would be possible to move to a fully priority-fee based ordering heuristic. This would be the ideal situation that solely relies on market dynamics to set pricing for execution order.\nRewards and Penalties\nIn this paradigm, the searchers adding transactions to the Inclusion Lists are the ones that would receive a bulk of the reward.  The block proposer would now be strictly following the protocol and have reduced agency. With the reduction of duty a majority of the proposal rewards should get shifted to searchers to avoid inflation. This will also have the added benefit of smoothing rewards from a single proposer to the whole IL committee. It could be argued that the proposer generating and gossiping the Inclusion Nonce at N-1 is now a critical point of failure though because the proposal process would hinge upon them.  Thus the block reward that is earned in slot N should be requisite of the nonce gossip at slot N-1.\nThe rewards mechanism of FOCIL would also need to be extended to accommodate the moving of partial block rewards to the searchers for including novel transactions.  This idea is referenced by the FOCIL team but is not finalized yet and thus needs to be updated here once that proposal progresses.\nBecause the IL Committee now hold the critical piece to transaction selection the task is now of high importance. Thusly, equivocation by a committee member should be slashable similar to creating conflicting attestations or blocks.\nLeft-Over Transactions\nThe implication of having multiple un-aggregated inclusion lists and all transactions for a block being sourced through combining them into a single list is the potential for more transactions being submitted than can fit into a single block. This means that the Inclusion Pool (transactions submitted by IL committee members) might need to be inserted into the subsequent block. How this happens is still an area of research but some suggestions proposed so far are:\nDo not guarantee IL transactions MUST be included. More like Inclusion Suggestions Lists and a heuristic will be used to include transactions in the block that is reproducible. All transactions that are not included may be added to subsequent block lists\nLimiting IL size to prevent overflow\nAdding a time weighting such that transactions that have waited for inclusion get higher precedence in the next block. This could be added to the priority fee for instance.  If a large collection of high fee transactions are submitted which prevent inclusion the time weighting could be increased for subsequent blocks to help push the transaction through eventually\nSetting a threshold for sequencing.  If the transaction has a very low priority fee, and does not make it into several blocks it gets removed from the overflow\nOf the suggestions the first is easiest, and most pragmatic to implement. The only thing that would be needed is to develop the heuristic for block inclusion for transactions on a list.\nAccount Abstractions\nIt would be possible to have the EL’s include AA generated transactions to the IL when a list is generated. There are some nuances to fees for prioritization with this though.\nePBS\nThis proposal would remove the need for ePBS and the complexity that it adds to the protocol\nProposer-Attester Separation\nThe chance for Execution Tickets, and other slot auction style proposals, to create further vertical integration by the builders is a real concern. If ordering was done in protocol it reduces this risk substantially. This would be a boon to all suggestions that separate the proposer duty from attester duties which would greatly increase the successful implementation of protocol developments that allow for very light attestation-only clients\nProtocol Simplification\nThe more complex systems become, the more difficult game theory is when planning and implementing protocols within the system. I argue that building widgets on top of widgets to solve problems creates more attacks surface within the system the widgets are designed to protect. In the case of building protocols to inhibit MEV/builders/relays without doing away with builder flow creates more nooks for exploitation to live in. We must address the existing system design and attack the root cause, selection and ordering of transactions without the help of “out of protocol” opaque solutions.\nPreserving Cypherpunk and Egalitarian Principles in the Protocol\nMake Ethereum Cypherpunk Again. This is a call to action on our pilgrimage to MECA. To preserve the ethos that brought us all here.  Builders and the ecosystem that has built up around them must not be allowed to usurp L1. Allowing actors to extract value from honest participants is not only antithetical to the ideals of blockchain but it dissuades adoption by traditional industries. Blockchain brings Rule of Law to the digital space. The idea that single participant is above the protocol. Restoring and preserving that will entice experimentation with blockchain as the root of trust for inter-system interactions.\nLayered Settlement\nThe traditional financial system is built upon layers of infrastructure. It is important to highlight this because there may be significant push-back to the trade-offs this proposal introduces. Settlement time will potentially increase for some participants. Some transactions may fail because builders provide some level of transaction coordination (at the expense of the MEV). Transaction fees might go up.  L1 is not, and should not be designed for instantaneous settlement.  Consensus takes time and the tradeoff for that speed is decentralization and the security it imbues.  Builders will be upset and if incentives are tuned correctly because value extraction will be converted into priority fees. Transaction volume will move to L2.  But these are healthy things for an L2 centric scaling roadmap.\nBeing able to facilitate liquidity pool price discovery, ie very large buys/sells between LPs, across L2s and other application chains like the Uniswap rollup will benefit from surety of price discovery and honest auctions.  The trade off is small transactions will be too expensive to run on L1. This is similar to how our existing financial system works today.  Shares are held by DTCC on behalf of brokers for the benefit of the broker’s clients.  It is costly and difficult for individuals to transact small positions because order books at that level are for very very large quantities.  Brokers facilitate this by providing localized liquidity and pay for the service through transaction sequencing (and in some instances sandwich trades).  The end consumers, however, get to create trades for no fee (other than the slippage that is introduced by the broker).\nThe bottom line is consensus takes time and the trade-off between decentralization and throughput is real. For the benefit of the protocol over the long term, preserving the decentralization at all costs is the secret sauce of what will preserve Ethereum far into the future.\n",
        "category": [
            "Proof-of-Stake",
            "Block proposer"
        ],
        "discourse": [
            "mev"
        ]
    },
    {
        "title": "RFC: Using this.balance as a Storage-Free Deactivation Mechanism Post-Cancun",
        "link": "https://ethresear.ch/t/rfc-using-this-balance-as-a-storage-free-deactivation-mechanism-post-cancun/21056",
        "article": "RFC: Using this.balance as a Storage-Free Deactivation Mechanism Post-Cancun\nAfter the Cancun upgrade and the changes introduced by EIP-6780, the ability to deactivate and reactivate a contract dynamically has become more challenging, particularly in scenarios where querying storage for every transaction is not desirable. I am proposing a minimalistic mechanism for activating and deactivating a contract (specifically a router contract) using the built-in this.balance variable as a control flag.\nUse Case\nThe router contract in question:\nHolds user approvals but no tokens or significant business logic.\nDoes not hold ETH in its normal operation.\nNeeds a reliable, storage-free way to toggle between an active and inactive state.\nThis is particularly useful in cases where:\nA bug is identified in the contract, and users need to be prevented from interacting with it until it is fixed.\nThere’s a need to deactivate the router swiftly without requiring manual revocation of user approvals.\nProposed Mechanism\nThe mechanism relies solely on the contract’s ETH balance (this.balance) to toggle its state. The logic is as follows:\nState Definition:\nWhy This Is Useful\nThis mechanism provides a lightweight way to handle router deactivation, especially in scenarios where:\nBugs in router contracts force users to manually revoke approvals.\nState querying for every transaction is undesirable due to gas costs or complexity.\nUsing the contract’s ETH balance as a toggle avoids the need for storage variables while enabling quick and efficient state changes. This approach could benefit other contract designs requiring similar activation/deactivation mechanisms in a storage-free context.\n",
        "category": [
            "Security"
        ],
        "discourse": []
    },
    {
        "title": "Bitcoin: A Peer-to-Peer Electronic CASH System ~ in part by: Overpass Channels",
        "link": "https://ethresear.ch/t/bitcoin-a-peer-to-peer-electronic-cash-system-in-part-by-overpass-channels/20987",
        "article": "Author: Brandon “Cryptskii” Ramsay\nDate: 2024-11-14\nAbstract\nIn response to the growing economic challenges faced by traditional financial systems, Bitcoin’s significance as a decentralized, censorship-resistant store of value continues to rise. Building on the Overpass Channels architecture, we propose a privacy-preserving, scalable Layer 2 solution that enables high-volume transactions on Bitcoin without altering its protocol or consensus model. This paper presents a comparative analysis of Overpass Channels and BitVM2, substantiating Overpass’s superiority in privacy, economic neutrality, and scalability. We formalize the system’s operational assumptions and provide rigorous theorems and proofs that validate Overpass’s ability to maintain Bitcoin’s security properties and monetary principles, setting a new benchmark for scalability on Bitcoin’s blockchain.\nThe escalating volatility within traditional financial systems underscores Bitcoin’s foundational role as a decentralized store of value. As Bitcoin adoption grows, the need for scalable and private transaction mechanisms is evident. Leveraging the Overpass Channels architecture\nOverpass.2024, we introduce a solution specifically designed to scale Bitcoin transactions without altering its consensus or core protocol. By contrasting Overpass Channels with BitVM2, we elucidate the distinct advantages of our approach in maintaining privacy and network integrity while ensuring economic neutrality.\n1.1 Motivation\nGiven the limitations of traditional Layer 2 solutions—often requiring protocol adjustments or trust-based assumptions—the Overpass Channels approach offers a uniquely adaptable, non-invasive solution that enables Bitcoin to scale without compromising its decentralized ethos. While recent advancements like BitVM2 have made strides in SNARK-based verification, Overpass Channels address these challenges through its established hierarchical structure [Section 9.1] and privacy-focused mechanisms [Section 3].\nDistributed Storage: Utilizes Overpass’s distributed storage model [Section 10] for efficient transaction handling.\nOptimized State Management: Employs hierarchical sparse Merkle trees [Section 12] for lightweight Bitcoin state management.\nPrivacy-Enhanced zk-SNARKs: Integrates Plonky2-based zk-SNARKs [Section 3.8] to preserve transaction privacy.\nCompatibility with Bitcoin’s HTLC: Ensures seamless Bitcoin integration through HTLC adaptation [Section 8.2].\n1.2 Core Principles\nOur design prioritizes the following principles to ensure Overpass Channels aligns with Bitcoin’s core properties:\nProtocol Integrity: Achieves scalability without protocol modifications to Bitcoin.\nEconomic Consistency: Preserves Bitcoin’s economic incentives and fee structure.\nTrustless Design: Implements trustless operation based on Overpass’s proven cryptographic assumptions [Section 6].\nPrivacy Assurance: Enhances transaction privacy by default, following Overpass’s established privacy guarantees [Section 18].\nDecentralization Support: Maintains economic neutrality to avoid concentration of network power.\nComparison Framework\nTo formalize the comparison between Overpass Channels and BitVM2, we establish a rigorous evaluation framework based on privacy, scalability, economic neutrality, and security. Each metric is substantiated through theorem-proof structures that quantify the systems’ respective capabilities.\nDefinition (Layer 2 Security Preservation): A Layer 2 solution S preserves Bitcoin’s security model if and only if:\n\n\\forall t \\in T, \\; P(\\text{attack} \\mid S) \\leq P(\\text{attack} \\mid \\text{Bitcoin})\n\nwhere T is the set of all transaction types, and P(\\text{attack}) represents the probability of a successful attack.\nTheorem (Security Preservation in Overpass Channels): Overpass Channels maintain Bitcoin’s security properties with respect to consensus and decentralization by ensuring that no additional vulnerabilities are introduced in state management or transaction validation:\n\nP(\\text{attack} \\mid \\text{Overpass}) = P(\\text{attack} \\mid \\text{Bitcoin}).\n\nProof: Let A be an adversary aiming to compromise transactions in Overpass Channels. For any attack strategy \\sigma:\n\nThe adversary must either:\n\nBreak Bitcoin’s security assumptions, or\nExploit a flaw in Overpass’s zk-SNARK verification or channel closure mechanism.\n\n\nBreak Bitcoin’s security assumptions, or\nExploit a flaw in Overpass’s zk-SNARK verification or channel closure mechanism.\n\nOverpass Channels enforce the following:\n\nzk-SNARK soundness guarantees transaction validity.\nChannel closure requires a valid Bitcoin transaction, preserving the network’s security model.\nNo additional cryptographic assumptions beyond standard zk-SNARK soundness are introduced.\n\n\nzk-SNARK soundness guarantees transaction validity.\nChannel closure requires a valid Bitcoin transaction, preserving the network’s security model.\nNo additional cryptographic assumptions beyond standard zk-SNARK soundness are introduced.\n\nConsequently, the security of Overpass Channels is bounded by Bitcoin’s own security assumptions and the integrity of zk-SNARK proofs:\nP(\\text{attack} \\mid \\text{Overpass}) = P(\\text{attack} \\mid \\text{Bitcoin})\n\nThe adversary must either:\nBreak Bitcoin’s security assumptions, or\nExploit a flaw in Overpass’s zk-SNARK verification or channel closure mechanism.\nOverpass Channels enforce the following:\nzk-SNARK soundness guarantees transaction validity.\nChannel closure requires a valid Bitcoin transaction, preserving the network’s security model.\nNo additional cryptographic assumptions beyond standard zk-SNARK soundness are introduced.\nConsequently, the security of Overpass Channels is bounded by Bitcoin’s own security assumptions and the integrity of zk-SNARK proofs:\nP(\\text{attack} \\mid \\text{Overpass}) = P(\\text{attack} \\mid \\text{Bitcoin})\nThis completes the proof, showing that Overpass Channels do not degrade Bitcoin’s security guarantees.\nTechnical Architecture\nThe integration of Overpass Channels with Bitcoin leverages several technical mechanisms to achieve scalability and privacy while preserving security. We provide a structured comparison with BitVM2 to highlight Overpass’s unique advantages.\nUnilateral Payment Channels\nOverpass Channels introduce a unilateral payment channel structure specifically optimized for Bitcoin, distinct from BitVM2’s state model.\nDefinition (Bitcoin-Compatible Unilateral Channel)\nA Bitcoin-compatible unilateral channel C is defined as a tuple (pk_s, pk_r, v, t, \\sigma) where:\npk_s: Sender’s public key\npk_r: Receiver’s public key\nv: Channel value in satoshis\nt: Timelock value\n\\sigma: Channel signature\nsatisfying the following property:\n{ValidChannel}(C) \\iff {VerifyBitcoinSig}(sigma, (pk_s, pk_r, v, t)) = {true}\nCryptographic Constructions for Bitcoin Channels\nOverpass Channels ensure privacy and security through cryptographic constructions designed to operate efficiently on Bitcoin’s existing infrastructure. This approach contrasts with BitVM2’s focus on sequential verification, yielding distinct privacy and efficiency advantages.\nTheorem (Channel State Privacy)\nGiven a channel state S and its corresponding zk-SNARK proof \\pi, no adversary A can determine the transaction history or current balances with probability greater than negligible, while still being able to verify the validity of the state.\nProof\nLet S be a channel state and \\pi its corresponding zk-SNARK proof. Privacy is ensured through a series of games:\n\nGame 0: The real privacy game, where an adversary A attempts to learn information about the channel state S.\n\n\nGame 1: Modify Game 0 by replacing the real zk-SNARK proof with a simulated proof.\nBy the zero-knowledge property of zk-SNARKs:\n\\left| \\Pr[A \\text{ wins Game 0}] - \\Pr[A \\text{ wins Game 1}] \\right| \\leq \\text{negl}(\\lambda)\nwhere \\text{negl}(\\lambda) is a negligible function in the security parameter \\lambda.\n\n\nGame 2: Replace the real channel state S with a random, valid state.\nBy the hiding property of the commitment scheme:\n$\\left| \\Pr[A \\text{ wins Game 1}] - \\Pr[A \\text{ wins Game 2}] \\right| \\leq \\text{negl}(\\lambda)$$\n\nGame 0: The real privacy game, where an adversary A attempts to learn information about the channel state S.\nGame 1: Modify Game 0 by replacing the real zk-SNARK proof with a simulated proof.\nBy the zero-knowledge property of zk-SNARKs:\n\\left| \\Pr[A \\text{ wins Game 0}] - \\Pr[A \\text{ wins Game 1}] \\right| \\leq \\text{negl}(\\lambda)\nwhere \\text{negl}(\\lambda) is a negligible function in the security parameter \\lambda.\nGame 2: Replace the real channel state S with a random, valid state.\nBy the hiding property of the commitment scheme:\n$\\left| \\Pr[A \\text{ wins Game 1}] - \\Pr[A \\text{ wins Game 2}] \\right| \\leq \\text{negl}(\\lambda)$$\nIn Game 2, the adversary receives no information about the actual channel state S, resulting in:\n\\Pr[A \\text{ wins Game 2}] = \\frac{1}{2}\nThrough this sequence of games, we conclude that A's advantage in the real game (Game 0) is negligible, establishing privacy for the Overpass Channels.\nChannel Operations and Bitcoin Script Integration\nOverpass Channels implement functionality through Bitcoin-compatible scripts, enabling secure channel operations without modifying Bitcoin’s protocol. This approach differs from BitVM2, which requires sequential verification stages, by focusing on privacy preservation and operational efficiency.\nAlgorithm: Channel Opening on Bitcoin\nRequire: Sender keys sk_s, pk_s, Receiver public key pk_r, Channel value v\n\nGenerate funding transaction T_f with the following script:\nOP_IF\n   OP_SHA256 H(revocation_key)\n   OP_EQUALVERIFY\n   pk_r OP_CHECKSIG\nOP_ELSE\n   timeout OP_CHECKLOCKTIMEVERIFY\n   OP_DROP\n   pk_s OP_CHECKSIG\nOP_ENDIF\n\n\n\nBroadcast T_f to the Bitcoin network.\n\n\nGenerate zk-SNARK proof \\pi of the channel state validity.\n\nGenerate funding transaction T_f with the following script:\nBroadcast T_f to the Bitcoin network.\nGenerate zk-SNARK proof \\pi of the channel state validity.\nEnsure: (T_f, \\pi)\nComparison with BitVM2\nOverpass Channels and BitVM2 both utilize zk-SNARKs to enable advanced transaction verification on Bitcoin. However, their approaches to state management, privacy, and scalability vary significantly. This section provides a detailed comparison to illustrate the advantages of Overpass Channels over BitVM2.\nArchitectural Differences\nThe core architectural design of each system impacts their performance and scalability. Overpass Channels leverage distributed state management and privacy-preserving mechanisms, while BitVM2 emphasizes sequential verification stages.\nEconomic Implications\nThe economic implications of each approach significantly affect Bitcoin’s fee market and miner incentives. While both systems maintain Bitcoin’s security model, their respective costs and operational overhead differ.\nTheorem (Incentive Compatibility)\nLet M represent Bitcoin miners, and let I(m) be the expected income of a miner m. Under both Overpass Channels and BitVM2:\n\\forall m \\in M: E[I(m) \\mid L2] \\geq E[I(m) \\mid Bitcoin]\nwith system-specific overhead distributions as follows:\nO_{\\text{Overpass}} = O_{\\text{constant}}\nO_{\\text{BitVM2}} = O_{\\text{verification}} + O_{\\text{setup}}\nProof\nFor Overpass Channels:\nChannel operations rely on standard Bitcoin transactions.\nVerification burden remains constant due to optimized SNARK proofs.\nMining decentralization and fee structures remain unaffected.\nFor BitVM2:\nSimilar reliance on standard Bitcoin transactions.\nInitial setup and verification costs introduced.\nVerification overhead potentially impacts miner fees due to increased computational requirements.\nTherefore, both systems preserve Bitcoin’s incentive model, although Overpass offers a more consistent and lower overhead for miners.\nNetwork Effects and Liquidity\nThe liquidity distribution and network effects of each system are crucial for Bitcoin’s economic stability. Overpass Channels achieve liquidity efficiency with minimized operational costs, offering an advantage over BitVM2’s verification overhead.\nTheorem (Liquidity Preservation)\nIn a network with total liquidity L, both systems preserve Bitcoin’s liquidity pool:\nL_{\\text{effective}} = L_{\\text{total}} - O_{\\text{system}}\nwhere:\nO_{\\text{Overpass}} < O_{\\text{BitVM2}}\ndue to Overpass’s optimized state management and lack of setup costs.\nSecurity Considerations and Risk Analysis\nLayer 2 solutions must be carefully analyzed for security implications to ensure they do not compromise Bitcoin’s core properties. This section provides a comprehensive examination of the security models for Overpass Channels and BitVM2, focusing on privacy, attack surface, and resistance to double-spend attacks.\nAttack Surface Analysis\nThe attack surface of each system represents the potential vulnerability points that could be exploited by adversaries. Overpass Channels and BitVM2 both introduce minimal attack surfaces, but their structural differences affect the composition of these surfaces.\nDefinition (Attack Surface Extension)\nFor a Layer 2 solution L, the attack surface extension E(L) is defined as:\nE(L) = \\{(v, p) \\mid v \\in V(L) \\setminus V(Bitcoin), p > 0\\}\nwhere V(L) is the set of potential vulnerability points in L and p is the probability of successful exploitation.\nTheorem (Equivalent Base Extension)\nBoth systems maintain minimal attack surface extension:\n|E(\\text{Overpass})| = O(1)\n|E(\\text{BitVM2})| = O(1)\nwith different vulnerability classes:\nV_{\\text{Overpass}} = \\{V_{\\text{privacy}}, V_{\\text{state}}\\}\nV_{\\text{BitVM2}} = \\{V_{\\text{setup}}, V_{\\text{verify}}\\}\nProof\nFor both Overpass Channels and BitVM2:\nState transitions and transaction validity are secured by zk-SNARKs.\nChannel operations rely on standard Bitcoin transaction security.\nNo additional consensus requirements are introduced.\nKey distinctions include:\n\nPrivacy Mechanism:\n\nOverpass: Full privacy achieved through state channels.\nBitVM2: Basic privacy limited by sequential verification.\n\n\nOverpass: Full privacy achieved through state channels.\nBitVM2: Basic privacy limited by sequential verification.\n\nSetup Requirements:\n\nOverpass: Direct channel initialization without additional setup.\nBitVM2: Requires an initial verification setup phase.\n\n\nOverpass: Direct channel initialization without additional setup.\nBitVM2: Requires an initial verification setup phase.\nPrivacy Mechanism:\nOverpass: Full privacy achieved through state channels.\nBitVM2: Basic privacy limited by sequential verification.\nSetup Requirements:\nOverpass: Direct channel initialization without additional setup.\nBitVM2: Requires an initial verification setup phase.\nThus, both systems achieve minimal and comparable attack surface extensions, though the structure of vulnerability classes differs.\nDouble-Spend Prevention\nDouble-spend prevention is essential for maintaining Bitcoin’s integrity as a monetary system. Both Overpass Channels and BitVM2 implement robust mechanisms to prevent double-spend attacks.\nTheorem (Double-Spend Prevention)\nFor both systems, the probability of a successful double-spend attack P(DS) is bounded by:\nP(DS) \\leq \\min(P(\\text{Bitcoin\\_DS}), P(\\text{zk\\_break}))\nwhere P(\\text{Bitcoin\\_DS}) represents the probability of a double-spend on Bitcoin and P(\\text{zk\\_break}) represents the probability of breaking the zk-SNARK system.\nProof\nLet A be an adversary attempting a double-spend attack. For success, A must either:\nCompromise Bitcoin’s underlying security model with probability P(\\text{Bitcoin\\_DS}).\nGenerate a false zk-SNARK proof with probability P(\\text{zk\\_break}).\nAdditionally, both systems enforce a channel closure mechanism that ensures:\n\\forall s_1, s_2 \\in \\text{States}: \\text{Close}(s_1) \\land \\text{Close}(s_2) \\implies s_1 = s_2\nThus, the probability of a successful double-spend attack is bounded by the minimum probability of either compromising Bitcoin’s security or breaking the zk-SNARK proof system, regardless of system-specific differences.\nImpact on Bitcoin’s Security Model\nEach Layer 2 solution must be assessed for its impact on Bitcoin’s core security properties, such as decentralization, censorship resistance, and immutability. Overpass Channels and BitVM2 maintain these properties, though their verification and state management differ.\nDefinition (Security Model Preservation)\nA Layer 2 solution S preserves Bitcoin’s security model if:\n\\forall p \\in \\text{Properties(Bitcoin)}: \\text{Guarantee}(p \\mid S) \\geq \\text{Guarantee}(p \\mid \\text{Bitcoin})\nwhere \\text{Properties(Bitcoin)} includes decentralization, censorship resistance, and immutability.\nTheorem (Security Model Impact)\nBoth Overpass Channels and BitVM2 maintain Bitcoin’s security model with distinct architectural trade-offs:\n\\Delta_{\\text{security}}(\\text{Overpass}) = \\Delta_{\\text{security}}(\\text{BitVM2}) = 0\nthough they follow different verification pathways:\n\\text{Path}_{\\text{Overpass}} = \\{\\text{Privacy}, \\text{StateManagement}\\}\n\\text{Path}_{\\text{BitVM2}} = \\{\\text{Setup}, \\text{VerificationFlow}\\}\nProof\nTo assess security preservation, consider the following for both systems:\n\nConsensus Requirements:\n\nBoth systems operate without modifying Bitcoin’s consensus.\n\n\nBoth systems operate without modifying Bitcoin’s consensus.\n\nCryptographic Assumptions:\n\nEach system relies on zk-SNARKs, ensuring equivalent cryptographic strength.\n\n\nEach system relies on zk-SNARKs, ensuring equivalent cryptographic strength.\n\nState and Transaction Management:\n\nOverpass: Employs integrated, privacy-preserving state channels, minimizing exposure.\nBitVM2: Utilizes a sequential verification process that introduces verification layers but maintains on-chain compatibility.\n\n\nOverpass: Employs integrated, privacy-preserving state channels, minimizing exposure.\nBitVM2: Utilizes a sequential verification process that introduces verification layers but maintains on-chain compatibility.\n\nImplementation Distinctions:\n\nOverpass prioritizes direct state transitions, reducing operational overhead.\nBitVM2 requires setup and verification sequences, increasing complexity.\n\n\nOverpass prioritizes direct state transitions, reducing operational overhead.\nBitVM2 requires setup and verification sequences, increasing complexity.\nConsensus Requirements:\nBoth systems operate without modifying Bitcoin’s consensus.\nCryptographic Assumptions:\nEach system relies on zk-SNARKs, ensuring equivalent cryptographic strength.\nState and Transaction Management:\nOverpass: Employs integrated, privacy-preserving state channels, minimizing exposure.\nBitVM2: Utilizes a sequential verification process that introduces verification layers but maintains on-chain compatibility.\nImplementation Distinctions:\nOverpass prioritizes direct state transitions, reducing operational overhead.\nBitVM2 requires setup and verification sequences, increasing complexity.\nTherefore, both systems preserve Bitcoin’s security model while following distinct approaches to verification and state management.\nLiveness and Availability Analysis\nThe liveness and availability of transactions are critical for user experience and adoption. Overpass Channels and BitVM2 achieve comparable liveness guarantees through different transaction handling mechanisms.\nTheorem (Liveness Guarantee)\nUnder both systems, transaction liveness L(t) for a transaction t is guaranteed with probability:\nP(L(t)) \\geq 1 - (1 - p)^k\nwhere p is the probability of successful Bitcoin transaction inclusion and k is the number of confirmation attempts.\nProof\nFor both systems:\n\nChannel Operations:\n\nRely on standard Bitcoin transactions for channel creation and closure.\n\n\nRely on standard Bitcoin transactions for channel creation and closure.\n\nVerification Methodology:\n\nBoth systems use zk-SNARK proofs for verification, enabling off-chain transaction finality.\n\n\nBoth systems use zk-SNARK proofs for verification, enabling off-chain transaction finality.\n\nChannel Closure Attempts:\n\nWith k attempts, the probability of successful closure is given by:\nP(\\text{closure\\_success}) = 1 - (1 - p)^k\n\n\nWith k attempts, the probability of successful closure is given by:\nP(\\text{closure\\_success}) = 1 - (1 - p)^k\nChannel Operations:\nRely on standard Bitcoin transactions for channel creation and closure.\nVerification Methodology:\nBoth systems use zk-SNARK proofs for verification, enabling off-chain transaction finality.\nChannel Closure Attempts:\nWith k attempts, the probability of successful closure is given by:\nP(\\text{closure\\_success}) = 1 - (1 - p)^k\nSince each system relies on Bitcoin’s underlying liveness properties for final settlement, they both achieve equivalent liveness guarantees.\nLong-term Security Implications\nBoth Overpass Channels and BitVM2 must be evaluated for their long-term security impacts, especially in terms of protocol longevity and resistance to future attack vectors.\nTheorem (Security Model Evolution)\nThe long-term security impact I(t) of both Layer 2 solutions at time t satisfies:\n\\lim_{t \\to \\infty} I(t) = 0\nwith differing composition vectors:\nV_{\\text{Overpass}}(t) = \\{v_{\\text{privacy}}(t), v_{\\text{state}}(t)\\}\nV_{\\text{BitVM2}}(t) = \\{v_{\\text{setup}}(t), v_{\\text{verify}}(t)\\}\nProof\nConsider the following security properties for both systems:\nLongevity of Cryptographic Assumptions:\nThus, the long-term security impact remains neutral for both systems, with each maintaining minimal additional risk over time.\nPrivacy Guarantees and Economic Implications\nThe privacy and economic characteristics of a Layer 2 solution significantly affect Bitcoin’s fungibility and monetary stability. Overpass Channels and BitVM2 both employ zk-SNARKs, yet their approaches to privacy and economic neutrality are fundamentally different.\nPrivacy Model\nPrivacy within a Layer 2 solution is critical for ensuring that transactions are indistinguishable, preserving Bitcoin’s fungibility. Overpass Channels provide enhanced privacy over BitVM2 due to its integrated, privacy-preserving state channels.\nDefinition (Transaction Privacy)\nA transaction T in a Layer 2 system provides \\delta-privacy if for any adversary A:\n\\left| \\Pr[A(T) = 1] - \\Pr[A(T') = 1] \\right| \\leq \\delta\nwhere T' is any other valid transaction with identical public parameters.\nTheorem (Privacy Guarantees)\nOverpass Channels achieve an enhanced level of privacy, denoted \\varepsilon-privacy:\n\\varepsilon_{\\text{Overpass}} \\leq \\frac{1}{2^\\lambda}\ncompared to BitVM2’s basic transaction privacy:\n\\varepsilon_{\\text{BitVM2}} \\leq \\frac{1}{2^\\lambda} + \\delta_{\\text{state}}\nwhere \\delta_{\\text{state}} represents additional information leakage due to BitVM2’s state verification.\nProof\nLet A be an adversary attempting to distinguish between transactions:\nBase zk-SNARK Privacy:\nOverpass: Full state privacy, leading to negligible information leakage:\n\\left| \\Pr[A(\\pi, P, U) = 1] - \\Pr[A(\\text{Sim}(\\pi), P, U) = 1] \\right| \\leq \\frac{1}{2^\\lambda}\nBitVM2: State verification introduces potential leakage:\n\\left| \\Pr[A(\\pi, P, U) = 1] - \\Pr[A(\\text{Sim}(\\pi), P, U) = 1] \\right| \\leq \\frac{1}{2^\\lambda} + \\delta_{\\text{state}}\nEconomic Impact Analysis\nThe economic implications of each system on Bitcoin’s fee market and miner incentives are essential to maintaining a balanced ecosystem.\nTheorem (Fee Market Preservation)\nUnder both systems, Bitcoin’s fee market equilibrium E remains stable:\n|E_{\\text{L2}} - E_{\\text{Bitcoin}}| \\leq \\epsilon\nwhere \\epsilon is a negligible factor, with differing overhead distributions:\n\\epsilon_{\\text{Overpass}} = O_{\\text{channel}} + O_{\\text{privacy}}\n\\epsilon_{\\text{BitVM2}} = O_{\\text{setup}} + O_{\\text{verify}}\nProof\nFor a transaction t, the fee function F(t) can be expressed as:\nF(t) = \\alpha \\cdot s(t) + \\beta \\cdot p(t)\nwhere s(t) is the transaction size, and p(t) is the priority.\nOverpass Channels:\nThus, while both systems preserve the equilibrium of Bitcoin’s fee market, Overpass offers a more efficient fee structure by minimizing extraneous costs.\nLiquidity Efficiency\nEfficient liquidity utilization is essential for a Layer 2 solution to scale while maintaining user accessibility and network sustainability. Overpass Channels provide a more optimized liquidity model than BitVM2 due to minimized verification and operational overhead.\nTheorem (Liquidity Utilization)\nBoth systems achieve efficient liquidity utilization U, with different optimization paths:\nFor Overpass Channels:\nU_{\\text{Overpass}} = \\frac{L_{\\text{active}}}{L_{\\text{total}}} \\cdot \\prod_{i=1}^n r_i\nFor BitVM2:\nU_{\\text{BitVM2}} = \\frac{L_{\\text{active}}}{L_{\\text{total}}} \\cdot \\prod_{i=1}^n (r_i - \\sigma_i)\nwhere L_{\\text{active}} is the active channel liquidity, L_{\\text{total}} is the total liquidity, r_i represents rebalancing factors, and \\sigma_i indicates verification overhead in BitVM2.\nProof\nConsider the set C of all channels in the system. For each channel c \\in C:\n\nLiquidity Utilization:\nu(c) = \\frac{v(c)}{V(c)} \\cdot r(c)\nwhere v(c) is the value utilized and V(c) is the channel capacity.\n\n\nSystem-Specific Utilization Factors:\n\nLiquidity Utilization:\nu(c) = \\frac{v(c)}{V(c)} \\cdot r(c)\nwhere v(c) is the value utilized and V(c) is the channel capacity.\nSystem-Specific Utilization Factors:\nOverpass Channels:\nU_{\\text{Overpass}} = \\frac{\\sum_{c \\in C} u(c) \\cdot V(c)}{\\sum_{c \\in C} V(c)}\nindicating minimal operational costs and high liquidity efficiency.\nBitVM2:\nU_{\\text{BitVM2}} = \\frac{\\sum_{c \\in C} (u(c) - \\sigma(c)) \\cdot V(c)}{\\sum_{c \\in C} V(c)}\nwhere \\sigma(c) reflects verification overhead, reducing effective liquidity.\nEconomic Centralization Resistance\nPreserving decentralization within the economic model is crucial to avoid power concentration in a Layer 2 solution. Overpass Channels and BitVM2 maintain Bitcoin’s decentralization, but Overpass’s structure is inherently more resistant to centralization.\nDefinition (Centralization Resistance)\nA system S is \\rho-centralization resistant if no entity e can control more than \\rho fraction of the system’s economic activity:\n\\forall e: \\frac{\\text{Control}(e)}{\\text{Total}} \\leq \\rho\nTheorem (Decentralization Maintenance)\nBoth systems maintain Bitcoin’s centralization resistance bound \\rho:\n\\rho_{\\text{L2}} \\leq \\rho_{\\text{Bitcoin}}\nthough they differ in their resistance mechanisms:\nR_{\\text{Overpass}} = \\{R_{\\text{privacy}}, R_{\\text{state}}\\}\nR_{\\text{BitVM2}} = \\{R_{\\text{setup}}, R_{\\text{verify}}\\}\nProof\nFor both systems, we examine centralization resistance as follows:\nArchitectural Aspects:\nOverpass Channels:\nPrivacy-preserving channels reduce reliance on trusted parties.\nDistributed state management minimizes central control.\nBitVM2:\nInitial setup and verification dependencies may centralize certain operations.\nThus, Overpass Channels provide a higher resistance to centralization due to minimized setup dependencies and enhanced privacy, while BitVM2 maintains resistance but with increased operational complexity.\nLong-term Economic Stability\nEnsuring economic stability over time is critical for the viability of a Layer 2 solution on Bitcoin. Both Overpass Channels and BitVM2 aim to preserve Bitcoin’s economic model; however, Overpass offers more consistent long-term stability due to its minimal operational overhead and direct transaction management.\nTheorem (Economic Model Preservation)\nBoth systems preserve Bitcoin’s long-term economic stability:\n\\lim_{t \\to \\infty} |M_{\\text{L2}}(t) - M_{\\text{Bitcoin}}(t)| = 0\nwhere M(t) represents the economic model at time t. Each system has different stability vectors:\nS_{\\text{Overpass}}(t) = \\{S_{\\text{privacy}}(t), S_{\\text{channel}}(t)\\}\nS_{\\text{BitVM2}}(t) = \\{S_{\\text{verify}}(t), S_{\\text{setup}}(t)\\}\nProof\nTo examine economic stability, we consider the following for each system:\nMonetary Properties:\nNetwork Effects:\nBoth systems are designed to maintain decentralization and support censorship resistance, ensuring long-term usability and user accessibility.\nAs t \\to \\infty, both systems converge towards stable economic models with minor fluctuations for BitVM2 due to its additional verification overhead. Overpass Channels, however, offer a smoother economic trajectory with fewer cost variations.\nComparative Analysis of Trustless Mechanisms\nA fundamental requirement for Layer 2 solutions on Bitcoin is the minimization of trust assumptions. Overpass Channels and BitVM2 each establish distinct trust models, yet Overpass achieves stronger trust minimization due to its direct channel structure and privacy integration.\nTrust Model Foundations\nThe level of trust required by a Layer 2 system impacts its alignment with Bitcoin’s trustless design. We formalize the trust minimization for each system.\nTheorem (Trust Minimization)\nFor both Layer 2 systems B, the trust requirement T(B) can be defined as:\nT(B) = \\sum_{i=1}^n w_i \\cdot t_i\nwhere w_i represents trust weights and t_i represents individual trust assumptions. Each system has unique trust vectors:\nT_{\\text{Overpass}} = \\{t_{\\text{privacy}}, t_{\\text{state}}\\}\nT_{\\text{BitVM2}} = \\{t_{\\text{setup}}, t_{\\text{verify}}\\}\nBridge Trust Models\nLayer 2 solutions require secure bridging mechanisms with Bitcoin’s Layer 1 to facilitate interoperability while preserving trust assumptions.\nDefinition (Bridge Security)\nA bridge transaction maintains Bitcoin’s trust assumptions if:\n\\forall \\text{tx} \\in \\text{Transactions}: \\text{Trust}(\\text{tx}) \\subseteq \\text{Trust}(\\text{Bitcoin})\nwhere \\text{Trust(Bitcoin)} encompasses Bitcoin’s base security assumptions.\nTheorem (Trust Preservation)\nBoth systems preserve Bitcoin’s trust model through different bridging mechanisms:\nT(\\text{L2}) = T(\\text{Bitcoin}) + T(\\text{SNARK})\nwhere T(\\text{SNARK}) represents the trust assumption introduced by zk-SNARKs. Distinct implementation paths are followed:\n\\text{Path}_{\\text{Overpass}} = \\{\\text{Privacy}, \\text{StateTransition}\\}\n\\text{Path}_{\\text{BitVM2}} = \\{\\text{Setup}, \\text{VerificationFlow}\\}\nProof\nThe preservation of trust assumptions is achieved by both systems through:\n\nzk-SNARK Trust Requirement:\n\nBoth systems introduce SNARK-based proofs, which assume soundness and non-interactivity.\n\n\nBoth systems introduce SNARK-based proofs, which assume soundness and non-interactivity.\n\nSystem-Specific Mechanisms:\n\n\nOverpass Channels:\n\nDirect channel state transitions ensure trust minimization.\nIntegrated privacy reduces the reliance on trusted setups.\n\n\n\nBitVM2:\n\nRequires an initial setup phase, adding a layer of trust for configuration integrity.\nSequential verification process may introduce dependencies on verification nodes.\n\n\n\n\n\nOverpass Channels:\n\nDirect channel state transitions ensure trust minimization.\nIntegrated privacy reduces the reliance on trusted setups.\n\n\nDirect channel state transitions ensure trust minimization.\nIntegrated privacy reduces the reliance on trusted setups.\n\nBitVM2:\n\nRequires an initial setup phase, adding a layer of trust for configuration integrity.\nSequential verification process may introduce dependencies on verification nodes.\n\n\nRequires an initial setup phase, adding a layer of trust for configuration integrity.\nSequential verification process may introduce dependencies on verification nodes.\nzk-SNARK Trust Requirement:\nBoth systems introduce SNARK-based proofs, which assume soundness and non-interactivity.\nSystem-Specific Mechanisms:\n\nOverpass Channels:\n\nDirect channel state transitions ensure trust minimization.\nIntegrated privacy reduces the reliance on trusted setups.\n\n\nDirect channel state transitions ensure trust minimization.\nIntegrated privacy reduces the reliance on trusted setups.\n\nBitVM2:\n\nRequires an initial setup phase, adding a layer of trust for configuration integrity.\nSequential verification process may introduce dependencies on verification nodes.\n\n\nRequires an initial setup phase, adding a layer of trust for configuration integrity.\nSequential verification process may introduce dependencies on verification nodes.\nOverpass Channels:\nDirect channel state transitions ensure trust minimization.\nIntegrated privacy reduces the reliance on trusted setups.\nBitVM2:\nRequires an initial setup phase, adding a layer of trust for configuration integrity.\nSequential verification process may introduce dependencies on verification nodes.\nIn summary, both systems maintain Bitcoin’s trust model, but Overpass achieves a higher degree of trust minimization by avoiding setup requirements and emphasizing privacy-preserving operations.\nConclusion\nThis paper has provided a detailed comparative analysis of Overpass Channels and BitVM2 as Layer 2 solutions for Bitcoin, focusing on scalability, privacy, security, and economic neutrality. Through rigorous theorem-proof structures, we have demonstrated Overpass Channels’ unique advantages in privacy preservation, efficient liquidity utilization, and trust minimization, establishing it as a leading solution for scaling Bitcoin without altering its core protocol.\nSummary of Key Findings\nOverpass Channels emerge as a compelling choice for high-volume, privacy-preserving transactions on Bitcoin, offering the following distinct advantages:\nEnhanced Privacy: Through integrated privacy-preserving state channels, Overpass ensures stronger privacy guarantees, minimizing information leakage compared to BitVM2.\nScalability and Efficiency: Achieving O(n) horizontal scaling with minimal verification overhead, Overpass efficiently supports high transaction throughput, whereas BitVM2 incurs higher verification and setup costs.\nEconomic Neutrality and Stability: Closely aligned with Bitcoin’s fee market structure, Overpass preserves Bitcoin’s economic neutrality without introducing additional cost burdens.\nTrustless Design: Overpass Channels eliminate the need for trusted setups and emphasize zk-SNARK-based verification, achieving stronger trust minimization than BitVM2’s setup-dependent model.\nOverpass Channels as the Cash Layer for Layer 1 Blockchains\nWhile Bitcoin serves as an optimal reserve asset and “gold layer” of a decentralized financial network, Overpass Channels have the potential to become the “cash layer” not only for Bitcoin but for any Layer 1 blockchain that integrates with its architecture. By extending Overpass Channels as a universal Layer 2 solution, any compatible blockchain can benefit from instant, privacy-preserving transactions with high scalability, thus providing a cash layer capable of supporting everyday transactional demands across various blockchain ecosystems.\nThis analysis specifically highlights Overpass Channels in the context of Bitcoin as an extension of the original Overpass Channels research. However, the modular design of Overpass allows seamless integration with multiple blockchains, enhancing each one with Overpass’s advanced privacy and scalability benefits. This interoperability offers a transformative vision: a decentralized, multi-chain economy where Bitcoin and Overpass work symbiotically, with Bitcoin as the global reserve and Overpass as the universal, privacy-preserving cash layer.\nFuture Directions\nSeveral areas of future research and development can help realize the full potential of Overpass Channels across multiple blockchain networks:\nzk-SNARK Optimization: Further research into zk-SNARK efficiency can reduce computational overhead, making verification faster and more accessible across diverse Layer 1 blockchains.\nExpanding Integration Capabilities: Developing tools and protocols for seamless Overpass integration with other blockchains will extend its applicability as a cash layer beyond Bitcoin.\nReal-world Deployment and Audits: Comprehensive security audits and real-world testing will validate Overpass’s privacy and scalability claims, ensuring robust performance across different blockchain networks.\nFinal Remarks\nIn conclusion, Overpass Channels represent a groundbreaking Layer 2 solution that enhances the scalability and privacy of Bitcoin and has the potential to serve as a universal cash layer across various Layer 1 blockchains. By offering a scalable, privacy-focused transaction layer, Overpass can redefine the usability and accessibility of decentralized finance. This cash layer for the Internet enables a flexible, interoperable financial system that respects user privacy and decentralization principles, positioning Bitcoin and Overpass as essential building blocks in the future of a decentralized global economy.\nRamsay, B., “Overpass Channels: Horizontally Scalable, Privacy-Enhanced, with Independent Verification, Fluid Liquidity, and Robust Censorship Proof Payments,” Cryptology ePrint Archive, Paper 2024/1526, 2024.\nLinus, R., Aumayr, L., Zamyatin, A., Pelosi, A., Avarikioti, Z., Maffei, M., “BitVM2: Bridging Bitcoin to Second Layers,” presented by ZeroSync, TU Wien, BOB, University of Pisa, University of Camerino, and Common Prefix, 2024.\nNakamoto, S., “Bitcoin: A Peer-to-Peer Electronic Cash System,” Bitcoin.org, 2008.\n",
        "category": [
            "Layer 2",
            "State channels"
        ],
        "discourse": []
    },
    {
        "title": "How obfuscation can help Ethereum",
        "link": "https://ethresear.ch/t/how-obfuscation-can-help-ethereum/7380",
        "article": "Obfuscation is in many ways the ultimate cryptographic primitive. Obfuscation allows you to turn a program P into an “obfuscated program” P' such that (i) P' is equivalent to P, ie. P'(x) = P(x) for all x, and (ii) P' reveals nothing about the “inner workings” of P. For example, if P does some computation that involves some secret key, P' should not reveal that key.\nObfuscation is not yet available; candidate constructions exist, but they all depend on cryptographic assumptions that cryptographers are not happy with and some candidates have already been broken. However, recent research suggests that we are very close to secure obfuscation being possible, even if inefficient.\nThe usual way to formalize the privacy property is that if there are two programs P and Q that implement the same functionality (ie. P(x) = Q(x) for all x) but maybe with different algorithms, then given obfuscate(P) and obfuscate(Q) you should not be able to tell which came from which (to see how this leads to secrecy of internal keys, consider a function that uses a key k to sign a message out of the set [1....n]; this could be implemented either by actually including k and signing a message that passes a range check, or by simply precomputing and listing all n signatures; the formal property implies that you can’t extract k from the first program, because the second program does not even contain k).\nObfuscation is considered to be so powerful because it immediately implies almost any other cryptographic primitive. For example:\nPublic key encryption: let enc/dec be a symmetric encryption scheme (which can be implemented easily using just hashes): the secret key is k, the public key is an obfuscated program of enc(k, x)\n\nSignatures: the signing key is k, the verification key is an obfuscated program that accepts M and sig and verifies that sig = hash(M, k)\n\nFully homomorphic encryption: let enc/dec be a symmetric encryption scheme. The secret key is k, the evaluation key is enc(k, dec(k, x1) + dec(k, x2)) and enc(k, dec(k, x1) * dec(k, x2))\n\nZero knowledge proofs: an obfuscated program is published that accepts x as input and publishes sign(k, x) only if P(x) = 1 for some P\n\nMore generally, obfuscation is viewed as a potential technology for creating general-purpose privacy-preserving smart contracts. This post will both go into this potential and other applications of obfuscation to blockchains.\nSmart contracts\nCurrently, the best available techniques for adding privacy to smart contracts use zero knowledge proofs, eg. AZTEC and Zexe. However, these techniques have an important limitation: they require the data in the contract to be broken up into “domains” where each domain is visible to a user and requires that user’s active involvement to modify. For a currency system, this is acceptable: your balance is yours, and you need your permission to spend money anyway. You can send someone else money by creating encrypted receipts that they can claim. But for many applications this does not work; for example, something like Uniswap contains a very important core state object which is not owned by anyone. An auction could not be conducted fully privately; there needs to be someone to run the calculation to determine who wins, and they need to see the bid amounts to compute the winning bid.\nObfuscation allows us to get around this limitation, getting much closer to “perfect privacy”. However, there is still a limitation remaining. One can naively assume obfuscation lets you create contracts of the form “only if event X happens, then release data Y”. However, outside observers can create a private fork of the blockchain, include and censor arbitrary transactions in this private fork (including copying over some but not all transactions from the main chain), and see the outputs of the contract in this private fork.\nTo give a particular example, key revocation for data vaults cannot work: if at some time in the past, a key k_1 could have released data D, but now that key was switched in the smart contract to k_2, then an attacker with k_1 could still locally rewind the chain to before the time of the switch, and send the transaction on this local chain where k_1 still suffices to release D and see the result.\nObfuscating an auction is a particular example of this: even if the auction is obfuscated, you can determine others’ bids by locally pretending to bid against them with every possible value, and seeing under what circumstances you win.\nOne can partially get around this, by requiring the obfuscated program to verify that an instruction was confirmed by the consensus, but this is not robust against failures of the blockchain (51% attacks or more than 1/3 going offline). Hence, it’s a lower security level than the full blockchain. Another way to get around this is by having the obfuscated program check a PoW instance based on the inputs; this limits the amount of information an attacker can extract by making executions of the program more expensive.\nWith this restriction, however, more privacy with obfuscation is certainly possible. Auctions, voting schemes (including in DAOs), and much more are potential targets.\nOther benefits\n\nZKPs with extremely cheap verification: this is basically the scheme mentioned above. Generate an obfuscated program which performs some pre-specified computation f on (x, y) (x is the public input, y is the private input), and signs (x, f(x)) with an internal key k. Verification is done by verifying the signature with the public key corresponding to k. This is incredibly cheap because verifying a proof is just verifying a signature. Additionally, if the signature scheme used is BLS, verification becomes very easy to aggregate.\n\nOne trusted setup to rule them all: generating obfuscated programs will likely require a trusted setup. However, we can make a single obfuscated program that can generate all future trusted setups for all future protocols, without needing any further trust. This is done as follows. Create a program which contains a secret key k, and takes as input a program P. The program executes P(h(P, k)) (ie. it generates a subkey h(P, k) specific to that program), and publishes the output and signs it with k. Any future trusted setup can be done trustlessly by taking the program P that computes the trusted setup and putting it into this trusted setup executor as an input.\n\nBetter accumulators: for example, given some data D, one can generate in O(|D|) time a set of elliptic curve point pairs (P, k*P) where P = hash(i, D[i]) and k is an internal secret key (K = k*G is a public verification key). This allows verifying any of these point pairs (P1, P2) by doing a pairing check e(P1, K) = e(P2, G) (this is the same technique as in older ZK-SNARK protocols). Particularly, notice that a single pairing check also suffices to verify any subset of the points. Even better constructions are likely possible.\n",
        "category": [
            "Cryptography"
        ],
        "discourse": []
    },
    {
        "title": "3-Slot-Finality: SSF is not about \"Single\" Slot",
        "link": "https://ethresear.ch/t/3-slot-finality-ssf-is-not-about-single-slot/20927",
        "article": "Authors: Francesco D’Amato, Roberto Saltini, Thanh-Hai Tran, Luca Zanolini\nTL;DR; In this post, we introduce 3-Slot Finality (3SF), a protocol designed to finalize blocks proposed by an honest proposer within 3 slots when network latency is bounded by a known value \\Delta – even if subsequent proposers might be dishonest – while requiring only one voting phase per slot. This approach contrasts with previously proposed protocols for Single-Slot Finality, which require three voting phases per slot to finalize an honestly proposed block within a single slot resulting in longer slot time. We also show that 3SF guarantees all the key properties expected from SSF, offering an efficient and practical alternative that reduces overhead while ensuring fast and predictable block finalization within a few slots, and a shorter practical slot time (as voting phases take practically longer than other phases). As a result, our protocol achieves a shorter expected confirmation time compared to the previously proposed protocol, at the expense of a slight delay in block finalization, which extends to the time required to propose three additional blocks, rather than finalizing before the next block proposal. However, we believe that a shorter expected confirmation time could be sufficient for most users. Also, slot time is a crucial parameter affecting economic leakage in on-chain automated market makers (AMMs) due to arbitrage. Specifically, arbitrage profits – and consequently, liquidity providers’ (LP) losses – are proportional to the square root of slot time. Therefore, reducing slot time is highly desirable for financial applications on smart contract blockchains. Finally, we show that we can make a further trade-off: we can increase the number of voting phases to two, without this affecting the actual slot length, and achieve finalization in only two slots.\nThis post represents a summary of our extended technical paper that you can find here.\nObserve that in this post, we will focus exclusively on the consensus protocol, setting aside the issues of validator set management. This aspect is independent of the design of an effective consensus protocol and can be addressed separately.\nSingle-Slot-Finality, or SSF, is a highly expected upgrade to the Ethereum consensus layer that is often associated with being able to finalize blocks within the same slot where they are proposed, a significant improvement over the current protocol, which finalizes blocks within 64 to 95 slots. This advancement would eliminate the trade-off between economic security and faster transaction confirmation.\nHowever, do we really need Single-Slot-Finality?\nTo answer this question, let us take a step back and ask ourselves another question: What are the properties not guaranteed by the current protocol (Gasper) that we would like SSF to give us?\nAs we can see, none of the properties listed above is necessarily about being able to finalize blocks within the same slot they are proposed. However, arguably, the sooner a block is finalized, the better. So, why don’t we just stick with Single-Slot-Finality given that we already have one such protocol (which, from here on, we refer to as SSF) that guarantees all of our desired properties above, including being able to finalize blocks within the same slot?\nThe reason is that SSF requires more than one voting phase per slot. The general issue with this is that, because of the large amount of data flooding the network when voting happens, in practice, extra latency must be accounted for each voting phase. For example, to reduce overall network bandwidth, Ethereum currently employs a signature aggregation scheme for each voting phase by which votes are first sent to aggregators who then distribute the aggregated signatures. This means that voting phases require 2\\Delta time, that is, double the normal network latency. Even if in the future it is found that we can do away with aggregators, it is reasonable to expect that voting phases take longer than other phase. In the rest of this section, we assume that a voting phase lasts 2\\Delta as per current Ethereum protocol.\nSo, the more voting phases, the longer the block time tends to be. Specifically, as detailed later, SSF divides each slot in 4 phases: (1) block proposing, (2) Head-voting, (3) FFG-voting and (4) Acknowledgment voting plus view freezing (view-freezing is not important for the argument here). Phase (3) and (4) must wait for phases (2) and (3) to complete, respectively. However, the start of the next slot and any of its phases do not depend on phase (4). So, the slot length in SSF is \\Delta + 2\\Delta + 2\\Delta + \\Delta = 6 \\Delta.\nIn our protocol (3SF) slots are also composed of 4 phases, but only one of them is a voting phase. Specifically, we have (1) block proposing, (2) Head/FFG-voting where both Head-votes and FFG-votes are cast, (3) fast-confirmation and (4) view-merging. Then, the slot length becomes \\Delta + 2\\Delta + \\Delta + \\Delta = 5\\Delta.\nFirst, block time has been shown to be an important parameter in determining the economic leakage of on-chain AMMs to arbitrage. For instance, arbitrage profits (and equivalently LP losses) are proportional to the square root of block time, so that a lower block time is very desirable by financial applications built on top of a smart contract blockchain.\nSecond, with a shorter block time, we can achieve a shorter expected confirmation time which is the expected delay from when a user submit a transaction to when such a transaction is confirmed, that is, to when it is included in a confirmed block. This corresponds to the time taken to confirm a block proposed by an honest proposer + \\frac{(1+\\beta) \\cdot \\text{slot-time}}{2(1 - \\beta)} where \\beta represents the adversarial power in the network. In both 3SF and SSF the time taken to confirm a block proposed by an honest validator is 3\\Delta. So, for \\beta = \\frac{1}{3}, the expected confirmation time for SSF is 9\\Delta whereas for our protocol is 8\\Delta, meaning an \\approx 11\\% improvement. For \\beta = 0, the expected confirmation time for SSF is 6\\Delta whereas for our protocols is 5.5\\Delta, meaning an \\approx 8\\% improvement.\nHowever, as one would expect, while the expected confirmation time of 3SF is shorter, the expected finalization time (that is, the expected time take to finalize a transaction) is longer. The expected finalization time is computed similarly to the expected confirmation time, namely, the time taken to finalize a block proposed by an honest proposer + \\frac{(1+\\beta) \\cdot \\text{slot-time}}{2(1 - \\beta)}. In SSF the time taken to finalize an honest block is 5\\Delta, in 3SF is 11\\Delta and the two-slot variant of 3SF is 8\\Delta. So, for \\beta = \\frac{1}{3}, the expected finalization time for SSF is 11\\Delta, for 3SF is 16\\Delta, and for the two-slot variant of 3SF is 13\\Delta, meaning that the expected finalization time for 3SF is \\approx 46\\% higher than that of SSF, but this reduces to \\approx 18\\% for its two-slot variant. For \\beta = 0, the expected finalization time for SSF is 8\\Delta, for 3SF is 13.5\\Delta, and for the two-slot variant of 3SF is 10.5\\Delta, meaning that the expected finalization time for 3SF is \\approx 69\\% higher than that of SSF, but this reduces to \\approx 31\\% for its two-slot variant.\nOverall, 3SF achieves a balance by trading a higher expected finalization time for a shorter expected confirmation time, which could be sufficient for most users. At the same time, it offers shorter slot time. As additional benefit, the slot structure of 3SF more closely resemble the current Ethereum’s slot structure.\nA previous research post already highlighted the issue about the number of voting phases in 3SF and put forward a solution for that. However, this solution could guarantee the desired properties only if the proposers in the two subsequent slots are also honest.\nThis takes a step in the right direction by reducing the protocol to only one voting phase per slot, but it also introduces a drawback: a lower probability of meeting the required conditions for finality. However, we now show that it is possible to design a protocol that can reduce the number of voting phase to one without requiring that proposers in subsequent slots are honest.\nRecalling the SSF protocol\nWe build our 3SF protocol by using the SSF protocol as starting point, which is summarized by the following picture.\nSSF1035×492 44 KB\nLet us now recall some of the most important concepts.\n(The reader is however encouraged to read the original post.)\nEbb-and-flow construction. SSF consists of two sub-protocols. The first is a PBFT-style (name that comes from this seminal paper) sub-protocol that always ensures safety, even in the case of network partitioning, but it requires at least 2/3 honest stake. The second is the dynamically-available (DA) sub-protocol, which guarantees progress even if some honest validators stop participating, under the assumption of a simple honest majority. A protocol combining these two types of sub-protocols is referred to as an ebb-and-flow protocol.\nCasper-based BFT sub-protocol. The PBFT-style sub-protocol is heavily based on Casper, the Finality Gadget currently used by Gasper. Compared to Gasper, in SSF checkpoints are pairs of blocks and slots, rather than blocks and epochs. This is because, in SSF, in some sense, epochs and slot are the same thing. However, the general rules for checkpoint justification and finalization are the same. Let us briefly recall such rules. First, block finalization is achieved through checkpoint finalization. Specifically, to finalize a block B, we need to finalize some checkpoint (B_\\mathsf{d},s) where B_\\mathsf{d} is a descendant of B and s \\geq \\mathsf{slot}(B_\\mathsf{d}). Second, checkpoint finalization is achieved in two stages: first checkpoints are justified, then a justified checkpoint is finalized. Let us now explain how this process works. The genesis checkpoint (B_\\mathsf{genesis},0) is by definition both justified and finalized. Justification and finalization of any other checkpoint is achieved through FFG-votes which are votes for links between checkpoints. They are of the form C_\\mathsf{s} \\to C_\\mathsf{t}, where C_\\mathsf{s} and C_\\mathsf{t} are called the source and target checkpoints, respectively.  A checkpoint (B,s) is justified if there are FFG-votes  from \\geq 2/3 of the validators (weighted by stake) for a link C_\\mathsf{s} \\to (B,s) where C_\\mathsf{s} is a justified checkpoint. A checkpoint (B,s) is then finalized if (B,s) is justified and there are FFG-votes  from \\geq 2/3 of the validators (weighted by stake) for a link (B,s) \\to (B',s+1) with B' descendant of B. In addition to this, in order to be able to finalize blocks within one slot, SSF introduces Acknowledgment votes ((B,s),s) that are, roughly speaking, a “compressed” FFG-vote saying that a validator saw a given checkpoint (B,s) justified at the end of slot s. Anyone who sees checkpoint (B,s) as justified and receives Acknowledgment votes ((B,s),s) from \\geq 2/3 of the validators (weighted by stake) can safely consider checkpoint (B,s) finalized.\nRLMD-GHOST as the DA sub-protocol. SSF leverages RLMD-GHOST as the DA sub-protocol to achieve Properties 2 and 3 above. In RLMD-GHOST, validators votes for blocks. We use the term Head-votes to refer to such votes. RLMD-GHOST works in the sleepy model where validators may fall asleep and stop participating for a period. A key property of RLMD-GHOST is that as long as the network latency is less than \\Delta and less than 1/2 of the validators (weighted by stake) are dishonest, the block proposed by an honest proposer will receive and keep receiving the Head-votes of all active validators. Importantly, RLMD-GHOST comes with the following confirmation rule that allows determining whether a block is confirmed, that is, it will always be part of the canonical chain of any honest valiadator: Any block that is at least \\kappa-deep with respect to the the current canonical chain is confirmed (the head of the canonical chain is 0-deep, its parent is 1-deep and so on). We refer to this rule as the \\kappa-deep confirmation rule. The value \\kappa represents the number of slots that we need in order to be sure, except for a negligible probability, that at least one of the proposers in these slots is honest. Clearly this means that \\kappa >> 1.\nFast confirmation. Compared to the \\kappa-deep confirmation rule, fast confirmation allows confirming a block within the same slot it has been proposed. However, it requires at least 2/3 of the validators (weighted by stake) to be honest and active. Fast confirmation of a block occurs when more than 2/3 of the Head-votes from all active validators (weighted by stake) are received after a time delay of \\Delta from when they were sent. In the diagram above, the Head-votes are cast at 4$\\Delta s$ + \\Delta, meaning they are sent \\Delta time within slot s. If by 4$\\Delta s  + 2\\Delta$ an honest validator receives more than 2/3 of the Head-votes (weighted by stake) for a specific block B, then B is considered fast confirmed.\nIntegration between the BFT and DA protocols. In SSF, honest validators determine the FFG-vote to cast as follows. The source checkpoint corresponds to the greatest justified checkpoints in their view, which is the justified checkpoint with the highest slot (no two different justified checkpoints for the same slot can every be justified unless > 1/3 of the stake is slashed). The slot of the target checkpoint corresponds to the current slot while the block corresponds to the block fast confirmed by the DA protocol, if there exists one, or the highest block confirmed by the DA protocol via the \\kappa-deep confirmation rule, otherwise.\nSlots of length 4\\Delta. Except when specifically stated, to align with previous literature, in the rest of this post, we assume that voting phases take just \\Delta time, that is, we do not explicitly account  for the extra latency introduced by voting phases. Then, in SSF, each slot has length 4\\Delta.\nBuilding our 3SF Protocol\nWe now start building our 3SF protocol by taking the SSF protocol and applying the following modifications to have one single voting phase per slot.\nFFG-votes are cast together with the Head-votes at \\Delta time into a slot, where \\Delta represents the network latency.\nRemove fast-confirmations.\nHence, the target of the FFG-votes is the longest chain confirmed by the DA protocol. Under this condition, the DA protocol ensures that if the proposer is honest, all honest validators see the same chain as confirmed by the DA protocol.\nNo Acknowledgment votes.\nBy removing one phase, the length of the slot then decreases to 3\\Delta. The resulting protocol can be schematized as follows.\nattempt11052×432 30 KB\nLet us now take a look at how a run of such a protocol would look like even under the simplifying assumption that we have two consecutive honest proposers and the usual assumptions that (i) < 1/3 of the validators (weighted by stake) are dishonest and the (ii) network delay is less than \\Delta.\nThe proposer in slot s proposes block B.\nDue to the DA protocol’s properties, all honest validators in slot s cast an Head-vote for B. However, given that B has just been proposed, B cannot also already be part of the chain confirmed by the DA protocol. This means that the FFG-votes sent by honest validators have as target a strict ancestor of B. All validators have the same view of the chain confirmed by the DA protocol. This means that they all send FFG-votes with the same target (B_\\mathsf{a},s) with B_\\mathsf{a} ancestor of B. (From now on, let us use the notation B_\\mathsf{a} \\preceq B to mean that B_\\mathsf{a} is a non-strict ancestor of B, and B_\\mathsf{a} \\prec B to mean that it is a strict ancestor, that is, B_\\mathsf{a} \\preceq B means B_\\mathsf{a} \\prec B or B_\\mathsf{a} = B.)\nThe proposer in slot s+1 proposes a block B' child of B and packs into it all the FFG-votes sent in slot s.\nValidators voting in slot s+1, then see (B_\\mathsf{a},s) as justified. This means that they cast an FFG-vote (B_\\mathsf{a},s) \\to (B'_\\mathsf{a},s+1) (we use the notation C_\\mathsf{s} \\to C_\\mathsf{t} to indicate an FFG-vote with source C_\\mathsf{s} and target C_\\mathsf{t}), where B'_\\mathsf{a} \\prec B'. However, given that we have dropped fast-confirmation, the DA protocol only confirms via the \\kappa-deep confirmation rule. Given that \\kappa > 1, this implies that B'_\\mathsf{a} \\prec B. As a consequence of this, we are at most able to justify a strict ancestor of B, not B, with the votes sent in this slot, which makes impossible to finalize B in the next slot.\nThe above shows us that we do need fast-confirmations. Therefore, let’s reintroduce them into the protocol just presented. Specifically, for FFG-votes, we use the chain fast-confirmed in the previous slot, rather than the one from the current slot, to determine the target block. This is because, given that Head-votes and FFG-votes are cast at the same time, clearly we cannot use the Head-votes cast at a given slot to determine the target of the FFG-votes cast in the same slot. We just need to wait \\Delta from the voting time to perform fast-confirmation. As a consequence of the above, we are back to a slot of length 4\\Delta as shown by the following picture illustrating this last protocol.\nattempt21236×432 34.2 KB\nLet us now take a look at whether this is going to work.\nIn slot s, the block in the target checkpoint of FFG-votes cast by honest validators is either the chain confirmed by the DA protocol (via the \\kappa-deep rule mentioned above), or the chain fast-confirmed at the slot before. This turns out to be an issue as some validators may have fast-confirmed a block B^{\\mathsf{fastconf}} in the slot before, some others might have not. So, it is very well possible that some validators cast an FFG-vote with target block B^{\\mathsf{conf}}, others an FFG-vote with target block B^{\\mathsf{fastconf}}_\\mathsf{a}. The problem is that, even though B^{\\mathsf{conf}}_\\mathsf{a} \\prec B^{\\mathsf{fastconf}}, by using the justification rules of SSF/Gasper this leads to a situation where we do not justify any checkpoint for slot s, which seems like a step backwards rather then forward.\nHow do we fix this?\nNote that when we finalize a checkpoint with block B, we finalize the entire chain, not only B. We could do the same for justified checkpoints. That is, if we justify (B,s) we also consider any checkpoint (B_\\mathsf{a},s) where B_\\mathsf{a} \\preceq B as justified. However, this is not enough to address the issue highlighted from the last example, as different validators might FFG-vote for different checkpoints, even though the blocks included in such checkpoints are on the same chain.\nWe need to take this a step further:\nAn FFG-vote (B_\\mathsf{s},s_\\mathsf{s}) \\to(B_\\mathsf{t},s_\\mathsf{t}), provided that the source checkpoint (B_\\mathsf{s},s_\\mathsf{s}) is justified, is considered contributing to the justification of any checkpoint (B',s_\\mathsf{t}) where B_\\mathsf{s}\\preceq B' \\preceq B_\\mathsf{t},that is, B’ is any block between B_\\mathsf{s} and B_\\mathsf{t} included. Note that the slot in the target checkpoint and the justified checkpoint are the same.\nIf we have FFG-votes from \\geq 2/3 of the validators (weighted by stake) contributing to the justification of (B',s_\\mathsf{t}), then (B',s_\\mathsf{t}) is considered justified.\nBefore proceeding, let us go over the example below to ensure that this new justification rule is clear. We consider 3 slots, slot 0, 4, and 6. For each of these slots, say s, the Figure below shows the list of possible target checkpoints for FFG-votes cast in slot s. This includes all the checkpoints comprising any of the blocks received up to the voting time in slot s, except for the block proposed in slot s, paired with slot s.\njustification2833×1133 144 KB\nWe assume just 4 validators, all with the same stake.\nLet us consider first the FFG-votes between (B0,0) and checkpoints for slot 4. If we take B to be such that B0 \\preceq B \\preceq B2, then we have 3 \\geq \\frac{2}{3} 4 FFG-votes between (B0,0) and a checkpoint (B_\\mathsf{d},4) such that B \\preceq B_\\mathsf{d}.\nGiven that B0 \\preceq B, as per our rule, (B0,4), (B1,4) and (B2,4) are justified. However, (B3,4) is not justified as only the FFG-vote (B0,0) \\to (B3,4) is in support of its justification.\nNow we move to the FFG-votes between checkpoints for slot 4 and checkpoints for slot 6. Note that by the reasoning above all of these FFG-votes have a justified checkpoint as source. Now, take B to be such that B2 \\preceq B \\preceq B4. Note that we have 3 FFG-votes between a justified checkpoint (B_\\mathsf{a},4) and a checkpoint (B_\\mathsf{d}.6) such that B_\\mathsf{a}\\preceq B\\preceq B_\\mathsf{d}. As per our rule, this means that (B2,6), (B3,6), and (B4,6) are justified. Importantly, (B0,6) and (B1,6) are not justified, despite (B0,4) and (B1,4) being justified. For (B1,6), this is because we only have the two FFG-votes (B1,4) \\to (B4,6) in support of its justification. Whereas, for (B0,6) we have no FFG-vote is support of its justification.\nThis new rule clearly allows having more than one justified checkpoint for a given slot. This is OK, as they would be on the same chain. However, when computing the best justified checkpoint, using the checkpoint slot is not sufficient anymore. Then, we naturally define the ordering between checkpoints that have the same slot number to be according to the block’s proposed slot so that if \\mathsf{slot}(B) < \\mathsf{slot}(B'), then (\\mathsf{slot}(B'),s) is greater than (\\mathsf{slot}(B),s).\nThen, let’s resume the previous example with this new protocol assuming that only the proposer of slot s is honest (in addition to the usual assumptions):\nIn slot s, any honest validator cast an FFG-vote C_\\mathsf{s} = (B_{\\mathsf{aa}},s') \\to C_d = (B_\\mathsf{a},s), with C_\\mathsf{s} and C_d potentially different for each validator, but with the property that B_\\mathsf{aa} \\preceq B_\\mathsf{a} \\prec B. This set of FFG-votes then leads to justifying at least a checkpoint (B_\\mathsf{j},s) with B_\\mathsf{j} \\prec B.\nAlso, all honest validators fast-confirm B.\nThe proposer of slot s+1 can propose whatever they want, or nothing at all.\nIn slot s+1, any honest validator casts an FFG-vote (B_\\mathsf{j},s) \\to (B,s+1), where B_\\mathsf{j} can be potentially different for each validator, but B_\\mathsf{j} \\prec B. This is because, from the point above, we know that there is at least a justified checkpoint for slot s. Overall, this means that this set of FFG-votes justifies (B,s+1). So far so good! It looks like we are headed in the right direction!\nThe proposer of slot s+2 can propose whatever they want, or nothing at all.\nGiven the rules on checkpoint ordering that we have established above, any honest validator sees (B,s+1) as the greatest justified checkpoint. Then, any honest validator cast an FFG-vote (B,s+1) \\to (B'',s+2) where B'' can be potentially different for each validator, but, due to the DA protocol’s properties, B \\prec B''. Using the Gasper/SSF finalization-rule this would not necessarily yield the  finalization (B,s+1). However, this can be easily fixed. Let’s see how!\nNote that in Gasper/SSF, in an FFG-vote (B,s+1) \\to (B',s+2), block B' doesn’t carry any meaningful information about whether it is safe to finalize (B, s+1). Such a vote is saying that the validator signing it saw (B,s+1) as the greatest justified checkpoint by the time they vote in slot s+2 (which is exactly what an Acknowledgment vote does in the SSF protocol). Hence, such a validator would then be slashed if it in a later slot (that is, in a slot > s + 2) it signs an FFG-vote with source a checkpoint lower than (B,s+1). This means that we can modify the finalization rule as follows.\nIf we have \\geq 2/3 of the validators (weighted by stake) FFG-voting (B,s+1) \\to (B',s+2), with B' potentially different for each validator, but such that B \\preceq B', then (B,s+1) is finalized.\nWith this last modification, at step 6 above, we finalize block B as we were hoping to do!\nIs this protocol safe?\nFair question! In order to make this protocol safe we only need to slightly modify the slashing rules.\nLet us start by recalling the slashing rules employed by Gasper which are the same used by SSF (to be precise SSF has an additional rule that uses for Acknowledgment votes, but we can ignore it as we do not use that type of vote).\nFirst, a validator is slashed if it sends two different FFG-votes but with target checkpoints for the same slot.\nSecond, a validator is slashed if it sends two FFG-votes (*,\\mathit{s1}_\\mathsf{s})\\to(*,\\mathit{s1}_\\mathsf{t}) and (*,\\mathit{s2}_\\mathsf{s})\\to(*,\\mathit{s2}_\\mathsf{t}) such that \\mathit{s1}_\\mathsf{s} < \\mathit{s2}_\\mathsf{s} and \\mathit{s2}_\\mathsf{t} < \\mathit{s1}_\\mathsf{t}. This is called a surround vote as the first vote surrounds the second one.\nGasper/SSF ensures that if two conflicting blocks are finalized, then we can slash at least 1/3 of the validator set.\nWe now show that the two rules above are not sufficient for 3SF.\nNote that in Gasper/SSF, it is perfectly legal for a validator to cast two FFG-votes with source checkpoints from the same slot. This fine as Gasper/SSF ensures that no two different checkpoints for the same slot can ever be justified (unless > 1/3 of the validators, weighted by stake, are dishonest). However, with our modified protocol, this is not true any more. This means that in our protocol, it is currently possible to cast two FFG-votes (B,s) \\to (B_d,s+1) and (B_\\mathsf{a},s) \\to (B',s+2), where both (B,s) and (B_\\mathsf{a},s) are justified, with B_\\mathsf{a} \\preceq B, and B' conflicting with B_d. This means that the first FFG-vote can contribute to finalizing checkpoint (B,s) and the second one to justifying the higher, but conflicting checkpoint, (B',s+2) from which then it is possible to finalize such a checkpoint without committing any slashsable offense.\nFortunately, the fix to this issue is straightforward. We extend the Gasper/FFG slashing rules with the following one.\nTwo FFG-votes (\\mathit{B1},s) \\to (*,\\mathit{s1}_\\mathsf{t}) and (\\mathit{B2},s) \\to (*,\\mathit{s2}_\\mathsf{t}) with \\mathsf{slot}(\\mathit{B1}) < \\mathsf{slot}(\\mathit{B2}) and \\mathit{s2}_\\mathsf{t} < \\mathit{s1}_\\mathsf{t} constitutes a slashable offense (surrounding vote).\nAs we can see, with this rule, the situation described above is prevented.\nThe newly introduced slashing rule requires accessing the block’s slot number. This poses a slight problem as checkpoint FFG-votes do not include the full block, but only the hash and we need to be able to determine validators’ slashability just by looking at the FFG-votes that they cast, without needing to have received the actual blocks that the refer to via their hashes.\nThis is easily solved by extending checkpoint to be triples (H,s,p), rather than just tuples (H,s), where H is the hash of a block (say B), s is the checkpoint’s slot and p is B's block slot, that is, p = \\mathsf{slot}(B). Naturally, this also means that before accepting an FFG-vote (H,s,p) \\to C_\\mathsf{t} as valid and accounting it for justifications/finalizations, we need to check that B.\\mathsf{slot}=p, where B is the block with hash H.\nWhat about the other properties?\nSo far, we have just addressed the first property of our interest: If the network latency is lower than \\Delta and >= 2/3 of the validator set (by stake) is honest and actively participate, then a block proposed by an honest node is finalized in a short and predictable amount of time.\nWhat about the other two properties that we have listed at the beginning?\nThose properties are already guaranteed by the DA protocol employed by SSF, namely RLMD-GHOST. So, we can either use RLMD-GHOST as our DA protocol or another DA protocol, like TOB-SVD.\nIn our extended technical report, we show in detail how both RLMD-GHOST and TOB-SVD can be used to achieve 3-Single-Slot Finality following the description we just presented.\nCan we do better than 3-Slot-Finality?\nIt turns our that we can actually reduce finalization to just two slots by re-introducing Acknowledgment votes.\nOf course, this mean two voting phases per slot, rather than one, but, as discussed above, this does not impact the actual slot length (that is, when we consider the extra latency required by voting phases), as the distribution of Acknowledgment votes can proceed in parallel to any subsequent phase.\nIn 3SF, Acknowledgment votes would be cast at the fast confirmation time, that is, at 2\\Delta in a slot.\nAs in the SSF protocol, to keep things safe, we would need to add the following slashing rule.\nAn FFG-vote (\\mathit{B1},s1_\\mathsf{s}) \\to (*,s1_\\mathsf{t}) and an Acknowledgment vote ((\\mathit{B2},\\mathit{sa}),\\mathit{sa}) with \\mathit{sa} < s1_\\mathsf{t} and either s1_\\mathsf{s} < \\mathit{sa},  or s1_\\mathsf{s} = \\mathit{sa} and \\mathsf{slot}(\\mathit{B1}) < \\mathsf{slot}(\\mathit{B2}) constitutes a slashable offense.\n",
        "category": [
            "Consensus"
        ],
        "discourse": [
            "single-slot-finality"
        ]
    },
    {
        "title": "Fusion Module - 7702 alternative with no protocol changes",
        "link": "https://ethresear.ch/t/fusion-module-7702-alternative-with-no-protocol-changes/20949",
        "article": "During the development and R&D of Modular Execution Environments and Supertransactions, we uncovered an intriguing mechanism that enables Externally Owned Accounts (EOAs) to function similarly to smart accounts. This innovation can be implemented immediately, without waiting for the 7702 Pectra protocol upgrade whilst at same time being compatible. We’ve conducted testing and developed a proof-of-concept. Let’s delve into the details!\nBackground\nThe blockchain community has engaged in extensive discussions, particularly within the Chain Abstraction and Account Abstraction groups, regarding the role of smart account models in the ecosystem and the potential impact of EIP-7702 on user experience.\nEOAs remain a fundamental element in blockchain infrastructure and while they represent one of the most prevalent wallet types in current use, EOAs lack several essential features that modern Web3 users consider standard requirements for day-to-day operations.\nMotivation\nSmart Contract Accounts (SCAs) represent a significant advancement in blockchain wallet technology, offering programmable functionality that extends far beyond the capabilities of traditional EOAs. The enhanced feature set of SCAs includes:\nTransaction batching\nSpending limits\nAccount recovery mechanisms\nGas fee abstraction\nCross-chain interoperability\nAutomation and scheduled transactions\nChallenges\nThe current implementation of SCAs presents several operational challenges:\nFirst, the requirement to transfer assets from an EOA to utilize an SCA creates unnecessary friction in the user experience and leads to liquidity fragmentation across accounts.\nSecond, the proliferation of different smart account providers across Apps fragments the unified wallet experience users have come to expect. This stands in contrast to the traditional EOA model, where users maintain a single wallet address and consolidated balance across multiple blockchain networks.\nAs illustrated below, the user would have to sign twice to fund their smart account and then start interacting with it. And to make it even worse, the first signature forces user to pay gas themselves, meaning they need to own some native coin. The common 2-step SCA onboarding flow is shown below:\ntest1002×748 42.1 KB\nTwo user signatures, shown in blue, are required for usingthe smart accoun for the first time.\nFurthermore, the implementation of embedded SCAs within dApps introduces additional complexity to asset management. Users must track and manage assets on an application-by-application basis, while placing trust in each application to maintain accessible interfaces for manual asset management.\nSolution - Fusing EOA & SCA!\nOur innovation, which we’ll present shortly, introduces a mechanism that can be utilized today to achieve a virtual merging of EOA and SCA into a single entity. This is accomplished without requiring users to explicitly upgrade their EOA or manually transfer funds to a new address. This breakthrough approach fundamentally resolves the fragmentation, complexity, and user experience challenges that have historically impeded SCA adoption, while maintaining backward compatibility with existing infrastructure.\nEOA Fused\nWe’ve developed a method that allows users to utilize their EOAs as if they were interacting with a smart account. This is achieved while only requiring a single signature for each action, eliminating the need for manual pre-activations, upgrades, or fund transfers to new addresses.\nOur proposal involves precalculating a Smart Account address for each EOA and utilizing it as a companion account. This approach builds upon the established mechanism for generating ERC4337 accounts. The innovation in our solution lies in the ability to transfer tokens to the companion Smart Contract Account and provide bundlers with all necessary additional instructions (UserOps) to be executed - all with a single signature.\nFurthermore, when a user first interacts with our system, the smart contract account is deployed on-demand (lazy deployment), ensuring that even the initial interaction only requires one user signature.\nKey concepts of our approach include:\nUsers need not be aware of their smart account wallet.\nUsers continue to use their EOA as their primary wallet, eliminating the need to explicitly move funds elsewhere.\nUsers never grant access to their entire EOA portfolio to execute complex operations on a small amount of tokens they hold.\nUsers sign ONCE for every interaction, providing a full EOA-like experience while utilizing funds from their EOA as if they were directly using the smart account.\nThe companion smart account processes complex user requests that would not be possible with a standard EOA alone. These include batching multiple operations, interacting with several DeFi protocols simultaneously, and distributing assets to multiple recipients in a single transaction.\nLive Demo\nDemo video\nTechnical Implementation\n…how is it possible to use EOA as if it was an SCA, before Pectra upgrade?\nWe’ve leveraged the fact that appending arbitrary bytes to a valid EVM transaction maintains its validity while including all the appended data on-chain.\nFor instance, if we’re executing a standard ERC20 transfer function:\nWe would typically encode this function as:\nresulting in a transaction data structure:\nHowever, we can append any arbitrary data to the callData field, and the transaction remains valid. The USDC transfer will execute correctly, while the additional appended hex data is simply ignored by the EVM.\nThis is a fundamental aspect of EVM design and how function selectors operate. For illustration, consider this test USDC transfer on Sepolia. The transaction transferred 1 USDC, but examining the input data field (viewed as UTF-8) reveals an additional message (image below).\nAppending ASCII text to a valid ERC20 transaction.1445×903 115 KB\nAppending ASCII text to a valid ERC20 transaction.\nAn important consideration is that the user’s signature covers the entire transaction, including all appended extra data. This approach allows us to execute multiple actions simultaneously:\nTransfer funds (ETH or other ERC-20 tokens) from an EOA to a new address.\nInclude extra data in this action, which is part of the transaction fully signed by the EOA and verifiable on-chain. For example, this extra data could be the UserOp(s) hash to be executed by bundlers.\nBy setting the destination of transferred funds to the user’s smart account address and including extra data (e.g., a userOp hash for operations to execute on the smart account), we achieve both actions with a single signature.\nFusing EOA with companion SCA bundles both deposit and execute operations into a single user signature shown in blue.937×568 45.8 KB\nFusing EOA with companion SCA bundles both deposit and execute operations into a single user signature shown in blue.\nThe only requirement for this to function is that the user’s companion smart account contains a module capable of verifying a userOp given the fully signed & serialized EVM transactions - which we have developed.\nFusion Module (7579)\nThe Fusion Module is a standardized 7579 smart account module capable of validating userOps not only by validating userOpHash signatures but also by validating userOps given the fully signed and serialized EVM transaction.\nThe module maintains universal compatibility with existing ERC-7579 smart account providers while introducing enhanced validation mechanisms, as demonstrated in our reference implementation.\n\nWhen processing a signed EVM transaction as userOp signature, the module executes the following validation sequence:\nTransaction Parsing\nDeconstructs the signed EVM transaction into its constituent components\nCryptographic Verification\nPerforms cryptographic recovery of the transaction signer and validates against the smart account owner’s signature\nData Extraction\nRetrieves extraData parameters from the transaction data field\nHash Verification\nValidates the correlation between userOp hash and extraData field contents\nUpon successful completion of all validation checks, the userOp achieves on-chain execution eligibility, permitting execution by any entity willing to pay for gas costs.\nBy utilizing this standardized module, we maintain compatibility with the 4337 infrastructure and enable any smart account provider to implement one-click onboarding and execution, providing users with an experience akin to direct EOA interaction.\nInfrastructure for Fusion Modules\n… or, who’s paying for gas?\nWhen a user executes an on-chain transaction that approves a userOp, there needs to be an entity willing to submit that userOp to the blockchain, knowing it’s now authorized and ready for execution. This entity must also have a mechanism to collect payment from the user for processing their complex request initiated from the EOA.\nThe current 4337 infrastructure can’t handle this efficiently due to limitations in how paymasters and bundlers are configured. The existing system expects users to either:\nHave native coins pre-staked in the EntryPoint contract\nReceive sponsorship from a paymaster for their operation\nRegular 4337 bundler will reject companion userOp if funds are on the EOA - bundler doesn’t know EOA & SCA are companions!887×582 42 KB\nRegular 4337 bundler will reject companion userOp if funds are on the EOA - bundler doesn’t know EOA & SCA are companions!\nNeither option works effectively for Fusion because bundlers and/or paymasters aren’t aware that the companion SCA has got the EOA address linked to it - who’s funds could be used for paying for fees and onboarding assets.\nOn the other hand, user shouldn’t need to manually stake ETH or fund their SCA with paymaster-accepted tokens - after all, we’re aiming for seamless user experience.\nThis is where Modular Execution Environments (MEE) become crucial. Fusion Modules are integrated as first-class citizens within the MEE stack. The MEE infrastructure, which is briefly introduced here, enables developers to build applications without concerning themselves with the complexities of transaction execution, whether across one chain or many.\nMEE stack supports diverse gas payment models, including scenarios where Fusion Module manages EOA assets through companion SCA accounts.\nRather than standard blockchain transactions, at the heart of MEE, lies a new primitive called Supertransactions. Supertransactions can contain multiple userOps that manage a single SCA, and every Supertransaction is uniquely identified by its hash. This architecture aligns perfectly with the Fusion Modules, enabling users to execute complex cross-chain interactions by signing one root Supertransaction hash. For example, users can sign once to approve Supertransaction which:\nTransfers USDC from their EOA to SCA\nTransfers a portion of USDC from the SCA to the MEE Node (as gas payment)\nSwaps remaining USDC for ETH\nBridges ETH to another chain and transfer it back to the user’s EOA\nThese and many other workflows become one-click experiences through the EOA+SCA pass-through model.\nTo illustrate: consider a user leveraging their EOA as if it were an SCA, without needing to know about the SCA’s existence. They can execute cross-chain swaps, directly approving and spending funds from their EOA, while companion SCA accounts handle the complex operations across multiple chains behind the scenes. The transaction lifecycle of such an operation is laid out below:\nSingle signature user interaction is again shown in blue - where user signed one EVM transaction to both fund their companion SCA AND execute multiple userOps once funded.1137×2766 353 KB\nSingle signature user interaction is again shown in blue - where user signed one EVM transaction to both fund their companion SCA AND execute multiple userOps once funded.\nAfter the dApp prepares the Supertransaction, users only need to sign a single ERC20 transfer transaction (highlighted in blue in the diagram above). The MEE stack then handles all operation orchestration and execution across different blockchain networks. Users simply see the end result: the received asset credited to their EOA on the destination chain.\nAdvanced Signing Schemes\nThe upgrade process described above enables an EOA to gain SCA capabilities. However, users must still maintain a balance of native coins in their EOA to execute Supertransactions with a single signature. This limitation stems from the core design of the EVM, where gas fees must be paid from the public address associated with the transaction-signing private key.\nHowever, there’s an interesting workaround. Through extensive exploration of different approaches to appending Supertransaction hash, we’ve discovered several alternative methods that allow users to spend assets from their EOA without paying gas fees directly.\nERC20Permit (Gasless EOA)\nWe leverage the ERC20Permit standard for supported tokens (such as USDC) to deliver a seamless, gasless experience where EOAs can execute Supertransactions without even having any ETH on their wallet, while still requiring only one off-chain singature!\nERC20Permit standard allows EOAs to approve spending of the tokens off-chain, by only signing an off-chain message of the following structure:\nSince the deadline parameter is already validated through the smart account/ERC-4337 model during userOp validation and execution (via validAfter and validUntil fields), we can repurpose it as a data carrier. This field stores the Supertransaction hash that fully describes the companion SCA operations that will be approved with the signed Permit Message. The deadline field’s uint256 type conveniently matches bytes32, allowing us to accomplish two objectives with a single off-chain signature:\nAuthorize the smart account to spend ERC20 assets by setting the spender address to be equal to the user’s companion SCA address (limited by the amount parameter) - which is what the ERC20Permit standard enables.\nApprove smart account operations that can use up to the specified amount of funds from the EOA by setting the deadline value to the given Supertransaction hash.\nWhen using the ERC20Permit signature, Fusion Module SCA module must:\nParse the ERC20Permit signature\nCall the token’s permit() function when necessary\nExecute the operations defined in the userOp - transferring funds from EOA to SCA and performing any additional specified actions\nThe paymaster can handle gas fees conventionally, deducting them from the same tokens transferred from EOA to SCA. Importantly, all operations occur effectively as a single atomic step - the user signs once, and this signature covers both gas payment and all user-defined actions.\nAs an example: let’s look at the example where user wants to transfer USDC from their EOA to some other address, but has no native coin on their wallet. In this case, they could leverage Fusion Module with the MEE stack and perform this operation with a single off-chain signature. The transaction lifecycle of such an operation is laid out below:\nSingle signature user interaction show in blue - where user signed an off-chain ERC20Permit message to both fund their SCA in a gasless way, and then execute userOp once funded.1137×2156 260 KB\nSingle signature user interaction show in blue - where user signed an off-chain ERC20Permit message to both fund their SCA in a gasless way, and then execute userOp once funded.\nThe only interaction performed by the user is highlighted in blue color in flow from above. This means the EOA still feels like an EOA while more complex flow is being approved and executed by the MEE stack.\nCross-chain Intent Signature (ERC-7683)\nAnother way of fusing EOA with a companion SCA is by packaging Supertransaction hash together with a signed cross-chain intent which is defined by ERC-7683 standard to have the following off-chain message structure:\nBy using the same approach as with the ERC20Permit, we can store the Supertransaction hash inside the bytes orderData field and have the signed off-chain intent be used for both the actual intent settlement and the subsequent operations performed once the intent has been settled.\nBy making Fusion Module compatible with the ERC-7683 standard, we’ve created a powerful synergy that bridges traditional smart contract interactions with intent-based systems. This integration enables users to sign a single intent that can both settle cross-chain operations and trigger complex subsequent actions across multiple chains. The result is a more streamlined, user-friendly experience that maintains the security and flexibility of both systems while significantly reducing complexity for end users and developers alike.\nImplications for Smart Accounts Today\nThe signature schemes described above demonstrate how single-signature onboarding enables users to interact exclusively with their EOA while utilizing Smart Accounts as a pass-through mechanism.\nThis architecture enhances the user experience for smart accounts broadly, even when they serve as primary accounts, by enabling one-signature onboarding of EOA assets and operation execution. In essence, the smart account address becomes a true extension of the EOA - any asset in the EOA becomes seamlessly accessible through the Smart Account address.\nOur Fusion Module validator currently supports three signature schemes:\nPlain userOpHash signature\nSigned EVM transaction signature\nSigned ERC20Permit message signature\nThe plain userOpHash signature maintains backwards compatibility, allowing the SCA to work with existing 4337 infrastructure, including bundlers and paymasters processing userOps. The EVM and ERC20Permit schemes extend this functionality, enabling the same SCA to act as a companion account and support Fusion Transactions.\nWe’ve made the module implementation publicly available in our GitHub repository. Through our collaboration with the Rhinestone team, we’re working to have the standardized ERC-7579 module audited and published in the whitelisted modules repository, making it accessible to all smart account providers.\nIntegration of these modules is straightforward, varying based on the dApp’s specific needs. Developers simply need to modify the userOp signature field in their frontend to enable single-signature EOA interactions that function like native smart account operations.\n7702 & Fusion - Different Approaches to the Same Problem\nWe propose this model as a solution that’s implementable today, without waiting for the Pectra upgrade, while supporting almost all features that 7702 will enable as well as being compatible with each other.\nThere are slight differences in capabilities between these models. Here’s a comparison of the pros and cons:\n indicates an advantage over  according to our assessment.\n indicates: “Uncertain, further testing required”\nEvidently, some actions are only possible with 7702, while others can only be performed using the Fusion Module approach. However, at a high level, both approaches address the same issue - how to safely enable EOA to have a SCA functionality.\nImportantly, both models are forward-compatible with the general AA roadmap.\nIt’s often noted that 7702 will enable a gradual transition towards a fully account-abstracted future, as users can upgrade to increasingly complex account implementations on-the-fly.\n7702 and the Fusion module differ in that Fusion module maintain separate account spaces: the EOA remains an EOA, while the SCA exists as a companion account, used only to spend funds that the user explicitly approves at the moment of execution.\nConversely, 7702 fully converts the EOA into an SCA - a stateful change. The EOA remains upgraded and points to an SCA implementation until an upgrade to a new account implementation is executed.\nWhile both approaches have their merits and enable EOA accounts to function like SCAs, Fusion Modules allow users to switch between implementations on every request, eliminating the need to store the active implementation.\nThey can be viewed as a pathway to adopting smart accounts by enabling:\nLevel 1 adoption: Using SCAs as a pass-through only - retaining assets in the EOA unless necessary, and transferring only what’s required to process the request\nLevel 2 adoption: Using SCAs as the primary wallet and onboarding funds by bundling the deposit transaction with the first userOp\nConclusion\nThe Fusion Module is a step forward in web3 account evolution, offering immediate benefits while staying compatible with future upgrades.\nImmediate Benefits\nUsers can start using smart account features today\nNo need to wait for protocol upgrades\nSimple one-signature experience\nWorks with existing infrastructure\nFuture Compatibility\nThe Fusion Module is designed to work alongside EIP-7702 when it launches. While 7702 will transform EOAs into SCAs directly, Fusion Module offers a complementary approach by keeping EOAs and SCAs separate but connected. This means:\nUsers can start with Fusion Module today and seamlessly transition to 7702 when ready\nProjects can build with Fusion Module now, knowing their implementations will remain valid post-7702\nSome features unique to Fusion (like single-signature multichain operations) will complement 7702’s capabilities\nPath Forward\nWe see Fusion Module as an important bridge to the future of account abstraction. It enables immediate adoption of smart account features while the ecosystem continues to evolve. Whether users eventually choose to fully upgrade their EOAs with 7702 or continue using the Fusion Module approach, they’ll have the flexibility to choose what works best for their needs.\nThe module is open source and ready for integration. We invite developers to try it out and help shape the future of wallet interactions in Web3. If you’d like  to enable smart contract capabilities on the EOA for your dapp or wallet, or just test it out, feel free to reach out here and we’ll help!\n",
        "category": [
            "Applications"
        ],
        "discourse": [
            "account-abstraction"
        ]
    },
    {
        "title": "Transport privacy exploration of the Validator-Relayer Builder API",
        "link": "https://ethresear.ch/t/transport-privacy-exploration-of-the-validator-relayer-builder-api/21050",
        "article": "Special thanks to @Nero_eth and @liobaheimbach\nAbstract\nThe availability of metadata from the networking layer of Ethereum, particularly in Proposer-Builder Separation (PBS)-enabled environments poses real and immediate privacy risks. Among other concerns, it allows and incentivizes adversaries to target and interfere with block production processes to prevent certain transactions or even entire blocks from being processed. Our experiment, “Metaclear,” investigates how transport-level privacy leaks at validator-relay interfaces allow targeted disruption via metadata analysis and low-cost network-layer attacks. We implemented a transport metadata harvesting pipeline of the full MEV architecture stack to link validators’ public keys to the IP address of their consensus client as well as that of the mev-boost software and executed attacks in a lab setting to demonstrate the practical applications of this work. We found that once de-anonymized, block proposers became vulnerable to network attacks aimed at interfering with future block proposals. We also identified scenarios where different parties besides proposers can be attacked when transport privacy is unprotected. Through this experiment, we want to i) challenge the trust assumptions in MEV infrastructure such as relayers, ii) advocate researchers and engineers to integrate network-level metadata privacy protocols into the design of enshrined PBS, and iii) expand the scope of MEV considerations to include transport-layer metadata.\nTable of contents\nAbstract\n1. Introduction\n1.1. MEV Relay in PBS\n1.1.1 Definition of terms\n1.2. Privacy issues in Relays\n1.3. Randomness generation by validators\n1.4. Contributions\n2. Methodology\n2.1. Metadata collection in Relay HTTP calls\n2.2. Attestation data collection\n2.3. De-anonymization\n2.4. Simulation of network attacks\n3. Setup and environment\n3.1. Architecture\n3.2. Environment and test parameters\n4. Results\n4.1. Linking public key and IP addresses of validators\n4.2. Identifying victim validators\n4.3. Viable network attacks\n5. Discussion: Further Attack scenarios and Network Consequences\n5.1. Network topology can be mapped using Relay metadata\n5.2. Solo stakers are more vulnerable to attack\n5.3. RANDAO can be exploited and manipulated\n5.4. Blobs can be disrupted\n5.5. Recently bootstrapped, sparsely connected beacon nodes are more vulnerable to “covert flash attacks”\n5.6. Selectively attack multi-block MEV\n5.7. Malicious Relay can more easily conduct a metadata-based attack without leaving traces\n5.8. Decentralizing Relays makes attacks more likely, not less\n5.9. Valuable metadata can make Relay a victim\n5.10. Builders can be made to underperform\n5.11. A new class of MEV?\n6. Limitations\n7. Future research\nBibliography\n1. Introduction\nOne standard definition of Maximal Extractable Value (MEV) defines MEV as “the maximum value that can be extracted from block production […] by including, excluding, and changing the order of transactions in a block [1].” However, these are not the only actions which can be taken by network participants to disrupt or extract value. Indeed, the current focus on mempool data – comprising pending transactions [2] – provides a limited perspective and overlooks other potentially valuable sources of data that could be exploited for extracting value.\nOne such underexplored source is the data collected from the networking layer that underpins blockchain systems. For a distributed network to function properly, various subprotocols coordinate states among nodes, such as the broadcasting of mempool transactions and the gossiping of attestations. We [3] and other researchers [4] have previously shown how data harvesting from these low-level communication protocols and collecting relevant metadata can be a source of valuable information, for example by linking validator public keys to their IP address. Once parties involved in block production have been de-anonymized, adversaries can target them, interfering with their block production processes and potentially excluding expected transactions from a block or entire blocks altogether.\nThis risk becomes even more pronounced in a Proposer-Builder Separation (PBS) - enabled environment, where the introduction of builders – responsible for constructing blocks – adds complexity to the communication protocol. When validators connect to a block-building service that contains a trusted party in the middle to moderate block delivery, the trusted party processes privileged information including validator metadata. When validators connect to a decentralized network of builders, their metadata is exposed to more parties. With more metadata to observe, the potential for privacy leakage increases, creating opportunities for MEV disruption and perhaps extraction. However, the potential for MEV extraction in PBS has not been thoroughly explored [5].\nThis experiment, named “Metaclear”, seeks to assess the feasibility of these attacks by examining transport-level privacy in the context of PBS in a practical manner and in a controlled lab environment, with a specific focus on privacy leaks at the interfaces between validators and relays. “Metaclear” reveals how metadata can de-anonymize stakeholders involved in block creation and demonstrates potential methods for exploiting these vulnerabilities to disrupt block production and extract additional value which falls outside the traditional definition of MEV.\n1.1. MEV Relay in PBS\nThe PBS scheme was introduced as part of Ethereum’s transition to Proof-of-Stake (PoS), aiming to bifurcate block production into two distinct roles: block builders, responsible for collecting and ordering transactions, and proposers, tasked with proposing new validated blocks. While the design and specifications have been discussed since 2018 [6], consensus on the design of enshrined PBS (ePBS) or in-protocol PBS (IP-PBS) has not yet been reached. Various proposals for ePBS, such as two-slot PBS [7], single-slot PBS [8], Protocol-Enforced Proposer Commitment (PEPC) [9], and Payload-timeliness committee (PTC) [10], emphasize different assumptions about the desired degree of decentralization, economic incentives, and appropriate positioning on the spectrum of censorship resistance.\nCurrently, the most widely adopted implementation by block proposers, based on slot share [11], is an out-of-protocol PBS called MEV-Boost. MEV-Boost is a market-driven block-building platform developed by Flashbots [12]. In a nutshell, MEV-Boost requires at least one trusted centralized middleman called “Relay”, which sits in between block builders and Ethereum validators and which prevents validators from extracting value from builders and also prevents builders from withholding information from validators. Validators run an instance of MEV-Boost alongside their client software so that when they are selected as a block proposer, MEV-Boost requests block-building information from the Relay. Block contents are fed by block builders at the beginning of a slot. The Relay implements the latest Builder API specification [13].\nFigure 1 Architecture of MEV-Boost, an implementation of PBS by Flashbots1498×410 220 KB\nFigure 1 Architecture of MEV-Boost, an implementation of PBS by Flashbots [14]\nThe terms “proposer” and “builder” can carry different meanings depending on the context in which they are discussed.\nIn the Ethereum Proof of Stake context, the term “proposer” refers to one of the main duties of validators. When a validator is randomly chosen for a specific slot, their responsibility is to propose a new block containing transactions for that slot. The term “builder” refers to the block builders who are responsible for creating blocks of transactions and offering them to the block proposer for each slot [5]. For example, MEV-Boost (from the mev-boost instance running along the beacon client to the entire marketplace with block builders and searchers) is considered a “builder” within PBS implementations [14].\nIn the context of MEV-Boost as one PBS implementation with a block-building marketplace, “MEV Boost” has three major components (see Figure 1 for its architecture): Builder, Relay, and mev-boost. Here, “Builder” refers to block builders who provide the ordered payload of a full block considering MEV extraction and reward distribution. “mev-boost” in blue in Figure 1 refers to the narrow sense of mev-boost, a sidecar middleware run by validators, which provides access to the “MEV-Boost” marketplace.\nWhen considering the “Relay” component of MEV-Boost, “builder” has the same meaning as in the MEV-Boost context, referring to block builders external to MEV-Boost. Here, builders call the Relay HTTP API [15]. “Proposers” are validators with mev-boost middleware. Relays implement the Ethereum Builder API specs [13] to handle API calls from mev-boost middleware.\nSince our research is focused on the architecture of MEV-Boost, for clarity we will use the same terminology as defined for the Relay component.\n1.2. Privacy issues in Relays\nRelays play a special role in current PBS, because they protect searchers and builders from proposers who may try to steal the value of bundles and blocks via, e.g., unbundling. Relays also provide validity checks on transactions with an EVM execution client [16]. However, the centralized and intermediary nature of Relays requires both validators and builders to trust Relays in two different ways: first, that the data delivered by the Relays is accurate, and second that the Relays themselves will not actively attack the validators or builders. Indeed, facets of Relay design such as payload sharing and value spoofing [17] have been exploited in the past [18].\nThese trust assumptions are well known, and countermeasures are already implemented such as signatures and mechanisms against known unbundling attacks which would prevent relays from allowing arbitrary insertion or deletion of transactions from a bundle [19, 20]. However, the true extent of the trust assumptions is much broader, and less explored. Relays possess the advantage of collecting metadata of validators and builders. Metadata leaks via HTTP calls can reveal information about nodes’ IP address and location within the networking layer [21, 22]. By correlating data collected on the consensus layer and more, nodes can be identified with relative ease and thus become a target of attacks on the networking layer. Proposals such as Block Negotiation Layer (BNL) [23] intend to improve network security by modifying the Relay mechanism with the proposer’s intent. However, despite the strong demand for protecting validator privacy and even an explicit desire to separate on-chain addresses from IP addresses [24], the issue of metadata privacy leaks at the transport layer remains unaddressed, leaving a significant gap in proposer security. In fact, the original MEV Boost architecture explicitly mentioned that “communication between MEV-Boost and relays […] must protect validator privacy by not associating validator key and IP address” [24]. Yet, no technical solutions were implemented to protect validator privacy.\n1.3. Randomness generation by validators\nRandomness plays an important role in selecting block proposers in Ethereum [25]. The RANDAO value is designed to be unpredictable to the other validators as other validators cannot predict a proposer’s entropy contribution without knowing their private keys. However, a proposer’s contribution to the randomness calculation can be influenced by a malicious proposer choosing between broadcasting or withholding a block [26], or by accidentally missed proposals due to network conditions [27].\nBut there is a third scenario we would like to highlight: block proposers can be purposefully disrupted by third parties to ensure they miss their proposal slot. This scenario is feasible thanks to the transport-layer privacy leaks outlined above.\n1.4. Contributions\nIn our work we i) showcase the feasibility of metadata collection by privileged Relays in a sandbox environment, ii) identify the true trust assumptions surrounding Relays and demonstrate how Relays can de-anonymize validators and conduct attacks without leaving traces, iii) construct a sandbox environment to conduct attacks on the network layer and demonstrate the negative consequences of such attacks on block production, iv) outline attack scenarios on other components, v) discuss possible ways to target these attacks to extract value, as a potential new class of MEV and vi) suggest changes in protocol to prevent and mitigate the effects of these metadata leaks, particularly on solo stakers.\n2. Methodology\nIn order to gather metadata from the HTTP calls utilized in MEV-Boost-Relay, we first identified all relevant HTTP calls used by the Relay. After that, we made changes to the Relay source code to allow for metadata collection. We then set up a sandbox Ethereum testnet using the Kurtosis “ethereum-package” from EthPandaOps [28] to test the new setup. The sandbox environment[1] includes services such as consensus clients, execution clients, validators, MEV flood simulator, transaction and blob spammer, beacon metrics gazer, beacon explorer, networking tools and containers, and Grafana dashboards. Using the metadata collected from the HTTP calls, as well as additional data about validators, we compiled a dashboard to reveal the identities of validators and to highlight important information that was subsequently used to conduct network attacks within the sandbox environment.\n2.1. Metadata collection in Relay HTTP calls\nFocusing on the most popular MEV-Boost-Relay implementation[2], validators and block builders communicate with Relays via RESTful HTTP APIs [29]. Specifically, MEV-Boost communicates with the Builder API (Proposer API) [13, 14], while block builders interact with the Relay API [15]. The Relay, in turn, communicates with its beacon client through the Beacon API [30]. We extended the sequence diagram of MEV-Boost [14] to provide an overview of HTTP calls within the Relay, as illustrated in Figure 2.\nFigure 2 Sequence diagram of MEV Relay, extracted from source code flashbots/mev-boost-relay.1080×843 50 KB\nFigure 2 Sequence diagram of MEV Relay, extracted from source code flashbots/mev-boost-relay [31]. See a zoomable version here.\nBelow is a summary of relevant HTTP calls:\nProposer APIs called by mev-boost implemented in the Relay\nRelay API called by block builders implemented in the Relay\nSelected BeaconAPIs used by the Relay:\nOur experiment focuses on extracting metadata, particularly the IP addresses and client types, from the `X-Forwarded-For` and `X-Real-IP` headers within the source code[3].\n2.2. Attestation data collection\nOur team at HOPR previously conducted research on “validator sniping” [3], which involved linking validators’ IP addresses with their public keys collected from validator attestations. In this experiment, we expanded our modifications to beacon clients to include aggregated attestations as an additional data source for inferring the most likely IP address of a validator.\nWe calculated the percentage of occurrences for each validator’s public key and peer ID pair. For each validator public key, we identified the pair with the highest occurrence percentage as the most probable match. This allowed us to link the validator’s IP address with their public key.\nEach consensus client may serve multiple validators. To locate a validator with their consensus client’s public-facing IP address, we first established the link between validators’ public keys and the peer ID used in the networking layer of consensus clients from collected attestations. We then listed the percentage of occurrences of a certain peer ID paired with a specific pub key from received attestations, and. We calculated the percentage of occurrences for each validator’s public key and peer ID pair. For each validator public key, we identified the pair with the highest occurrence percentage as the most probable match. We further associated the obtained public key and peer ID pairs with nodes’ multi-addresses, as the link between peer ID and multi-addresses is created when establishing connections in the libp2p protocol.\n2.3. De-anonymization\nWe started by matching the IP addresses we gathered from HTTP calls with the public keys that validators provided during the registerValidator call. This helped us connect validator public keys with their corresponding MEV-Boost IP address.\nNext, we used validator attestations to link the most likely validator public key with their Beacon Client IP address.\nFinally, by combining these connections, we were able to acquire both the MEV-Boost and Beacon Client (or consensus client) IP addresses for each validator. With this information, we could estimate the location of a validator, as these processes necessitate low latency and are therefore likely to be situated in close geographic proximity.\n2.4. Simulation of network attacks\nTo conduct production-level network attacks on production servers in the real world, a significant amount of resources is required. We have simplified the test to demonstrate the technical viability of network attacks. As more than 90% of denial of service (DoS) attacks use TCP and TCP SYN flooding is the most commonly used attack [32], we started the network attacks with a raw ICMP and TCP flood. We used the “syn_flood” service to simulate a network adversary, sending out an ICMP ping flood and TCP SYN flood using ‘iputils-ping’ and ‘hping3’ respectively to beacon client, validator clients, and mev-boost instances. To analyze the impact on memory and network performance, the changes were inspected using the Kubernetes dashboard, as shown in Figure 4 and onwards.\nFor a more efficient simulation of network attacks, the Attacknet [33] developed by Trail of Bits was utilized to execute memory attacks, bandwidth attacks, and clock skew attacks.\n3. Setup and environment\nThe simulation environment is an Ethereum testnet in a sandbox based on the Kurtosis Ethereum package developed by EthPandaOps. It has been modified for specific data collection and analysis purposes. Key modifications are given in the architecture breakdown. The source code can be found at hoprnet/metaclear-ethereum-package[4].\n3.1. Architecture\nWe modified the implementation of various components used in MEV-Boost to collect relevant data and metadata, as highlighted in yellow in Figure 3.\nFigure 3 Testing architecture, adapted from MEV-Boost Relay architecture diagram 34960×540 63.6 KB\nFigure 3 Testing architecture, adapted from MEV-Boost Relay architecture diagram [34]. The system (circled in dark blue) is divided into three main sections: Proposer, Relay, and Block Builders. Components highlighted in yellow indicate the “metaclear-” forked versions of the original implementation, where modifications have been introduced to collect data and metadata. Components filled with blue are services for testing system behaviors under network attacks.\nProposer Section:\nBeacon Node: Lighthouse client that manages consensus and coordinates the actions of validators. Here it has been enhanced with “metaclear” modifications to extract public keys from attestation and aggregated attestations, as well as the mapping of public keys and multi-address from the networking layer. The docker image that it runs is `hoprnet/metaclear-lighthouse`[5].\nMEV-Boost: A middleware used by validators to interact with the MEV-Boost marketplace.\nExecution Node: Geth client running `ethereum/client-go:latest`, which is the execution environment where transactions are processed.\nValidators: 64 validators per proposer instance. It manages the duties of an Eth2.0 validator, including proposing and attesting to blocks.\nRelay Section:\nServices: The main service of the metaclear MEV-Boost Relay is where HTTP calls are handled. The single instance of housekeeper coordinates sync between beacon, db, and APIs. The website displays data from the datastore. The major modifications are as follows.\n\nAPI: Handles all the external facing calls of proposer APIs and block builder APIs. Metadata is collected from the HTTP headers.\nHousekeeper: As it updates known validators and proposer duties, it’s modified to store those data persistently.\n\n\nAPI: Handles all the external facing calls of proposer APIs and block builder APIs. Metadata is collected from the HTTP headers.\nHousekeeper: As it updates known validators and proposer duties, it’s modified to store those data persistently.\nClients: A local beacon and an execution client to gain knowledge on validator activity and simulate blocks submitted from block builders. It uses the same modified image as nodes in the Prosper sections.\nData store: Data management component of the metaclear MEV-Boost Relay. It consists of an in-memory cache, Redis, and Postgres database. Here below are the major modifications.\n\nPostgres: Two new tables are created to collect HTTP header metadata, proposer duties, and RANDO values.\n\n\nPostgres: Two new tables are created to collect HTTP header metadata, proposer duties, and RANDO values.\nBlock Builders Section:\nBlock builder (MEV-Flood): Deploys mock Uniswap contracts and builds bundles on those mock Uniswap transactions. These entities are responsible for constructing blocks that are optimized for MEV extraction. In this architecture, one block builder instance is shown interacting with the Relay. The “flashbots/mev-flood” package is used to create mock MEV bundles.\nNetwork tools:\nSyn_flood: A dedicated service used to SYN flood targets, where it runs a containerized `hping3` TCP/IP packet assembler.\nTools such as `iputils-ping` and `tcpdump` are installed in the services, along with beacon client instances and mev-boost instances.\nAttackNet and Chaos-mesh: Tools used for injecting faults and simulating network attacks on the system. We created three types of test suites[6]: “memory-stress” for memory exhaustion, “network-bandwidth” for saturated bandwidth, and “clock-skew” for synchronization disruption.\n3.2. Environment and test parameters\nAll the images are run in minikube with docker as VM driver\nMinikube v1.33.1 on Darwin 14.6.1 (arm64) with memory of 4GB\nKubernetes v1.30.0 on Docker 26.1.1\nKurtosis engine 1.1.0\nMEV flood generates bundles every 15 seconds. The interval between slots is reduced to 6 seconds from the standard 12s for faster testing. The number of consensus layer (CL) clients varies between 2 to 3 in different testing scenarios. Each CL instance contains 64 validator public keys.\nIn the TCP SYN flood, 15000 packets are sent from random IP sources to the observed IP address and port. In the memory stress chaos test, the test plan has three scenarios, targeting one validator client (“vc-1-geth-lighthouse”), one mev-boost instance (“mev-boost-2-lighthouse-geth”), and one beacon client (“cl-3-lighthouse-geth”). Each belongs to a different beacon instance. For each scenario, 50 workers stress 10 MB/worker for a total of 10 minutes.\n4. Results\nWe compiled a Grafana dashboard to collect and visualize metrics from the metaclear experiment and visualize how a malicious Relay could identify validators with attestations from beacon clients and metadata from HTTP requests.\n4.1. Linking public key and IP addresses of validators\nPublic keys of validators can be linked with their IP addresses from two datastreams. One datastream is validator attestation. The link between validator public key to their consensus layer (CL) client peer IDs can be probabilistically established by observing aggregated and non-aggregated attestations, as shown in Figure 4(b). As peer IDs in CL can be translated into multiaddress and thus IP addresses, as shown in Figure 4(a), links between validator public keys with IP addresses can be obtained via CL peer IDs, as in Figure 4(c).\nFigure 4a) displays information from an unmodified Lighthouse implementation, including the link between peer ID and IP address, the current beacon epoch, and the current head slot.1920×620 57.3 KB\nFigure 4b) features a table that calculates the correlation between public keys and peer IDs derived from attestations. Higher percentages in this table indicate a stronger probability of the peer ID being associated with the corresponding public key.1920×1249 219 KB\nFigure 4c) uses the most probable associations between public keys and peer IDs to establish a mapping between the public key and the IP address of the validators' consensus layer (CL) client.1920×744 111 KB\nFigure 4 Grafana dashboard for MEV-Boost-Relay and their beacon nodes, labelled as a), b), and c) from top to bottom. Figure 4a) displays information from an unmodified Lighthouse implementation, including the link between peer ID and IP address, the current beacon epoch, and the current head slot. Figure 4b) features a table that calculates the correlation between public keys and peer IDs derived from attestations. Higher percentages in this table indicate a stronger probability of the peer ID being associated with the corresponding public key. Figure 4c) uses the most probable associations between public keys and peer IDs to establish a mapping between the public key and the IP address of the validators’ consensus layer (CL) client.\nThe other datastream is the HTTP API calls requested to MEV Relays.\nFigure 5 shows the Metaclear Relay dashboard, which displays how the MEV-Boost Relay can track the IP address of mev-boost instances of validators through the ‘registerValidator’ HTTP call, which occurs at the outset of launching the mev-boost instance. It’s important to note that multiple relays can be assigned to mev-boost, allowing the IP address to be shared with all the provided MEV-Boost Relays.\nAdditionally, it monitors the frequency (per second) of each HTTP call to detect the submission pattern of block builders. The green line, representing the ‘POST /eth/v1/builder/blinded_blocks’ endpoint, reflects the rate of block builders delivering blocks. Moreover, the MEV-Boost Relay gathers the IP addresses of block builders.\nFigure 5 Metaclear Relay dashboard displays the association between the validator’s public key, their peer ID, and the IP:port of their mev-boost instance at the launch of the mev-boost.1600×689 179 KB\nFigure 5 Metaclear Relay dashboard displays the association between the validator’s public key, their peer ID, and the IP:port of their mev-boost instance at the launch of the mev-boost. It also monitors the rate (per second, average over 1 minute) of each HTTP call to identify the submission pattern of block builders.\n4.2. Identifying victim validators\nThe Metaclear Relay dashboard also extracts the proposers of the next epoch, as shown in Figure 6. As proposers are unique once selected for a slot in a given epoch, they are the most obvious choice of victims for network-layer attacks after being deanonymized. However, we will outline possible attacks on other parties in the network later. Those proposers are computed by RANDAO [25].\nFigure 6 Metaclear Relay dashboard displays validators selected as proposers of the next epoch1920×876 170 KB\nFigure 6 Metaclear Relay dashboard displays validators selected as proposers of the next epoch and historical RANDAO values.\n4.3. Viable network attacks\nWe conducted a memory stress attack on three different clients around 15:13 one validator client (“vc-1-geth-lighthouse”), one mev-boost instance (“mev-boost-2-lighthouse-geth”), and one beacon client (“cl-3-lighthouse-geth”), each belonging to a different beacon instance as shown in Figure 7.\nThe attack caused the respective clients to become overwhelmed. As a result, the validator client and the beacon client stopped producing blocks and processing slots. As shown in Figure 7, the block production metric came to a halt for client number 3 (green line in Figure 7), where the consensus client was under attack. Block production stopped for client number 1 (blue line in Figure 7), where the validator stopped functioning. Block production continued as usual for client number 2 (yellow line in Figure 7), where mev-boost was under attack. However, blocks were produced only locally but not from mev-boost, as shown in Figure 8.\nFigure 7 Number of successfully produced blocks around the time of the attack.1574×904 53.3 KB\nFigure 7 Number of successfully produced blocks around the time of the attack. Instances number 1 (blue) and 3 (green), where the validator client and beacon client were under attack respectively, stopped producing slots.  Instance number 2 (yellow), where the mev-boost was under attack, could still produce blocks locally.\nFigure 8 shows the number of blocks constructed by the block builder instance and subsequently sent to proposers via the metaclear Relay instance. After the attack (after the dotted line), no blocks were delivered through the relay. This is due to instances 1 and 3 being unable to fulfill their proposer duties. Even though instance 2 was still able to propose blocks, they could not be proposed via the Relay because its MEV-boost functionality was not operational.\nFigure 8* Rate per minute of slots containing blocks built by the block builder and delivered to proposers via meta-clear Relay.1600×748 92.3 KB\nFigure 8 Rate per minute of slots containing blocks built by the block builder and delivered to proposers via meta-clear Relay.\nBy observing the HTTP endpoint request rate in Figure 9, it is clear that block builders kept posting blocks to the Relay (continuous red and orange lines). However, the request rate of other endpoints (other lines) dropped to 0, indicating they were not called.\nFigure 9 Metaclear-Relay HTTP endpoint request rate (per minute) around the attack time1600×948 190 KB\nFigure 9 Metaclear-Relay HTTP endpoint request rate (per minute) around the attack time\nThe block explorer gives us a more intuitive overview of slot production status, as seen in Figure 10. All slots were successfully proposed before the attack. After the attack, the only block proposer was client number 2, while clients 1 and 3 missed their slots.\nFigure 10 Explorer of block production before (left) and after (right) the attack.1920×1141 176 KB\nFigure 10 Explorer of block production before (left) and after (right) the attack.\n5. Discussion: Further Attack scenarios and Network Consequences\nOur Metaclear experiment focused on demonstrating the practicality of identifying and disrupting block proposers using leaked metadata. However, it is feasible that similar approaches to deanonymizing actors in the network could enable other attacks against other actors, including the Relay itself. The following sections outline briefly how the same metadata could be used to probabilistically identify the roles of different nodes and the kinds of attacks which could be conducted as a result, along with possible broader consequences for the entire network.\n5.1. Network topology can be mapped using Relay metadata\nThe MEV-Boost Relay has access to ample metadata to provide an advantage in mapping the network topological and geographical distribution of validators and builders. Validators rely on MEV-Boost instances to produce blocks with higher returns. This incentivizes the deployment of mev-boost and beacon nodes in a network configuration that reduces latency, likely in the same geographical area. By cross-checking the IP addresses of mev-boost collected from ‘registerValidator’ calls with the beacon IP addresses correlated from observing attestations, the MEV-Boost Relay can map out the topology and location of validators more reliably.\nBy grouping validators by IP addresses, Relayers can use IP ranges to infer the network topology. For example, home stakers can be identified by residential IP addresses and they would likely experience higher latency due to multiple network layers before reaching the device. Cloud-hosted validators can easily be identified by finding providers from IP address registries. Staking pools generally exhibit a dense concentration of validators per IP address, which implies that they apply some network security rules similar to other nodes in the same pool. These are just some of the examples of inferences which might be drawn, with differing levels of confidence. Determining the nature of a particular validator’s hardware and relationship to other validators opens up the possibility of more nuanced targeted attacks.\n5.2. Solo stakers are more vulnerable to attack\nFor reasons of security, decentralization and fairness, it is generally considered desirable for solo stakers to comprise a significant proportion of validators [35], although there is debate about the extent to which stakers with less powerful hardware should be supported. However, even well-equipped solo stakers are disproportionately vulnerable to the attacks outlined in our research. They are generally easier to identify [36], easier to attack, and would be less likely to notice that they have been attacked.\nSolo stakers and home stakers generally do not have ample resources for Distributed Denial of Service (DDoS) protection, such as alternative fallback IP addresses. Due to the usage of consumer-grade hardware, physical limitations on memory and computation power, as well as the bandwidth on the router and switches, solo stakers are more exposed to DDoS attacks. Additionally, DDoS attacks against a single home staker can be targeted extremely well; the attack only needs to be effective for a few seconds to cause a slot to be missed, making such attacks cheap to execute and hard to detect. Attackers can identify solo stakers via attestations alone, but the extra data collected from Relayer would make the validator de-anonymization more efficient.\nSince a DDoS attack can mimic the effects of other non-malicious errors, it is not always possible to identify when an attack has occurred. Solo stakers propose fewer blocks than other classes of proposer, and are much less likely to have robust hardware setups. Therefore, it will be harder for them to gather data to prove an attack, and they may be more likely to ascribe other causes such as a faulty setup or poor connection.\nAttacking solo validators during block proposals can provide significant advantages to professional validators. These entities typically have the resources and expertise to carry out such attacks efficiently and profitably, aiming to repeatedly disrupt the reward-earning potential of solo stakers. Over time, this strategy can force solo validators to leave the network after repeatedly failing to produce blocks, allowing professional validators to capture a larger market share and exert greater control over block production.\nThe disproportionate effects of these attacks on solo stakers could harm attempts to encourage them to join and remain in the network.\n5.3. RANDAO can be exploited and manipulated\nRANDAO is the random number generator used by Ethereum in various parts of its consensus mechanism, including determining which validators will be selected to propose upcoming blocks. RANDAO values generated in a particular epoch are used to assign duties in two epochs’ time, allowing chosen validators enough time to prepare for their roles. This delay presents an opportunity for malicious actors, however: by observing when a victim is selected as proposer by the RANDAO, the attacker has a time advantage of one epoch to prepare a DDoS attack (although this will change if secret leader elections are implemented) [37].\nBut it is possible to more actively interfere with RANDAO to manipulate future outcomes. Missing a slot, either deliberately or through unresponsiveness or delay, does not introduce entropy in the next RANDAO calculation and thus gains potentially multiple slots for free without increasing its stake fraction. Researchers have already identified the possibility of choosing to forego block rewards in exchange for generating a favourable RANDAO result in two epochs’ time [38]. Our research suggests a further possibility: proposers could be deliberately targeted by competing proposers to miss their block in order to achieve the same result.\nThis could be part of a single attack to benefit from a known high-value upcoming event in a particular epoch, or part of a more concerted effort to accelerate the k-tailed RANDAO takeover [26, 36].\n5.4. Blobs can be disrupted\nBy occupying network bandwidth, a validator may fail to send their attestation to data availability sampling (DAS) in time, which affects the result of attestation on blob data [39]. Additionally, a node could selectively respond to sampling requests while selectively providing malicious responses to data requests by other nodes. This could be made feasible by identifying nodes uniquely based on their IP address as outlined above.\n5.5. Recently bootstrapped, sparsely connected beacon nodes are more vulnerable to “covert flash attacks”\nThe higher the number of validators per beacon node, and the more sparse that beacon’s connections to the rest of the network, the more vulnerable it is to a “covert flash attack” [40]. A covert flash attack happens when the sybils of an attacker connect to the victim and behave properly long enough to build up scores in GossipSub protocol before executing a coordinated eclipse attack when the victim needs to propose a block. Such a covert flash attack executed at the time when a validator proposes a block could be considered a new type of MEV, if as a result the attacker is able to extract the value which the attacked proposer would have otherwise claimed for themselves.\n5.6. Selectively attack multi-block MEV\nEven in the narrow definition, MEV is not always confined to individual blocks. It is possible to extract MEV across multiple neighbouring blocks, known as multi-block MEV (MMEV), by having two or more consecutive block proposers collude and jointly create blocks in a row [41]. This is already known to be a high risk MEV tactic, but the disruptive attacks shown in our research increase this risk further. If a block proposer comes under attack and cannot produce the block at a given slot, this missing block will affect the MMEV, often with significant losses. For example, several proposers might collude to extract MMEV by manipulating on-chain time-weighted average price (TWAP) oracles. If a colluding proposer is attacked and fails to produce the promised block, the TWAP will not be manipulated, not only losing the MMEV but potentially resulting in a significant loss for MEV searchers. In an advanced attack scenario, the attacker would control the slot immediately after the targeted proposer. In this case, the attacker would be able to place a backrun order to exploit the original MMEV searcher.\nAlternatively, MMEV could be de-risked by taking out other proposers. If, for example, a large validator controls slots 5 and 8 in an epoch, they could disrupt the nodes of proposers responsible for slots 6 and 7 in order to launch a multi block MEV strategy. This is especially viable if the proposers of slots 6 and 7 have been identified as home stakers which can likely be taken out successfully by cheap and highly targeted attacks.\n5.7. Malicious Relay can more easily conduct a metadata-based attack without leaving traces\nDespite Relays being monitored on the reliability of block delivery, there is no measure in place to prevent them from leaking metadata on validators. When such metadata leaks are exploited by network attacks to kick out block proposers, they leave no definitive trace that would incriminate them. As outlined above, in operation, validators, especially solo stakers who get to produce blocks at a lower frequency than professional node operators, cannot differentiate a purposeful attack from poor network conditions.\n5.8. Decentralizing Relays makes attacks more likely, not less\nIt can be tempting to view issues like the ones outlined in this research as “teething troubles” which will be resolved once the network becomes more decentralized. But decentralization is unlikely to resolve the issue of high trust assumptions in Relays. In fact, it may make it worse: although validators may want to connect to multiple relays for more bids from more block builders and/or more reliable receipt of externally built blocks, connecting to multiple relays would leak their metadata to more parties in the network, each a potential attacker. It would also make it even harder to identify who the attacker was.\n5.9. Valuable metadata can make Relay a victim\nMuch of the foregoing has focused on an untrustworthy Relay as an attacker, but Relays themselves are also exposed to attack. As a result of reducing latency, Relay, e.g. ultrasound relay, actively disabled Cloudflare service to gain 10 ms of latency advantage [42]; although it eliminates the time for round-trip traffic routing in Cloudflare’s network, it also exposes Relay to DDoS attacks. As the validator registration information is publicly available via the Relay’s Data API, attackers can scrape the latest registration data via public endpoints to obtain all the relays that a given validator connects to. Attackers can then prepare DDoS attacks on a validator that will soon become a block proposer, with a maximum of one epoch time ahead. Ideally, this validator only connects to one Relay, so that the target for a DDoS attack is minimal. When Relay fails to deliver a block in time, the victim proposer would need to build a block using only the local mempool and thus lose their MEV profit.\nIt would be even more beneficial for attackers to attack Relay in the window right after the proposer broadcasts the signed header but before the payload gets published. This would not only damage the reputation of the proposer but also push down the stable operation rate of Relay, gradually making block builders and validators abandon their service. If the attacker happened to be a competitor of Relay, they might attract alienated validators to their service, increasing revenues and getting access to even more valuable metadata.\n5.10. Builders can be made to underperform\nLast but not least, block builders also expose their IP addresses to Relays. A malicious attacker could target builders connected to profitable searchers, in the hope that those searchers would move from the now underperforming builders to other block builders.\n5.11.  A new class of MEV?\nAlthough the standard definition of MEV restricts itself to actions taken when constructing blocks, we would argue that many of the targeted attacks shown to be possible via this research also qualify as MEV. Metadata can be used to deanonymize other players in the cutthroat MEV game, identify their role in the network and target them with disrupt DDoS attacks which prevent them from extracting MEV they would otherwise expect. In addition to harming competitors, attackers can take advantage of favourable slot ordering to claim this MEV for themselves. Particularly resourceful (in every sense of the words) attackers could even manipulate RANDAO to engineer these favourable slot positions, rather than waiting for circumstances to align.\nOne key difference: the fairness and ethics of MEV extraction as usually defined is subject to much debate. However, the attacks identified in our research seem unambiguously negative for the health and security of the network. Smooth and reliable block production is a fundamental part of blockchain utility. Incentives to disrupt the block production process and even manipulate RANDAO generation can only be bad for all parties in the long run.\nNot all of these attacks seem equally feasible or likely, and this is far from an exhaustive list of possible disruptive actions, but we hope this illustrates the broader point that failures in transport level metadata privacy leave all parties exposed. Even more concerning, many of these attacks can be carried out with no evidence of who the perpetrator is.\nIn general, all parties engaged in capturing MEV under the standard definition benefit from minimizing latency in the MEV pipeline [43]. Some argue that the co-location of builders with relay and relay with validators may become a driver for a more distributed MEV infrastructure [42]. However, a distributed MEV infrastructure does not necessarily translate into a more distributed Ethereum network, hence there’s no direct contribution to a more robust Ethereum network. Due to the risk of leaking metadata privacy, bootstrapping a rather isolated node makes it susceptible to networking-level attacks before it establishes a stable and truthful connection with the rest of the Ethereum network. Therefore, protecting metadata privacy is particularly important to a more distributed Ethereum network with more sparsely-connected nodes.\nTo mitigate these risks, a promising approach involves integrating network-level metadata privacy protection protocols directly into the networking layer of Ethereum clients when designing PBS specifications. An effective protocol should be easily integrable with the Ethereum protocol, provide out-of-the-box transport layer anonymity for REST-JSON Builder API calls, maintain low latency to ensure that necessary communications can be completed within a slot time, and avoid any centralized points of failure that could become vulnerabilities for privacy leaks. Additionally, generalizing Ethereum’s peer-to-peer networking protocols to support an overlay of metadata-private protocols, such as a mixnet, would further enhance privacy protections across the network. These solutions are already viable and could be implemented to significantly bolster the security of Ethereum’s infrastructure against potential MEV exploits.\nPrivacy-critical API calls such as `registerValidators` are one-time requests which are not time-critical. Using metadata-private protocols to protect such API calls are given. However, protecting one single endpoint does not prevent metadata leaks because the link between validator public keys and IP addresses can be derived from other endpoints. Partial protection is no protection.\nGranted that any overhead on the p2p networking layer would introduce additional latency, which is generally undesirable in MEV games, networking-level privacy protection makes the (re-)distribution of MEV fairer and enhances the resilience of the entire network by protecting solo stakers from network attacks.\nWhile the community seems to be focusing on faster commitment, i.e., single slot finality, single slot secret leader election, should we leave some space for a fair and secure environment for solo stakers and home stakers, and consider a reasonably longer slot time and eventually a longer epoch time, such that large majority of transactions can still be gossiped through the network, even from one sparsely connected node to another sparsely connected node. When there’s sufficient time for transactions to be propagated across the network, the speed of propagation becomes less important as the information asymmetry eventually gets canceled out.\n6. Limitations\nWhile the “Metaclear” experiment effectively demonstrates the potential for network disruption and potential new kinds of MEV extraction through metadata analysis, it has limitations. The simulated memory exhaustion attack is not practically feasible in most real-world deployments due to robust resource allocations. Attacks such as botnet-driven DDoS could achieve the same or better results, but these are hard to test ethically. Such botnets are readily available and inexpensive at a cost of 200$/day [44], which could make MEV extraction highly profitable: based on the average profit of 0.025 ETH/block at time of writing, up to $450,000 a day of value in total MEV is available.\nAdditionally, technical challenges prevented the simulation of network bandwidth and clock skew attacks, which are also known to exacerbate privacy leaks and disrupt block production. Despite these constraints, “Metaclear” highlights significant risks and underscores the need for stronger countermeasures.\n7. Future research\nOur current version of Metaclear ran in a contained environment with a limited number of proposers, relays, and block builders. We also assumed that validators had participated in the network for long periods and had gathered sufficient data to derive networking information on other parties.\nWe would like to extend the experiment to:\nRun in a more realistic environment\nExplore the possibilities for attacks when parties are more distributed yet still co-located and hold more computational resources.\nBuild a Proof-of-Concept (PoC) that integrates network-level metadata privacy protection protocol solutions like uHTTP [45].\nCarry out experiments of network bandwidth attack when validators send attestations to blob in data availability sampling (DAS).\nWe also hope to further explore questions such as:\nHow effectively can we de-anonymize validators and builders in the real world?\nHow much additional latency can the MEV-Boost design tolerate while remaining fair?\nHow do we quantify the fairness of the distributed Ethereum network?\nAre the attacks we have identified a new class of MEV, an extension of existing MEV, or something else?\nHow large an anonymity set can be obtained when applying network-level metadata privacy protection protocol solutions like uHTTP [45]?\nBibliography\n[1]\t“Maximal extractable value (MEV) | ethereum.org.” Accessed: Oct. 09, 2024. [Online]. Available: Maximal extractable value (MEV) | ethereum.org\n[2]\tG. Damalas and P. Ambrus, “An introduction to maximal extractable value on Ethereum,” Mar. 2023. [Online]. Available: https://assets.ey.com/content/dam/ey-sites/ey-com/en\\_us/topics/financial-services/ey-an-introduction-to-maximal-extractable-value-on-ethereum.pdf\n[3]\tS. Bürgel and L. Pohanka, “Proof-of-Stake Validator Sniping Research,” HOPR. [Online]. Available: Proof-of-Stake Validator Sniping Research | by Dr. Sebastian Bürgel | HOPR | Medium\n[4]\tL. Heimbach, Y. Vonlanthen, J. Villacis, L. Kiffer, and R. Wattenhofer, “Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue,” Sep. 06, 2024, arXiv: arXiv:2409.04366. doi: 10.48550/arXiv.2409.04366.\n[5]\tEthereum Foundation, “Ethereum Roadmap: PBS and MEV,” ethereum.org. [Online]. Available: Proposer-builder separation | ethereum.org\n[6]\tV. Buterin, “Proposer/block builder separation-friendly fee market designs - Economics,” Ethereum Research. [Online]. Available: Proposer/block builder separation-friendly fee market designs\n[7]\tV. Buterin, “Two-slot proposer/builder separation - Proof-of-Stake,” Ethereum Research. [Online]. Available: Two-slot proposer/builder separation\n[8]\tV. Buterin, “Single-slot PBS using attesters as distributed availability oracle - Proof-of-Stake,” Ethereum Research. [Online]. Available: Single-slot PBS using attesters as distributed availability oracle\n[9]\tB. Monnot, “Unbundling PBS: Towards protocol-enforced proposer commitments (PEPC) - Economics,” Ethereum Research. [Online]. Available: Unbundling PBS: Towards protocol-enforced proposer commitments (PEPC)\n[10]\tM. Neuder, “Payload-timeliness committee (PTC) – an ePBS design - Proof-of-Stake,” Ethereum Research. [Online]. Available: Payload-timeliness committee (PTC) – an ePBS design\n[11]\tT. Wahrstätter, “MEV-Boost Dashboard,” mevboost.pics. [Online]. Available: https://mevboost.pics/mevboost.pics\n[12]\tFlashbots Ltd, “MEV-Boost in a Nutshell,” MEV-Boost in a Nutshell. [Online]. Available: https://boost.flashbots.net/\n[13]\tEthereum Foundation, “Builder-API.” [Online]. Available: Builder-API\n[14]\tFlashbot/MEV-Boost. Go. Flashbots. [Online]. Available: GitHub - flashbots/mev-boost: MEV-Boost allows Ethereum validators to source high-MEV blocks from a competitive builder marketplace\n[15]\tFlashbots Ltd, “Relay-API.” [Online]. Available: Relay-API\n[16]\tM. Neuder, “Relays in a post-ePBS world - Proof-of-Stake,” Ethereum Research. [Online]. Available: Relays in a post-ePBS world\n[17]\tC. Hager, “Post-mortem for a relay vulnerability leading to proposers falling back to local block production (Nov. 10, 2022) - Relays,” Nov. 2022. [Online]. Available: Post-mortem for a relay vulnerability leading to proposers falling back to local block production (Nov. 10, 2022) - Relays - The Flashbots Collective\n[18]\tlotusbumi, “MEV-Boost Security Assessment (audit),” Jun. 2022. [Online]. Available: mev-boost/docs/audit-20220620.md at 4035cb3c8c8f9b0118a0170049203f0167c604a0 · flashbots/mev-boost · GitHub\n[19]\tR. Miller, “Post mortem: April 3rd, 2023 mev-boost relay incident and related timing issue - The Flashbots Ship,” Apr. 2023. [Online]. Available: Post mortem: April 3rd, 2023 mev-boost relay incident and related timing issue - The Flashbots Ship - The Flashbots Collective\n[20]\tM. Sproul, “Unbundling attacks on MEV relays using RPC,” 12:00:00+10:00. [Online]. Available: Unbundling attacks on MEV relays using RPC\n[21]\tG. D. Bissias, M. Liberatore, D. Jensen, and B. N. Levine, “Privacy Vulnerabilities in Encrypted HTTP Streams,” in Privacy Enhancing Technologies, G. Danezis and D. Martin, Eds., Berlin, Heidelberg: Springer, 2006, pp. 1–11. doi: 10.1007/11767831_1.\n[22]\tF. D. Smith, F. H. Campos, K. Jeffay, and D. Ott, “What TCP/IP Protocol Headers Can Tell Us About the Web,” ACM SIGMETRICS Perform. Eval. Rev., vol. 29, no. 1, pp. 245–256, Jun. 2001, doi: 384268.378789.\n[23]\tŁ. Miłkowski, “Realigning block building incentives and responsibilities - Block Negotiation Layer - Proof-of-Stake / Block proposer,” Ethereum Research. [Online]. Available: Realigning block building incentives and responsibilities - Block Negotiation Layer\n[24]\tS. Gosselin, “MEV-Boost: Merge ready Flashbots Architecture - The Merge,” Ethereum Research. [Online]. Available: MEV-Boost: Merge ready Flashbots Architecture\n[25]\tB. Edgington, Upgrading Ethereum | 2.9.2 Randomness. 2023. [Online]. Available: Upgrading Ethereum | 2.9.2 Randomness\n[26]\tK. Alpturer and S. M. Weinberg, “Optimal RANDAO Manipulation in Ethereum,” Sep. 29, 2024, arXiv: arXiv:2409.19883. [Online]. Available: [2409.19883] Optimal RANDAO Manipulation in Ethereum\n[27]\tM. Kalinin and D. Ryan, “EIP-4399: Supplant DIFFICULTY opcode with PREVRANDAO,” Ethereum Improvement Proposals. [Online]. Available: EIP-4399: Supplant DIFFICULTY opcode with PREVRANDAO\n[28]\tB. Busa and P. Jayanthi, “Kurtosis: A Deep Dive to Local Devnets.” [Online]. Available: Kurtosis: A Deep Dive to Local Devnets · ethPandaOps\n[29]\tFlashbots Ltd, “Relay API Documentation.” [Online]. Available: https://flashbots.notion.site/Relay-API-Documentation-5fb0819366954962bc02e81cb33840f5\\#854339c909a042d0bbca6e8f8069674e\n[30]\tEthereum Foundation, ethereum/beacon-APIs. (Oct. 09, 2024). HTML. ethereum. [Online]. Available: GitHub - ethereum/beacon-APIs: Collection of RESTful APIs provided by Ethereum Beacon nodes\n[31]\tflashbots/mev-boost-relay. (Apr. 03, 2024). Go. Flashbots. [Online]. Available: GitHub - flashbots/mev-boost-relay: MEV-Boost Relay for Ethereum proposer/builder separation (PBS)\n[32]\tH. Wang, D. Zhang, and K. G. Shin, “Detecting SYN flooding attacks,” in Proceedings.Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies, Jun. 2002, pp. 1530–1539. doi: 10.1109/INFCOM.2002.1019404.\n[33]\t“crytic/attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks.” [Online]. Available: GitHub - crytic/attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks\n[34]\tFlashbots Ltd, “Running MEV-Boost-Relay at scale.” [Online]. Available: Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.\n[35]\t“Vitalik Buterin supports lowering Ethereum solo staking requirement,” Cointelegraph. Accessed: Oct. 14, 2024. [Online]. Available: https://cointelegraph.com/news/vitalik-buterin-advocates-lowering-solo-staking-eth\n[36]\t“Solo stakers: The backbone of Ethereum,” Rated blog. [Online]. Available: Solo stakers: The backbone of Ethereum — Rated blog\n[37]\tEthereum Foundation, “Secret leader election,” ethereum.org. [Online]. Available: https://ethereum.org/en/roadmap/secret-leader-election/\n[38]\tT. Wahrstätter, “Selfish Mixing and RANDAO Manipulation - Consensus,” Ethereum Research. [Online]. Available: Selfish Mixing and RANDAO Manipulation\n[39]\tF. Damato, L. Zanolini, and R. Saltini, “DAS fork-choice - Consensus,” Ethereum Research. [Online]. Available: DAS fork-choice\n[40]\tD. Vyzovitis, Y. Napora, D. McCormick, D. Dias, and Y. Psaras, “GossipSub: Attack-Resilient Message Propagation in the Filecoin and ETH2.0 Networks,” 2019.  [2007.02754] GossipSub: Attack-Resilient Message Propagation in the Filecoin and ETH2.0 Networks\n[41]\tT. Mackinga, T. Nadahalli, and R. Wattenhofer, “TWAP Oracle Attacks: Easier Done than Said?,” in 2022 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), May 2022, pp. 1–8. doi: 10.1109/ICBC54727.2022.9805499.\n[42]\tBell Curve, Shining A Light On MEV  | Tarun Chitra, Justin Drake, (Apr. 05, 2023). [Online Video]. Available: https://www.youtube.com/watch?v=fWRboyGk\\_lc\n[43]\t0xTaker and Frontier Research, “Exploration of MEV Latencies,” Exploration of MEV Latencies. [Online]. Available: Exploration of MEV Latencies\n[44]\t“Dark web price of malware/DDoS services 2023,” Statista. [Online]. Available: Dark web price of malware/DDoS services 2023 | Statista\n[45]\tu(nlinked)HTTP-lib. (Oct. 03, 2024). TypeScript. HOPR. [Online]. Available: uHTTP-lib/ONBOARDING.md at fe38143e0b23ee7d81f8d8941f044261de74f320 · hoprnet/uHTTP-lib · GitHub\nSource code of metaclear-ethereum package GitHub - hoprnet/metaclear-ethereum-package: A Kurtosis package that deploys a private, portable, and modular Ethereum devnet ↩︎\nChecking the source code within the Github repos of top Relays in https://www.relayscan.io/ indicates that they are variants of Flashbots’ MEV-Relay ↩︎\nSource code of metadata handling in metaclear-mev-relay https://github.com/hoprnet/metaclear-mev-relay/blob/a96c4b6e9df7e7788a308bd10e3efe0ea3b6316d/services/api/service.go\\#L1003-L1042 ↩︎\nSource code of metaclear-ethereum package GitHub - hoprnet/metaclear-ethereum-package: A Kurtosis package that deploys a private, portable, and modular Ethereum devnet ↩︎\nSource code of metaclear-lighthouse GitHub - hoprnet/metaclear-lighthouse: Ethereum consensus client in Rust ↩︎\nSource code of metaclear-attacknet GitHub - hoprnet/metaclear-attacknet: Tool and testing methodology for subjecting blockchain devnets to simulated network and side channel attacks ↩︎\n",
        "category": [
            "Privacy"
        ],
        "discourse": [
            "p2p",
            "mev",
            "proposer-builder-separation"
        ]
    },
    {
        "title": "On Blob Markets, Base Fee Adjustments and Optimizations",
        "link": "https://ethresear.ch/t/on-blob-markets-base-fee-adjustments-and-optimizations/21024",
        "article": "Special thanks to Ansgar, Barnabé, Alex, Georgios, Roman and Dankrad for their input and discussions, as well as Bert, Gajinder and Max for their efforts on this topic!\nThe tl;dr:\nAll the suggested updates make sense in theory. Should we do them in Pectra - depends.\nRaising the min blob fee allows faster price discovery.\nAutomating the blob gas update fraction makes it future-proof.\nNormalizing the excess gas prevents an edge gas where the blob base fee drops after a fork that increases the target, though, nothing bad can happen if we don’t do it.\nMaking the base fee scaling symmetric ensures the mechanism stays as-is (scales \\pm 12.5% at the extremes of 0 and 6 blobs)\nI would propose to summarize all those changes in EIP-7762 and ship them together when we change the target/max from 3/6 to 4/6. If we’re afraid that Pectra might grow too big with adding yet more changes, I’d propose them in the following order with decreasing importance:\nIncrease blob count.\na) Do 4/6, being conservative, or something like 6/9 if we feel more confident.\nb) Ship EIP-7623 to ensure the EL payload size is significantly reduced to make room for more blobs.\nShip the outlined base fee changes.\nRecap of the Blob Fee Mechanism\nWith the launch of EIP-4844, Ethereum added a new dimension to its fee market. Blobs, coming with their own base fee, provide dedicated space for data, allowing applications and rollups to post information on-chain without requiring EVM execution.\nThe blob fee structure is governed by a base fee update rule, which approximates the formula:\nIn this equation, excess_blob_gas represents the total surplus of blob gas usage compared to the target amount (TARGET_BLOB_GAS_PER_BLOCK per block). Like the EIP-1559 fee mechanism, this formula is self-adjusting: as the excess blob gas increases, the base_fee_per_blob_gas rises exponentially, which discourages excessive usage and nudges the excess back toward a level at which rollups perceive the base fee as a “fair” price.\nThe process operates as follows: if block N consumes X blob gas, then in block N+1, the excess_blob_gas increases by X - TARGET_BLOB_GAS_PER_BLOCK. Consequently, the base_fee_per_blob_gas for block N+1 adjusts by a factor of e**((X - TARGET_BLOB_GAS_PER_BLOCK) / BLOB_BASE_FEE_UPDATE_FRACTION).\nThe parameter BLOB_BASE_FEE_UPDATE_FRACTION controls the maximum possible rate of change for the blob base fee. This rate is set to achieve a target maximum increase and decrease of approximately 1.125 per block, based on e**(TARGET_BLOB_GAS_PER_BLOCK / BLOB_BASE_FEE_UPDATE_FRACTION).\nIn the initial rollout, blob prices were expected to be low, with gradual increases until the market finds an equilibrium or “fair” price (i.e., price discovery). The blob fee market introduced by EIP-4844 follows a structure similar to EIP-1559, with a base fee that adjusts dynamically based on demand.\nAs of November 2024, Ethereum has reached a level of demand where rollups would stop posting blobs no matter what the base fee is, but instead post an amount of blobs that keeps the base fee quite stable. People like calling that a phase of “price discovery”, even though it just means that at that specific point in time a certain base fee X is regarded as a fair price. At the time of “price discovery”, rollups would no longer consistently post 6 blobs per block without considering the blob base fee, as it is no longer negligible. However, an increasing demand for blobs without an increasing supply (i.e. more blobs available) will lead to higher blob fees. For the following, forgive me when using this simplified concept of “price discovery,” even though prices are discovered every 12 seconds with every slot.\nProposed Adjustments, and their Pros & Cons\nLooking ahead to the upcoming Pectra fork, there is a clear demand for scaling blobs (from 3/6 to 4/6 or, more sophisticated, 6/9), which could necessitate adjustments to the blob fee market.\nIn the following sections, I will outline 4 potential changes to the blob fee market and discuss the associated benefits and challenges for each.\nAdjusting the Minimum Base Fee: One of the simplest adjustments is to modify the MIN_BASE_FEE parameter, as suggested by Max Resnick.\nAutomating Blob Base Fee Update Fraction: A simple change to ensure the blob base fee update fraction scales with the target number of blobs.\nNormalization of Excess Gas: Another proposal from Bert Kellerman and Gajinder suggests “normalizing” the calculation for excess gas usage.\nSymmetrizing the Base Fee Updates: A proposal to adjust the base fee formula.\n1. Increase MIN_BASE_FEE_PER_BLOB_GAS (EIP-7762)\nThe blob base fee starts at 0 and then slowly increases until the point of price discovery. Every ~6 blocks (with 6 blobs) the base fee doubles but it’s a long way to go from 1 wei to a price that is more reasonable, like, for example, 5 gwei. Until we’re at that level, the price may fluctuate a lot and rollups basically get “overly” cheap DA.\nScreenshot from 2024-11-07 05-33-46925×435 35.5 KB\nThe mentioned EIP proposes to increase the minimum base fee from 1 wei to ~0.034 gwei. This would shorten the time until price discovery and thus quicker pushes rollups towards a more stable price range that is considered “fair”.\nFor the base fee to climb from 1 to 5 gwei, it takes…\n\\frac{\\ln\\left(\\frac{\\text{base_fee_target}}{\\text{base_fee_start}}\\right)}{\\text{growth_rate}} = \\frac{\\ln(5 \\times 10^9 / 1)}{0.117} \\approx 190 \\text{ blocks} and all of those blocks need to have 6 blobs. This equals to approx. 38 minutes.\nWith the new, increased MIN_BASE_FEE_PER_BLOB_GAS, we would lower this duration to…\n\\frac{\\ln(5 \\times 10^9 / 2^{25})}{0.117} \\approx  \\text{42 blocks}, equaling 8.4 minutes.\nScreenshot from 2024-11-07 05-41-23925×435 33.3 KB\nOne highly important caveat/implementation detail:\nWe MUST reset the excess gas when updating the MIN_BASE_FEE_PER_BLOB_GAS.\nThe reason for that is that otherwise we would see an unpredictable, extreme spike in the blob base fee right after the fork. This is because the min base fee acts as a multiplier to the base fee and a small adjustment to it can greatly impact the base fee if the excess gas accumulated until that point is not reset.\nHere’s an example of that: A apparently “meaningless” increase from 1 to 5 wei is enough to make the base fee skyrocketing and then, rollups would have to wait for a certain period of time until the price calms down again.\nScreenshot from 2024-11-08 11-05-08970×429 27.5 KB\nThis can be simplified as “ossification vs. optimizations” and, imo, there’s no right or wrong, even though I personally think that NOT touching the protocol has value too.\n2. Automating Blob Base Fee Update Fraction\nThis change is super simple: As of now, the update fraction is set to 3338477, as specified in EIP-4844. One gets to that number by doing \\frac{target\\_gas}{ln(1.125)}. If we now replace the hardcoded number with this, we can make sure that we maintain the desired \\pm 12.5% change.\nThere is currently an open PR for this change to be included in EIP-7742 but I’d argue we shouldn’t bundle it together and instead propose this change individually together with the other changes to the base fee mechanism that we might want to ship.\n3. Normalization of Excess Gas\nThat’s a proposal that aims to avoid steep base fee drops after forks that increase the blob target.\nTo avoid a sudden drop in the base fee right after a fork that increases the blob target, this proposal suggests adjusting the way we calculate excess gas. Normally, the base fee should change smoothly—only up to 12.5% per slot. By normalizing excess gas, we can ensure the base fee remains stable and doesn’t drop sharply beyond the expected 12.5% limit when the target increases, keeping fee adjustments predictable and gradual.\nSimply put, instead of accumulating excess gas, we accumulate normalized excess gas. This way the target blob number doesn’t matter.\nScreenshot from 2024-11-14 04-14-09944×449 27.1 KB\nBy introducing a normalization factor in the excess gas calculation, we can adjust the accumulated excess gas to remain proportionate to the new target. This ensures that the base fee adjusts smoothly, maintaining consistent fee dynamics even when the target changes.\nNormalization involves scaling the excess gas using a constant factor, so that it reflects the deviation from the target in a consistent manner regardless of the target’s value. This approach keeps the base fee adjustments within the expected limits, preventing sudden drops or spikes that could disrupt the network’s fee market.\nFor more details on this change, check this commit.\nSymmetrizing Blob Base Fee Updates around the Target\nWith changing the target such that target=\\frac{max}{2} doesn’t hold anymore, we change the distance from the target to the min/max. E.g., going to a blob target of 4 and a max of 6, there is room for 2 blobs up and 4 blobs down.\n\nNow, with more room on the negative side than on the positive side, the base fee can move down faster than it can move up.\nbase_fee_symmetry843×1043 36 KB\nA simple fix to this is the following:\nWe first determine the delta between the gas used and the target (same as we do now). Then we apply a scaling factor that is simply target/(max-target) to the side that has less “room” (e.g., the “up” side when doing 4/6 target/max).\nThe calc_excess_blob_gas function would look like the following:\nOf course, this only works in cases where max-target<target and one could generalize it even more, but I’d guess it’s not worth it.\nTo better understand the effects of this symmetrizing process, let’s walk through a simple example:\nImagine we have three slots:\n\nThe first two slots contain 6 blobs each.\nThe last slot contains 0 blobs.\n\n\nThe first two slots contain 6 blobs each.\nThe last slot contains 0 blobs.\nThis results in a total of 12 blobs across the 3 slots.\nThe target is set to 4 blobs per block, with a maximum of 6 blobs.\nSymmetric Base Fee:\nThe base fee increases by approximately 12.5% twice (for the two 6-blob slots) and then decreases by 12.5% once (for the 0-blob slot).\nAfter these adjustments, the base fee ends up higher than its initial value.\nNon-Symmetric Base Fee:\nThe base fee remains unchanged after the 6-6-0 blob sequence.\nThis example highlights the benefits of a symmetric adjustment:\nThe 6-6-0 blob sequence pushes us toward extremes (max utilization followed by none), which is undesirable.\nIdeally, the load should be more evenly distributed across the three slots (e.g., 4 blobs in each block).\nA symmetric base fee discourages extreme behavior by “penalizing” uneven usage (6 blobs instead of 4) more with a higher base fee, promoting a more balanced load.\nOf course, one could argue that blob users might not care about pushing the base fee up because they might only need to post blobs a few times in every epoch and therefore don’t care about the slot following their 6-blob posting. Though, this argument is short-sighted.\n",
        "category": [
            "Sharding"
        ],
        "discourse": [
            "rollup",
            "data-availability"
        ]
    },
    {
        "title": "Endgame Staking Economics: A Case for Targeting",
        "link": "https://ethresear.ch/t/endgame-staking-economics-a-case-for-targeting/18751",
        "article": "by @adietrichs and @casparschwa .\nThis post explores the status quo of staking economics, its drawbacks as we see them and what the endgame of staking economics could look like.\nRead about our separate proposal for an immediate issuance policy update in Electra here.\nMany thanks to Anders, Barnabé, Francesco, Julian, Dankrad, Thomas, Vitalik, Mike, Justin, Jon, Nixo, and Sam for feedback and discussions.\nReviews \\neq endorsements. This post expresses opinions of the authors, which may not be shared by reviewers.\ntl;dr\nToday, 30M ETH or 1/4 of all ETH, is staked, with the trend of increasing staking showing no signs of stopping.\nWe argue that most of new stake will be driven by LSTs, which gain in money-ness with adoption and time.\nA world in which most of ETH is staked through LSTs has several implications that we consider negative: LSTs have winner-takes-most dynamics due to network effects of liquidity. Economies of scale increase competitive pressure for solo staking viability. Further, a LST replacing ETH as the de facto money of the network (apart from L1 tx fees) leads to Ethereum users being exposed to counterparty risk inherited by the LST by default. For true economic scalability the money of Ethereum should be maximally trustless.\nToday, the issuance yield does not ensure a limit to the amount that can be staked profitably. LSTs have significantly changed the cost structure of staking, making it possible that most ETH will be staked eventually.\nWe argue that endgame staking economics should include an issuance policy that targets a range of staking ratios instead, e.g. around 1/4 of all ETH. The intention is to be secure enough but avoid overpaying for security and thereby enabling said negative externalities.\nFinally, we highlight some open research questions that need answering to make a targeting policy feasible.\nThe figure below shows the amount of ETH staked over time. Historically, staking has known one direction: up only. In this post, we put forward arguments for why we think this trend will likely continue, what the negative externalities of it are, and make a case for a path forward to avoid this plausible future.\n2920×966 147 KB\nIn short, the current issuance policy allows for all ETH to be eventually staked. This arguably over-secures the protocol and comes with negative externalities. We elaborate why the endgame of staking economics should target a staking ratio range instead. The intention is to provide enough security for the protocol, while limiting the negative externalities of too much stake in the system.\nCurrent Issuance Policy – where we are & where it’s going\nTo frame our argument, we reason about what long term equilibria of stake participation are viable under the current issuance policy.\nThe Ethereum protocol requires some stake participation to secure itself. The demand for stake is very clearly defined in the form of the issuance curve. Given some level of stake participation, the protocol issues some maximum amount of rewards. Instead, the willingness to supply stake varies across ETH holders and is not public knowledge. Hence, we are left to reason about it.\nDemand for Stake – take my ETH and give me security\nEthereum inherits its security from its validators, who stake ETH for the right to earn rewards. The protocol’s demand for security is expressed in its  willingness to issue ETH for validators correctly performing their assigned duties. How much security is sufficient and how much might be too much is discussed here, here and here and otherwise beyond the scope of this post.\nThe Ethereum protocol issues new ETH to recruit ETH holders as stakers. It rewards correct validation according to a fixed issuance curve. Further, validators earn MEV rewards in their role as block proposers. Both sources of yield contribute to the incentive to stake.\n990×589 39.4 KB\nIssuance yield (solid green line). The current issuance yield curve is defined as y=\\frac{cF}{\\sqrt{ETH\\ Staked}} with c\\approx 2.6 and F=64 and represents the maximum issuance yield available for an optimally performing validator [1]. The rewards decrease as staking participation increases – first quickly, then more slowly. The protocol tries to ensure some minimum level of security and thus rewards validators more generously at low levels of staking participation. As more stake secures Ethereum, the marginal value of validators decreases and hence staking rewards are reduced.\nTotal staking yield (dashed green line). It is the sum of (nominal) issuance yield and MEV yield and forms the demand curve for stake [2]. MEV yield is exclusively available to validators in their role as block proposers. It is calculated as the annual amount of MEV extracted (~300,000 ETH over last year) over the amount of ETH staked. As a constant amount of MEV is shared by more validators, MEV yield decreases more rapidly than issuance yield, because it does not change with staking levels. The amount of MEV has stayed remarkably constant over time. While this could clearly change, for simplicity we will refer to the demand curve as fixed.\nThe current issuance curve was chosen over a plethora of reasonable functions. This particular curve conveys two clear messages:\nIt deliberately aims to avoid too low staking participation, by paying out very high rewards at low staking levels.\nIt suggests diminishing marginal utility for each additional staker, with issuance yield decreasing as staking participation increases.\nHowever, the issuance curve is not intentional about the level of staking participation it wants to achieve. Notably, there is no mechanism to prevent the staking ratio from exceeding some threshold. In fact, even if the entirety of ETH is staked, the incentive to stake still mounts up to ~2%, favoring those who stake over those who do not. While the issuance curve subtly suggests that the value of each additional staker diminishes, the protocol does not exert fine control over the eventual staking ratio reached. Essentially, beyond ensuring minimum security through initial high incentives, the protocol doesn’t encourage an optimal range of staking levels.\nNote that the issuance yield depicted above is nominal yield. It does not account for the dilution that takes place as more ETH is issued. The relevance of this dilution effect increases as more ETH is staked. However, as it affects both staked and unstaked ETH equally, it does not affect the net incentive to stake (i.e. the yield gap between holding raw ETH and staking it). We can therefore ignore dilution in this analysis for now – we will get back to it at a later point in this article.\nSupply Side – give me ETH and have my stake\nHaving explored the demand for stake, we now turn our attention to the supply side. The supply curve represents the willingness of ETH holders to stake at different staking yields, indicating the necessary incentives for each stake participation level. This curve typically slopes upwards, suggesting that higher staking participation levels require greater incentives. However, since the willingness to stake cannot directly be observed or measured, the exact shape of the supply curve remains unknown, only allowing for qualitative assessments.\nIn addition, the supply curve is not fixed over time. In the following sections we will illustrate how staking costs change over time, and such changes naturally impact the level at which ETH holders need to be incentivized to stake – in other words, they shift the supply curve.\nThe only direct observations of the supply curve that we do have are the historical staking levels. These represent the intersection between demand and supply curve at any given moment in time, and with the demand curve known, this gives us certain knowledge of a single point of the supply curve for each historical moment of the beacon chain, up to and including today.\nAs the first graph illustrated, the total amount of staked ETH has continued to grow since the launch of the Beacon Chain. Given the demand curve has remained constant [3], it then follows that this growth in ETH staked is due to a downward shift of the supply curve. In other words, this indicates that the willingness among ETH holders to participate in staking has increased – even at today’s lower issuance yield levels. The following graph illustrates this trend by sketching out plausible shapes of historical short term supply curves:\n2706×1634 207 KB\nJust looking at this historical trend, it is clear that for the near-term future, a continued downward shift of the supply curve and thus a continued net inflow of stake is reasonable to expect. However, the more interesting questions are those about the potential long term staking equilibria. To be able to make informed assessments here, we need to have a closer look at the composition of the supply side.\nThe decision for any ETH holder to stake depends on two things: The incentives provided to stake (total staking yield = issuance yield + MEV yield) and the costs of staking. While the former is relatively homogenous across stakers [4], the cost structure fundamentally varies for different staker types. In the following we contrast solo stakers and staking service providers.\nStaking Service Providers (SSPs) receive ETH from their users and stake it for them, charging a fee for their service. In most cases, they give users liquid staking tokens (LSTs) as a receipt for their ETH. An LST is fungible and can be freely used and traded. The extent to which this liquidity is useful for it’s holder varies among LSTs and is a function of overall adoption and support by third-party protocols for a given LST. In the following we will only talk about LST-issuing SSPs - those that do not issue LSTs can be considered as a special case of LSTs with zero liquidity value.\nSolo staking is trustless, but illiquid and inconvenient; liquid staking requires varying degrees of trust but is very convenient and importantly liquid.\nComparing these two staking types suggests two main conclusions, relevant to the supply discussion:\nThe cost structure for solo staking is heterogeneous across  ETH holders. The technical skill required, the variation in local cost for hardware and ongoing resources, and the difference in confidence in successful safeguarding of the ETH at stake, all contribute to a relatively steep supply curve for solo stakers. Without major improvements in solo staking UX, the pool of possible solo stakers at anything close to current issuance levels is likely mostly depleted.\nIn contrast, the cost structure for SSPs is much more homogeneous across ETH holders, with variation mostly in the assessment of operator risk and the LST-vs-ETH liquidity penalty. As a result, the SSP supply curve is considerably flatter, meaning that the issuance required to recruit more ETH holders as liquid stakers, only increases slowly.\nIn addition, the cost of solo staking remains independent of the level of staking participation, whereas the cost of holding LSTs will likely go down over time and with increased adoption:\nMoney-ness of LSTs increases. As one LST becomes more popular, it can be expected to be supported by more and more projects in addition to native ETH (e.g. more defi integrations, L2s starting to liquid-stake bridged ETH by default). With a sufficiently high staking ratio, the winning LST could eventually even surpass the remaining unstaked ETH in available volume, completely closing (and potentially even flipping) the gap in liquidity.\nSmart contract risk decreases: “battle tested”, formal verification,  etc.\nGovernance systems improve their robustness, e.g. this proposal.\nPerceived tail risks of mass slashing might decrease. Any LST scaling to subsume a significant portion of the overall ETH in existence could reasonably create a too-big-to-fail impression, where users expect a protocol bailout in case of failure, effectively driving down their perceived operator risk to 0.\nSSPs can offer lower fees to break even at higher staking ratios.\nIn summary, this suggests that the supply curve is significantly flattened by SSPs and LSTs in particular, meaning the incentives required to attract additional staking do not need to increase substantially as the total amount of staked ETH grows. A continued future increase of stake driven by liquid staking can be expected. But just how much more ETH will be staked in the long run?\nLong term equilibria – are we gonna keep staking or what?\nWe now put together the demand and supply considerations to reason about possible long term staking equilibria.\nWe argued that the demand curve is very opinionated for low staking participation but otherwise leaves it relatively open what staking ratios might be reached in the long run.\nWe then explained the dynamic of downward shifting supply curves over time, as the costs and risks of staking continue to decrease. The resulting new net inflow of stake will mostly flow towards LSTs. In particular, it is unclear whether the supply curve would be steep enough to set practical limits to staking participation.\nThus, there is a broad range of possible equilibria for the overall staking ratio, with ratios close to one among the plausible outcomes. The following sketch illustrates how even relatively small differences in the (hypothetical) long term equilibrium supply curve can lead to very different outcomes:\n2724×1628 211 KB\nThe key take-away is not that staking participation levels will necessarily be high, but that such high levels are plausible.\nWith this in mind, we now detail our concerns around high staking ratios, before presenting possible changes to the issuance policy preventing these.\nStaking Ratio – when is less stake more?\nWith ~30 million ETH staked from a total supply of ~120 million, the staking ratio s is defined as \\frac{ETH\\ Staked}{Total\\ Supply\\ of\\ ETH} and stands at \\frac{1}{4}.\nBefore diving into the concerns we see for high staking ratio regimes, we again point to some references, discussing what level of stake participation might be secure enough. In short, current staking levels are arguably sufficiently secure. This then raises the question - if we don’t need it for security, should we be okay with staking participation sufficiently higher than today?\nWe argue that high staking ratio regimes come with negative externalities that affect ETH holders, (solo) stakers, as well as the protocol itself.\nNetwork Effects of Money (LSTs) – no thanks to forced risk taking\nLSTs compete on money-ness with winner-takes-most if not all dynamics due to network effects. As a LST is more widely used, it becomes more useful driving further adoption. The money-ness of LSTs increases in aspects such as degree of integrations (on- and offchain), trading liquidity, resilience against governance/legal attacks, etc.\nIn a high staking ratio regime in which one SSP controls most stake, the LST may be considered too-big-to-fail. Is it a credible threat to slash, if a majority of all ETH in existence would be affected? More generally, the governance of such a dominant SSP would de facto become a part of the protocol, without being accountable to all Ethereum users.\nIn a world in which most existing ETH is liquid-staked, the de facto money of Ethereum for most use cases besides L1 transaction fees will be some LST(s). All LSTs, be they issued by an ETF, centralized exchange or onchain staking pool, come with different trust assumptions – some worse than others. But ultimately Ethereum users will end up holding LSTs, economically quasi-forced to expose themselves to those added risks (operator/governance/legal/smart contract/etc. risk). Is this desirable for Ethereum users? Further, such intermediated ETH is worse collateral. For true economic scalability, the money of Ethereum needs to be maximally trustless: ETH.\nReal Yield – the real deal\nAs we discussed in the beginning of the article, the nominal yield for stakers from issuance is progressively diluted with higher staking participation. To adjust for this effect, it is useful to look at the real staking yield.\nReal yield is nominal yield adjusted for the dilution effects of ETH issuance [6].\n6000×3600 617 KB\nThe graph depicts the impact of dilution on yield for both stakers and non-staking ETH holders. For ETH holders (red line) this implicit yield is of course negative, as their nominal balance remains unchanged, while they are exposed to the same dilution effect as stakers. To better characterize the impact of this effect, we can broadly distinguish between two different staking ratio (s) regimes:\nOn the left, for low s, the real yield curves relatively closely resemble the nominal yield curves we previously examined. With few stakers the total amount of new ETH minted is respectively small, resulting in only a small dilution effect. The net incentive to stake is made up primarily by the positive yield available to stakers – or visually it’s mostly green.\nOn the right, for high s, the real issuance yield diverges more strongly from the nominal yield curves. With an increased number of stakers earning validator rewards, total ETH issuance is higher, leading to this more noticeable dilution effect. Besides the diminished real yield, a significant portion of the net staking incentive now derives from “dilution protection”, essentially the avoidance of losses that would be incurred by passively holding ETH. In the extreme, as the staking ratio approaches 1, real total staking yield only consists of MEV yield.\nThe shifting composition of the net incentive to stake is a fundamental area of distinction between the two staking ratio regimes. At this point, we want to stress that this composition does not change the effectiveness of the incentive to stake. In other words, dilution protection incentivizes stakers just as much as real staking yields!\nWhat does change though is the desirability of the outcome resulting from that incentive. For low s, staking is a profitable service paid for by the protocol. But for high s, staking loses its profitability and instead becomes an unpleasant necessity to avoid losses from passively holding ETH. Thus, by allowing the staking ratio to “slide to the right”, we risk ending up in the worst of all worlds: Staking becomes a necessary indirection layer, exposing minimal real yield, but threatening dilution for those opting out of accepting LST trust assumptions.\nIf you give a staker the choice between living in some equilibrium (a) or equilibrium (b) for different issuance policies, this staker, assuming they would stake in both equilibria, would prefer the equilibrium paying higher real staking yield. For obvious reasons a staker cannot “choose” an equilibrium, but the protocol issuance curve effectively determines which equilibrium is reached (given some fixed long term supply curve). Higher issuance is associated with more nominal yield, but importantly, more nominal yield does not imply more real yield.\nSolo Staking Viability – down bad\nSSPs with fixed costs naturally benefit from economies of scale, allowing them to operate more profitably (or charge lower fees) as they have more ETH under management. Successful SSPs might be viewed as too-big-to-fail, reducing their perceived tail risks and further contributing to such scale effects. In contrast, solo staking comes with per-staker costs that do not decrease (rather even slightly increasing with networking load!) as the total amount of stake grows. In fact, EIP-7514 was in part merged for this reason.\nAs a larger share of issuance goes towards “dilution protection” and no longer contributes to real yield, stakers are left with more and more of their remaining real yield coming from MEV. This yield is by its nature highly variable, which leads to increased volatility of real total yield for solo stakers. For SSPs on the other hand this MEV income is smoothed over all validators they operate, removing staking yield volatility as a concern for them.\nThe liquidity gap between solo staking and LSTs widens with increased adoption and moneyness of LSTs. Put differently, the competitive disadvantage of solo staking relative to liquid staking increases as the staking ratio increases.\nIn many jurisdictions, the basis for government taxes on staking income is the nominal income, not the real income adjusted for dilution effects. LSTs can be structured in a way to shield holders from this effect, while for solo stakers this is usually not possible. This further increases the profitability gap as the difference between nominal yield and dilution-adjusted real yield widens.\nDiscussion\nThe considerations above lead us to argue for the following:\nHolding raw ETH should be economically feasible, to ensure user friendliness and preventing dilution beyond ensuring sufficient security.\nFor true economic scalability, Ethereum’s de facto money should be maximally trustless [7].\nAn outcome with most of the incentive to stake coming from dilution protection is undesirable for both stakers and ETH holders.\nHigh staking participation worsens the competitive disadvantage of solo staking.\nEthereum’s future staking ratio is uncertain; however, the absence of control over maximum staking levels warrants a proactive approach in determining optimal levels. Even if high staking ratios may be preferable to some, it should then be a deliberate choice, not an accidental result of exogenous market dynamics.\nThe following sections are concerned with proposing alternative issuance policies.\nStake Ratio Targeting – endgame staking economics\nAn endgame staking policy for Ethereum should target a staking ratio, rather than a fixed quantity of staked ETH. This approach ensures accounting for the variable supply of ETH, which changes over time due to EIP-1559 and issuance. In practice, the total supply of ETH currently changes so slowly (-0.3% per year since the merge) that this distinction does not matter in the medium term. However, the intention of an endgame policy is to not require adjustments, even over longer time horizons.\nAs discussed, while the current issuance curve is designed to ensure a minimum level of staking, it lacks a mechanism to cap staking at an upper bound, potentially resulting in high staking ratios. We argue that a robust endgame issuance policy should express desired staking participation levels by implementing controls on both the lower and upper bounds of staking ratio. Specifically, it should aim to maintain staking ratios within a defined optimal range that reflects the network’s security requirements without enabling negative externalities further.\nThe protocol can express strong opinions for “too-low” and “too-high” staking ratios alike, by issuing very high or low rewards (possibly also negative) respectively. By doing so the protocol regains control over its level of staking participation. To illustrate the existence of a curve with such properties consider the following plot, which is close to Vitalik’s curve here.\n6000×3600 475 KB\nIn this plot we observe that for low amounts of staking participation the issuance curve rewards generously, similarly to today’s issuance policy. As stake levels increase, issuance yield tapers off and finally turns negative. That way, staking becomes increasingly disincentivized, until it even becomes more profitable to hold ETH than to stake it. In practice, this range of negative total staking yields would not be maintained, with staking participation finding a lower equilibrium. Thus, any curve of such shape would give strong guarantees for the range of viable staking ratios.\nPractically it might not be necessary to choose a curve which turns negative so abruptly to achieve similar range targeting properties. In fact, curves that bring issuance rewards down to (or close to) zero beyond some point might even be sufficient for that purpose.\nImplications of Targeting\nThe main advantage of targeting is that it prevents all of the negative aspects of a high staking ratio regime enumerated in the section above. The notable exception to this is the concern around reward variability for solo stakers. Like in the high staking ratio regime, under targeting the share of real yield coming from MEV is also higher than today. A downside of a move to targeting is thus an  acceleration of this (already existing) dynamic. However, this increased variance can be mitigated through MEV capture mechanisms such as Execution Tickets or MEV Burn, or instead by introducing a staking fee.\nOne criticism sometimes brought forward against targeting is that it reduces the overall equilibrium yield, thus worsening the existing competitive pressure between solo staking and SSPs, as well as across different types of SSPs. The idea here is that when there is more money to go around, slightly less competitive forms of staking that might be desirable for the protocol have an easier time staying profitable. To address this concern, the distinction between nominal and real yield is crucial. A move to targeting unquestionably reduces nominal yields when compared to the equilibrium that would otherwise be reached in the long run. However, the same is not generally true about real yield, which is what really matters, as the following sketches illustrate.\n3533×1077 316 KB\nThe left graph depicts a plausible example for a long term equilibrium under targeting, the right graph similarly an example using today’s issuance curve. Crucially, both examples use the same hypothetical long term supply curve, allowing for a comparison of outcomes. As one can see, the chosen example would in the non-targeting case lead to a high staking participation of around 100M ETH. At that level, most of the staking incentive comes from dilution protection, with a real yield of only around 0.5%. Conversely, under targeting, the same example leads to an equilibrium with lower nominal yield, but little dilution and thus a real yield of around 1.4%.\nThis example illustrates how targeting can plausibly lead to significantly higher real yield levels than the equivalent outcome under the current system. To the extent that yield levels indeed matter for intensity of competition among different staking types, targeting can thus help prevent a near-zero outcome for real yield. It should be noted here that this is not only beneficial under the aspect of competitive pressure – keeping staking profitable is beneficial for stakers of all types. Moreover, it also benefits non-staking ETH holders, by minimizing the dilution they are exposed to.\nOpen Questions\nThis article advocates for the general principle of stake participation targeting. To move to an actual specification that could then be implemented on Ethereum, there are still several open questions that need to be addressed. In closing, we want to briefly touch on each of these questions.\nWe have deliberately only talked about undesirable ranges, but never specified a precise and desirable range. This is because it is inherently hard to objectively reason about it and needs to be discussed more broadly in the community. The main tradeoff is that little stake participation leaves the protocol open to cheap attacks, while too much staking creates negative externalities as discussed in earlier sections. We once again link to some preliminary discussion on the topic by Vitalik, Vitalik, and Justin. One helpful way to model the decision is by considering the utility of different staking ratios. One possible example for such a utility curve is sketched below.\n3017×1623 102 KB\nOnce a target range is picked, there is still a wide design space of possible issuance curves, that could be chosen to achieve the specified goals. Further work is necessary to compare the possible choices and pick the best candidate. In addition, alternative targeting mechanisms, such as an EIP-1559-like feedback controller, should continue to be explored.\nThe purpose of issuance is to reward the correct performance of validators fulfilling their consensus duties. However, under a targeting policy it might be possible for issuance yield to approach zero or even turn negative. A validator might still be incentivized to stake with issuance levels approaching zero or less, for the possibility to capture MEV. However, with no issuance it is rational for a validator to not fulfill all consensus duties. This goes to show that for low issuance levels consensus incentives risk breaking down. To mitigate this, the protocol could charge a fee for the right to validate (and again reward correct participation). This reestablishes incentive compatibility. However, this adds protocol and implementation complexity, of which the details need to be figured out.\nAs mentioned before, the mitigation of increased reward volatility is important for solo staking viability and can be achieved through MEV capture mechanisms such as Execution Tickets or MEV Burn, or instead by introducing a fee for validators, as per discussion above. While shipping targeting with one of these solutions already in place would be preferable, in principle this is not a strict dependency.\nA targeting policy could of course also target a fixed amount of ETH, e.g. 30M ETH. But for future-proofing the issuance policy it is preferable to directly target a staking ratio instead, e.g. \\frac{1}{4}. For the issuance policy to target some staking ratios the consensus layer needs to be aware of the amount of stake and supply of ETH. The latter is currently not the case, but could be achieved in a simple two-fork process:\nFork (1): Start tracking supply changes relative to the time of fork (1).\nFork (2): Add total supply of ETH at time of fork (1). This together with the ongoing tracking of supply changes since fork (1), gives a running total supply of ETH.\nThe simplest way to transition to a targeting policy is to of course start within the target staking range. However, in the likely scenario that the target range will be surpassed before the transition, this would necessitate a decrease in staking participation. Even with a gradual easing into the new curve, this would in practice entail a significant period, during which stakers would be insufficiently compensated, until the excess stake can exit. It remains an open question how to minimize the adverse impacts on stakers of such a transition.\nConclusion\nWe discussed the current issuance policy, explained negative externalities as we see them, and elaborated what a path forward could look like. In particular, we suggest targeting a range of staking ratios. However, given the open questions, and in particular the lack of a validator fee mechanism and/or in-protocol MEV capture mechanism, moving to a targeting policy will take time. In the meantime we should update the issuance policy, a stepping stone towards targeting. We make a case for a proposal to update the issuance policy in the upcoming network upgrade Electra here.\nFootnotes\n[1] on aggregate the network operates at ~97% effectiveness\n[2] Read more about why issuance yield and MEV yield together form the demand curve in section 2.1 here.\n[3] We want to reiterate that issuance yield and MEV yield together form the demand curve. Further, we make the simplifying assumption that MEV remains constant over time. While historically roughly true, this could of course change.\n[4] The performance across validators varies, both in terms of earning issuance rewards (consensus duties) and MEV rewards. However, this variability is relatively small and its discussion beyond the scope of this post.\n[5] Read this discussion for an analysis of risks for one such onchain SSP.\n[6] We analyze these yields in isolation, in particular we do not consider the tx fee burn mechanism (EIP 1559). The underlying analysis would remain unchanged nonetheless, as all yield curves would be shifted up- or downwards by some constant amount, independent of the staking level. We simply wish to reason about an ETH holder’s decision to stake or not to stake and for that only the difference in yields is relevant.\n[7] The asset ETH is more trustless than staked ETH for the slashing risk alone and then of course there is a spectrum of trust required across different SSPs.\n",
        "category": [
            "Proof-of-Stake",
            "Economics"
        ],
        "discourse": []
    },
    {
        "title": "On trustless cross-chain asset transfers",
        "link": "https://ethresear.ch/t/on-trustless-cross-chain-asset-transfers/20992",
        "article": "On trustless cross-chain asset transfers.\nNOTE: Early development of this protocol has begun under the name Viaduct.\nPreface\nIn the state of Ethereum today, layer 2 blockchains experience fragmentation between each other and with the Ethereum mainnet. One core piece of solving this problem and achieving effective interoperability is cross-chain asset transfers. Many bridges implement this functionality, but they do so in a way that, to some extent, relies on trust.\nBut, by using a signature-based transaction system, it is possible to trustlessly mirror token transfers across multiple networks. The only trust assumption is that at least one honest node exists and that the network can detect and relay transactions in a reasonably short period of time.\nPart I: Single-deployment\nThe first step is to create a system that can reliably handle signature-based transfers on a single smart contract. The protocol splits these transfers into three parts: retrieval, signing, and execution.\nThe retrieval step calls getValidHash() on the protocol’s core smart contract. This method will, given a sender, recipient, value, and nonce, calculate and return the requested transfer’s hash.\nThen comes the signing step. Once the contract finishes its calculations, the transaction sender can sign the resulting hash to approve the transaction. Signatures are valid for one transfer only. If the sender wishes to repeat a transaction, it can calculate a new hash and signature using a new nonce.\nFinally, in the execution step, anyone with access to the transaction signature can call objectiveTransfer() on the core contract. After checking for double spending and signature correctness, the contract transfers the correct number of tokens from the sender to the recipient. Either the signer or a so-called relay node could initiate the objectiveTransfer() call. Note that relay nodes may introduce fees into the transfer process.\nThis system allows for token transfers via signatures and relays. At this point, this design replicates the benefits and drawbacks of a system like Uniswap’s Permit2.\nPart II: Cross-deployment\nThe next step in creating a cross-chain trustless signature transfer system is enabling this protocol to function across multiple deployments or instances of the core contract, all on the same blockchain. While it might not seem relevant now, it forms an important piece in the infrastructure for cross-chain transfers.\nEach deployment can keep track of a list of other deployments. It is not necessary for every deployment to be aware of all others. The only requirement is that there is a route, direct or indirect, between any two given instances. Then, whenever a deployment receives a valid objective transfer, it can initiate an identical transfer call on each of its peer chains. Those chains can independently verify that the call is correct before executing the transfer and recalculating balances appropriately.\nAttempts to double-spend tokens on multiple instances at once will fail. Because each instance is on the same blockchain, signers can’t execute multiple transactions at once. This means that all deployments will remain in sync, blocking all double-spending attempts.\nPart III: Isolated cross-deployment\nNow, the core contract deployments or instances need a way to run a secure version of the protocol without communicating with one another. To enable cross-deployment transaction execution, there can be a permissionless relay node network that monitors onchain objective transfers and relays them to each deployment.\nHowever, at this point there remains a critical flaw in the protocol. If the same sender initiates two transactions on two different deployments simultaneously, both sending over 50% of the sender’s balance to two different addresses, the sender can double-spend. Both deployments will reject the transaction they did not receive initially, and their account balances will differ.\nTo resolve the double-spending issue, the core contracts use a challenge window . Each window lasts w_f seconds long and consists of two periods: the proposal period and the challenge-only period, which last for w_p and w_c seconds, respectively. The core smart contract disables the objectiveTransfer() function during the challenge-only period.\nWhen a signer submits an objective transfer, the contract checks it for double-spending and then stores it in the challengeableTransfers array. Note that the initial double-spending checks on objectiveTransfer() calls only check for double-spending against the address’ finalized balance. The protocol also checks for double-spending again when cleanChallengeWindow() is called [see below], where the total amount spent in all challangeable transfers are combined and then checked against the finalized balance.  The core contract can finalize the transfer once the active challenge period ends. Any address can call the cleanChallengeWindow() method, which will attempt to execute and delete all challengeable transfers that it can finalize. At this point, the transfer is complete.\nAny address can call the challengeAndRecord() method on a core contract, which, given one or more transfers, will check them alongside all challengeable transfers for double-spending. If this process detects double-spending (the total amount of tokens transferred from one address in the challenge window period exceeds its balance), it will mark all challengeable transfers originating from double-spending addresses as problematic. During the cleanChallengeWindow() call, the core contract will delete but not execute problematic transactions.\nThese rules create a system that:\nOnly needs one honest relay.\nPrevents double spending.\nMaintains consensus between deployments.\nBut at this point, the protocol isn’t very useful.\nPart IV: Cross-chain\nThere’s a surprisingly small gap between Part III and a cross-chain token. Because the isolated cross-deployment solution doesn’t require contracts to interact with each other, each deployment doesn’t need any awareness of or connection to any other deployments. They could be on entirely different blockchains, and the protocol would still function. We could deploy one instance on each network we want to connect to. Then, when an address initiates a transfer on one chain, relays will replicate it on every other chain.\nA trustless exchange contract repackages the protocol into a more familiar form, where EOA addresses can transfer assets from one chain to another. This works by utilizing the swapon-sync-swapoff model, which functions as follows:\nSwap an ERC20 token on the source chain for the cross-chain token.\nWait for relays to sync cross-chain balances.\nSwap the cross-chain token for an ERC20.\nHowever, standard liquidity pools like Uniswap can’t swap ERC20 tokens for cross-chain tokens. Instead of relying on the traditional model, we must take an orderbook-like approach.\nFor sellers:\nDeposit the ERC20 token into the exchange contract, specifying a price level for the trade.\nWait for a buyer to fulfill the order.\nFor buyers:\nFetch unfulfilled trades for the specified price level.\nCreate and sign transactions to transfer the cross-chain token to the appropriate sellers.\nSend the transactions and signatures to the exchange contract, which will verify them.\nThe exchange contract executes cross-chain token transfers using objectiveTransfer() calls.\nThe exchange contract releases sellers’ deposited funds to the buyer.\nThis design allows for a standard asset bridging interface between blockchains.\nConclusion\nThis protocol enables trustless cross-chain asset transfers using a standardized interface. It would also be possible to expand the protocol to execute arbitrary transactions on an EVM instance. This expanded version of the protocol could be the first step in building a class of blockchain that integrates a vast array of networks into its fundamental design, enabling trustless interoperability between all Ethereum-based blockchains in existence. It would be similar to an L2 blockchain in the sense that transaction data is stored onto another blockchain, but different in the sense that it stores data on multiple blockchains all at once to enhance interoperability.\nBy itself, this system is not very scalable. High fees are expected as there are multiple transactions required per transfer. However, this issue can be mitigated by either only using L2 networks to store transfer data or building L3s on top of the protocol. Alternatively, transactions could be bundled and their Merkle root submitted onchain alongside a ZK proof in a manner similar to that of a zero-knowledge rollup. In this way, multiple objective transfers could be submitted in only a few transactions.\nFinally, any and all feedback is welcome! The project is in its very early stages but I’ll attach the Github repository once I’ve made some more progress.\n",
        "category": [
            "Layer 2"
        ],
        "discourse": []
    },
    {
        "title": "GossipSub Topic Observation (proposed GossipSub 1.3)",
        "link": "https://ethresear.ch/t/gossipsub-topic-observation-proposed-gossipsub-1-3/20907",
        "article": "Authors: pop\ntldr; topic observation enables the nodes to get notified when there is a new message in\na topic without actually receiving the actual message.\nThis proposal enables you to tell your peers to notify you when there is a new message in the topic without consuming the bandwidth to download the actual message. When you do this, you are called an observing node in that topic.\nTopic observation is motivated by the amplification factor on bandwidth consumption due to the mesh degree of GossipSub. When you subscribe to a topic, you would need to download or forward as many copies of messages as the mesh degree. For example, if the mesh degree is 8, you would roughly download or forward 8 copies.\nWe have IDONTWANT implemented in GossipSub 1.2 which will reduce the number of copies you will download or forward, but it doesn’t guarantee exactly how many.\nWhen you observe a topic, you won’t receive any message. You will only get notified when there is a new message. If you want the actual message, you can request the message from the peer that notifies you first, so you will download only one copy. However, the message request part is out-of-scope of this proposal. This proposal only deals with notifications.\noutput1280×964 88.4 KB\nWhen you want to observe a topic, you would need to find subscribing nodes in the topic and tell them you want to observe the topic. Later, when there is a new message, those subscribing nodes will notify you.\nLet’s see examples in the figure, node 11 is observing the topic. Node 1, 9, and 10 will notify node 11 when there are new messages. Similarly, node 4 and 5 will notify node 12.\nNotice that the relationship is unidirectional rather then bidirectional like mesh connections.\nYou can also tell your subscribing peers when you don’t want to observe the topic anymore. That is when you want to unobserve it.\nNotice that observing nodes only receive notifications. They neither send notifications nor forward messages. In other words, they only consume, not contribute. So, they are only on the border of the network (as shown in the figure) and don’t provide any stability to the network. It means that there must be enough subscribing nodes in the network to provide stability.\nHowever, the good side of this is that the churn rate of observing nodes doesn’t matter at all. Nodes can observe and unobserve as often as they want.\nCurrently, when your peers want to observe the topic and tell you to notify, you are obligated to notify them without the option to decline. This has a downside that if too many of your peers want to observe, you will have too much overhead to do the job.\nHowever, the notifications consist only of messages ids which we now assume to be negligible. You may argue that if the number of observing peers is high enough, the total size of notifications will be significant. That’s true, but for the first iteration of the design, we should make this assumption to make the protocol simple.\nThere are two new control messages: OBSERVE and UNOBSERVE.\nYou send OBSERVE to your peer when you want to observe a topic and have that peer notify you.\nYou send UNOBSERVE to your peer to tell that you don’t observe the topic anymore.\nAfter sending OBSERVE to your peer, the peer will send IHAVE to you as a notification, when there is a new message in the topic.\nHowever, IHAVE in this proposal is different from the previous GossipSub versions. In the previous versions, IHAVE is sent only at the heartbeats, while in this version, it can also be sent right after peers receive messages. Previously, you can send IWANT after receiving IHAVE, but in topic observations, you aren’t expected to send IWANT, since IHAVE serves only as a notification.\n",
        "category": [
            "Networking"
        ],
        "discourse": [
            "p2p"
        ]
    },
    {
        "title": "On Block Sizes, Gas Limits and Scalability",
        "link": "https://ethresear.ch/t/on-block-sizes-gas-limits-and-scalability/18444",
        "article": "Thanks to Alex Stokes, Matt (lightclients) and Matt Solomon for feedback and review!\nThere has been much discussion about raising Ethereum’s block gas limit recently.\nSome argue for bigger blocks based on Moore’s law, some based on a personal gut feeling, some are just trolling around and others are afraid that other chains like Solana will outpace Ethereum when it comes to widespread user adoption.\nIn the following, I want to present some charts and figures that may be helpful in guiding us towards a decision that maxes out the gas limit without compromising Ethereum’s decentralization.\nFrom the beginning\nIn contrast to Bitcoin, Ethereum doesn’t have a fixed block size limit. Instead, Ethereum relies on a flexible block size mechanism governed by a unit called “gas.” Gas in Ethereum is a unit that measures the amount of computational effort required to execute operations like transactions or smart contracts. Each operation in Ethereum requires a certain amount of gas to complete, and each block has a gas limit, which determines how many operations can fit into a block.\nEthereum started with a gas limit of 5000 gas per block in 2015.\nThis limit was then quickly raised to ~3 million and then to ~4.7 million later in 2016.\nWith the Tangerine Whistle hardfork and, more specifically, EIP-150 in 2016, the gas limit was raised to 5.5 million, based on a repricing of various IO-heavy opcodes as a response to DoS attacks. After these attacks, the limit was continuously raised by miners to ~6.7 million in July 2017, then ~8 million in December 2017, then ~10 million in September 2019, then 12.5 million in August 2020 and finally to ~15 million in April 2021.\ngas_used_over_time1200×400 35 KB\nFurther on, with the Spurious Dragon, Byzantium, Constantinople, Istanbul and Berlin hardforks, the pricing of certain opcodes was further refined. Examples of these refinements are EIP-145, EIP-160, EIP-1052, EIP-1108, EIP-1884, EIP-2028, EIP-2200, EIP-2565 and EIP-2929.\nThe most significant change to Ethereum’s fee market happened with the London hardfork in August 2021 and more specifically EIP-1559.\nEIP-1559 introduced a base fee that dynamically adjusts over time/blocks depending on the demand for blockspace. At the same time a so called target has been introduced and set to 15 million gas per block. This target is used to guide the dynamic adjustment of the base fee. If the total gas used in a block exceeds this target, the base fee increases for the subsequent block. Conversely, if the total gas used is below the target, the base fee decreases. This mechanism aims to create a more predictable fee market and improve the user experience by stabilizing transaction costs. Additionally, EIP-1559 also introduced a burning mechanism for the base fee, permanently removing that portion of ether from circulation. This hardended the protocol’s sustainability while creating the ultra sound money meme.\nUnder EIP-1559, there is also a maximum (or “hard cap”) gas limit, set to twice the target, which is 30 million gas. This means that a block can include transactions using up to 30 million gas.\ngas_used_since_london1200×400 19.3 KB\nSince then Ethereum’s block gas limit remained the same and, as of 2024, it is still at 30 million gas per block.\nAre we ready for an increase?\nRecently, some raised concerns about Ethereum’s gas limit and demanded it to be increased. In the most recent Ethereum Foundation AMA on Reddit, Vitalik considered the idea of increasing the gas limit by 33% to 40 million. He based his reasoning on Moore’s law which states that the number of transistors on a microchip doubles approximately every two years, leading to a corresponding increase in computational power. This principle suggests that network capabilities, including processing and handling transactions, could also increase over time.\nSupport came from Dankrad and Ansgar, both researchers at the Ethereum Foundation, who like the idea of increasing the gas limit after evaluating the situation after the Dencun upgrade. In addition, Pari from the Ethereum Foundations published a post exploring paths for a potential gas limit increase.\nOthers like Peter and Marius from Geth raised concerns about increasing the gas limit, especially without having appropriate tooling/monitoring in place. These concerns were specifically based on accelerating state growth, syncing times and reorged block rates.\nWhat is the block size?\nThe size of a block can be measured in two ways:\nGas Usage\nBlock size (in bytes)\nWhile both of these measures correlate, they must be considered independently.\nFor example, a block that contains much non-zero calldata bytes might be big in terms of its size in bytes while the actual gas usage (16 gas for non-zero bytes) may still be relatively small.\nIgnoring compression, the maximum block size that can be achieved today while still obeying the 128 KB per transaction limit of Geth is ~6.88 MB. Such a block would max out the number of 128 KB transactions in a block. In practice, these are 55 transactions containing ~130,900 bytes of zero-byte calldata (4 gas per byte) and one transaction filling up the remaining space. However, after snappy compressing such a block we end up at ~0.32 MB, which is negligible.\nThe largest possible block after compression contains 15 transactions filled with non-zero calldata and can have a size of ~1.77 MB.\nSo, as of today, 1.77 MB represents the realistic upper-bound block size for an execution layer block.\nFocusing on this maximum block size, we can identify several factors that influence it:\nGas limit: Of course, the gas limit has an impact on the maximum block size. The higher it is, the more data can be put into a block.\nPricing of operations and data: The cheaper an operation in terms of gas, the more often the operation can be executed within a block. While operations such as CALLDATALOAD or CALLDATACOPY, both costing 3 gas, are relatively cheap, other opcodes such as CREATE are more expensive. The more expensive the opcodes used in a block, the less space for calldata (or other operations) in that block.\nClient limits: While not that obvious, client limits such as the 128kb limit per transaction of Geth can also impact the final block size. Since every transaction costs 21k gas as a fixed fee, the lower the client limit per transaction, the more often one has to pay the fixed fee, thus “wasting” gas that could otherwise be used for calldata. As a result, this limit can cause the maximum block size to be reduced by ~0.07 MB. Importantly, the client limits only impact the broadcasting of transactions and do not affect blocks that have already been confirmed.\nLet’s focus on the gas limit per block first:\nimpact_block_gas_limit1200×400 17.1 KB\nThe most straightforward and apparent way to scale a blockchain like Ethereum is increasing the block gas limit. A higher limit means more space for data. However, this also comes with larger blocks that everyone running a full node needs to propagate and download.\nAs visible in the chart above, the “worst-case” block size increases more or less linearly with the block gas limit. Those limits can be reached by creating blocks that use as many non-zero byte calldata transaction of maximum size.\nNext, let’s shift our focus to the second point - Ethereum’s pricing mechanism.\nMore specifically, we look at the costs for non-zero byte calldata that is currently set to 16 gas:\nimpact_calldata_price1200×400 20.4 KB\nAs we can see in the above chart, increasing the costs for non-zero calldata leads to decreasing block sizes. On the other hand, reducing the costs to, e.g. 8 gas per byte, doubles the size of worst-case blocks. This is very intuitive as halving the price allows to put double the amount of data into a block.\nWhat about EIP-4844 (Proto-Danksharding)?\nI won’t cover the details of 4844 here as there exists great documentation on eip4844.com, but simply speaking, EIP-4844 introduces “sidecars” that are named “blobs” with each blob carrying ~125kb of data. Similar to EIP-1559, there exists a “target” which determines the targeted number of blobs available. With the Dencun hardfork the target is set to 3 blobs with a maximum set to 6 blobs per block.\nImportantly, blobs come with their own fee market, creating a so-called multidimensional fee market. This means that blobs don’t have to compete with standard transactions but are decoupled from the EIP-1559 fees.\nSo far, so good. Let’s see how this upgrade affects the average block size of Ethereum.\nblobs1200×400 64 KB\nAs of today, the average block size of beacon chain blocks after employing snappy compression is around 125 KB. With 4844, we add another 375 KB to each block, thus 4x’ing the current avg. block size. By reaching the maximum number of blobs, we essentially increase the current block size by sevenfold.\nThe worst-case block increases from ~1.77 MB to ~2.5 MB. This estimation does not take into account the CL parts of a block. Nonetheless, in the event of a DoS attack, we must be prepared to deal with such maximum size blocks.\nConclusion\nFinally, increasing the current block gas limit requires thorough research and analysis before implementation. While sophisticated entities like Coinbase, Binance, Kraken, or Lido Node Operators might manage block gas limits over 40 million, solo stakers could struggle.\nThus, such decisions must be well-considered to make sure we do not hurt decentralization.\nIn the end, it’s rather easy to build something that is as scalable as Facebook but what matters is to not lose the property that most of us signed up for: decentralization.\nFind the code for the above estimates and charts here.\n",
        "category": [
            "Economics"
        ],
        "discourse": [
            "eip-1559",
            "resource-pricing",
            "scaling",
            "fee-market"
        ]
    },
    {
        "title": "Building an EVM for Bitcoin",
        "link": "https://ethresear.ch/t/building-an-evm-for-bitcoin/15402",
        "article": "Our team has been working on a fun experiment of combining an EVM with Bitcoin in the last couple of months.  Since the Taproot Upgrade, we can write more data onto Bitcoin.  Ordinals was the first one that let people write files onto Bitcoin.  We took a different approach.  Instead of jpegs and text files, we let developers deploy smart contracts and dapps onto Bitcoin.\nSo now we can write “bigger” smart contracts thanks to the large on-chain storage on Bitcoin.  Did anyone explore this direction?  Would love to exchange notes.\n\npasted image 01138×1600 81.3 KB\n\n",
        "category": [
            "EVM"
        ],
        "discourse": []
    },
    {
        "title": "AI-Powered Ethereum: From Rules-Based Smart Contracts to AI-Powered Smart Contracts",
        "link": "https://ethresear.ch/t/ai-powered-ethereum-from-rules-based-smart-contracts-to-ai-powered-smart-contracts/20981",
        "article": "Hi! We just released a whitepaper on adding onchain LLM to smart contracts. Any feedback is much appreciated. We have also implemented it already. You can play around with it at eternalai.org\nFrom Rules-Based Smart Contracts to AI-Powered Smart Contracts\nAbstract. One of the most fascinating aspects of Ethereum is its ability to create decentralized systems based on a set of smart contracts that can operate without human intervention. However, these smart contracts are still limited by their reliance on pre-programmed rules and logic. By integrating AI, we can begin to create systems that are not only decentralized but also autonomous, adaptive, and self-aware. This raises a range of interesting questions about the combined potential of blockchain technology and the role of AI in decentralized systems. To explore these questions, we propose the development of an AI Kernel that can be used to build AI-powered smart contracts on Ethereum. Eternal AI presents the architecture of the AI Kernel and examines the implications of integrating AI into smart contracts.\nLet’s take a step back and think about what we’re trying to achieve with smart contracts. We want to create systems that are not only decentralized but also autonomous, able to make decisions and respond to changing circumstances.\nBut if we look at the current state of dapps, we’re still a long way from achieving that vision. Most dapps today are simply rule-based programs coded in smart contracts with no ability to integrate AI capabilities. They’re like rigid machines, unable to adapt or learn from their environment.\nMeanwhile, in the Web2 world, we’re seeing a proliferation of AI-powered applications capable of making decisions in real-time. So, what’s holding us back from bringing this same level of sophistication to decentralized applications?\nTo address this challenge, we need to rethink the way we approach decentralized software development. We need to create a framework that allows developers to incorporate AI capabilities into their smart contracts, enabling the creation of truly smart contracts that can adapt and evolve over time.\nWe propose to achieve this by developing an AI Kernel for Ethereum.\nConsider a smart contract that manages a decentralized fantasy sports league. The contract needs to determine the winner of a matchup between two teams.\nRule-Based Approach\nWith a traditional rule-based approach, the contract might use a complex set of if-else statements to analyze each player’s performance and determine the matchup’s winner.\n1598×654 85.3 KB\nFigure 1. Rule-based smart contract.\nThis approach would be rigid and inflexible and unable to capture the game’s nuances and complexities.\nAI-Powered Approach\nIn contrast, the AI Kernel enables a new programming model that uses large language models (LLMs) to make decisions dynamically in real time. With the AI Kernel, the fantasy sports league contract could be written as providing a prompt to the LLM and receiving a structured response.\n1598×770 115 KB\nFigure 2. AI-powered smart contract.\nIn this example, the contract prompts the AI Kernel to analyze the two teams’ performance and determine the matchup winner. This approach allows for much more flexibility and dynamic decision-making. It could capture the nuances and complexities of the game in a way that a traditional rule-based approach would not.\nTo build truly smart contracts and AI-powered dapps, we need a decentralized framework that can facilitate the integration of AI inference, AI models, and GPU resources. This is where the AI Kernel comes in – the central component of the AI-powered Ethereum.\n1600×1441 140 KB\nFigure 3. The AI Kernel architecture.\nAt a high level, the AI Kernel can be broken down into four main components. Let’s explore each of these in turn, and think about how they fit together to enable decentralized AI.\nFirst, we have the User space – the domain where dapps operate. In this space, developers can build applications that interact with AI models, but they don’t have direct access to the underlying AI models or compute resources. Instead, they connect to the AI models via the Kernel space.\nThe Kernel space is where the magic happens. This component provides a simple programming interface for developers to interact with AI models, making it easier to build AI-powered dapps. Under the hood, the kernel space is divided into two sub-components: Decentralized Inference and Core AI Kernel. The Decentralized Inference provides a simple programming interface for developers to interact with AI models. At the same time, the Core AI Kernel takes care of the complex task of executing AI models on decentralized compute resources.\nNext, we have the Model space – a domain dedicated to managing AI models. Here, we take existing open-source models like Llama and FLUX, and adjust them to work onchain, enabling decentralized inference. By making these models available onchain, we can create a shared resource that developers can tap into, without having to replicate effort or manage complex model deployments.\nFinally, we have the Hardware space – the component that interacts with physical hardware, such as GPU nodes worldwide. This is where the compute resources are provisioned and the AI models are executed. By leveraging decentralized compute resources, we can create a scalable and flexible platform that can handle complex AI workloads.\nLet’s consider the user’s journey to interacting with the AI Kernel. It starts with a prompt — a request for the AI Kernel to generate an output. This prompt can come from a regular user or smart contract accounts. The prompt is sent to the Decentralized Inference smart contract.\nThe prompt itself is a simple data structure consisting of four fields:\nThe account: either a regular user account or a smart contract account\nThe topic: one of the many unique contexts between the account and the AI Kernel\nThe input: a question or a message to elicit an AI-generated output\nThe extra piece of context (optional)\n1600×712 66.5 KB\nFigure 4. A chain of prompts for a specific account and topic.\nThe topic is an interesting concept – it’s a unique context that’s shared between the account and the AI Kernel. This context is crucial for the AI Kernel to generate a meaningful output, and it’s something that evolves over time. The Context Manager smart contract is responsible for constructing and updating this context, based on the previous prompts, the input, and any extra context provided.\nOnce the prompt is submitted, the AI Kernel generates an output, and the Context Manager updates the prompt context with the new output. The prompt data is stored onchain, meaning anyone can verify the output by rerunning the prompt. This transparency is a key feature of the AI Kernel, which sets it apart from traditional AI systems.\nDevelopers have a choice regarding storing the prompt data — they can either store it directly on its native blockchain or store a hash that points to the raw data on an external decentralized storage network like Filecoin. This flexibility is important, as it allows developers to balance the trade-offs between cost, scalability, and security.\nOverall, the User Space is designed to provide a simple and intuitive interface for users to interact with the AI Kernel. By abstracting away the complexity of the underlying AI models and compute resources, we can create a seamless experience that allows users to focus on what matters - generating insights and solving problems.\nThe AI Kernel is the heart of our decentralized AI architecture. It is designed to be modular and flexible. At its core, the AI Kernel consists of a set of smart contracts that work together to manage resources and facilitate communication between different parts of the protocol.\n1600×968 85.4 KB\nFigure 5. The core smart contracts of the AI Kernel.\nLet’s take a look at the five main smart contracts that make up the AI Kernel.\nFirst, the Decentralized Inference contract provides a standardized interface for dapps to interact with the AI Kernel. This contract offers a set of “inference calls” that allow developers to tap into the AI Kernel’s capabilities in a simple and intuitive way.\nNext, we have the Prompt Scheduler contract, which is responsible for distributing GPU time and resources among all prompts in a fair and efficient manner. This is a critical component of the AI Kernel, ensuring that all prompts can be handled efficiently and simultaneously. The Prompt Scheduler uses various scheduling algorithms, such as round-robin and fee-based, to manage the flow of prompts and ensure that the system remains responsive and scalable.\nThe GPU Management contract is another key component of the AI Kernel. This contract manages the staking, status, and configurations of GPU nodes, which are the workhorses of the decentralized AI system. By providing a standardized interface for managing GPU nodes, we can ensure that the system remains flexible and scalable.\nThe Model File System contract provides access to stored models on various file systems, such as Filecoin and Arweave. This contract abstracts the details of different file systems, providing a consistent model I/O interface to GPU nodes. This allows developers to focus on building their dapps without worrying about the underlying complexities of model storage and retrieval.\nFinally, the Context Manager contract organizes and makes various user contexts accessible to GPU nodes. This contract is critical for ensuring the AI Kernel can provide personalized and context-dependent responses to user queries.\nThese five smart contracts form the core of the AI Kernel, working together to provide a decentralized and scalable AI system that can support a wide range of applications.\nThe Model Space is a critical component of the AI Kernel, where we adapt popular open-source AI models to the blockchain environment. At its core, the Model Space consists of two key components: AI Models and AI Model Drivers.\nThe AI Models are well-known open-source models like Llama, FLUX, and Hermes. These models have been widely adopted in the AI community and provide a solid foundation for our decentralized AI system.\nHowever, these models were not designed with the blockchain in mind, which is where the AI Model Drivers come in. These drivers play a crucial role in adapting the models to the blockchain environment, ensuring they can operate effectively in a decentralized setting.\nOne key challenge in adapting AI models to the blockchain is ensuring deterministic. In other words, we must ensure that the models produce the same results given the same input. This is critical for maintaining the integrity of the decentralized AI system, and the AI Model Drivers are designed to handle it.\nAnother important aspect of adapting AI models to the blockchain is quantization. By reducing the precision of model weights and activations, we can improve performance and reduce storage requirements. This is especially important in a decentralized setting, where storage and computational resources may be limited.\nThe AI Model Drivers are designed to be modular and extensible, allowing new models to be integrated into the AI Kernel easily. This means that developers can simply plug in new models as they become available without worrying about the underlying complexities of the blockchain environment via a standardized interface.\nThe Hardware Space is where the actual computation happens in the AI Kernel. At its core, this space consists of GPU nodes that serve as the atomic compute unit of the system. These nodes are responsible for taking user prompts, running inference, and returning outputs.\nBut what makes these GPU nodes tick? The answer lies in the GPU Management smart contract, which plays a critical role in managing the nodes and ensuring that they’re qualified to work. To participate in the system, nodes must stake EAI, which provides a level of accountability and ensures that nodes are invested in the success of the AI Kernel.\nIn addition to managing nodes, the GPU Management contract also tracks hardware configurations, such as GPU device models. This information is used by the Prompt Scheduler contract to assign prompts to nodes for processing.\nBut how do nodes get incentivized to participate in the system? That’s where the Proof-of-Compute mechanism comes in. This novel approach to node participation rewards nodes for generating outputs for prompts. The first node to generate an output for a prompt is rewarded with EAI, creating a built-in incentive mechanism that encourages nodes to support the AI Kernel.\nThink of it like gold mining. Miners expend resources to add gold to circulation, and in return, they’re rewarded with a piece of that gold. In our case, GPU nodes expend resources (electricity and GPU time) to process prompts, and in return, they’re rewarded with EAI. This mechanism creates a self-sustaining ecosystem where nodes are incentivized to participate and support the AI Kernel.\nEventually, as the system matures, the reward mechanism will transition to prompt fees, making it completely inflation-free. This approach ensures that the AI Kernel’s ecosystem is sustainable and self-sufficient, with nodes incentivized to participate and users paying for value-added services.\nTraditional consensus algorithms like Proof-of-Work (PoW) have been criticized for their lack of real-world utility. In contrast, our AI Kernel runs on a novel consensus algorithm called Proof-of-Compute (PoC), which challenges this paradigm by repurposing the computational energy expended in the network.\nInstead of solely solving a complex mathematical puzzle, GPU nodes in the PoC network perform meaningful computations on prompts requested by real users and real dapps. This generates valuable outputs for them, creating a self-sustaining ecosystem where nodes are incentivized to participate, and users receive tangible benefits.\nSo, how does Proof-of-Compute work? The process is straightforward:\nFirst, users or dapps submit a prompt to the Decentralized Inference smart contract. This prompt can be anything from a simple question to a complex AI task.\nNext, the Prompt Scheduler smart contract randomly assigns the prompt to a subset of available GPU nodes managed by the GPU Management smart contract. This ensures a decentralized and resilient computation process where no single node has too much control.\nOnce assigned, the GPU nodes process the prompt and generate outputs, competing to be the first to return a valid result. This competition incentivizes nodes to invest in computational resources and participate honestly in the network.\nThe first GPU node to return a valid output receives a reward comprising the prompt fee and the block reward. This reward mechanism incentivizes nodes to participate in the network and maintain its integrity.\nBut what about malicious behavior? To address this, other GPU nodes verify the output for accuracy, detecting and penalizing malicious behavior. This validation and penalty mechanism ensures the integrity of the computation process and maintains trust within the network.\nBy combining these elements, Proof-of-Compute creates a novel consensus algorithm that not only secures the network but also provides tangible benefits to users. It’s a new blockchain consensus paradigm that prioritizes utility and efficiency over mere security.\n1600×1118 124 KB\nFigure 6. Proof-of-Compute\nAs we integrate the AI Kernel into Ethereum, a new paradigm for decentralized applications begins to take shape. No longer limited by rigid, rule-based programming constraints, developers can now create dapps capable of adapting, learning, and evolving over time.\nOnchain Conversational AI Agents\nWith just a few lines of code, you can build an autonomous agent on top of the AI Kernel and earn passive income by charging a service fee when someone uses your agent.\n1518×478 58.7 KB\nFigure 7. Onchain AI agent.\nWhen someone chats with your agent, simply call the AI Kernel.\n1600×504 107 KB\nFigure 8. Conversational onchain AI agent.\nAI-Powered Crypto Wallet\nIn this example, we’re building an AI-powered wallet. Before sending the funds to an address, the wallet will call the suspiciousTransaction function.\n1600×883 178 KB\nFigure 9. AI-powered crypto wallet.\nBy providing the AI Kernel with a rich context of transaction history, the model can learn to identify potential red flags such as:\nLarge or unusual transaction amounts\nUnusual frequencies within a short period\nTransactions that are not consistent with the user’s typical wallet behavior\nTransactions to known flagged addresses\nAI-Powered Oracles\nIn this example, we’re building an AI-powered oracle for BTC prices.\n1430×834 98.2 KB\nFigure 10. AI-powered oracles.\nBy providing the AI Kernel with a rich context of BTC price feeds added continuously by Oracle feeders, the AI Kernel can learn to return the current BTC price by aggregating the fed prices and determining the most accurate value.\nAI-Powered DAOs\nIn this example, we’re building an AI-powered DAO.\n1600×889 180 KB\nFigure 11. AI-powered DAOs.\nBy feeding the AI Kernel a continuously updated context of proposal result history, the AI Kernel can develop an understanding of successful and unsuccessful proposals. This enables it to assess and predict the viability of new proposals and make informed decisions on whether to approve or reject a proposal.\nAI-Powered Wallet Credit Scores\nIn this example, we’re building an AI-powered Wallet Credit Scoring system.\n1600×841 164 KB\nFigure 12. AI-powered credit scoring.\nBy providing the AI kernel with a comprehensive context of a given address’s transaction history, including details such as transaction amounts, contract interactions (e.g., swaps, lending, borrowing), and other relevant data, the model can learn to accurately assess the address’s creditworthiness and provide a corresponding credit score.\nAI-Powered ENS\nIn this example, we’re building an AI-powered ENS generator.\n1600×952 133 KB\nFigure 13. AI-powered ENS.\nThe model can generate an available ENS domain that best fits the given description. If a suggested domain has already been taken, it will continue to retry until it finds a suitable one.\nThe integration of AI and blockchain technology represents a significant paradigm shift in the way we approach decentralized systems. Our work on the AI Kernel provides a framework for executing AI computations on the blockchain, opening up new possibilities for decentralized applications.\nAs AI is such an important technology and is growing into every fabric of our lives, the success of decentralized AI will depend on our ability to design systems that are not only technically robust but also socially and philosophically sound. This requires a deep understanding of the complex relationships between technology, society, and the individual.\nUltimately, our work on the AI Kernel is a starting point for a broader conversation about the future of AI-powered decentralized systems and their potential to reshape our world.\n",
        "category": [],
        "discourse": []
    },
    {
        "title": "The Shape of Issuance Curves to Come",
        "link": "https://ethresear.ch/t/the-shape-of-issuance-curves-to-come/20405",
        "article": "In this post we will analyze the consequences that the shape of the issuance curve has for the decentralization of the validator set.\nThe course of action is the following:\nFirst, we will introduce the concept of effective yield as the yield observed after taking into account the dilution generated by issuance.\nSecond, we will introduce the concept of real yield as the effective yield that a validator obtains post expenses (OpEx, CapEx, taxes…).\nArmed with these definitions we will be able to make some observations about how the shape of the issuance curve can result in centralization forces, as the real yield observed can push out small uncorrelated stakers at high stake rates.\nThen, we will propose a number of properties we would expect the issuance curve to satisfy to minimize these centralization forces. And explore some alternative issuance curves that could deal with the aforementioned issues.\nFinally, some heuristic arguments on how to fix a specific choice of issuance and yield curves.\nSource Code for all plots can be found here: GitHub - pa7x1/ethereum-issuance\nEffective Yield\nBy effective yield we mean the yield observed by an Ethereum holder after taking into account circulating supply changes. For instance, if everyone were to be a staker, the yield observed would be effectively 0%. As the new issuance is split evenly among all participants, the ownership of the circulating supply experienced by each staker would not change. Pre-taxes and other associated costs this situation resembles more a token re-denomination or a fractional stock split. So we would expect the effective yield to progressively reach 0% as stake rates grow to 100%.\nOn the other hand, non-staking holders are being diluted by the newly minted issuance. This causes holders to experience a negative effective yield due to issuance. We would expect this effect to be more and more acute as stake rates grow closer and closer to 100%.\nThese ideas can be put very simply in math terms.\nLet’s call s the amount of ETH held by stakers, h the amount of ETH held by non-stakers (holders), and t the total circulating supply. Then:\ns + h = t\nAfter staking for certain period of time, we will reach a new situation s' + h' = t'. Where s' and t' have been inflated by the new issuance i are obviously related to the nominal staking yield y_s:\ns' = s + i = s \\cdot y_s\nh' = h\nt' = t + i = t + s \\cdot (y_s - 1)\nNow, let’s introduce the normalized quantities s_n and h_n. They simply represent the proportion of total circulating supply that each subset represents:\ns_n \\equiv \\frac{s}{t}\nh_n \\equiv \\frac{h}{t}\nWe can do the same for s'_n and h'_n:\ns'_n \\equiv \\frac{s'}{t'} = \\frac{sy_s}{s(y_s - 1) +t}\nh'_n \\equiv \\frac{h'}{t'} = \\frac{t-s}{s(y_s - 1) + t}\nWith these definitions we can now introduce the effective yield as the change in the proportion of the total circulating supply observed by each subset.\ny_s^{eff} \\equiv \\frac{s'_n}{s_n} = \\frac{y_s}{\\frac{s}{t}(y_s-1) + 1}\ny_h^{eff} \\equiv \\frac{h'_n}{h_n} = \\frac{1}{\\frac{s}{t}(y_s-1) + 1} \neffective_yield1500×900 61.1 KB\nNet Yield\nStaking has associated costs. A staker must acquire a consumer-grade PC, it must pay some amount (albeit small) for electricity, it must have a high-speed internet connection. And they also must put their own labor and time to maintain the system operational and secure, or must pay someone to do that job for them. Stakers also observe other forms of costs that eat away from the nominal yield they observe, e.g. taxes. We would like to model this net yield observed after all forms of costs, because it can give us valuable information on how different stakers are impacted by changes in the nominal stake yield.\nTo model this we will introduce two types of costs; costs that scale with the nominal yield (e.g. taxes or fees charged by an LST would fit under this umbrella), and costs that do not (i.e. HW, electricity, internet, labor…).\nWith our definitions, after staking for a reference period stakers would have earned s' = y_s s = s + s(y_s - 1)\nBut if we introduce costs that eat away from the nominal yield (let’s call them k), and costs that eat away from the principal (let’s call them c). We arrive to the following formula for the net stake:\ns' = s(1-c) + s(y_s - 1) - \\max(0, sk(y_s - 1))\nNOTE: The max simply prevents that a cost that scales with yield becomes a profit if yield goes negative. For instance, if yield goes negative it’s unlikely that an LST will pay the LST holders 10%. Or if yield goes negative you may not be able to recoup in taxes as if it were negative income. In those cases we set it to 0. This will become useful later on when we explore issuance curves with negative issuance regimes.\nThis represents the net stake our stakers observe after all forms of costs have been taken into account. To be noted that this formula can be easily modified to take into account other types of effects like validator effectiveness (acts as multiplicative factor on the terms (y_s - 1)) or correlation/anti-correlation incentives (which alter y_s).\nTo fix ideas, let’s estimate the net yield observed by 3 different types of stakers. A home staker, an LST holder, and an institutional large-scale operator. The values proposed are only orientative and should be tuned to best reflect the realities of each stakeholder.\nA home staker will have to pay for a PC that they amortize over 5 years and costs around 1000 USD, so 200 USD/year. Pay for Internet, 50 USD per month, for around 600 USD/year. Something extra for electricity, less than 100 USD/year for a typical NUC. Let’s assume they are a hobbyist and decide to do this with their spare time, valuing their time at 0 USD/year. This would mean that his staking operation has a cost of around 1000 USD/year for them. If they have 32 ETH, with current ETH prices we can round that at ~100K USD. This would mean that for this staker, c = \\frac{1}{1000}. As their costs represent around 1 over 1000 their stake value.\nNow for the costs that scale with the yield. They will have to pay taxes, these are highly dependent on their tax jurisdiction, but may vary between 20% and 50% in most developed countries. Let’s pick 35% as an intermediate value. In that case, their stake after costs looks like:\ns' = s\\left(1-\\frac{1}{1000}\\right) + s(1-0.35)(y_s - 1)\nWe can do the same exercise for a staker using an LST. In this case, c=0 and k is composed of staking fees (10-15%) and taxes (20-50%) which depend on the tax treatment. Rebasing tokens have the advantage of postponing the realization of capital gains. If we assume a 5 year holding period, equivalent to the amortization time we assumed for solo staking, it could look something like this:\nFixed costs: 0\nStaking fees: 10%\nCapital gains tax: 20%\nHolding period: 5 years\ns' = s(1-0) + s(1-0.14)(y_s - 1)\nFinally, for a large scale operator. They have higher fixed costs, they will have to pay for labor, etc… But also will run much higher amount of validators. In that case, c can get much smaller as it’s a proportion of s. Perhaps 1 or 2 orders of magnitude smaller. And taxes will be typical of corporate tax rates (20-30%).\ns' = s\\left(1-\\frac{1}{10000}\\right) + s(1-0.25)(y_s - 1)\nNet Effective Yield (a.k.a Real Yield)\nFinally, we can blend the two concepts together to understand what’s the real yield a staker or holder obtains net of all forms of costs and after supply changes dilution. I would suggest calling this net effective yield as the real yield, because well, that’s the yield you are really getting.\ny_s^{real} = \\frac{(1-c) + (y_s - 1) - \\max(0,k(y_s - 1))}{\\frac{s}{t}(y_s-1)+1}\ny_h^{real} = y_h^{eff} = \\frac{1}{\\frac{s}{t}(y_s-1) + 1}\nIn the second equation we are simply stating the fact that there is no cost to holding, so the real yield (after costs) of holding is the same as the effective yield of holding.\nThe Issuance Curve and Centralization\nUp to here all the equations presented are agnostic of Ethereum’s specificities and in fact are equally applicable to any other scenario where stakeholders observe a yield but that yield is coming from new issuance.\nTo bring this analysis back to Ethereum-land it suffices to substitute y_s by Ethereum’s issuance yield as a function of the total amount staked s. And substitute t by the total circulating supply of ETH.\nt \\approx 120\\cdot 10^6 \\quad \\text{ETH}\ni(s) = 2.6 \\cdot 64 \\cdot \\sqrt{s} \\quad \\text{ETH}\\cdot\\text{year}^{-1}\ny_{s}(s) = 1 + \\frac{2.6 \\cdot 64}{\\sqrt{s}} \\quad \\text{year}^{-1}\nethereum_issuance_plot1500×900 50.6 KB\nethereum_nominal_yield_plot1500×900 40.4 KB\nWe can plot the real yield for the 4 different types of ETH stakeholders we introduced above, as a way to visualize the possible centralization forces that arise due to economies scale, and exogenous factors like taxes.\nethereum_real_yield_plot1500×900 78.7 KB\nWe can make the following observations, from which we will derive some consequences.\nObservations\n\nObservation 0: The economic choice to participate as a solo staker, LST holder, ETH holder or any other option is made on the gap between the real yields observed and the risks (liquidity, slashing, operational, regulatory, smart contract…) of each option. Typically higher risks demand a higher premium.\n\n\nObservation 1: Holding always has a lower real yield than staking, at least for the assumptions taken above for costs. But the gap shrinks with high stake rates.\n\n\nObservation 2: Different stakers cross the 0% real yield at different stake rate. Around 70M ETH staked solo validators start to earn negative real yield. At around 90M ETH institutional stakers start to earn negative real yield. At around 100M LST holders start to earn negative real yield.\n\n\nObservation 3: When every staker and every ETH holder is becoming diluted (negative real yield), staking is a net cost for everyone.\n\n\nObservation 4: There is quite a large gap between the stake levels where different stakers cross the 0% real yield.\n\n\nObservation 5: Low nominal yields affect disproportionately home stakers over large operators. From the cost structure formula above we can see that as long as nominal yields are positive, the only term that can make the real yield negative is c. This term is affected by economies of scale, and small operators will suffer larger c.\n\nObservation 0: The economic choice to participate as a solo staker, LST holder, ETH holder or any other option is made on the gap between the real yields observed and the risks (liquidity, slashing, operational, regulatory, smart contract…) of each option. Typically higher risks demand a higher premium.\nObservation 1: Holding always has a lower real yield than staking, at least for the assumptions taken above for costs. But the gap shrinks with high stake rates.\nObservation 2: Different stakers cross the 0% real yield at different stake rate. Around 70M ETH staked solo validators start to earn negative real yield. At around 90M ETH institutional stakers start to earn negative real yield. At around 100M LST holders start to earn negative real yield.\nObservation 3: When every staker and every ETH holder is becoming diluted (negative real yield), staking is a net cost for everyone.\nObservation 4: There is quite a large gap between the stake levels where different stakers cross the 0% real yield.\nObservation 5: Low nominal yields affect disproportionately home stakers over large operators. From the cost structure formula above we can see that as long as nominal yields are positive, the only term that can make the real yield negative is c. This term is affected by economies of scale, and small operators will suffer larger c.\nImplications\nObservation 0 and Observation 1 imply that as the gap between real yields becomes sufficiently small, participating in the network as some of those subsets may become economically irrational. For example, solo staking may be economically irrational given the operational risks, liquidity risks, slashing risks if the yield premium becomes sufficiently small vs holding. In that case solo stakers may become holders or switch to other forms of staking (e.g. LSTs) where the premium still satisfies the risk.\nTogether with Observation 2 and Observation 4 implies that as stake rates become higher and higher the chain is at risk of becoming more centralized, as solo stakers (which are the most uncorrelated staking set) must continue staking when it may be economically irrational to do so. Given the above assumptions LSTs will always observe at least 1% real yield higher than holding even at extreme stake rates (~100%), this may mean that there is always an incentive to hold an LST instead of ETH. Furthermore, when solo stakers cross to negative real yield but other stakers do not, other stakers are slowly but steadily gaining greater weight.\nFrom Observation 3 we know that the very high stake rate regime, where everyone is observing a negative real yield, is costly for everyone. Everyone observes dilution. The money is going to the costs that were included in the real yield calculation (tax, ISPs, HW, electricity, labor…).\nObservation 5 implies that nominal yield reductions need to be applied with care and certainly not in isolation without introducing uncorrelation incentives at the same time. As they risk penalizing home solo stakers disproportionately.\nRecommendations\nGiven the above analysis, we can put forward a few recommended properties the yield curve (respectively the issuance curve) should have. The idea of establishing these properties is that we should be able to discuss them individually and agree or disagree on their desirability. Once agreed they constrain the set of functions we should consider. At a minimum, it will make the discussion about issuance changes more structured.\n\nProperty 0: Although this property is already satisfied with the current issuance curve, it is worth stating explicitly. The protocol should incentivize some minimum amount of stake, to ensure the network is secure and the cost to attack much larger than the potential economic reward of doing so. This is achieved by defining a yield curve that ramps up the nominal yield as the total proportion of stake (s_n) is reduced.\n\n\nProperty 1: The yield curve should contain uncorrelation incentives such that stakers are incentivized to spin-up uncorrelated nodes and to stake independently, instead of joining large-scale operators. From a protocol perspective the marginal value gained from another ETH staked through a large staking operation is much smaller than if that same ETH does so through an uncorrelated node. The protocol should reward uncorrelation as that’s what allows the network to achieve the extreme levels of censorship resistance, liveness/availability and credible neutrality the protocol expects to obtain from its validator set. The economic incentives must be aligned with the expected outcomes, therefore the yield curve must contain uncorrelation incentives.\n\n\nProperty 2: The issuance curve (resp. yield curve) should have a regime where holding is strictly more economically beneficial than staking, at sufficiently high stake rates. This means that the real yield of holding is greater than the real yield of staking if the stake rate is sufficiently high. As explained above it’s the real yield gap the defining characteristic that establishes the economically rational choice to join one subset or another. If the holding real yield can be greater than the staking real yield at sufficiently high stake rates there is an economic incentive to hold instead of continue staking. To be noted, up to here we are not making an argument at what stake rate this should be set. It suffices to agree that 99.9% stake rate is unhealthy for the protocol (it’s a cost for everyone, LSTs will displace ETH as pristine collateral, etc…). If that’s the case, then we can prevent this outcome by setting the holding real yield to be higher than staking at that level. Unhealthy levels are likely found at much lower values of stake rate.\n\n\nProperty 3: To prevent centralization forces, the stake rates at which uncorrelated validators vs correlated validators cross to negative real yield should be small, as small as possible. A large gap between the thresholds to negative real yields of uncorrelated (e.g. home stakers) and correlated sets (e.g. large operators) creates a regime where the validator set can become more and more centralized. To make the case more clear, if uncorrelated validators reach 0 real yield at 30M ETH staked, while holding an LST composed of large operators (e.g. cbETH, wstETH) does so at 100M ETH. The regime where the stake ranges between 30M and 100M is such that solo stakers will tend to disappear, either quickly (they stop staking) or slowly (they become more and more diluted), the outcome in either case is a more centralized validator set.\n\n\nProperty 4: The yield curve should taper down relatively quick to enter the regime of negative real yields. From Property 2 and Property 3 we know we should build-in a regime where the real yield from issuance goes negative, but we want this regime to occur approximately at the same stake rate for the different types of stakers, to prevent centralization forces. Observation 5 implies that if the slope of this nominal yield reduction is slow, stakers with different cost structures will be pushed out at much different stake rates. Hence, we need to make this yield reduction quick.\n\n\nProperty 5: The issuance yield curve should be continuous. It’s tempting to play with discontinuous yield curves, but yield is the main incentive to regulate the total stake of the network. We would like that the changes induced to the total stake s are continuous, therefore the economic incentive should be a continuous function.\n\nProperty 0: Although this property is already satisfied with the current issuance curve, it is worth stating explicitly. The protocol should incentivize some minimum amount of stake, to ensure the network is secure and the cost to attack much larger than the potential economic reward of doing so. This is achieved by defining a yield curve that ramps up the nominal yield as the total proportion of stake (s_n) is reduced.\nProperty 1: The yield curve should contain uncorrelation incentives such that stakers are incentivized to spin-up uncorrelated nodes and to stake independently, instead of joining large-scale operators. From a protocol perspective the marginal value gained from another ETH staked through a large staking operation is much smaller than if that same ETH does so through an uncorrelated node. The protocol should reward uncorrelation as that’s what allows the network to achieve the extreme levels of censorship resistance, liveness/availability and credible neutrality the protocol expects to obtain from its validator set. The economic incentives must be aligned with the expected outcomes, therefore the yield curve must contain uncorrelation incentives.\nProperty 2: The issuance curve (resp. yield curve) should have a regime where holding is strictly more economically beneficial than staking, at sufficiently high stake rates. This means that the real yield of holding is greater than the real yield of staking if the stake rate is sufficiently high. As explained above it’s the real yield gap the defining characteristic that establishes the economically rational choice to join one subset or another. If the holding real yield can be greater than the staking real yield at sufficiently high stake rates there is an economic incentive to hold instead of continue staking. To be noted, up to here we are not making an argument at what stake rate this should be set. It suffices to agree that 99.9% stake rate is unhealthy for the protocol (it’s a cost for everyone, LSTs will displace ETH as pristine collateral, etc…). If that’s the case, then we can prevent this outcome by setting the holding real yield to be higher than staking at that level. Unhealthy levels are likely found at much lower values of stake rate.\nProperty 3: To prevent centralization forces, the stake rates at which uncorrelated validators vs correlated validators cross to negative real yield should be small, as small as possible. A large gap between the thresholds to negative real yields of uncorrelated (e.g. home stakers) and correlated sets (e.g. large operators) creates a regime where the validator set can become more and more centralized. To make the case more clear, if uncorrelated validators reach 0 real yield at 30M ETH staked, while holding an LST composed of large operators (e.g. cbETH, wstETH) does so at 100M ETH. The regime where the stake ranges between 30M and 100M is such that solo stakers will tend to disappear, either quickly (they stop staking) or slowly (they become more and more diluted), the outcome in either case is a more centralized validator set.\nProperty 4: The yield curve should taper down relatively quick to enter the regime of negative real yields. From Property 2 and Property 3 we know we should build-in a regime where the real yield from issuance goes negative, but we want this regime to occur approximately at the same stake rate for the different types of stakers, to prevent centralization forces. Observation 5 implies that if the slope of this nominal yield reduction is slow, stakers with different cost structures will be pushed out at much different stake rates. Hence, we need to make this yield reduction quick.\nProperty 5: The issuance yield curve should be continuous. It’s tempting to play with discontinuous yield curves, but yield is the main incentive to regulate the total stake of the network. We would like that the changes induced to the total stake s are continuous, therefore the economic incentive should be a continuous function.\nExploring Other Issuance Curves\nThe desired properties can be summarized very succinctly:\nThe yield curve should be continuous.\nThe yield curve should go up as stake rate goes to 0.\nThe yield curve should go to 0 as the stake rate goes up, crossing 0 at some point that bounds from above the desired stake rate equilibrium.\nThe yield curve should have uncorrelation incentives such that spinning up uncorrelated validators is rewarded and incentivized over correlated validators.\nThe real yield curves of correlated and uncorrelated stakers should become negative relatively close to each other.\nA very simple solution to meet the above is to introduce a negative term to Ethereum’s issuance yield and uncorrelation incentives.\nThe negative term should grow faster than the issuance yield as the stake grows, so that it eventually overcompensates issuance and makes the yield go negative quickly at sufficiently high stake rates. This negative term can be thought of as a stake burn, and should be applied on a slot or epoch basis such that it’s unavoidable (thanks to A. Elowsson for this observation).\nUncorrelation incentives are being explored in other posts. We will simply leave here the recommendation of adopting them as part of any issuance tweaks. Read further: Anti-Correlation Penalties by Wahrstatter et al.\nEthereum’s Issuance with Stake Burn\nThe following is an example of how such a negative term can be introduced.\ni(s) = 2.6 \\cdot 64 \\cdot \\sqrt{s} - 2.6 \\cdot \\frac{s \\ln s}{2048} \\quad \\text{ETH} \\cdot \\text{year}^{-1}\ny_{s}(s) = 1 + \\frac{2.6 \\cdot 64}{\\sqrt{s}} - \\frac{2.6 \\ln s}{2048} \\quad \\text{year}^{-1}\nethereum_issuance_with_burn_plot1500×900 51.7 KB\nethereum_nominal_yield_with_burn_plot1500×900 42.7 KB\nThe negative stake burn term eventually dominates issuance and can make it go negative. There is complete freedom deciding where this threshold occurs, by simply tweaking the constant pre-factors. In this particular case, the parameters have been chosen so they are rounded in powers of 2 and so that the negative issuance regime roughly happens around 50% stake rate.\nThis negative issuance regime induces a positive effective yield on holders, which provides the protocol with an economic incentive to limit the stake rate. As the real yield will eventually be greater holding ETH than staking. It also serves to protect the network from overloading its consensus layer, as it provides the protocol with a mechanism to charge exogenous sources of yield that occur on top of it. If priority fees, MEV, or restaking provide additional yield that would push stake rates above the desired limit, the protocol would start charging those extra sources of yield by making issuance go negative. Hence redistributing exogenous yield onto ETH holders.\nTo understand better the impact that this stake burn has on the different stakeholders we can plot the real yield curves.\nethereum_real_yield_with_burn_plot1500×900 74.2 KB\nWe can see how the introduction of a negative issuance yield regime has helped achieve most of the properties we desired to obtain. Particularly, we can notice the stake rates at which different stakeholders reach 0 real yield have compressed and are much closer to each other. And we can appreciate how when stake rates get close to 50% (given the choice of parameters) holders start to observe a positive real yield which disincentivizes additional staking. Holding real yields can become quite large so even large exogenous sources of yield can be overcome.\nGiven that we haven’t touched the positive issuance term, this results in a large reduction of the staking yield. We can increase the yield trivially while respecting the same yield curve shape. Here the same curve with larger yield:\ni(s) = 2.6 \\cdot 128 \\cdot \\sqrt{s} - 2.6 \\cdot \\frac{s \\ln s}{1024} \\quad \\text{ETH} \\cdot \\text{year}^{-1}\ny_{s}(s) = 1 + \\frac{2.6 \\cdot 128}{\\sqrt{s}} - \\frac{2.6 \\ln s}{1024} \\quad \\text{year}^{-1}\nethereum_issuance_with_burn_plot_21500×900 49.8 KB\nethereum_real_yield_with_burn_plot_21500×900 80.9 KB\nThis shows the target yield that is observed at a specific stake rate is a separate consideration to the curve shape discussion. So if you dislike this particular example because of the resulting yield at current stake rates. Fear not, that has an easy fix.\nAdding Uncorrelation Incentives to the Mix\nWe will not cover the specifics of how uncorrelation incentives should be introduced nor how they should be sized, but we will illustrate how the introduction of a correlation penalty can help align the economic incentives with the network interest of maintaining an uncorrelated validator set.\nTo do so we will simulate what would happen to the real yields observed by the following stakeholders:\nHome Validator (Very Uncorrelated): -0.0% subtracted to the nominal yield through correlation penalties\nLST Holder through a decentralized protocol (Quite Uncorrelated): -0.2% subtracted to the nominal yield through correlation penalties\nLST Holder through staking through large operators (Quite Correlated): -0.4% subtracted to the nominal yield through correlation penalties\nLarge Institutional Operator (Very Correlated): -0.6% subtracted to the nominal yield through correlation penalties\nThe following figure zooms in to the area where negative real yields are achieved:\nethereum_real_yield_with_burn_uncorrelation_plot1920×1050 130 KB\nImportant Note: The above values for correlation penalties are not based on any estimation or study. They have been chosen arbitrarily to showcase that the inclusion of uncorrelation incentives in the issuance curve can be used to disincentivize staking through large correlated operators. We refer the analysis of the right incentives to other papers.\nFixing the Issuance Yield Curve\nUp until now the focus has been on the shape of the yield curve (respectively the issuance curve) but very little has been said about the specific yield we should target at different stake rates. As illustrated above, by simply applying a multiplicative factor we can keep the same curve shape but make yields be higher or lower as wished.\nIn this section we will provide some heuristic properties to address this problem and be able to specify the prefactors that allow us to define a concrete yield curve.\nThese heuristic properties are orientative. There is no hard science behind them, just some soft arguments that provide reasonable justification for these choices.\nHeuristic 0: The nominal issuance yield should become negative at 50% stake rate or lower. Higher stake rates start to become problematic, above those levels the majority of circulating supply is staking. In case of supermajority bug the majority of ETH holders could be incentivized to break the consensus rules. The negative yield regime can be seen as a protection mechanism from the protocol to prevent this type of situations from happening, it sets an economic incentive to align the social layer with the protocol interests.\nHeuristic 1: Target 3% yield at 25% stake rate. When PoS was released there was no idea what would be the expected staking yield the market would consider appetizing. Would 5% be enough? Or 3%?\nNow we have data points, current staking yield is 3% as measured by https://beaconcha.in (issuance, MEV, and priority fees included). So we know the market certainly has appetite for ETH yield at 3%. There is also some soft-arguments by V. Buterin, J. Drake et al. that 25% stake rate should provide enough security.\nAnd finally, the current issuance curve happens to provide 3% yield at 25% stake rate. So by fixing the new curve to meet that same yield at 25% we anchor the same yield (and issuance) at the target rate. But any extra amount of stake will be met with a reduction in yield and issuance that makes it go to 0 before hitting 50%.\nAs current stake rate is a tad over 25% the proposed change to the issuance curve would imply a bit of issuance reduction, nothing very significant. But most importantly it avoids the ever growing issuance increase as stake rates become higher.\nIn conjunction with well designed uncorrelation incentives it could help the protocol ensure it does not overpay for security, stake rates are self-limiting, and the validator set very uncorrelated.\nFinal Words\nThe analytic form of the yield curve or the issuance curve matter much less than we may think. It might be tempting to spend time tinkering with its concrete analytic form but for all it matters it could be equally defined with a piece-wise continuous function.\nIts purpose is to provide an economic incentive to get stake rates where the protocol needs them to be (not too high, not too low) and maintaining a large uncorrelated validator set.\nThis post is an invitation to steer the discussion towards said properties instead of getting lost with the fine details. If we nail down the properties we will constrain the solution space enough so that almost any function we choose will do the job.\n",
        "category": [
            "Economics"
        ],
        "discourse": []
    }
]